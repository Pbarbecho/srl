DataBase,Title,Abstract,Keywords,Authors,Year,DocumentType,PublicationTitle,DOI,Link,Affiliations,Publisher,Language,ISSN,ISBN
Scopus,Systematic literature reviews in software engineering - A systematic literature review,"Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. © 2008 Elsevier B.V. All rights reserved.",Cost estimation; Evidence-based software engineering; Systematic literature review; Systematic review quality; Tertiary study,"Kitchenham B., Pearl Brereton O., Budgen D., Turner M., Bailey J., Linkman S.",2009,Review,Information and Software Technology,10.1016/j.infsof.2008.09.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649086628&doi=10.1016%2fj.infsof.2008.09.009&partnerID=40&md5=209d0cdfec109a5c5af11592697dcbf2,"Software Engineering Group, School of Computer Science and Mathematics, Keele University, Keele Village, Keele, Staffs ST5 5BG, United Kingdom; Department of Computer Science, Durham University, Durham, United Kingdom",,English,09505849,
Scopus,An Empirical Validation of Software Cost Estimation Models,"Practitioners have expressed concern over their inability to accurately estimate costs associated with software development. This concern has become even more pressing as costs associated with development continue to increase. As a result, considerable research attention is now directed at gaining a better understanding of the software-development process as well as constructing and evaluating software cost estimating tools. This paper evaluates four of the most popular algorithmic models used to estimate software costs (SLIM, COCOMO, Function Points, and ESTIMACS). Data on 15 large completed business data-processing projects were collected and used to test the accuracy of the models’ ex post effort estimation. One important result was that Albrecht's Function Points effort estimation model was validated by the independent data provided in this study [3]. The models not developed in business data-processing environments showed significant need for calibration. As models of the software-development process, all of the models tested failed to sufficiently reflect the underlying factors affecting productivity. Further research will be required to develop understanding in this area. © 1987, ACM. All rights reserved.",COCOMO; ESTIMACS; Function Points; SLIM; SLOC,Kemerer C.F.,1987,Journal,Communications of the ACM,10.1145/22899.22906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023349750&doi=10.1145%2f22899.22906&partnerID=40&md5=7e225e5f39731c61d6157a96a4183be1,"Graduate School of Industrial Administration, Carnegie-Mellon University, Schenley Park, Pittsburgh, PA 15213, United States",,English,00010782,
Scopus,Guidelines for conducting systematic mapping studies in software engineering: An update,"Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings. © 2015 Elsevier B.V.",Guidelines; Software engineering; Systematic mapping studies,"Petersen K., Vakkalanka S., Kuzniarz L.",2015,Conference,Information and Software Technology,10.1016/j.infsof.2015.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929464206&doi=10.1016%2fj.infsof.2015.03.007&partnerID=40&md5=f238ed2d4654adb16607b3e007b37292,"Department of Software Engineering, Blekinge Institute of Technology, Sweden",Elsevier,English,09505849,
Scopus,A systematic literature review on fault prediction performance in software engineering,"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively. © 2012 IEEE.",software fault prediction; Systematic literature review,"Hall T., Beecham S., Bowes D., Gray D., Counsell S.",2012,Review,IEEE Transactions on Software Engineering,10.1109/TSE.2011.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870561393&doi=10.1109%2fTSE.2011.103&partnerID=40&md5=31c5c83d993f4e293a6e46b487c88f08,"Department of Information Systems and Computing, Brunel University, Uxbridge, Middlesex UB8 3PH, United Kingdom; Lero, Irish Software Engineering Research Centre, University of Limerick, Limerick, Ireland; Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire AL10 9AB, United Kingdom",,English,00985589,
Scopus,Cross-project defect prediction: A large scale experiment on data vs. domain vs. process,"Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted. Copyright 2009 ACM.",Churn; Cross-project; Decision trees; Defect prediction; Logistic regression; Prediction quality,"Zimmermann T., Nagappan N., Gall H., Giger E., Murphy B.",2009,Conference,ESEC-FSE'09 - Proceedings of the Joint 12th European Software Engineering Conference and 17th ACM SIGSOFT Symposium on the Foundations of Software Engineering,10.1145/1595696.1595713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949378886&doi=10.1145%2f1595696.1595713&partnerID=40&md5=d601a429fb8a9aa3050d932a4a4ebf32,"Microsoft Research, India; University of Zurich, Switzerland",,English,,9781605580012
Scopus,Usability measurement and metrics: A consolidated model,"Usability is increasingly recognized as an important quality factor for interactive software systems, including traditional GUIs-style applications, Web sites, and the large variety of mobile and PDA interactive services. Unusable user interfaces are probably the single largest reasons why encompassing interactive systems - computers plus people, fail in actual use. The design of this diversity of applications so that they actually achieve their intended purposes in term of ease of use is not an easy task. Although there are many individual methods for evaluating usability; they are not well integrated into a single conceptual framework that facilitate their usage by developers who are not trained in the filed of HCI. This is true in part because there are now several different standards (e.g., ISO 9241, ISO/IEC 9126, IEEE Std.610.12) or conceptual models (e.g., Metrics for Usability Standards in Computing [MUSiC]) for usability, and not all of these standards or models describe the same operational definitions and measures. This paper first reviews existing usability standards and models while highlighted the limitations and complementarities of the various standards. It then explains how these various models can be unified into a single consolidated, hierarchical model of usability measurement. This consolidated model is called Quality in Use Integrated Measurement (QUIM). Included in the QUIM model are 10 factors each of which corresponds to a specific facet of usability that is identified in an existing standard or model. These 10 factors are decomposed into a total of 26 sub-factors or measurable criteria that are furtherdecomposed into 127 specific metrics. The paper explains also how a consolidated model, such as QUIM, can help in developing a usability measurement theory. © Springer Science + Business Media, Inc. 2006.",Effectiveness; Efficiency; Measurement; Metrics; Software engineering quality models; Usability; User satisfaction,"Seffah A., Donyaee M., Kline R.B., Padda H.K.",2006,Journal,Software Quality Journal,10.1007/s11219-006-7600-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746864055&doi=10.1007%2fs11219-006-7600-8&partnerID=40&md5=da0ec35cfd5b83d79addb20c626f3c94,"Human-Centered Software Engineering Group, Department of Computer Science and Software Engineering, Concordia University, 1455 De Maisonneuve Blvd. West, Montreu, Que. H3G 1M8, Canada; Department of Psychology (PY 151-6), Concordia University, 7141 Sherbrooke St. West, Montreal, Que. H4B 1R6, Canada",Kluwer Academic Publishers,English,09639314,
Scopus,Systematic literature reviews in software engineering-A tertiary study,"Context: In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007. Objective: The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search. Method: We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study. Results: Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines. Conclusion: SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality. © 2010 Elsevier B.V. All rights reserved.",Mapping study; Software engineering; Systematic literature review; Tertiary study,"Kitchenham B., Pretorius R., Budgen D., Brereton O.P., Turner M., Niazi M., Linkman S.",2010,Review,Information and Software Technology,10.1016/j.infsof.2010.03.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953727654&doi=10.1016%2fj.infsof.2010.03.006&partnerID=40&md5=9241ef400781a747a7b9fe611aee14af,"School of Computing and Mathematics, Keele University, Staffordshire ST5 5BG, United Kingdom; School of Engineering and Computing Sciences, Durham University, Durham DH1 3LE, United Kingdom",Elsevier B.V.,English,09505849,
Scopus,Software development cost estimation approaches - A survey,"This paper summarizes several classes of software cost estimation models and techniques: parametric models, expertise-based techniques, learning-oriented techniques, dynamics-based models, regression-based models, and composite-Bayesian techniques for integrating expertise-based and regression-based models. Experience to date indicates that neural-net and dynamics-based techniques are less mature than the other classes of techniques, but that all classes of techniques are challenged by the rapid pace of change in software technology. The primary conclusion is that no single technique is best for all situations, and that a careful comparison of the results of several approaches is most likely to produce realistic estimates.",,"Boehm B., Abts C., Chulani S.",2000,Journal,Annals of Software Engineering,10.1023/A:1018991717352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034557951&doi=10.1023%2fA%3a1018991717352&partnerID=40&md5=ef09727f773b1ecf0882fc3e61ac3dfc,"University of Southern California, Los Angeles, CA 90089-0781, United States; IBM Research, 650 Harry Road, San Jose, CA 95120, United States",,English,10227091,
Scopus,Does the technology acceptance model predict actual use? A systematic literature review,"Context: The technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage. Objective: This review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage. Method: We performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results. Results: The search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage. Conclusion: Care should be taken using the TAM outside the context in which it has been validated. © 2009 Elsevier B.V. All rights reserved.",Actual usage; Evidence-based software engineering; Literature review; Systematic literature review; Technology acceptance model (TAM),"Turner M., Kitchenham B., Brereton P., Charters S., Budgen D.",2010,Review,Information and Software Technology,10.1016/j.infsof.2009.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949489154&doi=10.1016%2fj.infsof.2009.11.005&partnerID=40&md5=ccb98f09b9b7d5b81a21844f0c23158b,"School of Computing and Mathematics, Keele University, Keele, Staffordshire ST55BG, United Kingdom; Department of Applied Computing, Lincoln University, PO Box 84, Lincoln 7647, Canterbury, New Zealand; School of Engineering and Computing Sciences, Durham University, Science Laboratories, South Road, Durham City, DH1 3LE, United Kingdom",,English,09505849,
Scopus,On the relative value of cross-company and within-company data for defect prediction,"We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company (CC) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where CC data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor (NN) filtering) to CC data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company (WC) data. As expected, we observe that defect predictors learned from WC data outperform the ones learned from CC data. However, our analyses also yield defect predictors learned from NN-filtered CC data, with performance close to, but still not better than, WC data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn WC defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use NN-filtered CC data to initiate the defect prediction process and simultaneously start collecting WC (local) data. Once enough WC data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from WC data. © 2008 Springer Science+Business Media, LLC.",Cross-company; Defect prediction; Learning; Metrics (product metrics); Nearest-neighbor filtering; Within-company,"Turhan B., Menzies T., Bener A.B., Di Stefano J.",2009,Journal,Empirical Software Engineering,10.1007/s10664-008-9103-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449126753&doi=10.1007%2fs10664-008-9103-7&partnerID=40&md5=81f9ed3f267876613ba3edbe3b4020a4,"Department of Computer Engineering, Bogazici University, Istanbul, Turkey; Lane Department of Computer Science and Electrical Engineering, Morgantown, WV, United States",,English,13823256,
Scopus,Towards a Framework for Software Measurement Validation,"In this paper we propose a framework for validating software measurement. We start by defining a measurement structure model that identifies the elementary component of measures and the measurement process, and then consider five other models involved in measurement: unit definition models, instrumentation models, attribute relationship models, measurement protocols and entity population models. We consider a number of measures from the viewpoint of our measurement validation framework and identify a number of shortcomings; in particular we identify a number of problems with the construction of function points. We also compare our view of measurement validation with ideas presented by other researchers and identify a number of areas of disagreement. Finally, we suggest several rules that practitioners and researchers can use to avoid measurement problems, including the use of measurement vectors rather than artificially contrived scalars. © 1995 IEEE",Measurement theory; software measurement; software metrics validation,"Kitchenham B., Pfleeger S.L., Fenton N.",1995,Journal,IEEE Transactions on Software Engineering,10.1109/32.489070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937653154&doi=10.1109%2f32.489070&partnerID=40&md5=c7afab9740a9e74f706c5443297870e1,"National Computing Centre, Oxford House, Oxford Rd., Manchester M1 7ED, United Kingdom; Systems/Software, Inc., 4519 Davenport St. NW, Washington, DC 20016-4415, United States; Centre for Software Reliability, City University, Northampton Sq., London EC1V OHB, United Kingdom",,English,00985589,
Scopus,Understanding and Controlling Software Costs,"Understanding of software costs is important because of the overall magnitude of these costs (in 1985, roughly $70 billion per year in the U.S. and over $140 billion per year worldwide) and the fundamental impact software will have on our future quality of life. Section I of this paper discusses these issues. Section II, the main portion of the paper, discusses the two primary ways of understanding software costs. The “black-box” or influence-function approach provides useful experimental and observational insights on the relative software productivity and quality leverage of various management, technical, environmental, and personnel options. The “glass-box” or cost distribution approach helps identify strategies for integrated software productivity and quality improvement programs, via such structures as the value chain and the software productivity opportunity tree. The individual strategies for improving software productivity identified in Section II are: • writing less code; • getting the best from people; • avoiding rework; • developing and using integrated project support environments. Section II provides overall surveys of early and recent progress along these and other lines identified by the opportunity tree. Better understanding of software costs leads to better methods of controlling software project costs, and vice versa. Section III discusses these issues. It points out that a good framework of techniques exists for controlling software budgets, schedules, and work completed, but that a great deal of further progress is needed to provide an overall set of planning and control techniques covering software product qualities and end-user system objectives. © 1988 IEEE",Programming productivity; software costs; software engineering economics; software management; software metrics; software productivity,"Boehm B.W., Papaccio P.N.",1988,Journal,IEEE Transactions on Software Engineering,10.1109/32.6191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024089511&doi=10.1109%2f32.6191&partnerID=40&md5=165eaa3928343b2a27ebd12bb287294d,"TRW Inc., One Space Park, Redondo Beach, CA 90278, United States",,English,00985589,
Scopus,Product platform and product family design: Methods and applications,"Today's highly competitive and volatile marketplace is reshaping the way many companies do business. Rapid innovation and mass customization offer a new form of competitive advantage. In response, companies like Sony, Black & Decker, and Kodak have successfully implemented strategies to design and develop an entire family of products based on a common product platform to satisfy a wide variety of customer requirements and leverage economies of scale and scope. Designing products and product families so that they may be customized for the global marketplace and achieving these goals in an abbreviated time period, while maintaining mass production efficiencies, is the key to successful manufacturing operations. Research in this area has matured rapidly over the last decade, and ""Product Platform and Product Family Design: Methods and Applications"" discusses how product platform and product family design can be used successfully to:-Increase variety within a product line,-Shorten manufacturing lead times, an-Reduce overall costs within a product line. The material available here serves as both a reference and a hands-on guide for researchers and practitioners devoted to the design, planning and production of families of products. Included are real-life case studies that explain the benefits of platform-based product development. © 2006 Springer Science+Business Media, LLC. All rights reserved.",,"Simpson T.W., Siddique Z., Jiao J.",2006,Book,Product Platform and Product Family Design: Methods and Applications,10.1007/0-387-29197-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891977337&doi=10.1007%2f0-387-29197-0&partnerID=40&md5=42e8e17e0838377f64483befffd1d73e,"Pennsylvania State University, 329 Leonhard Building, University Park, PA 16802, United States; University of Oklahoma, School of Aerospace and Mechanical Engineering, 865 ASP Avenue, Norman, OK 73019, United States; Nanyang Technological University, School of Mechanical and Aeropace Engineering, Nanyang Avenue 50, Singapore 639798, Singapore",Springer US,English,,0387257217; 9780387257211
Scopus,A Simulation Study of the Model Evaluation Criterion MMRE,"The Mean Magnitude of Relative Error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that used MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.",Empirical software engineering; Mean magnitude of relative error; Prediction accuracy; Prediction models; Regression analysis; Simulation; Software cost estimation; Software engineering; Software metrics,"Foss T., Stensrud E., Kitchenham B., Myrtveit I.",2003,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2003.1245300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346896359&doi=10.1109%2fTSE.2003.1245300&partnerID=40&md5=7bbc5169e3f75eb61f41f9e3e3562fea,"Norwegian School of Management BI, Box 580, Elias Smiths vei 15, N-1301 Sandvika, Norway; Myrtveit og Stensrud ANS, Austliveien 30, 0752 Oslo, Norway; Department of Computer Science, Keele University, Keele, Staffordshire ST5 5BG, United Kingdom",,English,00985589,
Scopus,What accuracy statistics really measure,"The paper aims to provide the software estimation research community with a better understanding of the meaning of, and relationship between, two statistics that are often used to assess the accuracy of predictive models: the mean magnitude relative error, MMRE, and the number of predictions within 25% of the actuals, pred(25). It is demonstrated that MMRE and pred(25) are, respectively, measures of the spread and the kurtosis of the variable z where z=estimate/actual. Thus, z is considered to be a measure of accuracy, and statistics such as MMRE and pred(25) to be measures of properties of the distribution of z. It is suggested that measures of the central location and skewness of z, as well as measures of spread and kurtosis, are necessary. Furthermore, since the distribution of z is non-normal, non-parametric measures of these properties may be needed. For this reason, boxplots of z are useful alternatives to simple summary metrics. It is also noted that the simple residuals are better behaved than the z variable, and could also be used as the basis for comparing prediction systems.",,"Kitchenham B.A., Pickard L.M., MacDonell S.G., Shepperd M.J.",2001,Journal,IEE Proceedings: Software,10.1049/ip-sen:20010506,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035381954&doi=10.1049%2fip-sen%3a20010506&partnerID=40&md5=af5290e06711ebc75d7de73f74f816ed,"Department of Computer Science, Keele University, Keele, ST5 5BG, United Kingdom",,English,14625970,
Scopus,Software Measurement: A Necessary Scientific Basis,"Software measurement, like measurement in any other discipline, must adhere to the science of measurement if it is to gain widespread acceptance and validity. The observation of some very simple, but fundamental, principles of measurement can have an extremely beneficial effect on the subject. Measurement theory is used to highlight both weaknesses and strengths of software metrics work, including work on metrics validation. We identify a problem with the well-known Weyuker properties, but also show that a criticism of these properties by Cherniavsky and Smith is invalid. We show that the search for general software complexity measures is doomed to failure. However, the theory does help us to define and validate measures of specific complexity attributes. Above all, we are able to view software measurement in a very wide perspective, rationalising and relating its many diverse activities. © 1994 IEEE.",complexity; empirical studies; measurement theory; metrics; Software measurement; validation,Fenton N.,1994,Journal,IEEE Transactions on Software Engineering,10.1109/32.268921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028393055&doi=10.1109%2f32.268921&partnerID=40&md5=8463b2ebf1c9f851a68473ff38494a97,"Centre for Software Reliability, ECIV OHB, United Kingdom",,English,00985589,
Scopus,A systematic review of software fault prediction studies,"This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle. © 2008 Elsevier Ltd. All rights reserved.",Automated fault prediction models; Expert systems; Machine learning; Method-level metrics; Public datasets,"Catal C., Diri B.",2009,Review,Expert Systems with Applications,10.1016/j.eswa.2008.10.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60249092995&doi=10.1016%2fj.eswa.2008.10.027&partnerID=40&md5=1b33c296012aa7543ffb5a35de959015,"The Scientific and Technological Research Council of Turkey, Marmara Research Center, Information Technologies Institute, Kocaeli, Turkey; Yildiz Technical University, Department of Computer Engineering, Istanbul, Turkey",,English,09574174,
Scopus,An attack surface metric,"Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method. Software developers can mitigate their software's security risk by measuring and reducing their software's attack surfaces. Our attack surface reduction approach complements the software industry's traditional code quality improvement approach for security risk mitigation and is useful in multiple phases of the software development lifecycle. Our collaboration with SAP demonstrates the use of our metric in the software development process. © 2011 IEEE Published by the IEEE Computer Society.",Code design; Life cycle; Product metrics; Protection mechanisms; Risk mitigation; Software security,"Manadhata P.K., Wing J.M.",2011,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2010.60,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957876122&doi=10.1109%2fTSE.2010.60&partnerID=40&md5=214de44fc1906208ae84b99972a4b8a3,"Symantec Research Labs, 900 Corporate Pointe, Culver City, CA 90230, United States; Carnegie Mellon University, Computer and Information Science and Engineering Directorate, US National Science Foundation, 4201 Wilson Boulevard, Arlington, United States",,English,00985589,
Scopus,Managerial use of metrics for object-oriented software: An exploratory analysis,"-With the increasing use of object-oriented methods in new software development there is a growing need to both document and improve current practice in object-oriented design and development. In response to this need, a number of researchers have developed various metrics for object-oriented systems as proposed aids to the management of these systems. In this research an analysis of a set of metrics proposed by Chidamber and Kemerer [10is performed in order to assess their usefulness for practicing managers. First, an informal introduction to the metrics is provided by way of an extended example of their managerial use. Second, exploratory analyses of empirical data relating the metrics to productivity, rework effort, and design effort on three commercial object-oriented systems are provided. The empirical results suggest that the metrics provide significant explanatory power for variations in these economic variables, over and above that provided by traditional measures, such as size in lines of code, and after controlling for the effects of individual developers. ©1998 IEEE.",CBO; Design; DIT; Effort; LCOM; NOC; Object-orientation; Productivity; Programmer; Project management; Reuse; RFC; SLOG; Software metrics; WMC,"Chidamber S.R., Darcy D.P., Kemerer C.F.",1998,Journal,IEEE Transactions on Software Engineering,10.1109/32.707698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003233849&doi=10.1109%2f32.707698&partnerID=40&md5=19268a4b4fd9361663ba7930abbc0ace,"Advisory Board Co, 600 Hampshire Ave, Washington, DC 20037, United States",,English,00985589,
Scopus,Using mapping studies as the basis for further research - A participant-observer case study,"Context: We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research. Objective: This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic. Method: We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies. Results: Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers. Conclusion: Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research. © 2010 Elsevier B.V. All rights reserved.",Case study; Mapping studies; Software engineering; Systematic literature review,"Kitchenham B.A., Budgen D., Pearl Brereton O.",2011,Journal,Information and Software Technology,10.1016/j.infsof.2010.12.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953724329&doi=10.1016%2fj.infsof.2010.12.011&partnerID=40&md5=a681727eb285f040d3b968199f73f3ba,"School of Computing and Mathematics, Keele University, Staffordshire ST5 5BG, United Kingdom; Durham University, South Road, Durham City, DH1 3LE, United Kingdom",,English,09505849,
Scopus,Cross versus within-company cost estimation studies: A systematic review,"The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., ≤20 projects) that used leave-one-out cross validation all found that the within-company model was significantly different (better) from the cross-company model. The results of this review are inconclusive. It is clear that some organizations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e., based on different data bases or at least different single company data sets) and should address specific hypotheses concerning the conditions that would favor cross-company or within-company models. In addition, experimenters need to standardize their experimental procedures to enable formal meta-analysis, and recommendations are made in Section 3. © 2007 IEEE.",Cost estimation; Management; Software engineering; Systematic review,"Kitchenham B.A., Mendes E., Travassos G.H.",2007,Review,IEEE Transactions on Software Engineering,10.1109/TSE.2007.1001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247611323&doi=10.1109%2fTSE.2007.1001&partnerID=40&md5=a0220b724f2497a1dd7566e8b9e35e7d,"School of Computing and Mathematics, University of Keele, Keele Village, Staffordshire ST5 5BG, United Kingdom; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand; UFRJ/COPPE, Systems Engineering and Computer Science Program, PO Box 68511, 21941-972 Rio de Janeiro, RJ, Brazil",,English,00985589,
Scopus,A review of software surveys on software effort estimation,"This paper summarizes estimation knowledge through a review of surveys on software effort estimation. Main findings were that: (1) most projects (60-80%) encounter effort and/or schedule overruns. The overruns, however, seem to be lower than the overruns reported by some consultancy companies. For example, Standish Group's ""Chaos Report"" describes an average cost overrun of 89%, which is much higher than the average overruns found in other surveys, i.e. 3040%. (2) The estimation methods in most frequent use of expert judgment is that there is no evidence that formal estimation models lead to more accurate estimates. (3) There is a lack of surveys including extensive analyses of the reasons for effort and schedule overruns. © 2003 IEEE.",Chaos; Computer industry; Costs; Job shop scheduling; Laboratories; Processor scheduling; Project management; Software engineering; Software performance; Writing,"Moløkken K., Jørgensen M.",2003,Conference,"Proceedings - 2003 International Symposium on Empirical Software Engineering, ISESE 2003",10.1109/ISESE.2003.1237981,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944314536&doi=10.1109%2fISESE.2003.1237981&partnerID=40&md5=051b51e3134558a36469b8e91acf017b,"Simula Research Laboratory, P.O. Box 134, Lysaker, NO-1325, Norway",Institute of Electrical and Electronics Engineers Inc.,English,,0769520022; 9780769520025
Scopus,The future of empirical methods in software engineering research,"We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research. © 2007 IEEE.",,"Sjøberg D.I.K., Dybå T., Jørgensen M.",2007,Conference,FoSE 2007: Future of Software Engineering,10.1109/FOSE.2007.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748911996&doi=10.1109%2fFOSE.2007.30&partnerID=40&md5=4380254916d99eade8cb5d4a5657cb7a,"Simula Research Laboratory, P.O.Box 134, NO-1325 Lysaker, Norway; SINTEF ICT, NO-7465 Trondheim, Norway",,English,,0769528295; 9780769528298
Scopus,How long will it take to fix this bug?,"Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naïve predictions by a factor of four. © 2007 IEEE.",,"Weiß C., Premraj R., Zimmermann T., Zeller A.",2007,Conference,"Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007",10.1109/MSR.2007.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548739034&doi=10.1109%2fMSR.2007.13&partnerID=40&md5=726d2743bed553faf0f1f91c353fd736,Saarland University,,English,,076952950X; 9780769529509
Scopus,Verification and validation in scientific computing,"Advances in scientific computing have made modelling and simulation an important part of the decision-making process in engineering, science, and public policy. This book provides a comprehensive and systematic development of the basic concepts, principles, and procedures for verification and validation of models and simulations. The emphasis is placed on models that are described by partial differential and integral equations and the simulations that result from their numerical solution. The methods described can be applied to a wide range of technical fields, from the physical sciences, engineering and technology and industry, through to environmental regulations and safety, product and plant safety, financial investing, and governmental regulations. This book will be genuinely welcomed by researchers, practitioners, and decision makers in a broad range of fields, who seek to improve the credibility and reliability of simulation results. It will also be appropriate either for university courses or for independent study. © W. L. Oberkampf & C. J. Roy 2010.",,"Oberkampf W.L., Roy C.J.",2011,Book,Verification and Validation in Scientific Computing,10.1017/CBO9780511760396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010433575&doi=10.1017%2fCBO9780511760396&partnerID=40&md5=1f04b232b7d8f8610a2becb55208f117,"Aerospace and Ocean Engineering Department, Virginia Tech, United States",Cambridge University Press,English,,9780511760396; 9780521113601
Scopus,Guide to advanced empirical software engineering,"Empirical studies have become an integral element of software engineering research and practice. This unique text/reference includes chapters from some of the top international empirical software engineering researchers and focuses on the practical knowledge necessary for conducting, reporting and using empirical methods in software engineering. Part 1, 'Research Methods and Techniques', examines the proper use of various strategies for collecting and analysing data, and the uses for which those strategies are most appropriate. Part 2, 'Practical Foundations', provides a discussion of several important global issues that need to be considered from the very beginning of research planning. Finally, 'Knowledge Creation' offers insight on using a set of disparate studies to provide useful decision support. Topics and features: Offers information across a range of techniques, methods, and qualitative and quantitative issues, providing a toolkit for the reader that is applicable across the diversity of software development contexts Presents reference material with concrete software engineering examples Provides guidance on how to design, conduct, analyse, interpret and report empirical studies, taking into account the common difficulties and challenges encountered in the field Arms researchers with the information necessary to avoid fundamental risks Tackles appropriate techniques for addressing disparate studies - ensuring the relevance of empirical software engineering, and showing its practical impact Describes methods that are less often used in the field, providing less conventional but still rigorous and useful ways of collecting data Supplies detailed information on topics (such as surveys) that often contain methodological errors This broad-ranging, practical guide will prove an invaluable and useful reference for practising software engineers and researchers. In addition, it will be suitable for graduate students studying empirical methods in software development. © Springer-Verlag London Limited 2008.",,"Shull F., Singer J., Sjøberg D.I.K.",2008,Book,Guide to Advanced Empirical Software Engineering,10.1007/978-1-84800-044-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890140936&doi=10.1007%2f978-1-84800-044-5&partnerID=40&md5=8f4ef74059b2d0068fcdda40a66b0c6a,"Fraunhofer Center for Empirical Software Engineering, College Park, MD, United States; NRC Institute for Information Technology, National Research Council, Ottawa, Canada; Simula Research Laboratory, Lysaker, Norway",Springer London,English,,9781848000438
Scopus,The situational factors that affect the software development process: Towards a comprehensive reference framework,"Context An optimal software development process is regarded as being dependent on the situational characteristics of individual software development settings. Such characteristics include the nature of the application(s) under development, team size, requirements volatility and personnel experience. However, no comprehensive reference framework of the situational factors affecting the software development process is presently available. Objective The absence of such a comprehensive reference framework of the situational factors affecting the software development process is problematic not just because it inhibits our ability to optimise the software development process, but perhaps more importantly, because it potentially undermines our capacity to ascertain the key constraints and characteristics of a software development setting. Method To address this deficiency, we have consolidated a substantial body of related research into an initial reference framework of the situational factors affecting the software development process. To support the data consolidation, we have applied rigorous data coding techniques from Grounded Theory and we believe that the resulting framework represents an important contribution to the software engineering field of knowledge. Results The resulting reference framework of situational factors consists of eight classifications and 44 factors that inform the software process. We believe that the situational factor reference framework presented herein represents a sound initial reference framework for the key situational elements affecting the software process definition. Conclusion In addition to providing a useful reference listing for the research community and for committees engaged in the development of standards, the reference framework also provides support for practitioners who are challenged with defining and maintaining software development processes. Furthermore, this framework can be used to develop a profile of the situational characteristics of a software development setting, which in turn provides a sound foundation for software development process definition and optimisation. © 2011 Elsevier B.V.",Process definition; Process implementation and change; Software engineering process,"Clarke P., O'Connor R.V.",1970,Journal,Information and Software Technology,10.1016/j.infsof.2011.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855616323&doi=10.1016%2fj.infsof.2011.12.003&partnerID=40&md5=53ecbe25a1504d5566c25d6d9b21b2dc,"Lero – Irish Software Engineering Research Centre, Dublin City University, Ireland; School of Computing, Dublin City University, Ireland",Elsevier B.V.,English,09505849,
Scopus,Systematic literature review of machine learning based software development effort estimation models,"Context: Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way. Objective: This research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context. Method: We performed a systematic literature review of empirical studies on ML model published in the last two decades (1991-2010). Results: We have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts. Conclusion: ML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners. © 2011 Elsevier B.V. All rights reserved.",Machine learning; Software effort estimation; Systematic literature review,"Wen J., Li S., Lin Z., Hu Y., Huang C.",2012,Review,Information and Software Technology,10.1016/j.infsof.2011.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055086827&doi=10.1016%2fj.infsof.2011.09.002&partnerID=40&md5=a9b36f8bf8c0fc6b08026d1ad28f9bde,"Department of Computer Science, Sun Yat-sen University, Guangzhou, China; Department of Computer Science, Guangdong Polytechnic Normal University, Guangzhou, China; Department of E-commerce, Guangdong University of Foreign Studies, Sun Yat-sen University, Guangzhou, China; Engineering Research Center of Computer Network and Information Systems, South China Normal University, Guangzhou, China",Elsevier B.V.,English,09505849,
Scopus,Reformulating software engineering as a search problem,"Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not' been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metaheuristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The software engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems, to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.",,"Clarke J., Dolado J.J., Harman M., Hierons R., Jones B., Lumkin M., Mitchell B., Mancoridis S., Rees K., Roper M., Shepperd M.",2003,Journal,IEE Proceedings: Software,10.1049/ip-sen:20030559,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038044864&doi=10.1049%2fip-sen%3a20030559&partnerID=40&md5=b839ded38f8a648d10aa0d5c5e70e163,"The University of York, Heslington, York YO10 5DD, United Kingdom; Facultad de Informática, University of the Basque Country, Basque Country, 20009, Spain; Brunel University, Uxbridge, Middlesex, UB8 3PH, United Kingdom; University of Glamorgan, Pontypridd, CF37 1DL, United Kingdom; Systems Integration Research, British Telecom, Adastral Park, Ipswich, IP5 3RE, United Kingdom; Dept. of Mathematics and Comp. Sci., Drexel University, Philadelphia, PA 19104, United States; Strathclyde University, Livingstone Tower, 26 Richmond Street, Glasgow G1 1XH, United Kingdom; Bournemouth University, Talbot Campus, Poole, BH12 5BB, United Kingdom",,English,14625970,
Scopus,Can genetic programming improve software effort estimation? A comparative evaluation,"Accurate software effort estimation is an important part of the software process. Originally, estimation was performed using only human expertise, but more recently, attention has turned to a variety of machine learning (ML) methods. This paper attempts to evaluate critically the potential of genetic programming (GP) in software effort estimation when compared with previously published approaches, in terms of accuracy and ease of use. The comparison is based on the well-known Desharnais data set of 81 software projects derived from a Canadian software house in the late 1980s. The input variables are restricted to those available from the specification stage and significant effort is put into the GP and all of the other solution strategies to offer a realistic and fair comparison. There is evidence that GP can offer significant improvements in accuracy but this depends on the measure and interpretation of accuracy used. GP has the potential to be a valid additional tool for software effort estimation but set up and running effort is high and interpretation difficult, as it is for any complex meta-heuristic technique. © 2001 Elsevier Science B.V. All rights reserved.",Case-based reasoning; Genetic programming; Machine learning; Neural networks; Software effort estimation,"Burgess C.J., Lefley M.",2001,Journal,Information and Software Technology,10.1016/S0950-5849(01)00192-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035892550&doi=10.1016%2fS0950-5849%2801%2900192-6&partnerID=40&md5=ca3d7c91b2da18e6adeea3e8752a96bc,"Department of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol BS8 1UB, United Kingdom; School of Design Engineering and Computing, University of Bournemouth, Talbot Campus, Poole BH12 5BB, United Kingdom",,English,09505849,
Scopus,Bayesian analysis of empirical software engineering cost models,"To date many software engineering cost models have been developed to predict the cost, schedule, and quality of the software under development. But, the rapidly changing nature of software development has made it extremely difficult to develop empirical models that continue to yield high prediction accuracies. Software development costs continue to increase and practitioners continually express their concerns over their inability to accurately predict the costs involved. Thus, one of the most important objectives of the software engineering community has been to develop useful models that constructively explain the software development life-cycle and accurately predict the cost of developing a software product. To that end, many parametric software estimation models have evolved in the last two decades [25], [17], [26], [15], [28], [1], [2], [33], [7], [10], [22], [23]. Almost all of the above mentioned parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in this paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The source data is also generally imprecise in reporting size, effort, and cost-driver ratings, particularly across different organizations. This results in the development of inaccurate empirical models that don't perform very well when used for prediction. This paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version [6]. It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach. Bayesian analysis is a well-defined and rigorous process of inductive reasoning that has been used in many scientific disciplines (the reader can refer to [11], [35], [3] for a broader understanding of the Bayesian Analysis approach). A distinctive feature of the Bayesian approach is that it permits the investigator to use both sample (data) and prior (expert-judgment) information in a logically consistent manner in making inferences. This is done by using Bayes' theorem to produce a 'postdata' or posterior distribution for the model parameters. Using Bayes' theorem, prior (or initial) values are transformed to postdata views. This transformation can be viewed as a learning process. The posterior distribution is determined by the variances of the prior and sample information. If the variance of the prior information is smaller than the variance of the sampling information, then a higher weight is assigned to the prior information. On the other hand, if the variance of the sample information is smaller than the variance of the prior information, then a higher weight is assigned to the sample information causing the posterior estimate to be closer to the sample information. The Bayesian approach discussed in this paper enables stronger solutions to one of the biggest problems faced by the software engineering community: the challenge of making good decisions using data that is usually scarce and incomplete. We note that the predictive performance of the Bayesian approach (i.e., within 30 percent of the actuals 75 percent of the time) is significantly better than that of the previous multiple regression approach (i.e., within 30 percent of the actuals only 52 percent of the time) on our latest sample of 161 project datapoints. Index Terms - Bayesian analysis, multiple regression, software estimation, software engineering cost models, model calibration, prediction accuracy, empirical modeling, COCOMO, measurement, metrics, project management. © 1999 IEEE.",,"Chulani S., Boehm B., Steece B.",1999,Journal,IEEE Transactions on Software Engineering,10.1109/32.799958,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033160009&doi=10.1109%2f32.799958&partnerID=40&md5=f2e5bf2a396a58d9d411db8e26bcb576,"IBM Research, Center for Software Engineering, 650 Harry Rd, San Jose, CA 95120, United States",Institute of Electrical and Electronics Engineers Inc.,English,00985589,
Scopus,Dealing with noise in defect prediction,"Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved. © 2011 ACM.",buggy changes; buggy files; data quality; defect prediction; noise resistance,"Kim S., Zhang H., Wu R., Gong L.",2011,Conference,Proceedings - International Conference on Software Engineering,10.1145/1985793.1985859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959899057&doi=10.1145%2f1985793.1985859&partnerID=40&md5=64ca52f0f6d3213d5d3f6c1ce07ea82c,"Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; School of Software, Tsinghua University, Beijing, China",,English,02705257,9781450304450
Scopus,Comparing software prediction techniques using simulation,"The need for accurate software prediction systems increases as software becomes much larger and more complex. A variety of techniques have been proposed; however, none has proven consistently accurate and there is still much uncertainty as to what technique suits which type of prediction problem. We believe that the underlying characteristics-size, number of features, type of distribution, etc. - of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. Also, in previous work, it has proven difficult to obtain significant results over small data sets. Consequently, it would be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1,000) validation cases. In this paper, we compared four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We also observed that the more ""messy"" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. The suggests that researchers will need to exercise caution when compairing different approaches and utilize procedures such as bootstrapping in order to generate multiple samples for training purposes. However, our most important result is that it more fruitful to ask which is the best prediction system in a particular context rather that which is the ""best"" prediction system.",Data set characteristic; Machine learning; Prediction system; Simulation,"Shepperd M., Kadoda G.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.965341,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035506767&doi=10.1109%2f32.965341&partnerID=40&md5=943f9ab42dcfa3bf63951442c7795edf,"Empirical Software Eng. Res. Group, Sch. of Design, Eng. and Comp., Bournemouth Univ., Poole, United Kingdom",,English,00985589,
Scopus,Reliability and validity in comparative studies software prediction models,"Empirical studies on software prediction models do not converge with respect to the question ""which prediction model is best?"" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models. © 2005 IEEE.",Accuracy indicators; Arbitrary function approximators; Cost estimation; Cross-validation; Empirical methods; Estimation by analogy; Machine learning; Regression analysis; Reliability; Simulation; Software metrics; Validity,"Myrtveit I., Stensrud E., Shepperd M.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.58,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944440671&doi=10.1109%2fTSE.2005.58&partnerID=40&md5=f4046789ea23ecaa6b114024b93ecbf3,"Norwegian School of Management BI, Elias Smiths vei 15, N-1301 Sandvika, Norway; Myrtveit og Stensrud ANS, Austliveien 30, 0752 Oslo, Norway; School of Design, Engineering and Computing, Bournemouth University, Poole House, P104d, Bournemouth BH12 5BB, United Kingdom",,English,00985589,
Scopus,Software metrics: Roadmap,"Software metrics as a subject area is over 30 years old, but it has barely penetrated into mainstream software engineering. A key reason for this is that most software metrics activities have not addressed their most important requirement: to provide information to support quantitative managerial decision-making during the software lifecycle. Good support for decision-making implies support for risk assessment and reduction. Yet traditional metrics approaches, often driven by regression-based models for cost estimation and defects prediction, provide little support for managers wishing to use measurement to analyse and minimise risk. The future for software metrics lies in using relatively simple existing metrics to build management decision-support tools that combine different aspects of software development and testing and enable managers to make many kinds of predictions, assessments and trade-offs during the software life-cycle. Our recommended approach is to handle the key factors largely missing from the usual metrics approaches, namely: causality, uncertainty, and combining different (often subjective) evidence. Thus the way forward for software metrics research lies in causal modelling (we propose using Bayesian nets), empirical software engineering, and multi-criteria decision aids. © ACM 2000.",Bayesian belief nets; Causal models; Multi-criteria decision aid; Risk assessment; Software metrics,"Fenton N.E., Neil M.",2000,Conference,"Proceedings of the Conference on the Future of Software Engineering, ICSE 2000",10.1145/336512.336588,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005986521&doi=10.1145%2f336512.336588&partnerID=40&md5=e64e27ee039db6e6cf9f5c5d76df5ada,"Computer Science Department, Queen Mary and Westfield College, London, E1 4NS, United Kingdom","Association for Computing Machinery, Inc",English,,1581132530; 9781581132533
Scopus,An empirical analysis of productivity and quality in software products,"We examine the relationship between life-cycle productivity and conformance quality in software products. The effects of product size, personnel capability, software process, usage of tools, and higher front-end investments on productivity and conformance quality were analyzed to derive managerial implications based on primary data collected on commercial software projects from a leading vendor. Our key findings are as follows. First, our results provide evidence for significant increases in life-cycle productivity from improved conformance quality in software products shipped to the customers. Given that the expenditure on computer software has been growing over the last few decades, empirical evidence for cost savings through quality improvement is a significant contribution to the literature. Second, our study identifies several quality drivers in software products. Our findings indicate that higher personnel capability, deployment of resources in initial stages of product development (especially design) and improvements in software development process factors are associated with higher quality products. © 2000 INFORMS.",CMM; Cost of quality; Front-end investments; Softivare process areas; Software quality and life-cycle productivity,"Krishnan M.S., Kriebel C.H., Kekre S., Mukhopadhyay T.",2000,Journal,Management Science,10.1287/mnsc.46.6.745.11941,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034205501&doi=10.1287%2fmnsc.46.6.745.11941&partnerID=40&md5=a14e6c7fce97db866c4cf6203a51389e,"University of Michigan, Business School, Ann Arbor, MI 48109, United States; Graduate School of Industrial Administration, Carnegie Mellon University, Pittsburgh, PA 15213, United States",INFORMS Inst.for Operations Res.and the Management Sciences,English,00251909,
Scopus,Evaluating prediction systems in software project estimation,"Context: Software engineering has a problem in that when we empirically evaluate competing prediction systems we obtain conflicting results. Objective: To reduce the inconsistency amongst validation study results and provide a more formal foundation to interpret results with a particular focus on continuous prediction systems. Method: A new framework is proposed for evaluating competing prediction systems based upon (1) an unbiased statistic, Standardised Accuracy, (2) testing the result likelihood relative to the baseline technique of random 'predictions', that is guessing, and (3) calculation of effect sizes. Results: Previously published empirical evaluations of prediction systems are re-examined and the original conclusions shown to be unsafe. Additionally, even the strongest results are shown to have no more than a medium effect size relative to random guessing. Conclusions: Biased accuracy statistics such as MMRE are deprecated. By contrast this new empirical validation framework leads to meaningful results. Such steps will assist in performing future meta-analyses and in providing more robust and usable recommendations to practitioners. © 2012 Elsevier B.V. All rights reserved.",Empirical validation; Prediction system; Randomisation techniques; Software engineering,"Shepperd M., MacDonell S.",2012,Journal,Information and Software Technology,10.1016/j.infsof.2011.12.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861556186&doi=10.1016%2fj.infsof.2011.12.008&partnerID=40&md5=75b67e128bf273243bb00bbe83bdb914,"Dept. of IS and Computing, Brunel University, Uxbridge, UB83PH, United Kingdom; Dept. of Computing and Mathematical Sciences, Auckland University of Technology, Private Bag 92006, Auckland 1142, New Zealand",Elsevier B.V.,English,09505849,
Scopus,Software Development Cost Estimation Using Function Points,This paper presents an assessment of several published statistical regression models that relate software development effort to software size measured in function points. The principal concern with published models has to do with the number of observations upon which the models were based and inattention to the assumptions inherent in regression analysis. The research describes appropriate statistical procedures in the context of a case study based on function point data for 104 software development projects and discusses limitations of the resulting model in estimating development effort. The paper also focuses on a problem with the current method for measuring function points that constrains the effective use of function points in regression models and suggests a modification to the approach that should enhance the accuracy of prediction models based on function points in the future. © 1994 IEEE.,cost estimation; Function points; regression analysis,"Matson J.E., Mellichamp J.M.",1994,Journal,IEEE Transactions on Software Engineering,10.1109/32.277575,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028418767&doi=10.1109%2f32.277575&partnerID=40&md5=8658201cfa6c05270e2720014afe0fd1,"Department of Industrial Engineering, University of Alabama, Tuscaloosa, AL 35487, United States; Department of Management Science and Statistics, University of Alabama, Tuscaloosa, AL, 35487, United States",,English,00985589,
Scopus,Analyzing data sets with missing data: An empirical evaluation of imputation methods and likelihood-based methods,"Missing data are often encountered in data sets used to construct effort prediction models. Thus far, the common practice has been to ignore observations with missing data. This may result in biased prediction models. In this paper, we evaluate four missing data techniques (MDTs) in the context of software cost modeling: listwise deletion (LD), mean imputation (MI), similar response pattern imputation (SRPI), and full information maximum likelihood (FIML). We apply the MDTs to an ERP data set, and thereafter construct regression-based prediction models using the resulting data sets. The evalution suggests that only FIML is appropriate when the data are not missing completely at random (MCAR). Unlike FIML, prediction models constructed on LD, MI and SRPI data sets will be biased unless the data are MCAR. Furthermore, compared to LD, MI and SRPI seem appropriate only if the resulting LD data set is too small to enable the construction of a meaningful regression-based prediction model.",Cost estimation; ERP; Full information maximum likelihood; Imputation methods; Listwise deletion; Log-log regression; Mean imputation; Missing data; Similar response pattern imputation; Software effort prediction,"Myrtveit I., Stensrud E., Olsson U.H.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.965340,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035506257&doi=10.1109%2f32.965340&partnerID=40&md5=af3956925cdf90fb2ccc718588d4d945,"IEEE, Norway; Norwegian School of Management, PO Box 580, N-1301 Sandvika, Norway",,English,00985589,
Scopus,Revisiting the impact of classification techniques on the performance of defect prediction models,"Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others. © 2015 IEEE.",,"Ghotra B., McIntosh S., Hassan A.E.",2015,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2015.91,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951805519&doi=10.1109%2fICSE.2015.91&partnerID=40&md5=1d0f3b6a7e246845a265cfb0bd908220,"Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada, Canada",IEEE Computer Society,English,02705257,9781479919345
Scopus,Empirical study of analogy-based software effort estimation,"Conventional approaches to software cost estimation have focused on algorithmic cost models, where an estimate of effort is calculated from one or more numerical inputs via a mathematical model. Analogy-based estimation has recently emerged as a promising approach, with comparable accuracy to algorithmic methods in some studies, and it is potentially easier to understand and apply. The current study compares several methods of analogy-based software effort estimation with each other and also with a simple linear regression model. The results show that people are better than tools at selecting analogues for the data set used in this study. Estimates based on their selections, with a linear size adjustment to the analogue's effort value, proved more accurate than estimates based on analogues selected by tools, and also more accurate than estimates based on the simple regression model.",,"Walkerden F., Jeffery R.",1999,Journal,Empirical Software Engineering,10.1023/A:1009872202035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033336424&doi=10.1023%2fA%3a1009872202035&partnerID=40&md5=5215c2f5763849b9dd5114d1c13f6e52,"Ctr. Adv. Empirical Software Res., School of Information Systems, University of New South Wales, Sydney, NSW 2052, Australia","Kluwer Academic Publishers, Dordrecht, Netherlands",English,13823256,
Scopus,Replicated assessment and comparison of common software cost modeling techniques,"Using data from the European Space Agency data set, a yet unexplored application domain, including military and space projects, is explored. It is shown that traditional techniques, such as ordinary least-squares regression and analysis of variance outperform Analogy-based estimation and regression trees.",,"Briand Lionel C., Langley Tristen, Wieczorek Isabella",2000,Conference,Proceedings - International Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033725599&partnerID=40&md5=44f19c15dacd6097a444616256d4216e,"Carleton Univ, Ottawa, Canada",,English,02705257,
Scopus,Software cost estimation with incomplete data,"The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. In this paper, we describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modelling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques perform well with small biases and high precision. This suggests that the simplest technique, listwise deletion, is a reasonable choice. However, this will not necessarily provide the best performance. Consistent best performance (minimal bias and highest precision) can be obtained by using hot-deck imputation with Euclidean distance and a z-score standardization.",Cost modelling; Data quality; Imputation; Missing data; Software cost estimation,"Strike K., Emam K.E., Madhavji N.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.962560,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035481267&doi=10.1109%2f32.962560&partnerID=40&md5=cbe8a7824a25e70d5b3df57dee8690df,"School of Computer Science, McGill University, McConnell Engineering Building, 3480 University Street, Montreal, Que. H3A 2A7, Canada; National Research Council of Canada, Institute for Information Technology, Building M-50, Montreal Road, Ottawa, Ont. K1A OR6, Canada",,English,00985589,
Scopus,Examining the feasibility of a case-based reasoning model for software effort estimation,"Existing algorithmic models fail to produce accurate software development effort estimates. To address this problem, a case-based reasoning model, called Estor, was developed based on the verbal protocols of a human expert solving a set of estimation problems. Estor was then presented with 15 software effort estimation tasks. The estimates of Estor were compared to those of the expert as well as those of the function point and COCOMO estimations of the projects. The estimates generated by the human expert and Estor were more accurate and consistent than those of the function point and COCOMO methods. In fact, Estor was nearly as accurate and consistent as the expert. These results suggest that a case-based reasoning approach for software effort estimation holds promise and merits additional research.",Case-based reasoning; Constructive cost model; Function points; Software effort estimation,"Mukhopadhyay T., Vicinanza S.S., Prietula M.J.",1992,Journal,MIS Quarterly: Management Information Systems,10.2307/249573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000356302&doi=10.2307%2f249573&partnerID=40&md5=7018a9c22f45ff1595212385b726b55b,"Graduate School of Industrial Administration, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Energy Management Associates, Inc., 100 Northcreek, Atlanta, GA 30327, United States",Management Information Systems Research Center,English,02767783,
Scopus,Empirical studies of pair programming for CS/SE teaching in higher education: A systematic literature review,"The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks. © 2006 IEEE.",Empirical studies; pair programming; systematic review,"Salleh N., Mendes E., Grundy J.C.",2011,Review,IEEE Transactions on Software Engineering,10.1109/TSE.2010.59,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959505364&doi=10.1109%2fTSE.2010.59&partnerID=40&md5=dd9b948cc5c31c78a3f1c1e00c5e5b5a,"Department of Computer Science, International Islamic University Malaysia, PO Box 10, 50728 Kuala Lumpur, Malaysia; Department of Computer Science, University of Auckland, Auckland Mail Centre, Private Bag 92019, Auckland 1142, New Zealand; Faculty of Information and Communication Technologies, Swinburne University of Technology, PO Box 218, Hawthorn, VIC 3122, Australia",,English,00985589,
Scopus,Review of aerospace engineering cost modelling: The genetic causal approach,"The primary intention of this paper is to review the current state of the art in engineering cost modelling as applied to aerospace. This is a topic of current interest and in addressing the literature, the presented work also sets out some of the recognised definitions of cost that relate to the engineering domain. The paper does not attempt to address the higher-level financial sector but rather focuses on the costing issues directly relevant to the engineering process, primarily those of design and manufacture. This is of more contemporary interest as there is now a shift towards the analysis of the influence of cost, as defined in more engineering related terms; in an attempt to link into integrated product and process development (IPPD) within a concurrent engineering environment. Consequently, the cost definitions are reviewed in the context of the nature of cost as applicable to the engineering process stages: from bidding through to design, to manufacture, to procurement and ultimately, to operation. The linkage and integration of design and manufacture is addressed in some detail. This leads naturally to the concept of engineers influencing and controlling cost within their own domain rather than trusting this to financers who have little control over the cause of cost. In terms of influence, the engineer creates the potential for cost and in a concurrent environment this requires models that integrate cost into the decision making process. © 2004 Published by Elsevier Ltd.",,"Curran R., Raghunathan S., Price M.",2004,Review,Progress in Aerospace Sciences,10.1016/j.paerosci.2004.10.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-15444373752&doi=10.1016%2fj.paerosci.2004.10.001&partnerID=40&md5=0651902ddbee86c3d00f2388586a9d4c,"Ctr. Excellence Intgd. Aircraft T., School of Aeronautical Engineering, Queens Univ. Belfast, David Keir B., Stranhillis Road, Belfast BT9 5AG, United Kingdom",,English,03760421,
Scopus,What we have learned about fighting defects,"The Center for Empirically Based Software Engineering helps improve software development by providing guidelines for selecting development techniques, recommending areas for further research, and supporting software engineering education. A central activity toward achieving this goal has been the running of ""e- Workshops"" that capture expert knowledge with a minimum of overhead effort to formulate heuristics on a particular topic. The resulting heuristics are a useful summary of the current state of knowledge in an area based on expert opinion. This paper discusses the results to date of a series of e-Workshops on software defect reduction. The original discussion items are presented along with an encapsulated summary of the expert discussion. The reformulated heuristics can be useful both to researchers (for pointing out gaps in the current state of the knowledge requiring further investigation) and to practitioners (for benchmarking or setting expectations about development practices). © 2002 IEEE.",Decision making; Electrical capacitance tomography; Feathers; Guidelines; Inspection; Programming; Software engineering; Testing,"Shull F., Basili V., Boehm B., Brown A.W., Costa P., Lindvall M., Port D., Rus I., Tesoriero R., Zelkowitz M., Allen E., Anger F., Chulani S., Davis N., Dyer M., Ebert C., Elliott B., Fagan E., Feather M., Green L., Forman I., Henninger S., Johnson P., Laitenberger O., Madachy R., Matsumoto Y., McGibbon T., Miller J., Moore J., O'Neill D., Rifkin S., Rombach D., Roy D., Saiedian H., Succi G., Thomas G., Vinter O.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948456877&doi=10.1109%2fMETRIC.2002.1011343&partnerID=40&md5=f82ed914311c8842ecc75c65c17bfa01,"Fraunhofer Center for Experimental Software EngineeringMD, United States; University of Southern California, Center for Software Engineering, United States; University of Maryland, Empirical Software Engineering Group, United States; MSU, United States; NSF, United States; IBM, United States; Davis Systems, United States; Lockheed Martin, United States; Alcatel, United States; Harris Corp., United States; Michael Fagan Associates, United States; JPL, United States; UNL, United States; U. Hawaii, United States; IESE, United States; USC, United States; Toshiba, Japan; ITT Industries, United States; U. Alberta, Canada; MITRE, United States; Don O'Neill Consulting, United States; Masters Systems, United States; STTP, Inc., United States; U. Kansas, United States; Raytheon, United States",IEEE Computer Society,English,15301435,0769513395
Scopus,What's up with software metrics? - A preliminary mapping study,"Background: Many papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research. Aim: This paper aims to identify trends in influential software metrics papers and assess the possibility of using secondary studies to integrate research results. Method: Search facilities in the SCOPUS tool were used to identify the most cited papers in the years 2000-2005 inclusive. Less cited papers were also selected from 2005. The selected papers were classified according factors such as to main topic, goal and type (empirical or theoretical or mixed). Papers classified as ""Evaluation studies"" were assessed to investigate the extent to which results could be synthesized. Results: Compared with less cited papers, the most cited papers were more frequently journal papers, and empirical validation or data analysis studies. However, there were problems with some empirical validation studies. For example, they sometimes attempted to evaluate theoretically invalid metrics and fail to appreciate the importance of the context in which data are collected. Conclusions: This paper, together with other similar papers, confirms that there is a large body of research related to software metrics. However, software metrics researchers may need to refine their empirical methodology before they can answer useful empirical questions. © 2009 Elsevier Inc. All rights reserved.",Empirical evaluation problems; Influential papers; Literature survey; Mapping study; Secondary study; Software metrics,Kitchenham B.,2010,Journal,Journal of Systems and Software,10.1016/j.jss.2009.06.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71949127456&doi=10.1016%2fj.jss.2009.06.041&partnerID=40&md5=cb4755c420488e7811ab51ea293f65a1,"Dept. Computer Science and Mathematics, Keele University, Staffordshire ST05, United Kingdom",,English,01641212,
Scopus,"Software metrics: successes, failures and new directions","The history of software metrics is almost as old as the history of software engineering. Yet, the extensive research and literature on the subject has had little impact on industrial practice. This is worrying given that the major rationale for using metrics is to improve the software engineering decision making process from a managerial and technical perspective. Industrial metrics activity is invariably based around metrics that have been around for nearly 30 years (notably Lines of Code or similar size counts, and defects counts). While such metrics can be considered as massively successful given their popularity, their limitations are well known, and mis-applications are still common. The major problem is in using such metrics in isolation. We argue that it is possible to provide genuinely improved management decision support systems based on such simplistic metrics, but only by adopting a less isolationist approach. Specifically, we feel it is important to explicitly model: (a) cause and effect relationships and (b) uncertainty and combination of evidence. Our approach uses Bayesian Belief nets, which are increasingly seen as the best means of handling decision-making under uncertainty. The approach is already having an impact in Europe.",,"Fenton N.E., Neil M.",1999,Journal,Journal of Systems and Software,10.1016/S0164-1212(99)00035-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032663370&doi=10.1016%2fS0164-1212%2899%2900035-7&partnerID=40&md5=115314fb8e038008e79829de7ff61d21,"Centre for Software Reliability, City Univ., Northampton Sq., EC1V O., London, United Kingdom","Elsevier Science Inc, New York",English,01641212,
Scopus,On the value of ensemble effort estimation,"Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9 × 10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n=13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods. © 2012 IEEE.",analogy; ensemble; k-NN; machine learning; neural nets; regression trees; Software cost estimation; support vector machines,"Kocaguneli E., Menzies T., Keung J.W.",2012,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2011.111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870566084&doi=10.1109%2fTSE.2011.111&partnerID=40&md5=11d35f485302f09031e1fd1ce30ca1d5,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506, United States; Department of Computing, Hong Kong Polytechnic University, Mong Man Wai Building, Hung Hom, Kowloon, Hong Kong, Hong Kong",,English,00985589,
Scopus,A systematic mapping study of software product lines testing,"Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. © 2010 Elsevier B.V. All rights reserved.",Mapping study; Software product lines; Software testing,"Da Mota Silveira Neto P.A., Carmo MacHado I.D., McGregor J.D., De Almeida E.S., De Lemos Meira S.R.",2011,Conference,Information and Software Technology,10.1016/j.infsof.2010.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952451558&doi=10.1016%2fj.infsof.2010.12.003&partnerID=40&md5=48146e279174abef9ae7749f41cec642,"RiSE - Reuse in Software Engineering, Recife PE, Brazil; Informatics Center, Federal University of Pernambuco, Recife, PE, Brazil; Computer Science Department, Federal University of Bahia, Salvador, BA, Brazil; Computer Science Department, Clemson University, Clemson, SC, United States",,English,09505849,
Scopus,A comparative study of cost estimation models for web hypermedia applications,"Software cost models and effort estimates help project managers allocate resources, control costs and schedule and improve current practices, leading to projects finished on time and within budget. In the context of Web development, these issues are also crucial, and very challenging given that Web projects have short schedules and very fluidic scope. In the context of Web engineering, few studies have compared the accuracy of different types of cost estimation techniques with emphasis placed on linear and stepwise regressions, and case-based reasoning (CBR). To date only one type of CBR technique has been employed in Web engineering. We believe results obtained from that study may have been biased, given that other CBR techniques can also be used for effort prediction. Consequently, the first objective of this study is to compare the prediction accuracy of three CBR techniques to estimate the effort to develop Web hypermedia applications and to choose the one with the best estimates. The second objective is to compare the prediction accuracy of the best CBR technique against two commonly used prediction models, namely stepwise regression and regression trees. One dataset was used in the estimation process and the results showed that the best predictions were obtained for stepwise regression.",Case-based reasoning techniques; Effort prediction models; Multiple regression models; Prediction accuracy; Web hypermedia applications,"Mendes E., Watson I., Triggs C., Mosley N., Counsell S.",2003,Journal,Empirical Software Engineering,10.1023/A:1023062629183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037950133&doi=10.1023%2fA%3a1023062629183&partnerID=40&md5=7eeb6ba1eec65ec706a0b2319a75b894,"Computer Science Department, The University of Auckland, Auckland, New Zealand; Statistics Department, The University of Auckland, Auckland, New Zealand; MxM Technology, Auckland, New Zealand; Computer Science Department, Birkbeck College, University of London, London, United Kingdom",,English,13823256,
Scopus,A better measure of relative prediction accuracy for model selection and model estimation,"Surveys show that the mean absolute percentage error (MAPE) is the most widely used measure of prediction accuracy in businesses and organizations. It is, however, biased: when used to select among competing prediction methods it systematically selects those whose predictions are too low. This has not been widely discussed and so is not generally known among practitioners. We explain why this happens. We investigate an alternative relative accuracy measure which avoids this bias: the log of the accuracy ratio, that is, log (prediction/actual). Relative accuracy is particularly relevant if the scatter in the data grows as the value of the variable grows (heteroscedasticity). We demonstrate using simulations that for heteroscedastic data (modelled by a multiplicative error factor) the proposed metric is far superior to MAPE for model selection. Another use for accuracy measures is in fitting parameters to prediction models. Minimum MAPE models do not predict a simple statistic and so theoretical analysis is limited. We prove that when the proposed metric is used instead, the resulting least squares regression model predicts the geometric mean. This important property allows its theoretical properties to be understood. © 2015 Operational Research Society Ltd. All rights reserved.",forecasting; loss function; model selection; prediction; regression; time series,Tofallis C.,2015,Journal,Journal of the Operational Research Society,10.1057/jors.2014.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937509634&doi=10.1057%2fjors.2014.103&partnerID=40&md5=e56e00a9dc4940b07c93aef01798f69c,"University of Hertfordshire, Hertfordshire Business School, College Lane, Hatfield, Herts, AL10 9AB, United Kingdom",Palgrave Macmillan Ltd.,English,01605682,
Scopus,Measuring knowledge worker productivity: A taxonomy,"The structure of the economy continues to change; where once they are dependent on the productivity of a manual workforce, companies increasingly depend on the productivity of knowledge workers. Today, knowledge workers account for more than two-thirds of the workforce, and thus should be the focus of strategic plans to improve productivity. Currently there are no universally accepted methods to measure knowledge worker productivity, or even generally accepted categories. This paper provides a taxonomy of knowledge worker productivity measurements, and identifies a number of productivity dimensions that are used to categorize the findings of previous research. Also describes the relative density of discussions along these dimensions and identifies critical areas for future research. © 2004, Emerald Group Publishing Limited",Employee productivity; Knowledge organizations; Performance measures; Quality,"Ramírez Y.W., Nembhard D.A.",2004,Journal,Journal of Intellectual Capital,10.1108/14691930410567040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986116693&doi=10.1108%2f14691930410567040&partnerID=40&md5=7a4f40cfbe7d1198d00c615340bbf94c,"The University of Wisconsin-Madison, Madison, Wisconsin, United States; Penn State University, University Park, Pennsylvania, United States",,English,14691930,
Scopus,Predicting object-oriented software maintainability using multivariate adaptive regression splines,"Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique. © 2006 Elsevier Inc. All rights reserved.",Maintainability; Multiple adaptive regression splines; Object-oriented; Prediction,"Zhou Y., Leung H.",2007,Journal,Journal of Systems and Software,10.1016/j.jss.2006.10.049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248571358&doi=10.1016%2fj.jss.2006.10.049&partnerID=40&md5=f61ff57ba040a3b4badf1b66506001d2,"Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, Hong Kong",,English,01641212,
Scopus,Software economics: A roadmap,"The fundamental goal of all good design and engineering is to create maximal value added for any given investment. There are many dimensions in which value can be assessed, from monetary profit to the solution of social problems. The benefits sought are often domain-specific, yet the logic is the same: design is an investment activity. Software economics is the field that seeks to enable significant improvements in software design and engineering through economic reasoning about product, process, program, and portfolio and policy issues. We summarize the state of the art and identify shortfalls in existing knowledge. Past work focuses largely on costs, not on benefits, thus not on value added; nor are current technical software design criteria linked clearly to value creation. We present a roadmap for research emphasizing the need for a strategic investment approach to software engineering. We discuss how software economics can lead to fundamental improvements in software design and engineering, in theory and practice. © ACM 2000.",,"Boehm B.W., Sullivan K.J.",2000,Conference,"Proceedings of the Conference on the Future of Software Engineering, ICSE 2000",10.1145/336512.336584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013589577&doi=10.1145%2f336512.336584&partnerID=40&md5=61328667f211b4e09f736e076f3a203d,"University of Southern California, Department of Computer Science, Los Angeles, CA  90089-0781, United States; University of Virginia, Thornton Hall, Department of Computer Science, Charlottesville, VA  22901, United States","Association for Computing Machinery, Inc",English,,1581132530; 9781581132533
Scopus,Using public domain metrics to estimate software development effort,"In this paper we investigate the accuracy of cost estimates when applying most commonly used modeling techniques to a large-scale industrial data set which is professionally maintained by the International Software Standards Benchmarking Group (ISBSG). The modeling techniques applied are ordinary least squares regression (OLS), Analogy-based estimation, stepwise ANOVA, CART, and robust regression. The questions we address in this study are related to important issues. The first is the appropriate selection of a technique in a given context. The second is the assessment of the feasibility of using multi-organizational data compared to the benefits from company-specific data collection. We compare company-specific models with models based on multi-company data. This is done by using the estimates derived for one company that contributed to the ISBSG data set and estimates from using carefully matched data from the rest of the ISBSG data. When using the ISBSG data set to derive estimates for the company generally poor results were obtained. Robust regression and OLS performed most accurately. When using the company's own data as the basis for estimation, OLS, a CART-variant, and Analogy performed best. In contrast to previous studies, the estimation accuracy when using the company's data is significantly higher than when using the rest of the ISBSG data set. Thus, from these results, the company that contributed to the ISBSG data set, would be better off when using its own data for cost estimation.",,"Jeffery R., Ruhe M., Wieczorek I.",2001,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035023813&partnerID=40&md5=8c7929cac6f9e744842d3fb6aa5aff18,"University of New South Wales, CAESAR, Sydney, NSW 2052, Australia",,English,,
Scopus,An empirical study of maintenance and development estimation accuracy,"We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company's standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63% of the estimates being within 25% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers. © 2002 Elsevier Science Inc. All rights reserved.",Development estimates; Duration; Effort; Estimation accuracy; Function points; Maintenance estimates,"Kitchenham B., Pfleeger S.L., McColl B., Eagan S.",2002,Journal,Journal of Systems and Software,10.1016/S0164-1212(02)00021-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037110428&doi=10.1016%2fS0164-1212%2802%2900021-3&partnerID=40&md5=363bb9687e047929aeafa00748d6f624,"Department of Computer Science, Keele University, Keele, Staffs ST5 5BG, United Kingdom; Systems/Software Inc., 4159, Davenport St. NW, Washington, DC 20016-4415, United States; Computer Sciences Corporation, Vergason Building, 100 Winnendon Road, Norwich, CT 06360, United States",Elsevier Inc.,English,01641212,
Scopus,Adoption of open source software in software-intensive organizations - A systematic literature review,"Context: Open source software (OSS) is changing the way organizations develop, acquire, use, and commercialize software. Objective: This paper seeks to identify how organizations adopt OSS, classify the literature according to these ways of adopting OSS, and with a focus on software development evaluate the research on adoption of OSS in organizations. Method: Based on the systematic literature review method we reviewed publications from 24 journals and seven conference and workshop proceedings, published between 1998 and 2008. From a population of 24,289 papers, we identified 112 papers that provide empirical evidence on how organizations actually adopt OSS. Results: We show that adopting OSS involves more than simply using OSS products. We moreover provide a classification framework consisting of six distinctly different ways in which organizations adopt OSS. This framework is used to illustrate some of the opportunities and challenges organizations meet when approaching OSS, to show that OSS can be adopted successfully in different ways, and to organize and review existing research. We find that existing research on OSS adoption does not sufficiently describe the context of the organizations studied, and it fails to benefit fully from related research fields. While existing research covers a large number of topics, it contains very few closely related studies. To aid this situation, we offer directions for future research. Conclusion: The implications of our findings are twofold. On the one hand, practitioners should embrace the many opportunities OSS offers, but consciously evaluate the consequences of adopting it in their own context. They may use our framework and the success stories provided by the literature in their own evaluations. On the other hand, researchers should align their work, and perform more empirical research on topics that are important to organizations. Our framework may be used to position this research and to describe the context of the organization they are studying. © 2010 Elsevier B.V. All rights reserved.",Open source software; Organizations; Software development; Systematic literature review,"Hauge Ø., Ayala C., Conradi R.",2010,Conference,Information and Software Technology,10.1016/j.infsof.2010.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956415573&doi=10.1016%2fj.infsof.2010.05.008&partnerID=40&md5=c162b9ab2e3754d5905cbace7a15b96f,"IDI, NTNU, Sem Sælands vei 7-9, NO-7491 Trondheim, Norway",,English,09505849,
Scopus,A probabilistic model for predicting software development effort,"Recently, Bayesian probabilistic models have been used for predicting software development effort. One of the reasons for the interest in the use of Bayesian probabilistic models, when compared to traditional point forecast estimation models, is that Bayesian models provide tools for risk estimation and allow decision-makers to combine historical data with subjective expert estimates. In this paper, we use a Bayesian network model and illustrate how a belief updating procedure can be used to incorporate decision-making risks. We develop a causal model from the literature and, using a data set of 33 real-world software projects, we illustrate how decision-making risks can be incorporated in the Bayesian networks. We compare the predictive performance of the Bayesian model with popular nonparametric neural-network and regression tree forecasting models and show that the Bayesian model is a competitive model for forecasting software development effort. © 2005 IEEE.",Bayesian belief networks; Probability theory; Software effort estimation,"Pendharkar P.C., Subramanian G.H., Rodger J.A.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.75,https://www.scopus.com/inward/record.uri?eid=2-s2.0-25844452838&doi=10.1109%2fTSE.2005.75&partnerID=40&md5=909b233428571ed784c5b659099de31d,"Information Systems, School of Business Administration, Pennsylvania State University, 777 W. Harrisburg Pike, Middletown, PA 17057, United States; MIS and Decision Sciences, Eberly College of Business and Information Technology, Indiana University of Pennsylvania, Indiana, PA 15705, United States",,English,00985589,
Scopus,Data mining techniques for software effort estimation: A comparative study,"A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained. © 1976-2012 IEEE.",Data mining; regression; software effort estimation,"Dejaeger K., Verbeke W., Martens D., Baesens B.",2012,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2011.55,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859746562&doi=10.1109%2fTSE.2011.55&partnerID=40&md5=71ee13accc59095a0237613047249de5,"Department of Decision Sciences and Information Management, Katholieke Universiteit Leuven, Naamsestraat 69, Leuven B-3000, Belgium; Faculty of Applied Economics, University of Antwerp, Prinsstraat 13, Antwerp B-2000, Belgium; School of Management, University of Southampton, Highfield Southampton, SO17 1BJ, United Kingdom",,English,00985589,
Scopus,Comparison of artificial neural network and regression models for estimating software development effort,"Estimating the amount of effort required for developing an information system is an important project management concern. In recent years, a number of studies have used neural networks in various stages of software development. This study compares the prediction performance of multilayer perceptron and radial basis function neural networks to that of regression analysis. The results of the study indicate that when a combined third generation and fourth generation languages data set were used, the neural network produced improved performance over conventional regression analysis in terms of mean absolute percentage error. © 2002 Elsevier Science B.V. All rights reserved.",Artificial neural network; Regression models; Software development effort,Heiat A.,2002,Journal,Information and Software Technology,10.1016/S0950-5849(02)00128-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036888053&doi=10.1016%2fS0950-5849%2802%2900128-3&partnerID=40&md5=d25b9bd2e23e18b435d6823a75066e31,"Management Information Systems, Montana State University Billings, 1500 North 30th Street, Billings, MT 59101, United States",,English,09505849,
Scopus,Comparative study of two software development cost modeling techniques using multi-organizational and company-specific data,"This research examined the use of the International Software Benchmarking Standards Group (ISBSG) repository for estimating effort for software projects in an organization not involved in ISBSG. The study investigates two questions: (1) What are the differences in accuracy between ordinary least-squares (OLS) regression and Analogy-based estimation? (2) Is there a difference in accuracy between estimates derived from the multi-company ISBSG data and estimates derived from company-specific data? Regarding the first question, we found that OLS regression performed as well as Analogy-based estimation when using company-specific data for model building. Using multi-company data the OLS regression model provided significantly more accurate results than Analogy-based predictions. Addressing the second question, we found in general that models based on the company-specific data resulted in significantly more accurate estimates.",,"Jeffery R., Ruhe M., Wieczorek I.",2000,Journal,Information and Software Technology,10.1016/S0950-5849(00)00153-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034319879&doi=10.1016%2fS0950-5849%2800%2900153-1&partnerID=40&md5=86488c0314b34fd4b99ed5e35987c52f,"Centre for Advanced Empirical, Software Res. (CAESAR), Univ. New S., Sydney, NSW, Australia; Department of Computer Science, University of Kaiserslautern, 67663, Kaiserslautern, Germany; Fraunhofer Inst. Exp. Software E., 67661, Kaiserslautern, Germany","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,Expert judgement as an estimating method,This paper reviews some past research that may be relevant to the way that 'expert judgement' is used to produce estimates of software size and implementation effort. It then describes a survey of software estimators in one organization conducted via written questionnaires and some follow-up interviews. The survey paid particular attention to the information requirements of the estimators. A large proportion of the system development was found to involve the modification of existing systems and as a consequence a large part of the estimating process comprised the assessment of the impact of changes on existing code. The paper concludes by suggesting that some work could profitably be applied to attempting to support expert judgement rather than displacing it.,Software estimation; Software maintenance,Hughes R.T.,1996,Journal,Information and Software Technology,10.1016/0950-5849(95)01045-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029707174&doi=10.1016%2f0950-5849%2895%2901045-9&partnerID=40&md5=46e275d162240f0678fc4c357b71eae0,"Department of Computing, University of Brighton, Moulsecoomb, Brighton BN2 4GJ, United Kingdom",Elsevier,English,09505849,
Scopus,On the problem of the software cost function,"The question of finding a function for software cost estimation is a long-standing issue in the software engineering field. The results of other works have shown different patterns for the unknown function, which relates software size to project cost (effort). In this work, the research about this problem has been made by using the technique of Genetic Programming (GP) for exploring the possible cost functions. Both standard regression analysis and GP have been applied and compared on several data sets. However, regardless of the method, the basic size-effort relationship does not show satisfactory results, from the predictive point of view, across all data sets. One of the results of this work is that we have not found significant deviations from the linear model in the software cost functions. This result comes from the marginal cost analysis of the equations with best predictive values.",,Dolado J.J.,2001,Journal,Information and Software Technology,10.1016/S0950-5849(00)00137-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035151922&doi=10.1016%2fS0950-5849%2800%2900137-3&partnerID=40&md5=75562a8e5ff5accf319b1860f1de259c,"Facultad de Informática, Univ. Pais Vasco-Euskal Herriko U., Donostia, Spain","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,Empirical studies of quality models in object-oriented systems,"Measuring structural design properties of a software system, such as coupling, cohesion, or complexity, is a promising approach toward early quality assessments. To use such measurement effectively, quality models that quantitatively describe how these internal structural properties relate to relevant external system qualities such as reliability or maintainability are needed. This chapter's objective is to summarize, in a structured and detailed fashion, the empirical results reported so far with modeling external system quality based on structural design properties in object-oriented systems. We perform a critical review of existing work in order to identify lessons learned regarding the way these studies are performed and reported. Constructive guidelines for facilitating the work of future studies are also provided, thus facilitating the development of an empirical body of knowledge. © 2002 Elsevier B.V. All rights reserved.",,"Briand L.C., Wüst J.",2002,Book Chapter,Advances in Computers,10.1016/S0065-2458(02)80005-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957159740&doi=10.1016%2fS0065-2458%2802%2980005-5&partnerID=40&md5=1256de9aefbf034462dacc6007f42689,"Software Quality Engineering Laboratory Systems, Computer Engineering Carleton University, 1125 Colonel By Drive, Ottawa, K1S 5B6, Canada; Fraunhofer IESE, Sauerwiesen 6, 67661 Kaiserslautern, Germany",Academic Press Inc.,English,00652458,
Scopus,A controlled experiment to assess the benefits of estimating with analogy and regression models,"To have general validity, empirical results must converge. To be credible, an experimental science must understand the limitations and be able to explain the disagreements of empirical results. We describe an experiment to replicate previous studies which claim that estimation by analogy outperforms regression models. In the experiment, 68 experienced practitioners each estimated a project from a dataset of 48 industrial COTS projects. We applied two treatments, an analogy tool and a regression model, and we used the estimating performance when aided by the historical data as the control. We found that our results do not converge with previous results. The reason is that previous studies have used other datasets and partially different data analysis methods, and last but not least, the tools have been validated in isolation from the tool users. This implies that the results are sensitive to the experimental design: the characteristics of the dataset, the norms for removing outliers and other data points from the original dataset, the test metrics, significance levels, and the use of human subjects and their level of expertise. Thus, neither our results nor previous results are robust enough to claim any general validity. Index Terms - Software cost estimation, commercial off-the-shelf (COTS) software projects, multivariate regression analysis, estimation by analogy, human performance, controlled experiment, enterprise resource planning (ERP) systems. © 1999 IEEE.",,"Myrtveit I., Stensrud E.",1999,Journal,IEEE Transactions on Software Engineering,10.1109/32.799947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033161922&doi=10.1109%2f32.799947&partnerID=40&md5=8288ab403493f1454f99742e56eeb9bc,"Norwegian School of Management, PO Box 580, N-1301 Sandvika, Norway",Institute of Electrical and Electronics Engineers Inc.,English,00985589,
Scopus,Better cross company defect prediction,"How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning. © 2013 IEEE.",Cross company; Data mining; Defect prediction,"Peters F., Menzies T., Marcus A.",2013,Conference,IEEE International Working Conference on Mining Software Repositories,10.1109/MSR.2013.6624057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889018963&doi=10.1109%2fMSR.2013.6624057&partnerID=40&md5=07b1ea23eb68e688b7a6ea80b446fdf3,"Lane Department of CS and EE, West Virginia University, United States; Computer Science, Wayne State University, United States",,English,21601852,9781467329361
Scopus,A systematic literature review to identify and classify software requirement errors,"Most software quality research has focused on identifying faults (i.e., information is incorrectly recorded in an artifact). Because software still exhibits incorrect behavior, a different approach is needed. This paper presents a systematic literature review to develop taxonomy of errors (i.e., the sources of faults) that may occur during the requirements phase of software lifecycle. This taxonomy is designed to aid developers during the requirement inspection process and to improve overall software quality. The review identified 149 papers from the software engineering, psychology and human cognition literature that provide information about the sources of requirements faults. A major result of this paper is a categorization of the sources of faults into a formal taxonomy that provides a starting point for future research into error-based approaches to improving software quality. © 2009 Elsevier B.V. All rights reserved.",Human errors; Software quality; Systematic literature review,"Walia G.S., Carver J.C.",2009,Review,Information and Software Technology,10.1016/j.infsof.2009.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349095299&doi=10.1016%2fj.infsof.2009.01.004&partnerID=40&md5=f9fb1878478a71dbc97e629e17798ade,"Mississippi State University, Department of Computer Science and Engineering, 300 Butler Hall, Mississippi State, MS 39762, United States; University of Alabama, Computer Science, Box 870290, Tuscaloosa, AL 35487-0290, United States",,English,09505849,
Scopus,Local versus global lessons for defect prediction and effort estimation,"Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data. © 1976-2012 IEEE.",clustering; Data mining; defect prediction; effort estimation,"Menzies T., Butcher A., Cok D., Marcus A., Layman L., Shull F., Turhan B., Zimmermann T.",2013,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2012.83,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878556735&doi=10.1109%2fTSE.2012.83&partnerID=40&md5=d0f697127866c92f1ce3981dfd91b8a6,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, United States; Gramma Tech, Ithaca, NY, United States; Wayne State University, Detroit, MI, United States; Fraunhofer Center, University of Maryland, College Park, MD, United States; Department of Information Processing Science, University of Oulu, Oulu, Finland; Microsoft Research, Redmond, WA, United States",,English,00985589,
Scopus,An Investigation of machine learning based prediction systems,"Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by problems with explanatory value and configurability. For example, we found that considerable effort was required to configure the ANN and that this compared very unfavourably with the other techniques, particularly CBR and least squares regression (LSR). We suggest that further work be carried out, both to further explore interaction between the enduser and the prediction system, and also to facilitate configuration, particularly of ANNs.",,"Mair C., Kadoda G., Lefley M., Phalp K., Schofield C., Shepperd M., Webster S.",2000,Journal,Journal of Systems and Software,10.1016/S0164-1212(00)00005-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034224993&doi=10.1016%2fS0164-1212%2800%2900005-4&partnerID=40&md5=f619747a676921134f0fb7834e0c4ebf,"Empirical Software Engineering Research Group, Design, Engineering and Computing Department, Bournemouth University, P608, Poole House, Talbot Campus, Poole BH12 5BB, United Kingdom; Nortel, Canada",,English,01641212,
Scopus,A comparison of techniques for developing predictive models of software metrics,"The use of regression analysis to derive predictive equations for software metrics has recently been complemented by increasing numbers of studies using non-traditional methods, such as neural networks, fuzzy logic models, case-based reasoning systems, and regression trees. There has also been an increasing level of sophistication in the regression-based techniques used, including robust regression methods, factor analysis, and more effective validation procedures. This paper examines the implications of using these methods and provides some recommendations as to when they may be appropriate. A comparison of the various techniques is also made in terms of their modelling capabilities with specific reference to software metrics. © 1997 Elsevier Science B.V.",Analysis techniques; Metrics; Predictive models,"Gray A.R., MacDonell S.G.",1997,Journal,Information and Software Technology,10.1016/S0950-5849(96)00006-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031170604&doi=10.1016%2fS0950-5849%2896%2900006-7&partnerID=40&md5=a9db4b65f0208b5ebc6301163c3dd8cb,"Department of Information Science, University of Otago, P.O. Box 56, Dunedin, New Zealand",Elsevier,English,09505849,
Scopus,A flexible method for software effort estimation by analogy,"Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods? First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number N of analogies (similar objects) taken for adaptation and the threshold T for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of N and T likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods. © Springer Science+Business Media, LLC 2007.",Analogy-based effort estimation; Comparative analysis; Learning; Missing values; Non-quantitative attributes; Software development effort,"Li J., Ruhe G., Al-Emran A., Richter M.M.",2007,Journal,Empirical Software Engineering,10.1007/s10664-006-7552-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846691171&doi=10.1007%2fs10664-006-7552-4&partnerID=40&md5=6e93fe7c443cd2544538d89cccd287db,"Software Engineering Decision Support Laboratory, University of Calgary, Calgary, Alta. T2N1N4, Canada; TU Kaiserslautern, FB Informatik, P.O. Box 3049, 67653 Kaiserslautern, Germany",,English,13823256,
Scopus,An application of Bayesian network for predicting object-oriented software maintainability,"As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system. © 2005 Elsevier B.V. All rights reserved.",Bayesian network; Maintainability; Object-oriented systems; Regression; Regression tree,"Van Koten C., Gray A.R.",2006,Journal,Information and Software Technology,10.1016/j.infsof.2005.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-28844434630&doi=10.1016%2fj.infsof.2005.03.002&partnerID=40&md5=124b793da3411fdd2025bfd323115b9e,"Department of Information Science, University of Otago, P.O. Box 56, Dunedin 9001, New Zealand",,English,09505849,
Scopus,Reuse and productivity in integrated computer-aided software engineering: An empirical study,"Growing competition in the investment banking industry has given rise to increasing demand for high functionality software applications that can be developed in a short period of time. Yet delivering such applications creates a bottleneck in software development activities. This dilemma can be addressed when firms shift to development methods that emphasize software reusability. This article examines the productivity implications of object and repository-based integrated computer-aided software engineering (ICASE) software development in the context of a major investment bank's information systems strategy. The strategy emphasizes software reusability. Our empirical results, based on data from 20 projects that delivered software for the bank's New Trades Processing Architecture (NTPA), indicate an order of magnitude gain in software development productivity and the importance of reuse as a driver in realizing this result. In addition, results are presented on the extent of the learning that occurred over a two-year period after ICASE was introduced, and on the influence of the link between application characteristics and the ICASE tool set in achieving development performance. This work demonstrates the viability of the firm's IS strategy and offers new ideas for code reuse and software development productivity measurement that can be applied in development environments that emphasize reuse.",CASE; ICASE; Productivity measurement; Reuse; Software development; Software economics; Software engineering,"Banker R.D., Kauffman R.J.",1991,Journal,MIS Quarterly: Management Information Systems,10.2307/249649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53349163770&doi=10.2307%2f249649&partnerID=40&md5=f07e6c5bcf10697c0845f5e1f602b6c5,"Carlson School of Management, University of Minnesota, Minneapolis, MN 55455, United States; Stern School of Business, New York University, New York, NY 10006, United States",Management Information Systems Research Center,English,02767783,
Scopus,The effects of time constraints on test case prioritization: A series of controlled experiments,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost-effectiveness. © 2010 IEEE.",Bayesian networks; cost-benefits; empirical studies; Regression testing; test case prioritization,"Do H., Mirarab S., Tahvildari L., Rothermel G.",2010,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2010.58,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957752029&doi=10.1109%2fTSE.2010.58&partnerID=40&md5=a8b3ea4eb5d52b15851fc1dd14c0e8dc,"Department of Computer Science 2740, North Dakota State University, 258 IACC Bldg., 1320 Albrecht Blvd., Fargo, ND 58102, Canada; IBM, 409-3520 Crowley Drive, Vancouver, BC V5R 6G9, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Davis Center Room 2524, 200 University Ave. West, Waterloo, ON N2L 3G1, Canada; Department of Computer Science and Engineering, University of Nebraska Lincoln, Lincoln, NE 68588-0115, Canada",,English,00985589,
Scopus,Software Measurement and Estimation: A Practical Approach,"An effective, quantitative approach for estimating and managing software projects How many people do I need? When will the quality be good enough for commercial sale? Can this really be done in two weeks? Rather than relying on instinct, the authors of Software Measurement and Estimation offer a new, tested approach that includes the quantitative tools, data, and knowledge needed to make sound estimations. The text begins with the foundations of measurement, identifies the appropriate metrics, and then focuses on techniques and tools for estimating the effort needed to reach a given level of quality and performance for a software project. All the factors that impact estimations are thoroughly examined, giving you the tools needed to regularly adjust and improve your estimations to complete a project on time, within budget, and at an expected level of quality. This text includes several features that have proven to be successful in making the material accessible and easy to master: * Simple, straightforward style and logical presentation and organization enables you to build a solid foundation of theory and techniques to tackle complex estimations * Examples, provided throughout the text, illustrate how to use theory to solve real-world problems * Projects, included in each chapter, enable you to apply your newfound knowledge and skills * Techniques for effective communication of quantitative data help you convey your findings and recommendations to peers and management Software Measurement and Estimation: A Practical Approach allows practicing software engineers and managers to better estimate, manage, and effectively communicate the plans and progress of their software projects. With its classroom-tested features, this is an excellent textbook for advanced undergraduate-level and graduate students in computer science and software engineering. An Instructor Support FTP site is available from the Wiley editorial department. © 2006 IEEE Computer Society.",,"Laird L.M., Brennan M.C.",2006,Book,Software Measurement and Estimation: A Practical Approach,10.1002/0471792535,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889466345&doi=10.1002%2f0471792535&partnerID=40&md5=9d4e05c825f2c3db3d1c4e46825bd266,,John Wiley and Sons,English,,0471676225; 9780471676225
Scopus,Implications of ceiling effects in defect predictors,"Context: There are many methods that input static code features and output a predictor for faulty code modules. These data mining methods have hit a ""performance ceiling""; i.e., some inherent upper bound on the amount of information offered by, say, static code features when identifying modules which contain faults. Objective: We seek an explanation for this ceiling effect. Perhaps static code features have ""limited information content""; i.e. their information can be quickly and completely discovered by even simple learners. Method: An initial literature review documents the ceiling effect in other work. Next, using three sub-sampling techniques (under-, over-, and micro-sampling), we look for the lower useful bound on the number of training instances. Results: Using micro-sampling, we find that as few as 50 instances yield as much information as larger training sets. Conclusions: We have found much evidence for the limited information hypothesis. Further progress in learning defect predictors may not come from better algorithms. Rather, we need to be improving the information content of the training data, perhaps with case-based reasoning methods. Copyright 2008 ACM.",Defect prediction; Naive bayes; Over-sampling; Under-sampling,"Menzies T., Turhan B., Bener A., Gay G., Cukic B., Jiang Y.",2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370788.1370801,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049155106&doi=10.1145%2f1370788.1370801&partnerID=40&md5=7517ab04d0abc5eee79d3c4dc2b1f494,"Dept. of CS and EE, West Virginia University, Morgantown, WV, United States; Dept. of Computer Engineering, Bogazici University, Turkey",,English,02705257,9781605580364
Scopus,A realistic empirical evaluation of the costs and benefits of UML in software maintenance,"The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54% increase in the functional correctness of changes (p=0.03), and an insignificant 7% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56%) - at the expense of an insignificant 14% increase in development time caused by the overhead of updating the UML documentation (p=0.35). © 2008 IEEE.",Empirical software engineering; Modeling; Object-oriented programming; Quasiexperiment; Software maintainability; UML,"Dzidek W.J., Arisholm E., Briand L.C.",2008,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2008.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-45449102151&doi=10.1109%2fTSE.2008.15&partnerID=40&md5=d1f3ec4139f73eeed9a6dc8d254f945f,"Simula Research Laboratory, Department of Software Engineering, PO Box 134, N-1325 Lysaker, Norway; Department of Informatics, University of Oslo, PO Box 1080, Blindern N-0316 Oslo, Norway",,English,00985589,
Scopus,A validation of the component-based method for software size estimation,"Estimation of software size is a crucial activity among the tasks of software management. Work planning and subsequent estimations of the effort required are made based on the estimate of the size of the software product. Software size can be measured in several ways: Lines of code (LOG) is a common measure and is usually one of the independent variables in equations for estimating effort. There are several methods for estimating the final LOG count of a software system in the early stages. In this article, we report the results of the validation of the component-based method (initially proposed by Verner and Täte) for software sizing. This was done through the analysis of 46 projects involving more than 100,000 LOG of a fourth-generation language. We present several conclusions concerning the predictive capabilities of the method. We observed that the component-based method behaves reasonably, although not as well as expected for ""global"" methods such as Mark II function points for software size prediction. The main factor observed that affects the performance is the type of component. © 2000 IEEE.",Function points; Genetic programming; Linear regression; Neural networks; Software management; Software measurement; Software size estimation,Dolado J.J.,2000,Journal,IEEE Transactions on Software Engineering,10.1109/32.879821,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034291123&doi=10.1109%2f32.879821&partnerID=40&md5=d32335b0bdb65ca08ac2f9074a0b16f7,"Facultad de Informdtica, Universidad Del Pat's Vasco, P. M. Lardizdbal 1, 20.009 - San Sebastian, Spain",,English,00985589,
Scopus,Classifying web metrics using the web quality model,"Purpose - The purpose of this paper is to classify the most important metrics proposed for web information systems, with the aim of offering the user a global vision of the state of the research within this area. Design/methodology/approach - WQM distinguishes three dimensions related to web features, lifecycle processes and quality characteristics. A range of recently published (1992-2004) works that include web metrics definitions have been studied and classified within this model. Findings - In this work, a global vision of web metrics is provided. Concretely, it was found that about 44 percent of metrics are related to ""presentation"" and that most metrics (48 percent) are usability metrics. Regarding the life cycle, the majority of metrics are related to operation and maintenance processes. Nevertheless, focusing on metrics validation, it was found that there is not too much work done, with only 3 percent of metrics validated theoretically and 37 percent of metrics validated empirically. Practical implications - The classification presented tries to facilitate the use and application of web metrics for different kinds of stakeholders (developers, maintainers, etc.) as well as to clarify where web metric definition efforts are centred, and thus where it is necessary to focus future works. Originality/value - This work tries to cover a deficiency in the web metrics field, where many proposals have been stated but without any kind of rigour and order. Consequently, the application of the proposed metrics is difficult and risky, and it is dangerous to base decisions on their values. © Emerald Group Publishing Limited.",Measurement; Quality; Worldwide web,"Calero C., Ruiz J., Piattini M.",2005,Journal,Online Information Review,10.1108/14684520510607560,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344442821&doi=10.1108%2f14684520510607560&partnerID=40&md5=7d89865629cea8b7aab846f2efb75e02,"ALARCOS Research Group, Computer Science Department, University of Castilla-La Mancha, Ciudad Real, Spain",,English,14684527,
Scopus,An evaluation of k-nearest neighbour imputation using lIkert data,"Studies in many different fields of research suffer from the problem of missing data. With missing data, statistical tests will lose power, results may be biased, or analysis may not be feasible at all. There are several ways to handle the problem, for example through imputation. With imputation, missing values are replaced with estimated values according to an imputation method or model. In the kNearest Neighbour (k-NN) method, a case is imputed using values from the k most similar cases. In this paper, we present an evaluation of the k-NN method using Likert data in a software engineering context. We simulate the method with different values of k and for different percentages of missing data. Our findings indicate that it is feasible to use the k-NN method with Likert data. We suggest that a suitable value of k is approximately the square root of the number of complete cases. We also show that by relaxing the method rules with respect to selecting neighbours, the ability of the method remains high for large amounts of missing data without affecting the quality of the imputation. © 2004 IEEE.",,"Jönsson P., Wohlin C.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357895,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844321601&doi=10.1109%2fMETRIC.2004.1357895&partnerID=40&md5=b6af1d692d0d5799c7aa57caed284bc1,"School of Engineering, Blekinge Institute of Technology, PO-Box 520, SE-372 25, Ronneby, Sweden",,English,15301435,0769521290
Scopus,An exploration of enterprise architecture research,"Management of the enterprise architecture has become increasingly recognized as a crucial part of both business and IT management. Still, a common understanding and methodological consistency seems far from being developed. Acknowledging the significant role of research in moving the development process along, this article employs different bibliometric methods, complemented by an extensive qualitative interpretation of the research field, to provide a unique overview of the enterprise architecture literature. After answering our research questions about the collaboration via co-authorships, the intellectual structure of the research field and its most influential works, and the principal themes of research, we propose an agenda for future research based on the findings from the above analyses and their comparison to empirical insights from the literature. In particular, our study finds a considerable degree of co-authorship clustering and a positive impact of the extent of co-authorship on the diffusion of works on enterprise architecture. In addition, this article identifies three major research streams and shows that research to date has revolved around specific themes, while some of high practical relevance receive minor attention. Hence, the contribution of our study is manifold and offers support for researchers and practitioners alike. © 2013 by the Association for Information Systems.",Enterprise architecture; Information technology management; Scientometrics; Survey,"Simon D., Fischbach K., Schoder D.",2013,Journal,Communications of the Association for Information Systems,10.17705/1cais.03201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877089831&doi=10.17705%2f1cais.03201&partnerID=40&md5=ece3f2d4c79eb8bd91556450ad1a38f2,"Department of Information Systems and Information Management, University of Cologne, Germany; University of Bamberg, Germany",Association for Information Systems,English,15293181,
Scopus,Further comparison of cross-company and within-company effort estimation models for Web applications,"This paper extends a previous study, using data on 67 Web projects from the Tukutuku database, investigating to what extent a cross-company cost model can be successfully employed to estimate effort for projects that belong to a single company, where no projects from this company were used to build the cross-company model. Our within-company model employed data on 14 Web projects from a single Web company. Our results were similar to those from the previous study, showing that predictions based on the within-company model were significantly more accurate than those based on the cross-company model. We also found that predictions were very poor when the within-company cost model was used to estimate effort for 53 Web projects from different companies. We analysed the data using two techniques, forward stepwise regression and case-based reasoning. We found estimates produced using stepwise regression models were better for the within company model while case-based reasoning predictions were better for the cross-company model. © 2004 IEEE.",Case-based reasoning; Cross-company estimation models; Effort estimation; Regression-based estimation models; Replication study; Web projects; Within-company estimation model,"Mendes E., Kitchenham B.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844320530&doi=10.1109%2fMETRIC.2004.1357920&partnerID=40&md5=21ac53c036518230cf33aa00cee788f0,"Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand; Department of Computer Science, Keele University, Staffordshire ST5 5GB, United Kingdom; National ICT Australia, Locked Bag 9013, Alexandria, NSW 1435, Australia",,English,15301435,0769521290
Scopus,Software Metrics and Software Metrology,"Most of the software measures currently proposed to the industry bring few real benefits to either software managers or developers. This book looks at the classical metrology concepts from science and engineering, using them as criteria to propose an approach to analyze the design of current software measures and then design new software measures (illustrated with the design of a software measure that has been adopted as an ISO measurement standard). The book includes several case studies analyzing strengths and weaknesses of some of the software measures most often quoted. It is meant for software quality specialists and process improvement analysts and managers. © 2010 IEEE Computer Society.",,Abran A.,2010,Book,Software Metrics and Software Metrology,10.1002/9780470606834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955340093&doi=10.1002%2f9780470606834&partnerID=40&md5=50357df635bfa652081a27a3e8db775a,"École de Technologie Supérieure (ETS), Université du Québec, Montréal, Canada",John Wiley and Sons,English,,9780470597200
Scopus,"Quality, productivity and economic benefits of software reuse: A review of industrial studies","Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges. © 2007 Springer Science+Business Media, LLC.",Evidence; Productivity; Quality; Review; Software reuse,"Mohagheghi P., Conradi R.",2007,Journal,Empirical Software Engineering,10.1007/s10664-007-9040-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748858031&doi=10.1007%2fs10664-007-9040-x&partnerID=40&md5=2adcaf8b331cfb2c91a7fb2e58c551ab,"SINTEF, ICT, Blindern P.O. BOX 124, Oslo 0314, Norway; Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim 7491, Norway",,English,13823256,
Scopus,Evolutionary optimization of software quality modeling with multiple repositories,"A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling. © 2010 IEEE.",defects; Genetic programming; machine learning; optimization; software measurement; software quality,"Liu Y., Khoshgoftaar T.M., Seliya N.",2010,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2010.51,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649782445&doi=10.1109%2fTSE.2010.51&partnerID=40&md5=80b03126db5d405ba12fc6a183704e4b,"J. Whitney Bunting School of Business, Georgia College, State University, 231 W. Hancock St, Milledgeville, GA 31061, United States; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, 777 Glades Road, Boca Raton, FL 33431, United States; Computer and Information Science Department, University of MichiganDearborn, 4901 Evergreen Rd, Dearborn, MI 48128, United States",,English,00985589,
Scopus,A model for software development effort and cost estimation,"Several algorithmic models have been proposed to estimate software costs and other management parameters. Early prediction of completion time is absolutely essential for proper advance planning and aversion of the possible ruin of a project. Putnam's SLIM model offers a fairly reliable method that is used extensively to predict project completion times and manpower requirements as the project evolves. However, the nature of the Norden/Rayleigh curve used by Putnam, renders it unreliable during the initial phases of the project, especially in projects involving a fast manpower buildup, as is the case with most software projects. In this paper, we propose the use of a model that improves early prediction considerably over the Putnam model. An analytic proof of the model's improved performance is also demonstrated on simulated data. © 1997 IEEE.",Completion time; Development time; Early prediction; Gamma model; Manpower; Norden/rayleigh model,"Pillai K., Sukumaran Nair V.S.",1997,Journal,IEEE Transactions on Software Engineering,10.1109/32.624305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001657482&doi=10.1109%2f32.624305&partnerID=40&md5=749d7e7db7c22f74f2d82cf4f9938387,"Department of Computer Science and Engineering, Southern Methodist University, Dallas, TX 75275, United States",,English,00985589,
Scopus,Modeling software measurement data,"This paper proposes a method for specifying models Of software data sets in order to capture the definitions and relationships among software measures. We believe a method of defining software data sets is necessary to ensure that software data are trustworthy. Software companies introducing a measurement program need to establish procedures to collect and store trustworthy measurement data. Without appropriate definitions it is difficult to ensure data values are repeatable and comparable. Software metrics researchers need to maintain collections of software data sets. Such collections allow researchers to assess the generality of software engineering phenomena. Without appropriate safeguards, it is difficult to ensure that data from different sources are analyzed correctly. These issues imply the need for a standard method of specifying software data sets so they are fully documented and can be exchanged with confidence. We suggest our method of defining data sets can be used as such a standard. We present our proposed method in terms of a conceptual Entity-Relationship data model that allows complex software data sets to be modeled and their data values stored. The standard can, therefore, contribute both to the definition of a company measurement program and to the exchange of data sets among researchers.",Data collection; Data set exchange; Data storage; Software measurements,"Kitchenham B.A., Hughes R.T., Linkman S.G.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.950316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035441737&doi=10.1109%2f32.950316&partnerID=40&md5=0541967a12ccc92e1691b6496e17f4d1,"IEEE Computer Society, United Kingdom; Department of Computer Science, Keele University, Staffordshire ST5 5BG, United Kingdom; School of Information Management, University of Brighton, Watts Building, Moulsecoomb, Brighton BN2 4GJ, United Kingdom",,English,00985589,
Scopus,Software development cost estimation using wavelet neural networks,"Software development has become an essential investment for many organizations. Software engineering practitioners have become more and more concerned about accurately predicting the cost and quality of software product under development. Accurate estimates are desired but no model has proved to be successful at effectively and consistently predicting software development cost. In this paper, we propose the use of wavelet neural network (WNN) to forecast the software development effort. We used two types of WNN with Morlet function and Gaussian function as transfer function and also proposed threshold acceptance training algorithm for wavelet neural network (TAWNN). The effectiveness of the WNN variants is compared with other techniques such as multilayer perceptron (MLP), radial basis function network (RBFN), multiple linear regression (MLR), dynamic evolving neuro-fuzzy inference system (DENFIS) and support vector machine (SVM) in terms of the error measure which is mean magnitude relative error (MMRE) obtained on Canadian financial (CF) dataset and IBM data processing services (IBMDPS) dataset. Based on the experiments conducted, it is observed that the WNN-Morlet for CF dataset and WNN-Gaussian for IBMDPS outperformed all the other techniques. Also, TAWNN outperformed all other techniques except WNN. © 2008 Elsevier Inc. All rights reserved.",Software cost estimation; Software development effort; Threshold accepting based wavelet neural network; Wavelet neural networks,"Vinay Kumar K., Ravi V., Carr M., Raj Kiran N.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2007.12.793,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52049116923&doi=10.1016%2fj.jss.2007.12.793&partnerID=40&md5=5be776d64d38b6b802b10711c41100fe,"Institute for Development and Research in Banking Technology, Castle, Hills Road #1, Masab Tank, Hyderabad, 500 057 AP, India",,English,01641212,
Scopus,Making resource decisions for software projects,"Software metrics should support managerial decision making in software projects. We explain how traditional metrics approaches, such as regression-based models for cost estimation fall short of this goal. Instead, we describe a causal model (using a Bayesian network) which incorporates empirical data, but allows it to be interpreted and supplemented using expert judgement. We show how this causal model is used in a practical decision-support tool, allowing a project manager to trade-off the resources used against the outputs (delivered functionality, quality achieved) in a software project. The model and toolset have evolved in a number of collaborative projects and hence capture significant commercial input. Extensive validation trials are taking place among partners on the EC funded project MODIST (this includes Philips, Israel Aircraft Industries and QinetiQ) and the feedback so far has been very good. The estimates are sensible and the causal modelling approach enables decision-makers to reason in a way that is not possible with other project management and resource estimation tools. To ensure wide dissemination and validation a version of the toolset with the full underlying model is being made available for free to researchers.",,"Fenton N., Marsh W., Neil M., Cates P., Forey S., Tailor M.",2004,Conference,Proceedings - International Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544298446&partnerID=40&md5=25d35d4a4b03e7e10a08351e52ddb80a,"Department of Computer Science, Queen Mary, University of London, Agena Ltd., United Kingdom",,English,02705257,
Scopus,An Empirical Validation of Object-Oriented Metrics in Two Different Iterative Software Processes,"Object-oriented (OO) metrics are used mainly to predict software engineering activities/efforts such as maintenance effort, error proneness, and error rate. There have been discussions about the effectiveness of metrics in different contexts. In this paper, we present an empirical study of OO metrics in two iterative processes: the short-cycled agile process and the long-cycled framework evolution process. We find that OO metrics are effective in predicting design efforts and source lines of code added, changed, and deleted in the short-cycled agile process and ineffective in predicting the same aspects in the long-cycled framework process. This leads us to believe that OO metrics' predictive capability is limited to the design and implementation changes during the development iterations, not the long-term evolution of an established system in different releases.",Agile process; Empirical validation; Framework evolution; Software metrics,"Alshayeb M., Li W.",2003,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2003.1245305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346896353&doi=10.1109%2fTSE.2003.1245305&partnerID=40&md5=851a923b8b81d1d83fceeb9e22d460a8,"Computer Science Department, University of Alabama in Huntsville, Huntsville, AL 35899, United States",,English,00985589,
Scopus,Towards an early software estimation using log-linear regression and a multilayer perceptron model,"Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects. © 2012 Elsevier Inc. All rights reserved.",Log-linear regression model; Multilayer perceptron; Software effort estimation; Use case points,"Nassif A.B., Ho D., Capretz L.F.",2013,Journal,Journal of Systems and Software,10.1016/j.jss.2012.07.050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868662770&doi=10.1016%2fj.jss.2012.07.050&partnerID=40&md5=4d9202728fec01473f7608c4c9814c03,"Department of ECE, Western University, London, ON, Canada; NFA Estimation Inc., Richmond Hill, ON, Canada",,English,01641212,
Scopus,Simplifying effort estimation based on Use Case Points,"Context: The Use Case Points (UCP) method can be used to estimate software development effort based on a use-case model and two sets of adjustment factors relating to the environmental and technical complexity of a project. The question arises whether all of these components are important from the effort estimation point of view. Objective: This paper investigates the construction of UCP in order to find possible ways of simplifying it. Method: The cross-validation procedure was used to compare the accuracy of the different variants of UCP (with and without the investigated simplifications). The analysis was based on data derived from a set of 14 projects for which effort ranged from 277 to 3593 man-hours. In addition, the factor analysis was performed to investigate the possibility of reducing the number of adjustment factors. Results: The two variants of UCP - with and without unadjusted actor weights (UAW) provided similar prediction accuracy. In addition, a minor influence of the adjustment factors on the accuracy of UCP was observed. The results of the factor analysis indicated that the number of adjustment factors could be reduced from 21 to 6 (2 environmental factors and 4 technical complexity factors). Another observation was made that the variants of UCP calculated based on steps were slightly more accurate than the variants calculated based on transactions. Finally, a recently proposed use-case-based size metric TTPoints provided better accuracy than any of the investigated variants of UCP. Conclusion: The observation in this study was that the UCP method could be simplified by rejecting UAW; calculating UCP based on steps instead of transactions; or just counting the total number of steps in use cases. Moreover, two recently proposed use-case-based size metrics Transactions and TTPoints could be used as an alternative to UCP to estimate effort at the early stages of software development. © 2010 Elsevier B.V. All rights reserved.",Software cost estimation; TTPoints; Use Case Points; Use cases; Use-case transactions,"Ochodek M., Nawrocki J., Kwarciak K.",2011,Journal,Information and Software Technology,10.1016/j.infsof.2010.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251644866&doi=10.1016%2fj.infsof.2010.10.005&partnerID=40&md5=919c46b8b297b0b9ed390896036b28fd,"Poznan University of Technology, Institute of Computing Science, ul. Piotrowo 2, 60-965 Poznań, Poland",,English,09505849,
Scopus,On the effectiveness of early life cycle defect prediction with Bayesian nets,"Standard practice in building models in software engineering normally involves three steps: collecting domain knowledge (previous results, expert knowledge); building a skeleton of the model based on step 1 including as yet unknown parameters; estimating the model parameters using historical data. Our experience shows that it is extremely difficult to obtain reliable data of the required granularity, or of the required volume with which we could later generalize our conclusions. Therefore, in searching for a method for building a model we cannot consider methods requiring large volumes of data. This paper discusses an experiment to develop a causal model (Bayesian net) for predicting the number of residual defects that are likely to be found during independent testing or operational usage. The approach supports (1) and (2), does not require (3), yet still makes accurate defect predictions (an R 2 of 0.93 between predicted and actual defects). Since our method does not require detailed domain knowledge it can be applied very early in the process life cycle. The model incorporates a set of quantitative and qualitative factors describing a project and its development process, which are inputs to the model. The model variables, as well as the relationships between them, were identified as part of a major collaborative project. A dataset, elicited from 31 completed software projects in the consumer electronics industry, was gathered using a questionnaire distributed to managers of recent projects. We used this dataset to validate the model by analyzing several popular evaluation measures (R 2, measures based on the relative error and Pred). The validation results also confirm the need for using the qualitative factors in the model. The dataset may be of interest to other researchers evaluating models with similar aims. Based on some typical scenarios we demonstrate how the model can be used for better decision support in operational environments. We also performed sensitivity analysis in which we identified the most influential variables on the number of residual defects. This showed that the project size, scale of distributed communication and the project complexity cause the most of variation in number of defects in our model. We make both the dataset and causal model available for research use. © 2008 Springer Science+Business Media, LLC.",Bayesian network; Decision support; Qualitative factors; Quantitative data; Sensitivity analysis; Software defect prediction,"Fenton N., Neil M., Marsh W., Hearty P., Radliński Ł., Krause P.",2008,Journal,Empirical Software Engineering,10.1007/s10664-008-9072-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52549100186&doi=10.1007%2fs10664-008-9072-x&partnerID=40&md5=c4f612b40bc920e9b5aed3c396a2f261,"Department of Computer Science, Queen Mary, University of London, Mile End Road, London, United Kingdom; Institute of Information Technology in Management, University of Szczecin, Szczecin, Poland; Department of Computing, University of Surrey, Guildford, Surrey, United Kingdom",,English,13823256,
Scopus,Cost estimation for web applications,"In this paper, we investigate the application of the COBRA™ method (Cost Estimation, Benchmarking, and Risk Assessment) in a new application domain, the area of web development. COBRA combines expert knowledge with data on a small number of projects to develop cost estimation models, which can also be used for risk analysis and benchmarking purposes. We modified and applied the method to the web applications of a small Australian company, specializing in web development. In this paper we present the modifications made to the COBRA method and results of applying the method. In our study, using data on twelve web applications, the estimates derived from our Web-COBRA model showed a Mean Magnitude of Relative Error (MMRE) of 0.17. This result significantly outperformed expert estimates from Allette Systems (MMRE 0.37). A result comparable to Web-COBRA was obtained when applying ordinary least squares regression with size in terms of Web Objects as an independent variable (MMRE 0.23).",,"Ruhe M., Jeffery R., Wieczorek I.",2003,Conference,Proceedings - International Conference on Software Engineering,10.1109/icse.2003.1201208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037587476&doi=10.1109%2ficse.2003.1201208&partnerID=40&md5=4bc238eb4170e6a4cfb6e64e8bcdb4fd,"Siemens AG, Corporate Technology, Software Engineering, 80730 Munich, Germany; University of New South Wales, School of Computer Science and Eng., CAESER, Sydney, NSW 2052, Australia; Federal Ministry of Educ. and Res., 53170 Bonn, Germany",IEEE Computer Society,English,02705257,
Scopus,Modeling development effort in object-oriented systems using design properties,"In the context of software cost estimation, system size is widely taken as a main driver of system development effort. But, other structural design properties, such as coupling, cohesion, and complexity, have been suggested as additional cost factors. In this paper, using effort data from an object-oriented development project, we empirically investigate the relationship between class size and the development effort for a class and what additional impact structural properties such as class coupling have on effort. This paper proposes a practical, repeatable, and accurate analysis procedure to investigate relationships between structural properties and development effort. This is particularly important as it is necessary, as for any empirical study, to be able to replicate the analysis reported here. More specifically, we use Poisson regression and regression trees to build cost prediction models from size and design measures and use these models to predict system development effort. We also investigate a recently suggested technique to combine regression trees with regression analysis which aims at building more accurate models. Results indicate that fairly accurate predictions of class effort can be made based on simple measures of the class interface size alone (mean MREs below 30 percent). Effort predictions at the system level are even more accurate as, using Bootstrapping, the estimated 95 percent confidence interval for MREs is 3 to 23 percent. But, more sophisticated coupling and cohesion measures do not help to improve these predictions to a degree that would be practically significant. However, the use of hybrid models combining Poisson regression and CART regression trees clearly improves the accuracy of the models as compared to using Poisson regression alone.",Cost estimation; Empirical validation; Object-oriented measurement,"Briand L.C., Wüst J.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.965338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035506256&doi=10.1109%2f32.965338&partnerID=40&md5=8edd0cab6d7c573d8081e60219aae290,"IEEE, Canada; Systems and Computer Engineering Department, Carleton University, 1125 Colonel By Drive, Ottawa, Ont. K1S 5B6, Canada; Fraunhofer Institute for Experimental Software Engineering, Sauerwiesen 6, 67661 Kaiserslautern, Germany",,English,00985589,
Scopus,Effort estimation in Agile Software Development: A systematic literature review,"Context: Ever since the emergence of agile methodologies in 2001, many software companies have shifted to Agile Software Development (ASD), and since then many studies have been conducted to investigate effort estimation within such context; however to date there is no single study that presents a detailed overview of the state of the art in effort estimation for ASD. Objectives: The aim of this study is to provide a detailed overview of the state of the art in the area of effort estimation in ASD. Method: To report the state of the art, we conducted a systematic literature review in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: A total of 25 primary studies were selected; the main findings are: i) Subjective estimation techniques (e.g. expert judgment, planning poker, use case points estimation method) are the most frequently applied in an agile context; ii) Use case points and story points are the most frequently used size metrics respectively; iii) MMRE (Mean Magnitude of Relative Error) and MRE (Magnitude of Relative Error) are the most frequently used accuracy metrics; iv) team skills, prior experience and task size are cited as the three important cost drivers for effort estimation in ASD; and v) Extreme Programming (XP) and SCRUM are the only two agile methods that are identified in the primary studies. Conclusion: Subjective estimation techniques, e.g. expert judgment-based techniques, planning poker or the use case points method, are the one used the most in agile effort estimation studies. As for the size metrics, the ones that were used the most in the primary studies were story points and use case points. Several research gaps were identified, relating to the agile methods, size metrics and cost drivers, thus suggesting numerous possible avenues for future work. Copyright 2014 ACM.",Agile software development; Effort estimation; Systematic literature review,"Usman M., Mendes E., Weidt F., Britto R.",2014,Conference,ACM International Conference Proceeding Series,10.1145/2639490.2639503,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905666395&doi=10.1145%2f2639490.2639503&partnerID=40&md5=3a589a32a9893400f14a098a22a06b8a,"Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Federal University of Juiz de Fora, Brazil",Association for Computing Machinery,English,,9781450328982
Scopus,Software productivity and effort prediction with ordinal regression,"In the area of software cost estimation, various methods have been proposed to predict the effort or the productivity of a software project. Although most of the proposed methods produce point estimates, in practice it is more realistic and useful for a method to provide interval predictions. In this paper, we explore the possibility of using such a method, known as ordinal regression to model the probability of correctly classifying a new project to a cost category. The proposed method is applied to three data sets and is validated with respect to its fitting and predictive accuracy. © 2004 Elsevier B.V. All rights reserved.",Interval prediction; Ordinal regression; Software cost estimation,"Sentas P., Angelis L., Stamelos I., Bleris G.",2005,Journal,Information and Software Technology,10.1016/j.infsof.2004.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444251063&doi=10.1016%2fj.infsof.2004.05.001&partnerID=40&md5=9f8f41afe7e00b544ba380ab4fe87fc0,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,09505849,
Scopus,Characterizing software architecture changes: A systematic review,"With today's ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS's purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture. © 2009 Elsevier B.V. All rights reserved.",Change characterization; Software architecture; Software changes; Software evolution; Software maintenance; Systematic review,"Williams B.J., Carver J.C.",2010,Review,Information and Software Technology,10.1016/j.infsof.2009.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449084875&doi=10.1016%2fj.infsof.2009.07.002&partnerID=40&md5=2ccbb3bc7c592974f2549cb793e34892,"Department of Computer Science and Engineering, Mississippi State University, United States; Department of Computer Science, University of Alabama, 101 Houser Hall, Tuscaloosa, AL 35487-0290, United States",,English,09505849,
Scopus,Evidence on economies of scale in software development,"Researchers and practitioners have found it useful for cost estimation and productivity evaluation purposes to think of software development as an economic production process, whereby inputs, most notably the effort of systems development professionals, are converted into outputs (systems deliverables), often measured as the size of the delivered system. One central issue in developing such models is how to describe the production relationship between the inputs and outputs. In particular, there has been much discussion about the existence of either increasing or decreasing returns to scale. The presence or absence of scale economies at a given size are important to commercial practice in that they influence productivity. A project manager can use this knowledge to scale future projects so as to maximize the productivity of software development effort. The question of whether the software development production process should be modelled with a non-linear model is the subject of some recent controversy. This paper examines the issue of non-linearities through the analysis of 11 datasets using, in addition to standard parametric tests, new statistical tests with the non-parametric Data Envelopment Analysis (DEA) methodology. Results of this analysis support the hypothesis of significant non-linearities, and the existence of both economies and diseconomies of scale in software development. © 1994.",data envelopment analysis; function points; productivity measurement; returns to scale; scale economies; software development; software management; software metrics; source lines of code,"Banker R.D., Chang H., Kemerer C.F.",1994,Journal,Information and Software Technology,10.1016/0950-5849(94)90083-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001195181&doi=10.1016%2f0950-5849%2894%2990083-3&partnerID=40&md5=2d5324a258d2619a500a5d35074282b2,"University of Minnesota, United States; Massachusetts Institute of Technology, United States",,English,09505849,
Scopus,Estimation and prediction metrics for adaptive maintenance effort of object-oriented systems,"Many software systems built in recent years have been developed using object-oriented technology and, in some cases, they already need adaptive maintenance in order to satisfy market and customer needs. In most cases, the estimation and prediction of maintenance effort is performed with difficulty due to the lack of metrics and suitable models. In this paper, a model and metrics for estimation/prediction of adaptive maintenance effort are presented and compared with some other solutions taken from the literature. The model proposed can be used as a general approach for adopting well-known metrics (typically used for the estimation of development effort) for the estimation/prediction of adaptive maintenance effort. The model and metrics proposed have been validated against real data by using multilinear regression analysis. The validation has shown that several well-known metrics can be profitably employed for the estimation/prediction of maintenance effort.",Adaptive maintenance effort; Estimation/prediction; Object-oriented metrics,"Fioravanti F., Nesi P.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.988708,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035673534&doi=10.1109%2f32.988708&partnerID=40&md5=2944e4492f21c304322d08f0c07097e4,"IEEE, Italy; Department of Systems and Informatics, University of Florence, Via di S. Marta 3, 50139, Florence, Italy",,English,00985589,
Scopus,Analogy-based software development effort estimation: A systematic mapping and review,"Context: Analogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques. Objective: The objective of this research is twofold: (1) to classify ASEE studies which primary goal is to propose new or modified ASEE techniques according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools. Method: We performed a systematic mapping of studies for which the primary goal is to develop or to improve ASEE techniques published in the period 1990-2012, and reviewed them based on an automated search of four electronic databases. Results: In total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with Fuzzy Logic (FL) or Genetic Algorithms (GA). Conclusion: Based on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry. © 2014 Elsevier B.V. All rights reserved.",Analogy; Case-based reasoning; Mapping study; Software development effort estimation; Systematic literature review,"Idri A., Amazal F.A., Abran A.",2015,Review,Information and Software Technology,10.1016/j.infsof.2014.07.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027945931&doi=10.1016%2fj.infsof.2014.07.013&partnerID=40&md5=d03c7e583c0f8f4f05f0a478ea2ac866,"Software Projects Management Research Team, ENSIAS, Mohammed V Souissi University, Madinate Al Irfane, Rabat, 10100, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, Montréal, QC  H3C IK3, Canada",Elsevier B.V.,English,09505849,
Scopus,Effective software fault localization using an RBF neural network,"We propose the application of a modified radial basis function neural network in the context of software fault localization, to assist programmers in locating bugs effectively. This neural network is trained to learn the relationship between the statement coverage information of a test case and its corresponding execution result, success or failure. The trained network is then given as input a set of virtual test cases, each covering a single statement. The output of the network, for each virtual test case, is considered to be the suspiciousness of the corresponding covered statement. A statement with a higher suspiciousness has a higher likelihood of containing a bug, and thus statements can be ranked in descending order of their suspiciousness. The ranking can then be examined one by one, starting from the top, until a bug is located. Case studies on 15 different programs were conducted, and the results clearly show that our proposed technique is more effective than several other popular, state of the art fault localization techniques. Further studies investigate the robustness of the proposed technique, and illustrate how it can easily be applied to programs with multiple bugs as well. © 2006 IEEE.",Fault location; radial basis function neural networks; software debugging,"Wong W.E., Debroy V., Golden R., Xu X., Thuraisingham B.",2012,Journal,IEEE Transactions on Reliability,10.1109/TR.2011.2172031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858150805&doi=10.1109%2fTR.2011.2172031&partnerID=40&md5=52f3d477d6054ee09233aa47ef0edc15,"Department of Computer Science, University of Texas at Dallas, Dallas, TX 75247, United States; School of Behavioral and Brain Sciences, University of Texas at Dallas, Dallas, TX 75247, United States",,English,00189529,
Scopus,Why comparative effort prediction studies may be invalid,"Background: Many cost estimation papers are based on finding a ""new"" estimation method, trying out the method on one or two past datasets and ""proving"" that the new method is better than linear regression. Aim: This paper aims to explain why this approach to model comparison is often invalid and to suggest that the PROMISE repository may be making things worse. Method: We identify some of the theoretical problems with studies that compare different estimation models. We review some of the commonly used datasets from the viewpoint of the reliability of the data and the validity of the proposed linear regression models. Discussion points: It is invalid to select one or two datasets to ""prove"" the validity of a new technique because we cannot be sure that, of the many published datasets, those chosen are the only ones that favour the new technique. When new models are compared with regression models, researchers need to understand how to use regression analysis appropriately. The use of linear regression presupposes: a linear relationship between dependent and independent variables, no significant outliers, no significant skewness, no relationship between the variance of the dependent variable and the magnitude of the variable. If all these conditions are not true, standard statistical practice is to use a robust regression or transform the data. The logarithmic transformation is appropriate in many cases, and for the Desharnais dataset gives better results than the regression model presented in the PROMISE repository. Conclusions: Simplistic studies comparing data intensive methods with linear regression will be scientifically valueless, if the regression techniques are applied incorrectly. They are also suspect if only a small number of datasets are used and the selection of those datasets is not scientifically justified. © ACM 2009.",effort estimation; linear regression; model comparison; model construction,"Kitchenham B., Mendes E.",2009,Conference,ACM International Conference Proceeding Series,10.1145/1540438.1540444,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953739200&doi=10.1145%2f1540438.1540444&partnerID=40&md5=c56b83f297af6646471830736ce4e129,"School of Computing and Mathematics, Keele University, Keele, Stoke on Trent, ST5 5BG, United Kingdom; Computer Science Department, University of Auckland, Auckland, New Zealand",,English,,9781605586342
Scopus,Analogy-X: Providing statistical inference to analogy-based software cost estimation,"Data-intensive analogy has been proposed as a means of software cost estimation as an alternative to other data intensive methods such as linear regression. Unfortunately, there are drawbacks to the method. There is no mechanism to assess its appropriateness for a specific dataset. In addition, heuristic algorithms are necessary to select the best set of variables and identify abnormal project cases. We introduce a solution to these problems based upon the use of the Mantel correlation randomization test called Analogy-X. We use the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. The method is demonstrated using the Desharnais dataset and two random datasets, showing (1) the use of Mantel's correlation to identify whether analogy is appropriate, (2) a stepwise procedure for feature selection, as well as (3) the use of a leverage statistic for sensitivity analysis that detects abnormal data points. Analogy-X, thus, provides a sound statistical basis for analogy, removes the need for heuristic search and greatly improves its algorithmic performance. © 2008 IEEE.",Analogy; Cost estimation; Management; Software engineering,"Keung J.W., Kitchenham B.A., Jeffery D.R.",2008,Conference,IEEE Transactions on Software Engineering,10.1109/TSE.2008.34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49349110547&doi=10.1109%2fTSE.2008.34&partnerID=40&md5=eadd876223d30426945ec0c70c9025dd,"ESE, NICTA, Garden St., Eveleigh, NSW 1430, Australia; Keele University, Keele, Staffordshire ST5 5BG, United Kingdom; School of Computer Science and Engineering, University of New South Wales, Sydney, NSW 2052, Australia",,English,00985589,
Scopus,Mining software repositories for comprehensible software fault prediction models,"Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models. © 2007 Elsevier Inc. All rights reserved.",Ant Colony Optimization; Classification; Comprehensibility; Fault prediction; Software mining,"Vandecruys O., Martens D., Baesens B., Mues C., De Backer M., Haesen R.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2007.07.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749154789&doi=10.1016%2fj.jss.2007.07.034&partnerID=40&md5=ccd1c2ccbf4ff2ee424e78c2fa10a258,"Department of Decision Sciences and Information Management, Naamsestraat 69, B-3000 Leuven, Belgium; School of Management, University of Southampton, Highfield, Southampton, SO17 1BJ, United Kingdom; Vlerick Management School, Reep 1, B-9000 Gent, Belgium",,English,01641212,
Scopus,Global Software Development Handbook,"Economics and technology have dramatically re-shaped the landscape of software development. It is no longer uncommon to find a software development team dispersed across countries or continents. Geographically distributed development challenges the ability to clearly communicate, enforce standards, ensure quality levels, and coordinate tasks. Global Software Development Handbook explores techniques that can bridge distances, create cohesion, promote quality, and strengthen lines of communication. The book introduces techniques proven successful at international electronics and software giant Siemens AG. It shows how this multinational uses a high-level process framework that balances agility and discipline for globally distributed software development. The authors delineate an organizational structure that not only fosters team building, but also achieves effective collaboration among the central and satellite teams. The handbook explores the issues surrounding quality and the processes required to realize quality in a distributed environment. Communication is a tremendous challenge, especially for teams separated by several time zones, and the authors elucidate how to uncover patterns of communication among these teams to determine effective strategies for managing communication. The authors analyze successful and failed projects and apply this information to how a project can be successful with distributed teams. They also provide lightweight processes that can be dynamically adapted to the demands of any project. © 2007 by Taylor & Francis Group, LLC.",,Sangwan R.,2006,Book,Global Software Development Handbook,10.1201/9781420013856,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349142978&doi=10.1201%2f9781420013856&partnerID=40&md5=5a43a5997ba4b8348af267543b1d2120,,CRC Press,English,,9781420013856; 0849393841; 9780849393846
Scopus,Software engineering processes: Principles and applications,"Software engineering is playing an increasingly significant role in computing and informatics, necessitated by the complexities inherent in large-scale software development. To deal with these difficulties, the conventional life-cycle approaches to software engineering are now giving way to the ""process system"" approach, encompassing development methods, infrastructure, organization, and management. Until now, however, no book fully addressed process-based software engineering or set forth a fundamental theory and framework of software engineering processes. Software Engineering Processes: Principles and Applications does just that. Within a unified framework, this book presents a comparative analysis of current process models and formally describes their algorithms. It systematically enables comparison between current models, avoidance of ambiguity in application, and simplification of manipulation for practitioners. The authors address a broad range of topics within process-based software engineering and the fundamental theories and philosophies behind them. They develop a software engineering process reference model (SEPRM) to show how to solve the problems of different process domains, orientations, structures, taxonomies, and methods. They derive a set of process benchmarks-based on a series of international surveys-that support validation of the SEPRM model. Based on their SEPRM model and the unified process theory, they demonstrate that current process models can be integrated and their assessment results can be transformed between each other. Software development is no longer just a black art or laboratory activity. It is an industrialized process that requires the skills not just of programmers, but of organization and project managers and quality assurance specialists. Software Engineering Processes: Principles and Applications is the key to understanding, using, and improving upon effective engineering procedures for software development. © 2000 by Taylor & Francis Group, LLC.",,"Wang Y., King G.",2000,Book,Software Engineering Processes: Principles and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001518301&partnerID=40&md5=594bc71bcba81f0aec41672c7e72e9a3,"Center for Software Engineering, IVF, Gothenburg, Sweden; Southampton Institute, United Kingdom",CRC Press,English,,9781482274547; 9780849323669
Scopus,Software Process Dynamics,"© 2008 the Institute of Electrical and Electronics Engineers, Inc..",,Madachy R.J.,2007,Book,Software Process Dynamics,10.1002/9780470192719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889630251&doi=10.1002%2f9780470192719&partnerID=40&md5=55557e8bedf6e9cf0fa4acb42f7c924a,,John Wiley and Sons,English,,9780471274551
Scopus,Optimal project feature weights in analogy-based cost estimation: Improvement and limitations,"Cost estimation is a vital task in most important software project decisions such as resource allocation and bidding. Analogy-based cost estimation is particularly transparent, as it relies on historical information from similar past projects, whereby similarities are determined by comparing the projects' key attributes and features. However, one crucial aspect of the analogy-based method is not yet fully accounted for: the different impact or weighting of a project's various features. Current approaches either try to find the dominant features or require experts to weight the features. Neither of these yields optimal estimation performance. Therefore, we propose to allocate separate weights to each project feature and to find the optimal weights by extensive search. We test this approach on several real-world data sets and measure the improvements with commonly used quality metrics. We find that this method 1) increases estimation accuracy and reliability, 2) reduces the model's volatility and, thus, is likely to increase its acceptance in practice, and 3) indicates upper limits for analogy-based estimation quality as measured by standard metrics. © 2006 IEEE.",Analogy-based cost estimation; Project clustering; Project features; Software cost estimation,"Auer M., Trendowicz A., Graser B., Haunschmid E., Biffl S.",2006,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2006.1599418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644660383&doi=10.1109%2fTSE.2006.1599418&partnerID=40&md5=930d37955d58834ff4c154f8c0e25cf0,"Institute of Software Technology and Interactive Systems, Vienna University of Technology, Favoritenstr. 9-11/188, A-1040 Vienna, Austria; 145 W. 58 St., New York, NY 10019, United States; Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Act Management Consulting, Haslingergasse 55/5/17, A-1160 Vienna, Austria; Computing Services Department, Vienna University of Technology, Wiedner Hauptstrasse 8-10, A-1040 Vienna, Austria",,English,00985589,
Scopus,Software productivity measurement using multiple size measures,"Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p < 0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an example of its use for Web application projects. We also explain the relationship between effort prediction models and productivity models. © 2004 IEEE.",Software cost estimation; Software productivity measurement,"Kitchenham B., Mendes E.",2004,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2004.104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844464576&doi=10.1109%2fTSE.2004.104&partnerID=40&md5=13619c5b034f4fd35840646a776e35c4,"National ICT Australia, Locked Bag 9013, Alexandria, NSW 1435, Australia; Department of Computer Science, Keele University, Staffordshire ST5 1BG, United Kingdom; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,00985589,
Scopus,An empirical validation of a neural network model for software effort estimation,"As software becomes more complex and its scope dramatically increases, the importance of research on developing methods for estimating software development efforts has perpetually increased. Such accurate estimation has a prominent impact on the success of projects. Out of the numerous methods for estimating software development efforts that have been proposed, line of code (LOC)-based constructive cost model (COCOMO), function point-based regression model (FP), neural network model (NN), and case-based reasoning (CBR) are among the most popular models. Recent research has tended to focus on the use of function points (FPs) in estimating the software development efforts, however, a precise estimation should not only consider the FPs, which represent the size of the software, but should also include various elements of the development environment for its estimation. Therefore, this study is designed to analyze the FPs and the development environments of recent software development cases. The primary purpose of this study is to propose a precise method of estimation that takes into account and places emphasis on the various software development elements. This research proposes and evaluates a neural network-based software development estimation model. © 2007 Elsevier Ltd. All rights reserved.",Function points; Neural networks; Software effort estimation; Variable selection strategy,"Park H., Baek S.",2008,Journal,Expert Systems with Applications,10.1016/j.eswa.2007.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949093510&doi=10.1016%2fj.eswa.2007.08.001&partnerID=40&md5=cb035e93a974e004bc807bd451f36e23,"College of Engineering, Yonsei University, Seoul, South Korea; School of Business Administration, Hanyang University, Seoul, South Korea",,English,09574174,
Scopus,Effort estimation of use cases for incremental large-scale software development,"This paper describes an industrial study of an effort estimation method based on use cases, the Use Case Points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates. Copyright 2005 ACM.",Estimation; Incremental development; Use cases,"Mohagheghi P., Anda B., Conradi R.",2005,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2005.1553573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244495310&doi=10.1109%2fICSE.2005.1553573&partnerID=40&md5=424afc2aa718f86dcfd0420d0b470038,"Department of Computer and Information Science, Norwegian University of Science and Technology, NO-7491 Trondheim, Norway; Faculty of Engineering, Agder University College, NO-4876 Grimstad, Norway; Simula Research Laboratory, P.O.Box 134, NO-1325 Lysaker, Norway",,English,02705257,
Scopus,CLAMI: Defect prediction on unlabeled datasets,"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning. © 2015 IEEE.",,"Nam J., Kim S.",2016,Conference,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",10.1109/ASE.2015.56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963818075&doi=10.1109%2fASE.2015.56&partnerID=40&md5=8e44aad9d9cf4a0535891f3746c09f44,"Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong",Institute of Electrical and Electronics Engineers Inc.,English,,9781509000241
Scopus,On using planning poker for estimating user stories,"While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students' estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts' estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases. © 2012 Elsevier Inc. All rights reserved.",Agile software development; Effort estimation; Expert judgment; Planning poker; Scrum; User story,"Mahnič V., Hovelja T.",2012,Conference,Journal of Systems and Software,10.1016/j.jss.2012.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863099260&doi=10.1016%2fj.jss.2012.04.005&partnerID=40&md5=b04e14142829337ecb9932c6e53fc0a6,"University of Ljubljana, Faculty of Computer and Information Science, Tržaška 25, SI-1000 Ljubljana, Slovenia",,English,01641212,
Scopus,Cost-reliability-optimal release policy for software reliability models incorporating improvements in testing efficiency,"Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. One of the most important applications of SRGMs is to determine the software release time. Most software developers and managers always want to know the date on which the desired reliability goal will be met. In this paper, we first review a SRGM with generalized logistic testing-effort function and the proposed generalized logistic testing-effort function can be used to describe the actual consumption of resources during the software development process. Secondly, if software developers want to detect more faults in practice, it is advisable to introduce new test techniques, tools, or consultants, etc. Consequently, here we propose a software cost model that can be used to formulate realistic total software cost projects and discuss the optimal release policy based on cost and reliability considering testing effort and efficiency. Some theorems and several numerical illustrations are also presented. Based on the proposed models and methods, we can specifically address the problem of how to decide when to stop testing and when to release software for use. © 2004 Elsevier Inc. All rights reserved.",Estimation methods; Release criteria; Software cost; Software reliability; Software testing,Huang C.-Y.,2005,Journal,Journal of Systems and Software,10.1016/j.jss.2004.10.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18144416946&doi=10.1016%2fj.jss.2004.10.014&partnerID=40&md5=8242025b1763aa5361afff9e16b346fe,"Department of Computer Science, National Tsing Hua University, No. 101, Section 2, Kuang Fu Road, Hsinchu 30055, Taiwan",,English,01641212,
Scopus,Class point: An approach for the size estimation of object-oriented systems,"In this paper, we present an FP-like approach, named Class Point, which was conceived to estimate the size of object-oriented products. In particular, two measures are proposed, which are theoretically validated showing that they satisfy well-known properties necessary for size measures. An initial empirical validation is also performed, meant to assess the usefulness and effectiveness of the proposed measures to predict the development effort of object-oriented systems. Moreover, a comparative analysis is carried out, taking into account several other size measures. © 2005 IEEE.",Effort prediction model; Empirical validation; Function point analysis; Object-oriented systems; Size measures; Theoretical validation,"Costagliola G., Ferrucci F., Tortora G., Vitiello G.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644395947&doi=10.1109%2fTSE.2005.5&partnerID=40&md5=46d78f933f23f2950907637d1c165323,"Dipto. di Matematica e Informatica, Università of Salerno, Via Ponte Don Melillo, 84084 Fisciano, SA, Italy",,English,00985589,
Scopus,Better sure than safe? Over-confidence in judgement based software development effort prediction intervals,"The uncertainty of a software development effort estimate can be indicated through a prediction interval (PI), i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be ""90% confident"" or believe that is it ""very likely"" that the effort required to complete a project will be between 8000 and 12,000 work-hours. This paper describes results from four studies (Studies A-D) on human judgement (expert) based PIs of software development effort. Study A examines the accuracy of the PIs in real software projects. The results suggest that the PIs were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B-D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process. Study C examines the possibility that the concept of confidence level is difficult to interpret for software estimators. Finally, Study D examines the possibility that there are unfortunate feedback mechanisms that reward over-confidence. © 2003 Elsevier Inc. All rights reserved.",,"Jørgensen M., Teigen K.H., Moløkken K.",2004,Journal,Journal of Systems and Software,10.1016/S0164-1212(02)00160-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346694424&doi=10.1016%2fS0164-1212%2802%2900160-7&partnerID=40&md5=b9d587c0382074cc0af6c686b8615c13,"Simula Research Laboratory, Software Engineering, P.O. Box 134, 1325 Lysaker, Norway; Department of Psychology, University of Oslo, P.O. Box 1094, Blindern, 0317 Oslo, Norway",,English,01641212,
Scopus,Identifying high performance ERP projects,"Learning from high performance projects is crucial for software process improvement. Therefore, we need to Identify outstanding projects that may serve as role models. It is common to measure productivity as an indicator of performance. It is vital that productivity measurements deal correctly with variable returns to scale and multivariate data. Software projects generally exhibit variable returns to scale, and the output from ERP projects Is multivariate. We propose to use Data Envelopment Analysis Variable Returns to Scale (DEA VRS) to measure the productivity of software projects. DEA VRS fulfills the two requirements stated above. The results from this emplrical study of 30 ERP projects extracted from a benchmarking database in Accenture Identified six projects as potential role models. These projects deserve to be studied and probably copied as part of a software process improvement initiative. The results also suggest that there is a 50 percent potential for productivity Improvement, on average. Finally, the results support the assumption of variable returns to scale in ERP projects. We recommend DEA VRS be used as the default technique for appropriate productivity comparisons of individual software projects. Used together with methods for hypothesis testing, DEA VRS is also a useful technique for assessing the effect of alleged process improvements.",Benchmarking; Best practice identification; Data envelopment analysis (DEA); Enterprise resource planning (ERP); Multivariate productivity measurements; Software development; Software metrics; Software process improvement; Software project management,"Stensrud E., Myrtveit I.",2003,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2003.1199070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038043480&doi=10.1109%2fTSE.2003.1199070&partnerID=40&md5=c4ce7902d9bf176cb2c5b75886813e77,"Norwegian School of Management, PO Box 580, N-1301 Sandvika, Norway",,English,00985589,
Scopus,A procedure for analyzing unbalanced datasets,This paper describes a procedure for analyzing unbalanced datasets that include many nominal- and ordinal-scale factors. Such datasets are often found in company datasets used for benchmarking and productivity assessment. The two major problems caused by lack of balance are that the impact of factors can be concealed and that spurious impacts can be observed. These effects are examined with the help of two small artificial datasets. The paper proposes a method of forward pass residual analysis to analyze such datasets. The analysis procedure is demonstrated on the artificial datasets and then applied to the COCOMO dataset. The paper ends with a discussion of the advantages and limitations of the analysis procedure. © 1998 IEEE.,Analysis of variance; Benchmarking data; Residual analysis; Software metrics; Statistical analysis; Unbalanced datasets,Kitchenham B.,1998,Journal,IEEE Transactions on Software Engineering,10.1109/32.677185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001688427&doi=10.1109%2f32.677185&partnerID=40&md5=e35d518c208ffaa7badc67deaebe2e82,"Department of Computer Science, Keele University, Staffordshire, ST5 5BG, United Kingdom",,English,00985589,
Scopus,Ranking and clustering software cost estimation models through a multiple comparisons algorithm,"Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods. © 1976-2012 IEEE.",Cost estimation; management; metrics/measurement; statistical methods,"Mittas N., Angelis L.",2013,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2012.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875690871&doi=10.1109%2fTSE.2012.45&partnerID=40&md5=88db10a907f77073af62b59793dece76,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,00985589,
Scopus,Maintainability defects detection and correction: A multi-objective approach,"Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort. © 2012 Springer Science+Business Media, LLC.",By example; Effort; Maintainability defects; Multi-objective optimization; Search-based software engineering; Software maintenance,"Ouni A., Kessentini M., Sahraoui H., Boukadoum M.",2013,Conference,Automated Software Engineering,10.1007/s10515-011-0098-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872029667&doi=10.1007%2fs10515-011-0098-8&partnerID=40&md5=f728aa4233857991505a85456ac454c4,"DIRO, Université de Montréal, Montréal, Canada; CS, Missouri University of Science and Technology, Rolla, United States; DI, Université du Québec À Montréal, Montréal, Canada",,English,09288910,
Scopus,"Think locally, act globally: Improving defect and effort prediction models","Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds. © 2012 IEEE.",models; software metrics; techniques,"Bettenburg N., Nagappan M., Hassan A.E.",2012,Conference,IEEE International Working Conference on Mining Software Repositories,10.1109/MSR.2012.6224300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865149858&doi=10.1109%2fMSR.2012.6224300&partnerID=40&md5=d7d0849ed85b37c094fe60877aeea150,"Software Analysis and Intelligence Lab. (SAIL), Queen's University, School of Computing, Kingston, ON K1N 3L6, Canada",,English,21601852,9781467317610
Scopus,Bayesian network models for Web effort prediction: A comparative study,"Objective: The objective of this paper is to compare, using a cross-company dataset, several Bayesian Network (BN) models for Web effort estimation. Method: Eight BNs were built; four automatically using Hugin and PowerSoft tools with two training sets, each with 130 Web projects from the Tukutuku database; four using a causal graph elicited by a domain expert, with parameters automatically fit using the same training sets used in the automated elicitation (hybrid models). Their accuracy was measured using two validation sets, each containing data on 65 projects, and point estimates. As a benchmark, the BN-based estimates were also compared to estimates obtained using Manual StepWise Regression (MSWR), Case-Based Reasoning (CBR), mean- and median-based effort models. Results: MSWR presented significantly better predictions than any of the BN models built herein, and in addition was the only technique to provide significantly superior predictions to a Median-based effort model. Conclusions: This paper investigated data-driven and hybrid BN models using project data from the Tukutuku database. Our results suggest that the use of simpler models, such as the median effort, can outperform more complex models, such as BNs. In addition, MSWR seemed to be the only effective technique for Web effort estimation. © 2008 IEEE.",Project management; Software engineering; Web cost estimation; Web engineering,"Mendes E., Mosley N.",2008,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2008.64,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049200453&doi=10.1109%2fTSE.2008.64&partnerID=40&md5=18d881c963c4a41da305b0b8fe00bce5,"Computer Science Department, The University of Auckland, Private Bag 92019, Auckland, New Zealand; MetriQ Limited, PO Box 837, Waiheke 1840 Auckland, New Zealand",,English,00985589,
Scopus,"Reasons for software effort estimation error: Impact of respondent role, information collection approach, and data analysis method","This study aims to improve analyses of why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: 1) interviews with employees in different roles who are responsible for estimation, 2) estimation experience reports from 68 completed projects, and 3) statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons given for estimation error. We found, for example, a strong tendency to perceive factors outside the respondents' own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents' own control and were determined by the estimators' skill or experience. This bias in types of reason means that the collection only of project managers' viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics. © 2004 IEEE.",Cost estimation; Performance evaluation; Review and evaluation,"Jørgensen M., Moløkken-Østvold K.",2004,Review,IEEE Transactions on Software Engineering,10.1109/TSE.2004.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844532694&doi=10.1109%2fTSE.2004.103&partnerID=40&md5=e153c2abee485c21b87207974338468e,"Simula Research Laboratory, PO Box 134, N-1325 Lysaker, Norway",,English,00985589,
Scopus,Analogy-Based Practical Classification Rules for Software Quality Estimation,"Software metrics-based quality estimation models can be effective tools for identifying which modules are likely to be fault-prone or not fault-prone. The use of such models prior to system deployment can considerably reduce the likelihood of faults discovered during operations, hence improving system reliability. A software quality classification model is calibrated using metrics from a past release or similar project, and is then applied to modules currently under development. Subsequently, a timely prediction of which modules are likely to have faults can be obtained. However, software quality classification models used in practice may not provide a useful balance between the two misclassification rates, especially when there are very few faulty modules in the system being modeled. This paper presents, in the context of case-based reasoning, two practical classification rules that allow appropriate emphasis on each type of misclassification as per the project requirements. The suggested techniques are especially useful for high-assurance systems where faulty modules are rare. The proposed generalized classification methods emphasize on the costs of misclassifications, and the unbalanced distribution of the faulty program modules. We illustrate the proposed techniques with a case study that consists of software measurements and fault data collected over multiple releases of a large-scale legacy telecommunication system. In addition to investigating the two classification methods, a brief relative comparison of the techniques is also presented. It is indicated that the level of classification accuracy and model-robustness observed for the case study would be beneficial in achieving high software reliability of its subsequent system releases. Similar observations are made from our empirical studies with other case studies.",Case-based reasoning; Classification models; Data clustering; Majority voting; Multiple releases; Software metrics; Software reliability estimation,"Khoshgoftaar T.M., Seliya N.",2003,Journal,Empirical Software Engineering,10.1023/A:1025316301168,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141924286&doi=10.1023%2fA%3a1025316301168&partnerID=40&md5=c55c23db4d307f95d3a30705f91b2729,"Empirical Software Eng. Laboratory, Dept. of Comp. Sci. and Engineering, Florida Atlantic University, Boca Raton, FL 33431, United States",,English,13823256,
Scopus,Improving the COCOMO model using a neuro-fuzzy approach,"Accurate software development cost estimation is important for effective project management such as budgeting, project planning and control. So far, no model has proved to be successful at effectively and consistently predicting software development cost. A novel neuro-fuzzy Constructive Cost Model (COCOMO) is proposed for software cost estimation. This model carries some of the desirable features of a neuro-fuzzy approach, such as learning ability and good interpretability, while maintaining the merits of the COCOMO model. Unlike the standard neural network approach, the proposed model can be interpreted and validated by experts, and has good generalization capability. The model deals effectively with imprecise and uncertain input and enhances the reliability of software cost estimates. In addition, it allows input to have continuous rating values and linguistic values, thus avoiding the problem of similar projects having large different estimated costs. A detailed learning algorithm is also presented in this work. The validation using industry project data shows that the model greatly improves estimation accuracy in comparison with the well-known COCOMO model. © 2005 Elsevier B.V. All rights reserved.",COCOMO; Fuzzy set; Neural network; Soft computing; Software cost estimation,"Huang X., Ho D., Ren J., Capretz L.F.",2007,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2005.06.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750964694&doi=10.1016%2fj.asoc.2005.06.007&partnerID=40&md5=098281e566a6b1cdbb9e52389b18ed95,"Department of ECE, University of Western Ontario, London, Ont. N6A 5B9, Canada; Toronto Design Center, Motorola Canada Ltd., Markham, Ont. L6G 1B3, Canada",,English,15684946,
Scopus,On the use of Bayesian belief networks for the prediction of software productivity,"In spite of numerous methods proposed, software cost estimation remains an open issue and in most situations expert judgment is still being used. In this paper, we propose the use of Bayesian belief networks (BBNs), already applied in other software engineering areas, to support expert judgment in software cost estimation. We briefly present BBNs and their advantages for expert opinion support and we propose their use for productivity estimation. We illustrate our approach by giving two examples, one based on the COCOMO81 cost factors and a second one, dealing with productivity in ERP system localization. © 2002 Elsevier Science B.V. All rights reserved.",Bayesian belief network; COCOMO; Cost estimation; Expert judgment,"Stamelos I., Angelis L., Dimou P., Sakellaris E.",2003,Journal,Information and Software Technology,10.1016/S0950-5849(02)00163-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037223390&doi=10.1016%2fS0950-5849%2802%2900163-5&partnerID=40&md5=a216f80e1ef6d8a9e1cffcae7f251d8e,"Department of Informatics, Aristotle University of Thessaloniki, 54006 Thessaloniki, Greece; Singular International, 10A Dodekanisou Str., 54626 Thessaloniki, Greece",,English,09505849,
Scopus,Function Points Analysis: An Empirical Study of Its Measurement Processes,"Function point analysis (FPA) was initially designed on the basis of expert judgments, without explicit reference to any theoretical foundation. From the point of view of the measurement scales used in its measurement process, FPA constitutes a potpourri of scales not admissible without the transformations imbedded in the implicit models of expert judgments. The results of this empirical study demonstrate that in a homogeneous environment not burdened with major differences in productivity factors there is a clear relationship between FPA' primary components and Work-Effort. This empirical study also indicates that there is such a relationship for each step of the FPA measurement process prior to the mixing of scales and the assignments of weights. Comparisons with FPA productivity models based on weights confirm, on the one hand, that the weights do not add information and, on the other, that the weights are fairly robust and can be used when little historical data is available. The full data set is provided for future studies. -. ©1996 IEEE.",Estimation models; Function point analysis; Functional metrics; Measurement process; Productivity models,"Abran A., Robillard P.N.",1996,Journal,IEEE Transactions on Software Engineering,10.1109/32.553638,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001658335&doi=10.1109%2f32.553638&partnerID=40&md5=e58510aeec940cce4dbf7b8a652d6d30,"IEEE; Université du Québec Àmontréal, P.O. Box 8888, Suce. Centre-Ville Montreal,Quebec, Canada; École Polytechnique de Montréal, P.O. Box 6079, Suce. Centre-Ville Montreal,Que., Canada",,English,00985589,
Scopus,Predicting software project effort: A grey relational analysis based method,"The inherent uncertainty of the software development process presents particular challenges for software effort prediction. We need to systematically address missing data values, outlier detection, feature subset selection and the continuous evolution of predictions as the project unfolds, and all of this in the context of data-starvation and noisy data. However, in this paper, we particularly focus on outlier detection, feature subset selection, and effort prediction at an early stage of a project. We propose a novel approach of using grey relational analysis (GRA) from grey system theory (GST), which is a recently developed system engineering theory based on the uncertainty of small samples. In this work we address some of the theoretical challenges in applying GRA to outlier detection, feature subset selection, and effort prediction, and then evaluate our approach on five publicly available industrial data sets using both stepwise regression and Analogy as benchmarks. The results are very encouraging in the sense of being comparable or better than other machine learning techniques and thus indicate that the method has considerable potential. © 2010 Elsevier Ltd. All rights reserved.",Effort prediction; Feature subset selection; Grey relational analysis; Outlier detection; Software project estimation,"Song Q., Shepperd M.",2011,Journal,Expert Systems with Applications,10.1016/j.eswa.2010.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951581706&doi=10.1016%2fj.eswa.2010.12.005&partnerID=40&md5=d0814361a5014164ec393413b35f0a7c,"Xi'An Jiaotong University, 28 Xian-Ning West Road, Xi'an, Shaanxi 710049, China; Brunel University, UB8 3PH, United Kingdom",,English,09574174,
Scopus,Functional size measurement revisited,"There are various approaches to software size measurement. Among these, the metrics and methods based on measuring the functionality attribute have become widely used since the original method was introduced in 1979. Although functional size measurement methods have gone a long way, they still provide challenges for software managers. This article identifies improvement opportunities based on empirical studies we performed on ongoing projects. We also compare our findings with the extended dataset provided by the International Software Benchmarking Standards Group (ISBSG). © 2008 ACM.",COSMIC FFP; Functional size measurement; MkII FPA; Software benchmarking; Software estimation,"Gencel C., Demirors O.",2008,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/1363102.1363106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449121855&doi=10.1145%2f1363102.1363106&partnerID=40&md5=878f1b13f4bd45bcae482ee112ab6952,"Middle East Technical University, Ankara, Turkey; Middle East Technical University, Informatics Institute, Ankara, 06531, Turkey",,English,1049331X,
Scopus,A formal model of the software test process,"A novel approach to model the system test phase of the software life cycle is presented. This approach is based on concepts and techniques from control theory and is useful in computing the effort required to reduce the number of errors and the schedule slippage under a changing process environment. Results from these computations are used, and possibly revised, at specific checkpoints in a feedback-control structure to meet the schedule and quality objectives. Two case studies were conducted to study the behavior of the proposed model. One study reported here uses data from a commercial project. The outcome from these two studies suggests that the proposed model might well be the first significant milestone along the road to a formal and practical theory of software process control.",Feedback control; Modeling; Process control; Software test process; Software testing; State variable,"Cangussu J.W., DeCarlo R.A., Mathur A.P.",2002,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2002.1027800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036704716&doi=10.1109%2fTSE.2002.1027800&partnerID=40&md5=ff27da9c3556118fea1ee133fd7f169c,"Department of Computer Science, Purdue University, West Lafayette, IN 47907-1398, United States; Dept. of Elec. and Comp. Engineering, Purdue University, West Lafayette, IN 47907-1285, United States",,English,00985589,
Scopus,Ensembles and locality: Insight on improving software effort estimation,"Context: Ensembles of learning machines and locality are considered two important topics for the next research frontier on Software Effort Estimation (SEE). Objectives We aim at (1) evaluating whether existing automated ensembles of learning machines generally improve SEEs given by single learning machines and which of them would be more useful; (2) analysing the adequacy of different locality approaches; and getting insight on (3) how to improve SEE and (4) how to evaluate/choose machine learning (ML) models for SEE. Method A principled experimental framework is used for the analysis and to provide insights that are not based simply on intuition or speculation. A comprehensive experimental study of several automated ensembles, single learning machines and locality approaches, which present features potentially beneficial for SEE, is performed. Additionally, an analysis of feature selection and regression trees (RTs), and an investigation of two tailored forms of combining ensembles and locality are performed to provide further insight on improving SEE. Results Bagging ensembles of RTs show to perform well, being highly ranked in terms of performance across different data sets, being frequently among the best approaches for each data set and rarely performing considerably worse than the best approach for any data set. They are recommended over other learning machines should an organisation have no resources to perform experiments to chose a model. Even though RTs have been shown to be more reliable locality approaches, other approaches such as k-Means and k-Nearest Neighbours can also perform well, in particular for more heterogeneous data sets. Conclusion Combining the power of automated ensembles and locality can lead to competitive results in SEE. By analysing such approaches, we provide several insights that can be used by future research in the area. © 2012 Elsevier B.V. All rights reserved.",Empirical validation; Ensembles of learning machines; Keywords; Locality; Software effort estimation,"Minku L.L., Yao X.",2013,Conference,Information and Software Technology,10.1016/j.infsof.2012.09.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878336652&doi=10.1016%2fj.infsof.2012.09.012&partnerID=40&md5=8fb60e8e7874831b1147d1b25d0a684c,"CERCIA, School of Computer Science, University of Birmingham, Edgbaston, Birmingham B15 2TT, United Kingdom",,English,09505849,
Scopus,An investigation of artificial neural networks based prediction systems in software project management,"A critical issue in software project management is the accurate estimation of size, effort, resources, cost, and time spent in the development process. Underestimates may lead to time pressures that may compromise full functional development and the software testing process. Likewise, overestimates can result in noncompetitive budgets. In this paper, artificial neural network and stepwise regression based predictive models are investigated, aiming at offering alternative methods for those who do not believe in estimation models. The results presented in this paper compare the performance of both methods and indicate that these techniques are competitive with the APF, SLIM, and COCOMO methods. © 2007 Elsevier Inc. All rights reserved.",Artificial neural networks; Data mining; Linear regression; Predictive accuracy; Software effort estimation,"de Barcelos Tronto I.F., da Silva J.D.S., Sant'Anna N.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2007.05.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049173611&doi=10.1016%2fj.jss.2007.05.011&partnerID=40&md5=0e3ee3b6dcbfbd52ee00b4593c6c87aa,"Laboratory for Computing and Applied Mathematics - LAC, Brazilian National Institute for Space Research, INPE, Av. Astronautas, 1758, Jardim da Granja, Zip 13081-970, Sao Jose dos Campos, SP, Brazil",,English,01641212,
Scopus,Dealing with missing software project data,"Whilst there is a general consensus that quantitative approaches are an important part of successful software project management, there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so we explore the application of two existing data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets. Both are quite problematic from an effort modelling perspective because they contain few cases, have a significant number of missing values and the projects are quite heterogeneous. We examine the quality of fit of effort models derived by stepwise regression on the raw data and data sets with values imputed by various techniques is compared. In both data sets we find that k-nearest neighbour (k-NN) and sample mean imputation (SMI) significantly improve the model fit, with k-NN giving the best results. These results are consistent with other recently published results, consequently we conclude that imputation can assist empirical software engineering. © 2003 IEEE.",Application software; Costs; Data analysis; Data engineering; Design engineering; Industrial training; Information analysis; Project management; Software engineering; Software metrics,"Cartwright M.H., Shepperd M.J., Song Q.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232464,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943171173&doi=10.1109%2fMETRIC.2003.1232464&partnerID=40&md5=fba09cf288db429efb09e32e4a5bbf2b,"Empirical Software Engineering Research Group, School of Design, Engineering and Computing, Bournemouth University, Bournemouth, BH1 3LT, United Kingdom",IEEE Computer Society,English,15301435,0769519873
Scopus,"Software development productivity of european space, military, and industrial applications","The identification, combination, and interaction of the many factors which influence software development productivity makes the measurement, estimation, comparison and tracking of productivity rates very difficult. Through the analysis of a European Space Agency database consisting of 99 software development projects from 37 companies in 8 European countries, this paper seeks to provide significant and useful information about the major factors which influence the productivity of European space, military, and industrial applications, as well as to determine the best metric for measuring the productivity of these projects. Several key findings emerge from the study. The results indicate that some organizations are obtaining significantly higher productivity than others. Some of this variation is due to the differences in the application category and programming language of projects in each company; however, some differences must also be due to the ways in which these companies manage their software development projects. The use of tools and modern programming practices were found to be major controllable factors in productivity improvement. Finally, the lines-of-code productivity metric is shown to be superior to the process productivity metric for projects in our database. © 1996 IEEE.",And industrial software projects; Empirical study of software projects; European software projects; Military; Software effort estimation; Software productivity; Space,"Maxwell K.D., Van Wassenhove L., Dutta S.",1996,Journal,IEEE Transactions on Software Engineering,10.1109/32.544349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000786473&doi=10.1109%2f32.544349&partnerID=40&md5=af302ec678591d30f3361f3541f6202a,"Research Initiative in Software Excellence (RISE), INSEAD, Boulevard de Constance, 77305 Fontainebleau Cedex, France; Haas School of Business, University of California at Berkeley, Berkeley, CA 94720, United States",,English,00985589,
Scopus,Empirical evaluation of the effects of mixed project data on learning defect predictors,"Context: Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects (i.e. within project (WP)) through retrospective analyses. On the other hand, recent studies try to utilize data across projects (i.e. cross project (CP)) for building defect prediction models for new projects. There are no cases where the combination of within and cross (i.e. mixed) project data are used together. Objective: Our goal is to investigate the merits of using mixed project data for binary defect prediction. Specifically, we want to check whether it is feasible, in terms of defect detection performance, to use data from other projects for the cases (i) when there is an existing within project history and (ii) when there are limited within project data. Method: We use data from 73 versions of 41 projects that are publicly available. We simulate the two above-mentioned cases, and compare the performances of naive Bayes classifiers by using within project data vs. mixed project data. Results: For the first case, we find that the performance of mixed project predictors significantly improves over full within project predictors (p-value < 0.001), however the effect size is small (Hedges′ g = 0.25). For the second case, we found that mixed project predictors are comparable to full within project predictors, using only 10% of available within project data (p-value = 0.002, g = 0.17). Conclusion: We conclude that the extra effort associated with collecting data from other projects is not feasible in terms of practical performance improvement when there is already an established within project defect predictor using full project history. However, when there is limited project history, e.g. early phases of development, mixed project predictions are justifiable as they perform as good as full within project models. © 2012 Elsevier B.V. All rights reserved.",Cross project; Defect prediction; Fault prediction; Mixed project; Product metrics; Within project,"Turhan B., Tosun Misirli A., Bener A.",2013,Conference,Information and Software Technology,10.1016/j.infsof.2012.10.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876294660&doi=10.1016%2fj.infsof.2012.10.003&partnerID=40&md5=354dc5a5ce6c6d01207322afc781d582,"Dept. of Information Processing Science, University of Oulu, 90014 Oulu, Finland; Ted Rogers School of ITM, Ryerson University, Toronto, ON M5B 2K3, Canada",,English,09505849,
Scopus,Software Science Revisited: A Critical Analysis of the Theory and Its Empirical Support,"The theory of software science was developed by the late M. H. Halstead of Purdue University during the early 1970's. It was first presented in unified form in the monograph Elements of Software Science published by Elsevier North-Holland in 1977. Since it claimed to apply scientific methods to the very complex and important problem of software production, and since experimental evidence supplied by Halstead and others seemed to support the theory, it drew widespread attention from the computer science community. Some researchers have raised serious questions about the underlying theory of software science. At the same time, experimental evidence supporting some of the metrics continue to be presented. This paper is a critique of the theory as presented by Halstead and a review of experimental results concerning software science metrics published since 1977. Copyright © 1983 by The Institute of Electrical and Electronics Engineers, Inc.",Software complexity; software engineering; software management; software measurement; software metrics; software science,"Shen V.Y., Conte S.D., Dunsmore H.E.",1983,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.1983.236460,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020718740&doi=10.1109%2fTSE.1983.236460&partnerID=40&md5=4ee8042221c0001daa99ebc9bfaeda46,"Department of Computer Sciences, Purdue University, West Lafayette, IN 47907, United States",,English,00985589,
Scopus,Empirical findings on team size and productivity in software development,"The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository. © 2011 Elsevier Inc. All rights reserved.",Effort estimation datasets; ISBSG repository; Productivity; Team size,"Rodríguez D., Sicilia M.A., García E., Harrison R.",2012,Journal,Journal of Systems and Software,10.1016/j.jss.2011.09.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857361268&doi=10.1016%2fj.jss.2011.09.009&partnerID=40&md5=247dabc541019a5c2ffd28673cd89a32,"Department of Computer Science, University of Alcalá, Ctra. Barcelona, Km. 33.6, 28871 Alcalá de Henares, Madrid, Spain; Department of Computing and Communication Technologies, Oxford Brookes University, Wheatley, Oxford OX33 1HX, United Kingdom",,English,01641212,
Scopus,Chapter 6 Factors Influencing Software Development Productivity-State-of-the-Art and Industrial Experiences,"Managing software development productivity is a key issue in software organizations. Business demands for shorter time-to-market while maintaining high product quality force software organizations to look for new strategies to increase development productivity. Traditional, simple delivery rates employed to control hardware production processes have turned out not to work when simply transferred to the software domain. The productivity of software production processes may vary across development contexts dependent on numerous influencing factors. Effective productivity management requires considering these factors. Yet, there are thousands of possible factors and considering all of them would make no sense from the economical point of view. Therefore, productivity modeling should focus on a limited number of factors with the most significant impact on productivity. In this chapter, we present a comprehensive overview of productivity factors recently considered by software practitioners. The study results are based on the review of 126 publications as well as international experiences of the Fraunhofer Institute, including the most recent 13 industrial projects, four workshops, and eight surveys on software productivity. The aggregated results show that the productivity of software development processes still depends significantly on the capabilities of developers as well as on the tools and methods they use. © 2009 Elsevier Inc. All rights reserved.",,"Trendowicz A., Münch J.",2009,Review,Advances in Computers,10.1016/S0065-2458(09)01206-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65749095362&doi=10.1016%2fS0065-2458%2809%2901206-6&partnerID=40&md5=b86476b8d8a11598c8dbd4d91978a924,"Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany",,English,00652458,9780123748126
Scopus,Competitive Engineering,"Competitive Engineering documents Tom Gilb's unique, ground-breaking approach to communicating management objectives and systems engineering requirements, clearly and unambiguously. Competitive Engineering is a revelation for anyone involved in management and risk control. Already used by thousands of project managers and systems engineers around the world, this is a handbook for initiating, controlling and delivering complex projects on time and within budget. The Competitive Engineering methodology provides a practical set of tools and techniques that enable readers to effectively design, manage and deliver results in any complex organization - in engineering, industry, systems engineering, software, IT, the service sector and beyond. Elegant, comprehensive and accessible, the Competitive Engineering methodology provides a practical set of tools and techniques that enable readers to effectively design, manage and deliver results in any complex organization - in engineering, industry, systems engineering, software, IT, the service sector and beyond. * Provides detailed, practical and innovative coverage of key subjects including requirements specification, design evaluation, specification quality control and evolutionary project management * Offers a complete, proven and meaningful 'end-to-end' process for specifying, evaluating, managing and delivering high quality solutions * Tom Gilb's clients include HP, Intel, CitiGroup, IBM, Nokia and the US Department of Defense. © 2005 Elsevier Ltd. All rights reserved.",,"Gilb T., Brodie L.",2005,Book,Competitive Engineering,10.1016/B978-0-7506-6507-0.X5000-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011004545&doi=10.1016%2fB978-0-7506-6507-0.X5000-4&partnerID=40&md5=51780452d745758a7711a80e9e09a425,"Result Planning Limited, United Kingdom",Elsevier Ltd,English,,9780750665070
Scopus,"An empirical study on the relationship between software design quality, development effort, and governance in open source projects","The relationship among software design quality, development effort, and governance practices is a traditional research problem. However, the extent to which consolidated results on this relationship remain valid for open source (OS) projects is an open research problem. An emerging body of literature contrasts the view of open source as an alternative to proprietary software and explains that there exists a continuum between closed and open source projects. This paper hypothesizes that as projects approach the OS end of the continuum, governance becomes less formal. In turn a less formal governance is hypothesized to require a higher-quality code as a means to facilitate coordination among developers by making the structure of code explicit and facilitate quality by removing the pressure of deadlines from contributors. However, a less formal governance is also hypothesized to increase development effort due to a more cumbersome coordination overhead. The verification of research hypotheses is based on empirical data from a sample of 75 major OS projects. Empirical evidence supports our hypotheses and suggests that software quality, mainly measured as coupling and inheritance, does not increase development effort, but represents an important managerial variable to implement the more open governance approach that characterizes OS projects which, in turn, increases development effort. © 2008 IEEE.",Organizational management and coordination; Qualitative process analysis; Software complexity measures; Software cost estimation; Software quality concepts; Structural equation modeling,"Capra E., Francalanci C., Merlo F.",2008,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2008.68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149145618&doi=10.1109%2fTSE.2008.68&partnerID=40&md5=b213703e4c8433e1696a775062134803,"Department of Electronics and Information, Politecnico di Milano, via Ponzio 34/5, I-20133 Milano, Italy",,English,00985589,
Scopus,Investigating Web size metrics for early Web cost estimation,"This paper's aim is to bring light to this issue by identifying size metrics and cost drivers for early Web cost estimation based on current practices of several Web Companies worldwide. This is achieved using two surveys and a case study. The first survey (S1) used a search engine to obtain Web project quote forms employed by Web companies worldwide to provide initial quotes on Web development projects. The 133 Web project quote forms gathered data on size metrics, cost factors, contingency and possibly profit metrics. These metrics were organised into categories and ranked. Results indicated that the two most common size metrics used for Web cost estimation were ""total number of Web pages"" (70%) and ""which features/functionality to be provided by the application"" (66%). The results of S1 were then validated by a mature Web company that has more than 12 years of experience in Web development and a portfolio of more than 50 Web applications. The analysis was conducted using an interview. Finally, once the case study was finished, a second validation was conducted using a survey (S2) involving local New Zealand Web companies. The results of both validations were used to prepare Web project data entry forms to gather data on Web projects worldwide. After gathering data on 67 real Web projects worldwide, multivariate regression applied to the data confirmed that the number of Web pages and features/functionality provided by the application to be developed were the two most influential effort predictors. © 2004 Elsevier Inc. All rights reserved.",,"Mendes E., Mosley N., Counsell S.",2005,Journal,Journal of Systems and Software,10.1016/j.jss.2004.08.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18144382618&doi=10.1016%2fj.jss.2004.08.034&partnerID=40&md5=b1523921c7dc56b9c6587668001d553d,"Department of Computer Science, The University of Auckland, Auckland, New Zealand; Okki Software, Auckland, New Zealand; Department of Computer Science, Birkbeck College, University of London, London, United Kingdom",,English,01641212,
Scopus,Fuzzy grey relational analysis for software effort estimation,"Accurate and credible software effort estimation is a challenge for academic research and software industry. From many software effort estimation models in existence, Estimation by Analogy (EA) is still one of the preferred techniques by software engineering practitioners because it mimics the human problem solving approach. Accuracy of such a model depends on the characteristics of the dataset, which is subject to considerable uncertainty. The inherent uncertainty in software attribute measurement has significant impact on estimation accuracy because these attributes are measured based on human judgment and are often vague and imprecise. To overcome this challenge we propose a new formal EA model based on the integration of Fuzzy set theory with Grey Relational Analysis (GRA). Fuzzy set theory is employed to reduce uncertainty in distance measure between two tuples at the k th continuous feature (|(xo(k) xi(k)|) .GRA is a problem solving method that is used to assess the similarity between two tuples with M features. Since some of these features are not necessary to be continuous and may have nominal and ordinal scale type, aggregating different forms of similarity measures will increase uncertainty in the similarity degree. Thus the GRA is mainly used to reduce uncertainty in the distance measure between two software projects for both continuous and categorical features. Both techniques are suitable when relationship between effort and other effort drivers is complex. Experimental results showed that using integration of GRA with FL produced credible estimates when compared with the results obtained using Case-Based Reasoning, Multiple Linear Regression and Artificial Neural Networks methods. © 2009 Springer Science+Business Media, LLC.",Feature selection; Fuzzy set theory; Grey relational analysis; Similarity measurement; Software effort estimation by analogy,"Azzeh M., Neagu D., Cowling P.I.",2010,Journal,Empirical Software Engineering,10.1007/s10664-009-9113-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749129323&doi=10.1007%2fs10664-009-9113-0&partnerID=40&md5=1aa7a90483d1c7521eb59d2177b398e9,"AI Research Centre, Department of Computing, University of Bradford, Bradford BD7 1DP, United Kingdom",,English,13823256,
Scopus,Estimating software project effort by analogy based on linguistic values,"Estimation models in software engineering are used to predict some important attributes of future entities such as software development effort, software reliability and programmers' productivity. Among these models, those estimating software effort have motivated considerable research in recent years. The prediction procedure used by these software-effort models can be based on a mathematical function or other techniques such as analogy based reasoning, neural networks, regression trees, and rule induction models. Estimation by analogy is one of the most attractive techniques in the software effort estimation field. However, the procedure used in estimation by analogy is not yet able to handle correctly linguistic values (categorical data) such as 'very low', 'low' and 'high'. We propose a new approach based on reasoning by analogy, fuzzy logic and linguistic quantifiers to estimate software project effort when it is described either by numerical or linguistic values; this approach is referred to as Fuzzy Analogy. This paper also presents an empirical validation of our approach based on the COCOMO'81 dataset. © 2002 IEEE.",Fuzzy logic; Fuzzy reasoning; Mathematical model; Neural networks; Predictive models; Productivity; Programming profession; Regression tree analysis; Software engineering; Software reliability,"Idri A., Abran A., Khoshgoftaar T.M.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948470360&doi=10.1109%2fMETRIC.2002.1011322&partnerID=40&md5=87684064a73b4d68bfc421c42ae8edd3,"ENSIAS, Université Mohamed V Souissi, BP. 713, Agdal, Rabat, Morocco; École de Technologie Supérieure, 1180 Notre-Dame Ouest, Montréal, QC  H3C 1K3, Canada; Empirical Software Engineering Laboratory, Florida Atlantic University, United States",IEEE Computer Society,English,15301435,0769513395
Scopus,A study of the non-linear adjustment for analogy based software cost estimation,"Cost estimation is one of the most important but most difficult tasks in software project management. Many methods have been proposed for software cost estimation. Analogy Based Estimation (ABE), which is essentially a case-based reasoning (CBR) approach, is one popular technique. To improve the accuracy of ABE method, several studies have been focusing on the adjustments to the original solutions. However, most published adjustment mechanisms are based on linear forms and are restricted to numerical type of project features. On the other hand, software project datasets often exhibit non-normal characteristics with large proportions of categorical features. To explore the possibilities for a better adjustment mechanism, this paper proposes Artificial Neural Network (ANN) for Non-linear adjustment to ABE (NABE) with the learning ability to approximate complex relationships and incorporating the categorical features. The proposed NABE is validated on four real world datasets and compared against the linear adjusted ABEs, CART, ANN and SWR. Subsequently, eight artificial datasets are generated for a systematic investigation on the relationship between model accuracies and dataset properties. The comparisons and analysis show that non-linear adjustment could generally extend ABE's flexibility on complex datasets with large number of categorical features and improve the accuracies of adjustment techniques. © 2009 Springer Science+Business Media, LLC.",Analogy based estimation; Artificial datasets; Artificial neural network; Case based reasoning; Non-linear adjustment; Software cost estimation,"Li Y.F., Xie M., Goh T.N.",2009,Journal,Empirical Software Engineering,10.1007/s10664-008-9104-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349217241&doi=10.1007%2fs10664-008-9104-6&partnerID=40&md5=93db250ac1dc2e9e93ef42a24dff0862,"Department of Industrial and Systems Engineering, National University of Singapore, Singapore 119 260, Singapore",,English,13823256,
Scopus,A comparison of software project overruns - Flexible versus sequential development models,"Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates claim that among the benefits of using these models is reduced overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those which employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference was that more of flexible projects than sequential projects cited good requirement specifications and good collaboration/ communication with clients as contributing to accurate estimates. © 2005 IEEE.",Cost estimation; Management; Project control and modeling; Software development models,"Moløkken-Østvold K., Jørgensen M.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.96,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644439483&doi=10.1109%2fTSE.2005.96&partnerID=40&md5=66e17b23169352a0597745e2499bfcce,"Simula Research Laboratory, PO Box 134, 1325 Lysaker, Norway",,English,00985589,
Scopus,Multi-objective software effort estimation,"We introduce a bi-objective effort estimation algorithm that combines Confidence Interval Analysis and assessment of Mean Absolute Error. We evaluate our proposed algorithm on three different alternative formulations, baseline comparators and current state-of-the-art effort estimators applied to five real-world datasets from the PROMISE repository, involving 724 different software projects in total. The results reveal that our algorithm outperforms the baseline, state-of-the-art and all three alternative formulations, statistically significantly (p &lt; 0:001) and with large effect size (A12≥ 0:9) over all five datasets. We also provide evidence that our algorithm creates a new state-of-the-art, which lies within currently claimed industrial human-expert-based thresholds, thereby demonstrating that our findings have actionable conclusions for practicing software engineers. © 2016 ACM.",Confidence interval; Estimates uncertainty; Multi-objective evolutionary algorithm; Software effort estimation,"Sarro F., Petrozziello A., Harman M.",2016,Conference,Proceedings - International Conference on Software Engineering,10.1145/2884781.2884830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971435118&doi=10.1145%2f2884781.2884830&partnerID=40&md5=485453ddecdacdba35f819b10ed637b3,"University College London, London, United Kingdom; University of Portsmouth, Portsmouth, United Kingdom",IEEE Computer Society,English,02705257,9781450339001; 9781450342056
Scopus,Automated Defect Prevention: Best Practices in Software Management,"This book describes an approach to software management based on establishing an infrastructure that serves as the foundation for the project. This infrastructure defines people roles, necessary technology, and interactions between people and technology. This infrastructure automates repetitive tasks, organizes project activities, tracks project status, and seamlessly collects project data to provide measures necessary for decision making. Most importantly, this infrastructure sustains and facilitates the improvement of human-defined processes. The methodology described in the book, which is called Automated Defect Prevention (ADP) stands out from the current software landscape as a result of two unique features: its comprehensive approach to defect prevention, and its far-reaching emphasis on automation. ADP is a practical and thorough guide to implementing and managing software projects and processes. It is a set of best practices for software management through process improvement, which is achieved by the gradual automation of repetitive tasks supported and sustained by this flexible and adaptable infrastructure, an infrastructure that essentially forms a software production line. In defining the technology infrastructure, ADP describes necessary features rather than specific tools, thus remaining vendor neutral. Only a basic subset of features that are essential for building an effective infrastructure has been selected. Many existing commercial and non-commercial tools support these, as well as more advanced features. Appendix E contains such a list. © 2007 John Wiley & Sons, Inc.",,"Huizinga D., Kolawa A.",2007,Book,Automated Defect Prevention: Best Practices in Software Management,10.1002/9780470165171,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889308306&doi=10.1002%2f9780470165171&partnerID=40&md5=bfd5df64bb2dc17dae584e22372ffce6,,John Wiley and Sons,English,,9780470042120
Scopus,A survey on software estimation in the norwegian industry,"This paper provides an overview of the estimation methods that software companies apply to estimate their projects, why those methods are chosen, and how accurate they are. In order to improve estimation accuracy, such knowledge is essential. We conducted an in-depth survey, where information was collected through structured interviews with senior managers from 18 different companies and project managers of 52 different projects. We analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, delivered functionality and other estimation related information. Our results suggest, for example, that average effort overruns are 41%, that the estimation performance has not changed much the last 10-20 years, that expert estimation is the dominating estimation method, that estimation accuracy is not much impacted by use of formal estimation models, and that software managers tend to believe that the estimation accuracy of their company is better than it actually is. © 2004 IEEE.",,"Moløkken-Østvold K., Jørgensen M., Tanilkan S.S., Gallis H., Lien A.C., Hove S.E.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844303750&doi=10.1109%2fMETRIC.2004.1357904&partnerID=40&md5=016766403cf91e35b87bf9576f6b8650,"Simula Research Laboratory, P.O.Box 134, 1325 Lysaker, Norway; Department of Informatics, P.O.Box 1080 Blindern, 0316 Oslo, Norway",,English,15301435,0769521290
Scopus,Toward a discipline of software engineering,"Despite rapid changes in computing and software development, some fundamental ideas have remained constant. This article describes eight such concepts that together constitute a viable foundation for a software engineering discipline: abstraction, analysis and design methods and notations, user interface prototyping, modularity and architecture, software life cycle and process, reuse, metrics, and automated support.",,Wasserman A.I.,1996,Journal,IEEE Software,10.1109/52.542291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030289540&doi=10.1109%2f52.542291&partnerID=40&md5=a4d9b614723c231def2d845e48c4d9b6,"Development Environments, Inc; IDE; Dept. of Medical Information Science, University of California, San Francisco, CA, United States; Computer Science Division, University of California, Berkeley, CA, United States; UC Berkeley; University of Wisconsin, Madison, WI, United States; ACM, IEEE",,English,07407459,
Scopus,Situational method engineering,"While previously available methodologies for software “ like those published in the early days of object technology “ claimed to be appropriate for every conceivable project, situational method engineering (SME) acknowledges that most projects typically have individual characteristics and situations. Thus, finding the most effective methodology for a particular project needs specific tailoring to that situation. Such a tailored software development methodology needs to take into account all the bits and pieces needed for an organization to develop software, including the software process, the input and output work products, the people involved, the languages used to describe requirements, design, code, and eventually also measures of success or failure. The authors have structured the book into three parts. Part I deals with all the basic concepts, terminology and overall ideas underpinning situational method engineering. As a summary of this part, they present a formal meta-model that enables readers to create their own quality methods and supporting tools. In Part II, they explain how to implement SME in practice, i.e., how to find method components and put them together and how to evaluate the resulting method. For illustration, they also include several industry case studies of customized or constructed processes, highlighting the impact that high-quality engineered methods can have on the success of an industrial software development. Finally, Part III summarizes some of the more recent and forward-looking ideas. This book presents the first summary of the state of the art for SME. For academics, it provides a comprehensive conceptual framework and discusses new research areas. For lecturers, thanks to its step-by-step explanations from basics to the customization and quality assessment of constructed methods, it serves as a solid basis for comprehensive courses on the topic. For industry methodologists, it offers a reference guide on features and technologies to consider when developing in-house software development methods or customising and adopting off-the-shelf ones. © Springer-Verlag Berlin Heidelberg 2014.",,"Henderson-Sellers B., Ralyté J., Ågerfalk P.J., Rossi M.",2014,Book,Situational Method Engineering,10.1007/978-3-642-41467-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955718874&doi=10.1007%2f978-3-642-41467-1&partnerID=40&md5=1cde9f2e3550ce0dc76cc2c60d1aea28,"School of Software, University of Technology, Sydney Broadway, New South Wales, Australia; CUI, Battelle - bâtiment A University of Geneva Carouge, Switzerland; Dept. of Informatics and Media, Uppsala University, Uppsala, Sweden; Aalto University, Aalto, Finland",Springer Berlin Heidelberg,English,,9783642414671; 9783642414664
Scopus,Building a software cost estimation model based on categorical data,"This paper explores the possibility of generating a multi-organisational software cost estimation model by analysing the software cost data collected by the International Software Benchmarking Standards Group. This database contains data about recently developed projects characterised mostly by attributes of categorical nature such as the project business area, organisation type, application domain and usage of certain tools or methods. The generation of the model is based on a statistical technique which has been proposed as alternative to the standard regression approach, namely the categorical regression or regression with optimal scaling. This technique starts with the quantification of the qualitative attributes (expressed either on nominal or ordinal scale), that appear frequently within such data, and proceeds by using the obtained scores as independent variables of a regression model. The generated model is validated by measuring certain indicators of accuracy.",,"Angelis L., Stamelos I., Morisio M.",2001,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034999177&partnerID=40&md5=6593e46644c52060a5d7f84e0a920b84,"Department of Informatics, University of Thessaloniki, Thessaloniki, Greece",,English,,
Scopus,Impact of budget and schedule pressure on software development cycle time and effort,"As excessive budget and schedule compression becomes the norm in today's software industry, an understanding of its impact on software development performance is crucial for effective management strategies. Previous software engineering research has implied a nonlinear impact of schedule pressure on software development outcomes. Borrowing insights from organizational studies, we formalize the effects of budget and schedule pressure on software cycle time and effort as U-shaped functions. The research models were empirically tested with data from a $25 billion/year international technology firm, where estimation bias is consciously minimized and potential confounding variables are properly tracked. We found that controlling for software process, size, complexity, and conformance quality, budget pressure, a less researched construct, has significant U-shaped relationships with development cycle time and development effort. On the other hand, contrary to our prediction, schedule pressure did not display significant nonlinear impact on development outcomes. A further exploration of the sampled projects revealed that the involvement of clients in the software development might have ""eroded"" the potential benefits of schedule pressure. This study indicates the importance of budget pressure in software development. Meanwhile, it implies that achieving the potential positive effect of schedule pressure requires cooperation between clients and software development teams. © 2006 IEEE.",Cost estimation; Schedule and organizational issues; Systems development; Time estimation,"Nan N., Harter D.E.",2009,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2009.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449095757&doi=10.1109%2fTSE.2009.18&partnerID=40&md5=0f987256cd1c9a99b022ad4b952933a0,"Price College of Business, University of Oklahoma, 307 West Brooks, Norman, OK 73019, United States; Whitman School of Management, Syracuse University, 721 University Avenue, Syracuse, NY 13244, United States",,English,00985589,
Scopus,Data sets and data quality in software engineering,"OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels. Copyright 2008 ACM.",Data quality; Data sets; Empirical research; Prediction,"Liebchen G.A., Shepperd M.",2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370788.1370799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049148158&doi=10.1145%2f1370788.1370799&partnerID=40&md5=21ccb9b70d37e6fc5ca9f6c5aeec640b,"Brunei University, United Kingdom",,English,02705257,9781605580364
Scopus,Adapting a fault prediction model to allow inter language reuse,"An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects. Copyright 2008 ACM.",Error prone; Inter language prediction; Metrics; Open source,"Watanabe S., Kaiya H., Kaijiri K.",2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370788.1370794,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049118679&doi=10.1145%2f1370788.1370794&partnerID=40&md5=cc2f84166654ad4eda29d78293d927f1,"Faculty of Engineering, Shinshu University, 4-17-1 Wakasato, Nagano 380-0935, Japan",,English,02705257,9781605580364
Scopus,Software engineering foundations: A software science perspective,"A groundbreaking book in this field, Software Engineering Foundations: A Software Science Perspective integrates the latest research, methodologies, and their applications into a unified theoretical framework. Based on the author's 30 years of experience, it examines a wide range of underlying theories from philosophy, cognitive informatics, denotational mathematics, system science, organization laws, and engineering economics. The book contains in-depth information, annotated references, real-world problems, heuristics, and research opportunities. Highlighting the inherent limitations of the historical programming-language-centered approach, the author explores an interdisciplinary approach to software engineering. He identifies fundamental cognitive, organizational, and resource constraints and the need for multi-faceted and transdisciplinary theories and empirical knowledge. He then synergizes theories, principles, and best practices of software engineering into a unified framework and delineates overarching, durable, and transdisciplinary theories as well as alternative solutions and open issues for further research. The book develops dozens of Wang's laws for software engineering and outlooks the emergence of software science. The author's rigorous treatment of the theoretical framework and his comprehensive coverage of complicated problems in software engineering lay a solid foundation for software theories and technologies. Comprehensive and written for all levels, the book explains a core set of fundamental principles, laws, and a unified theoretical framework. © 2008 by Taylor & Francis Group, LLC.",,Wang Y.,2007,Book,Software Engineering Foundations: A Software Science Perspective,10.1201/9780203496091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903168303&doi=10.1201%2f9780203496091&partnerID=40&md5=b44fcacc264b50ae6a72cec7511b3c26,"Canadian Conferences on Electrical and Computer Engineering (CCECE), International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), CRC book series in Software Engineering, Canada",CRC Press,English,,9780203496091; 9780849319310
Scopus,An effort prediction interval approach based on the empirical distribution of previous estimation accuracy,"When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals') judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach. © 2002 Elsevier Science B.V. All rights reserved.",Effort estimation; Estimation uncertainty; Human judgment; Prediction intervals,"Jørgensen M., Sjøberg D.I.K.",2003,Journal,Information and Software Technology,10.1016/S0950-5849(02)00188-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037349232&doi=10.1016%2fS0950-5849%2802%2900188-X&partnerID=40&md5=edd6f3139077d0f53ba05bf0acdb86f1,"Simula Research Laboratory, P.O. Box 134, Lysaker NO-1325, Norway",,English,09505849,
Scopus,Using genetic programming to improve software effort estimation based on general data sets,"This paper investigates the use of various techniques including genetic programming, with public data sets, to attempt to model and hence estimate software project effort. The main research question is whether genetic programs can offer 'better' solution search using public domain metrics rather than company specific ones. Unlike most previous research, a realistic approach is taken, whereby predictions are made on the basis of the data available at a given date. Experiments are reported, designed to assess the accuracy of estimates made using data within and beyond a specific company. This research also offers insights into genetic programming's performance, relative to alternative methods, as a problem solver in this domain. The results do not find a clear winner but, for this data, GP performs consistently well, but is harder to configure and produces more complex models. The evidence here agrees with other researchers that companies would do well to base estimates on in house data rather than incorporating public data sets. The complexity of the GP must be weighed against the small increases in accuracy to decide whether to use it as part of any effort prediction estimation. © Springer-Verlag Berlin Heidelberg 2003.",,"Lefley M., Shepperd M.J.",2003,Journal,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-45110-2_151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248891530&doi=10.1007%2f3-540-45110-2_151&partnerID=40&md5=b7def995479fb343a425853b69ea24c6,"School of Design Engineering and Computing, University of Bournemouth, Talbot Campus, Poole, BH12 5BB, United Kingdom",Springer Verlag,English,03029743,3540406034; 9783540406037
Scopus,Can neural networks be easily interpreted in software cost estimation?,"Software development effort estimation with the aid of neural networks has generally been viewed with skepticism by a majority of the software cost estimation community. Although, neural networks have shown their strengths in solving complex problems, their shortcoming of being 'black boxes' models has prevented them from being accepted as a common practice for cost estimation. In this paper, we study the interpretation of cost estimation models based on a Backpropagation three-layer Perceptron network. Our proposed idea comprises mainly of the use of a method that maps this neural network to a fuzzy rule-based system. Consequently, if the obtained fuzzy rules are easily interpreted, the neural network will also be easy to interpret. Our case study is based on the COCOMO' 81 dataset.",,"Idri A., Khoshgoftaar T.M., Abran A.",2002,Conference,IEEE International Conference on Fuzzy Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036460230&partnerID=40&md5=249f3b15054a61ab67d30a4a195431d1,"Empirical Software Engineering Lab., Dept. of Comp. Sci. and Eng., Florida Atlantic University, Boca Raton, FL 33431, United States; Software Engineering Mgmt. Lab., Ecole de Technologie Superieure-ETS, 1100, Notre-Dame Ouest, Montréal, Que. H3C 1K3, Canada",,English,10987584,
Scopus,Expert judgement in cost estimating: Modelling the reasoning process,"Expert Judgement (EJ) is used extensively during the generation of cost estimates. Cost estimators have to make numerous assumptions and judgements about what they think a new product will cost. However, the use of EJ is often frowned upon, not well accepted or understood by non-cost estimators within a concurrent engineering environment. Computerised cost models, in many ways, have reduced the need for EJ but by no means have they, or can they, replace it. The cost estimates produced from both algorithmic and non-algorithmic cost models can be widely inaccurate; and, as the work of this paper highlights, require extensive use of judgement in order to produce a meaningful result. Very little research tackles the issues of capturing and integrating EJ and rationale into the cost estimating process. Therefore, this paper presents a case with respect to the wide use of EJ within cost estimating. EJ is examined in terms of what thought processes are used when a judgement is made. This paper highlights that most judgements are based on the results of referring to historical costs data, and then adjusting up or down accordingly in order to predict the cost of a new project. This is often referred to as analogy. The reasoning processes of EJ are identified and an inference structure has been developed, which represents an abstraction of the reasoning steps used by an expert as they generate an estimate. This model has been validated through both literature and interviews with cost estimating experts across industry sectors. Furthermore, the key inferences of the experts are identified. These inferences are considered as those where many of the assumptions and expert judgements are made. The thesis of this paper is that through modelling the reasoning processes of EJ, it becomes possible to capture, structure, and integrate EJ and rationale into the cost estimating process as estimates are being generated. Consequently, the rationale capture will both improve the understanding of estimates throughout a product life cycle, and improve management decisions based upon these cost estimates.",Analogy based cost estimating; Cost estimating knowledge; Cost estimating rationale; Expert judgement; Inference modelling,"Rush C., Roy R.",2001,Journal,Concurrent Engineering Research and Applications,10.1177/1063293x0100900404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035744598&doi=10.1177%2f1063293x0100900404&partnerID=40&md5=73dfa7e268829ff03e31bfb22e3d7bf4,"Department of Enterprise Integration, Sch. of Indust. Manufac. and Sci., Cranfield University, Cranfield, Bedford MK43 OAL, United Kingdom",SAGE Publications Ltd,English,1063293X,
Scopus,Human performance estimating with analogy and regression models: An empirical validation,"Most cost estimation models seem to be validated without testing human performance and using data sets front custom software projects where the software typically is sized in lines of code (SLOC) or function points. From a practitioner's point of view this research seem not to address some important aspects of IT projects that we observe: i) Estimating in an industrial environment is performed by people, not models ii) COTS projects are increasing their market share replacing traditional custom software projects iii) Industrial projects use a large variety of metrics to size the project deliverables and estimate the costs. Estimation by analogy tools like ANGEL and multiple regression analysis provide the necessary flexibility in terms of choice of input parameters. We describe an experiment to evaluate human performance where the subjects were aided by analogy and regression tools respectively. 68 partners and managers in Andersen Consulting estimated 48 different COTS projects. The results in terms of MMRE indicate that users benefit from both tools, however more from regression models than from analogy, models as ANGEL. Furthermore, the performance of the ANGEL tool itself is not superior to the performance of the regression model. This result is contradictory to previous studies that claim that ANGEL outperforms multiple regression.",,"Stensrud Erik, Myrtveit Ingunn",1998,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032289950&partnerID=40&md5=f1e9ee4e5f1e693f83ca5d7486a238c6,"Andersen Consulting, Northbrook, United States","IEEE Comp Soc, Los Alamitos, CA, United States",English,,
Scopus,A software size model,"A major problem of software cost estimation is first obtaining an accurate size estimate of the software to be developed. A bottom-up approach to software size estimation is described, which first identifies factors affecting software size, thus obtaining size explanation equations, and then seeks suitable predictors based on those explanation factors which can be used for size estimation. The approach, or model, is bottom-up in that it sizes individual software components or modules first, and then obtains subsystem and system sizes by summing component sizes. Since components may have different purposes and characteris-tics—e.g., menu, computation, and reporting components—the model allows for the partitioning of system components into several different types, each component type having different size explanation and estimation equations. The approach is similar to function point analysis, which partitions components of business systems (transactions) into five types: inputs, outputs, inquiries, entities (or files), and external interfaces. The partitioning here, however, is not fixed, but depends on the particular software development technology. In this sense the approach can be regarded as a generalization of function point analysis. The model is applied to several different software systems, including both business applications and systems programs. © 1992 IEEE",Function point analysis; lines of code; metrics; product; software metrics; software size estimation,"Verner J., Tate G.",1992,Journal,IEEE Transactions on Software Engineering,10.1109/32.129216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026850596&doi=10.1109%2f32.129216&partnerID=40&md5=b522d62e04920e21d0381b36c05f93d0,"School of Information Systems, University of New South Wales, Kensington, NSW 2033, Australia; Department of Information Systems, City Polytechnic of Hong Kong, Kowloon, Hong Kong",,English,00985589,
Scopus,The consistency of empirical comparisons of regression and analogy-based software project cost prediction,"Objective - to determine the consistency within and between results in empirical studies of software engineering cost estimation. We focus on regression and analogy techniques as these are commonly used. Method - we conducted an exhaustive literature search using predefined inclusion and exclusion criteria and identified 67 journal papers and 104 conference papers. From this sample we identified 11 journal papers and 9 conference papers that used both methods. Results - our analysis found that about 25% of studies were internally inconclusive. We also found that there is approximately equal evidence in favour of, and against analogy-based methods. Conclusions - we confirm the lack of consistency in the findings and argue that this inconsistent pattern from 20 different studies comparing regression and analogy is somewhat disturbing. It suggests that we need to ask more detailed questions than just: ""What is the best prediction system?"" © 2005 IEEE.",,"Mair C., Shepperd M.",2005,Conference,"2005 International Symposium on Empirical Software Engineering, ISESE 2005",10.1109/ISESE.2005.1541858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749062142&doi=10.1109%2fISESE.2005.1541858&partnerID=40&md5=e3f58fa87bfa5b44c2f7fd2da137a90a,"Brunel University, United Kingdom",,English,,0780395085; 9780780395084
Scopus,Predicting with sparse data,"It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. In this paper, we describe our sparse data method (SDM) based upon a pairwise comparison technique and Saaty's Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as Data Salvage. We show, for data from two companies, how our approach-based upon expert judgement-adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that out approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction.",Empirical data; Expert judgement; Prediction; Software project effort; Sparse data,"Shepperd M., Cartwright M.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.965339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035506994&doi=10.1109%2f32.965339&partnerID=40&md5=db3b92aa70efed5056b1fbbaba583f02,"Empirical Software Eng. Res. Group, Sch. of Design Eng. and Computing, Bournemouth University, Talbot Campus, Poole, United Kingdom",,English,00985589,
Scopus,Comparative studies of the model evaluation criterions MMRE and PRED in software cost estimation research,"Software cost model research results depend on model accuracy criteria such as MMRE and PRED. Despite criticism, MMRE has emerged as the de facto standard criterion. Many alternatives have been proposed and studied, surprisingly however PRED, the second most popular criterion, has not been extensively studied. This work attempts to fill this gap in the literature and expand the understanding and use of evaluation criterion in general. The majority of this work is empirically based, applying MMRE and PRED to a number of COCOMO model variations with respect to a simulated data set and four publicly available cost estimation data sets. We replicate a number of results based on MMRE and extend them to PRED. We study qualities of MMRE and PRED as sample estimator statistics for parameters of a cost model error distribution. Standard error is used to ensure greater confidence in replicated and new results based on sample data. Copyright 2008 ACM.",Bootstrapping; Calibration; Confidence; Confidence interval; Cost estimation; Cost model; MMRE; Model selection; Parameters; PRED; Standard error,"Port D., Korte M.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949083282&doi=10.1145%2f1414004.1414015&partnerID=40&md5=678d185824abfdbf4d5664ad7c6999a9,"University of Hawai'i at Manoa, 2404 Maile Way, E303, Honolulu, HI 96822, United States; University of Applied Sciences and Arts Dortmund, Emil-Figge-Str. 42, 44227 Dortmund, Germany",,English,,9781595939715
Scopus,Adaptive fuzzy logic-based framework for software development effort prediction,"Algorithmic effort prediction models are limited by their inability to cope with uncertainties and imprecision present in software projects early in the development life cycle. In this paper, we present an adaptive fuzzy logic framework for software effort prediction. The training and adaptation algorithms implemented in the framework tolerates imprecision, explains prediction rationale through rules, incorporates experts knowledge, offers transparency in the prediction system, and could adapt to new environments as new data becomes available. Our validation experiment was carried out on artificial datasets as well as the COCOMO public database. We also present an experimental validation of the training procedure employed in the framework. © 2004 Elsevier B.V. All rights reserved.",COCOMO; Effort prediction; Fuzzy logic; Soft computing,"Ahmed M.A., Saliu M.O., Alghamdi J.",2005,Journal,Information and Software Technology,10.1016/j.infsof.2004.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444246384&doi=10.1016%2fj.infsof.2004.05.004&partnerID=40&md5=4cfc95f2add91074c094ca712c234f85,"Dept of Info. Science, King Fahd Univ. Petrol. and Minerals, Dhahran 31261, Saudi Arabia; Dept of Computer Science, University of Calgary, 2500 University Drive NW, Calgary, Alta. T2N 1N4, Canada",,English,09505849,
Scopus,Status report on software measurement,"The most successful measurement programs are ones in which researcher, practitioner, and customer work hand in hand to meet goals and solve problems. But such collaboration is rare. The authors explore the gaps between these groups and point toward ways to bridge them.",,"Pfleeger S.L., Jeffery R., Curtis B., Kitchenham B.",1997,Journal,IEEE Software,10.1109/52.582973,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031102213&doi=10.1109%2f52.582973&partnerID=40&md5=5a1f6b8cfd2872f9b9014cdcc802311b,"Systems/Software, Howard University, United States; University of New South Wales, Australia; TeraQuest Metrics; Keele University, United Kingdom; CREST, Howard University, Department of Systems, Washington, DC 20059, United States",,English,07407459,
Scopus,How reliable are systematic reviews in empirical software engineering?,"BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method. © 2010 IEEE.",cost estimation; Empirical software engineering; meta-analysis; systematic review,"MacDonell S., Shepperd M., Kitchenham B., Mendes E.",2010,Review,IEEE Transactions on Software Engineering,10.1109/TSE.2010.28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957749322&doi=10.1109%2fTSE.2010.28&partnerID=40&md5=932e56b39575eaee618188c34cb5f5de,"School of Computing and Mathematical Sciences, Auckland University of Technology, Private Bag 92006, Auckland 1142, New Zealand; Department of IS and Computing, Brunel University, West London, UB8 3PH, United Kingdom; School of Computing and Mathematics, Keele University, Keele, Staffordshire, ST5 5BG, United Kingdom; Department of Computer Science, University of Auckland, Private Bag 92019, Auckland 1142, New Zealand",,English,00985589,
Scopus,Agile Software Development Methodologies and Practices,"Beginning in the mid-1990s, a number of consultants independently created and evolved what later came to be known as agile software development methodologies. Agile methodologies and practices emerged as an attempt to more formally and explicitly embrace higher rates of change in software requirements and customer expectations. Some prominent agile methodologies are Adaptive Software Development, Crystal, Dynamic Systems Development Method, Extreme Programming (XP), Feature-Driven Development (FDD), Pragmatic Programming, and Scrum. This chapter presents the principles that underlie and unite the agile methodologies. Then, 32 practices used in agile methodologies are presented. Finally, three agile methodologies (XP, FDD, and Scrum) are explained. Most often, software development teams select a subset of the agile practices and create their own hybrid software development methodology rather than strictly adhere to all the practices of a predefined agile methodology. Teams that use primarily agile practices are most often small- to medium-sized, colocated teams working on less complex projects. © 2010 Elsevier Inc.",,Williams L.,2010,Book Chapter,Advances in Computers,10.1016/S0065-2458(10)80001-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958546387&doi=10.1016%2fS0065-2458%2810%2980001-4&partnerID=40&md5=072e8fb4b818cdf0776a715ec95b9d33,"Department of Computer Science, North Carolina State University, Raleigh, North Carolina, United States",,English,00652458,
Scopus,Software measurement: Establish - Extract - Evaluate - Execute,"Our world and our society are shaped and increasingly governed by software. Since software is so ubiquitous and embedded in nearly everything we are doing, we need to stay in control. We have to make sure that the systems and their software are running as we intend - or better. Software measurement is the discipline that assures that we stay in control. In this volume, Ebert and Dumke provide a comprehensive introduction to software measurement. They detail knowledge and experiences about software measurement in an easily understood, hands-on presentation. Brief references are embedded from world-renown experts such as Alain Abran, Luigi Buglione, Manfred Bundschuh, David N. Card, Ton Dekkers, Robert L. Glass, David A. Gustafson, Marek Leszak, Peter Liggesmeyer, Andreas Schmietendorf, Harry Sneed, Charles Symons, Ruediger Zarnekow and Horst Zuse. Many examples and case studies are provided from Global 100 companies such as Alcatel-Lucent, Atos Origin, Axa, Bosch, Deloitte, Deutsche Telekom, Shell, Siemens and Vector Consulting. This combination of methodologies and applications makes the book ideally suited for both professionals in the software industry and for scientists looking for benchmarks and experiences. Besides the many practical hints and checklists readers will also appreciate the large reference list, which includes links to metrics communities where project experiences are shared. Further information, continuously updated, can also be found on the Web site related to this book: http://metrics.cs.uni-magdeburg.de/. © Springer-Verlag Berlin Heidelberg 2007. All rights are reserved.",,"Ebert C., Dumke R.",2007,Book,Software Measurement: Establish - Extract - Evaluate - Execute,10.1007/978-3-540-71649-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892096361&doi=10.1007%2f978-3-540-71649-5&partnerID=40&md5=7037f9d9e7f8a5555c7e22dd394d8dea,"Vector Consulting Services, Ingersheimer Strasse 24, 70499 Stuttgart, Germany; Otto-von-Guericke Universität Magdeburg, Postfach 4120, 39016 Magdeburg, Germany",Springer Berlin Heidelberg,English,,9783540716488
Scopus,An empirical study of system design instability metric and design evolution in an agile software process,"Software project tracking and project plan adjustment are two important software engineering activities. The class growth shows the design evolution of the software. The System Design Instability (SDI) metric indicates the progress of an object-oriented (OO) project once the project is set in motion. The SDI metric provides information on project evolution to project managers for possible adjustment to the project plan. The objectives of this paper are to test if the System Design Instability metric can be used to estimate and re-plan software projects in an XP-like agile process and study system design evolution in the Agile software process. We present an empirical study of the class growth and the SDI metric in two OO systems, developed using an agile process similar to Extreme Programming (XP). We analyzed the system evolutionary data collected on a daily basis from the two systems. We concluded that: the systems' class growth follows observable trends, the SDI metric can indicate project progress with certain trends, and the SDI metric is correlated with XP activities. In both of the analyzed systems, we observed two consistent jumps in the SDI metric values in early and late development phases. Part of the results agrees with a previous empirical study in another environment. © 2004 Elsevier Inc. All rights reserved.",Design evolution in agile software process; Empirical study; Extreme programming; System design instability (SDI) metric,"Alshayeb M., Li W.",2005,Journal,Journal of Systems and Software,10.1016/j.jss.2004.02.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344262370&doi=10.1016%2fj.jss.2004.02.002&partnerID=40&md5=394d3985c0ab2645f131add8fd4e9688,"Information Science Department, King Fahd Univ. of Petrol./Minerals, P.O. Box 1172, Dhahran 31261, Saudi Arabia; Computer Science Department, University of Alabama in Huntsville, Huntsville, AL 35899, United States",,English,01641212,
Scopus,Combining techniques to optimize effort predictions in software project management,"This paper tackles two questions related to software effort prediction. First, is it valuable to combine prediction techniques? Second, if so, how? Many commentators have suggested the use of more than one technique in order to support effort prediction, but to date there has been little or no empirical investigation to support this recommendation. Our analysis of effort data from a medical records information system reveals that there is little, or even negative, covariance between the accuracy of our three chosen prediction techniques, namely, expert judgment, least squares regression and case-based reasoning. This indicates that when one technique predicts poorly, one or both of the others tends to perform significantly better. This is a particularly striking result given the relative homogeneity of our data set. Consequently, searching for the single ""best"" technique, at least in this case, leads to a sub-optimal prediction strategy. The challenge then becomes one of identifying a means of determining a priori which prediction technique to use. Unfortunately, despite using a range of techniques including rule induction, we were unable to identify any simple mechanism for doing so. Nevertheless, we believe this remains an important research goal. © 2002 Elsevier Science Inc. All rights reserved.",Empirical analysis; Multiple techniques; Software effort prediction,"MacDonell S.G., Shepperd M.J.",2003,Journal,Journal of Systems and Software,10.1016/S0164-1212(02)00067-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037845141&doi=10.1016%2fS0164-1212%2802%2900067-5&partnerID=40&md5=64d711feaf07f13ae58613170201dd4a,"School of Information Technology, Auckland University of Technology, Private Bag 92006, Auckland 1020, New Zealand; Empirical Software Eng. Res. Group, Sch. of Des., Eng. and Computing, Bournemouth University, Bournemouth BH1 3LT, United Kingdom",,English,01641212,
Scopus,Software Size Estimation of Object-Oriented Systems,"Software size estimation has been the object of a lot of research in the software engineering community due to the need of reliable size estimates in the utilization of existing software project cost estimation models. This paper discusses the strengths and weaknesses of existing size estimation techniques, considers the nature of software size estimation, and presents a software size estimation model which has the potential for providing more accurate size estimates than existing methods. The proposed method takes advantage of a characteristic of object-oriented systems, the natural correspondence between specification and implementation, in order to enable users to come up with better size estimates at early stages of the software development cycle. Through a statistical approach the method also provides a confidence interval for the derived size estimates. The relation between the presented software sizing model and project cost estimation has also been considered. © 1990, IEEE.",Functional specification; object-oriented systems; software cost estimation; software size estimation,Laranjeira L.A.,1990,Journal,IEEE Transactions on Software Engineering,10.1109/32.52774,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025434552&doi=10.1109%2f32.52774&partnerID=40&md5=2526c14b2e7530b147facb17aa7148f3,"Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX 78712, United States",,English,00985589,
Scopus,On the dataset shift problem in software engineering prediction models,"A core assumption of any prediction model is that test data distribution does not differ from training data distribution. Prediction models used in software engineering are no exception. In reality, this assumption can be violated in many ways resulting in inconsistent and non-transferrable observations across different cases. The goal of this paper is to explain the phenomena of conclusion instability through the dataset shift concept from software effort and fault prediction perspective. Different types of dataset shift are explained with examples from software engineering, and techniques for addressing associated problems are discussed. While dataset shifts in the form of sample selection bias and imbalanced data are well-known in software engineering research, understanding other types is relevant for possible interpretations of the non-transferable results across different sites and studies. Software engineering community should be aware of and account for the dataset shift related issues when evaluating the validity of research outcomes. © 2011 Springer Science+Business Media, LLC.",Dataset shift; Defect prediction; Effort estimation; Prediction models,Turhan B.,2012,Journal,Empirical Software Engineering,10.1007/s10664-011-9182-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857374320&doi=10.1007%2fs10664-011-9182-8&partnerID=40&md5=3ad87bbbbf0ff6f35aa0fd21a941b6a5,"Department of Information Processing Science, University of Oulu, POB.3000, Oulu 90014, Finland",,English,13823256,
Scopus,Managed evolution: A strategy for very large information systems,"Many organizations critically depend on very large information systems. In the authors' experience these organizations often struggle to find the right strategy to sustainably develop their systems. Based on their own experience at a major bank, over more than a decade, the authors have developed a successful strategy to deal with these challenges, including: - A thorough analysis of the challenges associated with very large information systems - An assessment of possible strategies for the development of these systems, resulting in managed evolution as the preferred strategy - Describing key system aspects for the success of managed evolution, such as architecture management, integration architecture and infrastructure - Developing the necessary organizational, cultural, governance and controlling mechanisms for successful execution. © Springer-Verlag Berlin Heidelberg 2011.",,"Murer S., Bonati B., Furrer F.J.",2011,Book,Managed Evolution: A Strategy for Very Large Information Systems,10.1007/978-3-642-01633-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892803631&doi=10.1007%2f978-3-642-01633-2&partnerID=40&md5=5c9821462a21e9338fbb8681e7add3aa,"Credit Suisse, 8070 Zürich, Switzerland; Bruno Bonati Consulting, Bernoldweg 4a, 6300 Zug, Switzerland; Guldifuess 3, 8260 Stein-am-Rhein, Switzerland",Springer Berlin Heidelberg,English,,9783642016325
Scopus,Analogy-based software effort estimation using Fuzzy numbers,"Background: Early stage software effort estimation is a crucial task for project bedding and feasibility studies. Since collected data during the early stages of a software development lifecycle is always imprecise and uncertain, it is very hard to deliver accurate estimates. Analogy-based estimation, which is one of the popular estimation methods, is rarely used during the early stage of a project because of uncertainty associated with attribute measurement and data availability. Aims: We have integrated analogy-based estimation with Fuzzy numbers in order to improve the performance of software project effort estimation during the early stages of a software development lifecycle, using all available early data. Particularly, this paper proposes a new software project similarity measure and a new adaptation technique based on Fuzzy numbers. Method: Empirical evaluations with Jack-knifing procedure have been carried out using five benchmark data sets of software projects, namely, ISBSG, Desharnais, Kemerer, Albrecht and COCOMO, and results are reported. The results are compared to those obtained by methods employed in the literature using case-based reasoning and stepwise regression. Results: In all data sets the empirical evaluations have shown that the proposed similarity measure and adaptation techniques method were able to significantly improve the performance of analogy-based estimation during the early stages of software development. The results have also shown that the proposed method outperforms some well know estimation techniques such as case-based reasoning and stepwise regression. Conclusions: It is concluded that the proposed estimation model could form a useful approach for early stage estimation especially when data is almost uncertain. © 2010 Elsevier Inc. All rights reserved.",Cost estimation; Early stage software effort estimation; Estimation by analogy; Generalized Fuzzy numbers; Similarity measurement,"Azzeh M., Neagu D., Cowling P.I.",2011,Journal,Journal of Systems and Software,10.1016/j.jss.2010.09.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650610359&doi=10.1016%2fj.jss.2010.09.028&partnerID=40&md5=ee0b78e013a433303dade6f00045b693,"AI Research Group, Department of Computing, University of Bradford, Bradford BD7 1DP, United Kingdom",,English,01641212,
Scopus,Analysis of attribute weighting heuristics for analogy-based software effort estimation method AQUA+,"Estimation by analogy (EBA) predicts effort for a new project by aggregating effort information of similar projects from a given historical data set. Existing research results have shown that a careful selection and weighting of attributes may improve the performance of the estimation methods. This paper continues along that research line and considers weighting of attributes in order to improve the estimation accuracy. More specifically, the impact of weighting (and selection) of attributes is studied as extensions to our former EBA method AQUA, which has shown promising results and also allows estimation in the case of data sets that have non-quantitative attributes and missing values. The new resulting method is called AQUA+. For attribute weighting, a qualitative analysis pre-step using rough set analysis (RSA) is performed. RSA is a proven machine learning technique for classification of objects. We exploit the RSA results in different ways and define four heuristics for attribute weighting. AQUA+ was evaluated in two ways: (1) comparison between AQUA+ and AQUA, along with the comparative analysis between the proposed four heuristics for AQUA+, (2) comparison of AQUA + with other EBA methods. The main evaluation results are: (1) better estimation accuracy was obtained by AQUA+ compared to AQUA over all six data sets; and (2) AQUA+ obtained better results than, or very close to that of other EBA methods for the three data sets applied to all the EBA methods. In conclusion, the proposed attribute weighing method using RSA can improve the estimation accuracy of EBA method AQUA+ according to the empirical studies over six data sets. Testing more data sets is necessary to get results that are more statistical significant. © 2007 Springer Science+Business Media, LLC.",Attribute weighting; Effort estimation by analogy; Feature selection; Heuristics; Learning; Rough set analysis,"Li J., Ruhe G.",2008,Conference,Empirical Software Engineering,10.1007/s10664-007-9054-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37649001168&doi=10.1007%2fs10664-007-9054-4&partnerID=40&md5=b5932cd9f278a0455c6d521cdb62cfa4,"Software Engineering Decision Support Laboratory, University of Calgary, Calgary, AB T2N1N4, Canada",,English,13823256,
Scopus,Alternative approaches to effort prediction of ERP projects,"There exist many effort prediction systems but none specifically devised for enterprise resource planning (ERP) projects, and the empirical evidence is neither convincing nor adequate from a human user perspective. Consequently, this non-empirical evaluation contributes knowledge by investigating: (i) their applicability to ERP projects, (ii) their added value to a human user beyond making a prediction, and (iii) if they make sense. The analysis suggests that regression analysis seems to be the best choice as an ERP prediction system, and that ANGEL, ACE, CART and OSR primarily add value to a user in exploratory data analysis by their ability to identify similar projects. © 2001 Elsevier Science B.V.",Effort prediction systems; ERP projects; Non-parametric models; Parametric models,Stensrud E.,2001,Journal,Information and Software Technology,10.1016/S0950-5849(01)00147-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035371491&doi=10.1016%2fS0950-5849%2801%2900147-1&partnerID=40&md5=75bb903a78aca9b73c95cb4053b1f1c4,"Stensrud Consulting, Austliveien 30, N-0752, Oslo, Norway",,English,09505849,
Scopus,Predicting the implementation effort of ERP projects: Empirical evidence on SAP/R3,"This paper investigates the impact of the technical size and organizational complexity of SAP/R3 projects on implementation effort. Traditional models for predicting software implementation effort tie measures of code size and programming complexity to development time. ERP projects shift a significant proportion of the implementation effort from code development to the parameterization of a pre-existing software package. At the same time, they move complexity from technical to organizational factors since they force companies to adapt to predefined work processes embedded in the software. This paper redefines the concepts of size and complexity for ERP projects and empirically verifies their impact on implementation effort. Specifically, project size is measured in terms of the number of SAP modules and submodules that are implemented, while complexity is defined as the organizational scope of the project in terms of users involved and overall company size. Hypotheses are tested on 43 SAP/R3 projects conducted in a cross-section of manufacturing companies. The findings show that both the technical size and organizational complexity of projects are relevant drivers of implementation effort. The results indicate that implementation effort not only grows with the number of modules and submodules that are selected for implementation, but that SAP is found to require increasing resources to be implemented in larger companies and for a higher number of users, thus indicating that, while there is a technical component of effort that is independent of the organizational breadth of the project, each user adds an organizational component of costs. © 2001, Association for Information Technology Trust. All rights reserved.",,Francalanci C.,2001,Journal,Journal of Information Technology,10.1080/02683960010035943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035531216&doi=10.1080%2f02683960010035943&partnerID=40&md5=f1c12f48b6877296b0d7eff28bb6e2cf,"Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo Da Vinci, Milano, 20133, Italy",,English,02683962,
Scopus,A new imputation method for small software project data sets,"Effort prediction is a very important issue for software project management. Historical project data sets are frequently used to support such prediction. But missing data are often contained in these data sets and this makes prediction more difficult. One common practice is to ignore the cases with missing data, but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction. The alternative is missing data imputation. There are many imputation methods. Software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets. For this reason we explore using simple methods to impute missing data in small project effort data sets. We propose a class mean imputation (CMI) method based on the k-NN hot deck imputation method (MINI) to impute both continuous and nominal missing data in small data sets. We use an incremental approach to increase the variance of population. To evaluate MINI (and k-NN and CMI methods as benchmarks) we use data sets with 50 cases and 100 cases sampled from a larger industrial data set with 10%, 15%, 20% and 30% missing data percentages respectively. We also simulate Missing Completely at Random (MCAR) and Missing at Random (MAR) missingness mechanisms. The results suggest that the MINI method outperforms both CMI and the k-NN methods. We conclude that this new imputation technique can be used to impute missing values in small data sets. © 2006 Elsevier Inc. All rights reserved.",Class mean imputation; Data imputation; k-NN imputation; Missing data; Software effort prediction,"Song Q., Shepperd M.",2007,Journal,Journal of Systems and Software,10.1016/j.jss.2006.05.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750994891&doi=10.1016%2fj.jss.2006.05.003&partnerID=40&md5=e1b9798f08bc32b66227c0fbab18672f,"Xi'an Jiaotong University, Xi'an, Shaanxi 710049, China; Brunel University, Uxbridge, UB8 3PH, United Kingdom",,English,01641212,
Scopus,Comparison of Web size measures for predicting Web design and authoring effort,"Software practitioners recognise the importance of realistic estimates of efforts for the successful management of software projects, the Web being no exception. Estimates are necessary throughout the whole development life cycle. They are fundamental when bidding for a contract or when determining a project's feasibility in terms of cost-benefit analysis. In addition, they allow project managers and development organisations to manage resources effectively. Size, which can be described in terms of length, functionality and complexity, is often a major determinant of effort. Most effort prediction models to date concentrate on functional measures of size, although length and complexity are also essential aspects of size. A case study evaluation is described, in which size metrics characterising length, complexity and functionality are obtained and used to generate effort prediction models for Web authoring and design. The comparison of these size metrics as effort predictors is described by generating corresponding prediction models, and their accuracy is compared using boxplots of the residuals. Results suggest that in general all categories present a similar prediction accuracy.",,"Mendes E., Mosley N., Counsell S.",2002,Journal,IEE Proceedings: Software,10.1049/ip-sen:20020337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036601388&doi=10.1049%2fip-sen%3a20020337&partnerID=40&md5=787cc597fc3ae85e9a5528f05a0651a7,"Department of Computer Science, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,14625970,
Scopus,A comparison of development effort estimation techniques for Web hypermedia applications,"Several studies have compared the prediction accuracy of different types of techniques with emphasis placed on linear and stepwise regressions, and case-based reasoning (CBR). We believe the use of only one type of CBR technique may bias the results, as there are others that can also be used for effort prediction. This paper has two objectives. The first is to compare the prediction accuracy of three CBR techniques to estimate the effort to develop Web hypermedia applications. The second objective is to compare the prediction accuracy of the best CBR technique, according to our findings, against three commonly used prediction models, namely multiple linear regression, stepwise regression and regression trees. One dataset was used in the estimation process and the results showed that different measures of prediction accuracy gave different results. MMRE and MdMRE showed better prediction accuracy for multiple regression models whereas box plots showed better accuracy for CBR. © 2002 IEEE.",Accuracy; Application software; Computer science; Educational institutions; Linear regression; Machine learning; Predictive models; Project management; Regression tree analysis; Resource management,"Mendes E., Watson I., Triggs C., Mosley N., Counsell S.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943185827&doi=10.1109%2fMETRIC.2002.1011332&partnerID=40&md5=bfeb07a1ddadd0f0d12cc8f0d265e3b1,"Computer Science Department, University of Auckland, Auckland, New Zealand; Statistics Department, University of Auckland, Auckland, New Zealand; MxM Technology, P.O. Box 3539, Shortland Street, Auckland, New Zealand; Computer Science Department, Birkbeck College, University of London, London, United Kingdom",IEEE Computer Society,English,15301435,0769513395
Scopus,Software metrics: An overview of recent results,"The groundwork for software metrics was established in the seventies, and from these earlier works, interesting results have emerged in the eighties. Over 120 of the many publications on software metrics that have appeared since 1980 are classified and presented in five tables that comprise, respectively, 1. (1) the use of classic metrics, 2. (2) a description of new metrics, 3. (3) software metrics through the life cycle, 4. (4) code metrics and popular programming languages, and 5. (5) various metric-based estimation models. © 1987.",,"Côté V., Bourque P., Oligny S., Rivard N.",1988,Journal,The Journal of Systems and Software,10.1016/0164-1212(88)90005-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023982931&doi=10.1016%2f0164-1212%2888%2990005-2&partnerID=40&md5=68666b5231d5835da153ed90d24f5b4d,"Université de Sherbrooke, Que., Canada",,English,01641212,
Scopus,Six strategies for generalizing software engineering theories,"General theories of software engineering must balance between providing full understanding of a single case and providing partial understanding of many cases. In this paper we argue that for theories to be useful in practice, they should give sufficient understanding of a sufficiently large class of cases, without having to be universal or complete. We provide six strategies for developing such theories of the middle range. In lab-to-lab strategies, theories of laboratory phenomena are developed and generalized to other laboratory phenomena. This is a characteristic strategy for basic science. In lab-to-field strategies, theories are developed of artifacts that first operate under idealized laboratory conditions, which are then scaled up until they can operate under uncontrolled field conditions. This is the characteristic strategy for the engineering sciences. In case-based strategies, we generalize about components of real-world cases, that are supposed to exhibit less variation than the cases as a whole. In sample-based strategies, we generalize about the aggregate behavior of samples of cases, which can exhibit patterns not visible at the case level. We discuss three examples of sample-based strategies. Throughout the paper, we use examples of theories and generalization strategies from software engineering to illustrate our analysis. The paper concludes with a discussion of related work and implications for empirical software engineering research. © 2014 Elsevier B.V.",Architectural mechanisms; External validity; Generalization; Scaling up; Statistical inference,"Wieringa R., Daneva M.",2015,Conference,Science of Computer Programming,10.1016/j.scico.2014.11.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925075873&doi=10.1016%2fj.scico.2014.11.013&partnerID=40&md5=cb48cb2cfa5189ebe203a97ea8479d08,"University of Twente, Netherlands",Elsevier,English,01676423,
Scopus,Real-Time Systems Design and Analysis: Tools for the Practitioner,"The leading text in the field explains step by step how to write software that responds in real time From power plants to medicine to avionics, the world increasingly depends on computer systems that can compute and respond to various excitations in real time. The Fourth Edition of Real-Time Systems Design and Analysis gives software designers the knowledge and the tools needed to create real-time software using a holistic, systems-based approach. The text covers computer architecture and organization, operating systems, software engineering, programming languages, and compiler theory, all from the perspective of real-time systems design. The Fourth Edition of this renowned text brings it thoroughly up to date with the latest technological advances and applications. This fully updated edition includes coverage of the following concepts: Multidisciplinary design challenges Time-triggered architectures Architectural advancements Automatic code generation Peripheral interfacing Life-cycle processes The final chapter of the text offers an expert perspective on the future of real-time systems and their applications. The text is self-contained, enabling instructors and readers to focus on the material that is most important to their needs and interests. Suggestions for additional readings guide readers to more in-depth discussions on each individual topic. In addition, each chapter features exercises ranging from simple to challenging to help readers progressively build and fine-tune their ability to design their own real-time software programs. Now fully up to date with the latest technological advances and applications in the field, Real-Time Systems Design and Analysis remains the top choice for students and software engineers who want to design better and faster real-time systems at minimum cost. © 2012 the Institute of Electrical and Electronics Engineers, Inc.",,"Laplante P.A., Ovaska S.J.",2011,Book,Real-Time Systems Design and Analysis: Tools for the Practitioner,10.1002/9781118136607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891584397&doi=10.1002%2f9781118136607&partnerID=40&md5=32f20adf4483a050e512d6eb3f2cf9ce,"Hyvinkää, Finland",John Wiley and Sons,English,,9780470768648
Scopus,"Verification, Validation, and Testing of Engineered Systems","Systems' Verification Validation and Testing (VVT) are carried out throughout systems' lifetimes. Notably, quality-cost expended on performing VVT activities and correcting system defects consumes about half of the overall engineering cost. Verification, Validation and Testing of Engineered Systems provides a comprehensive compendium of VVT activities and corresponding VVT methods for implementation throughout the entire lifecycle of an engineered system. In addition, the book strives to alleviate the fundamental testing conundrum, namely: What should be tested? How should one test? When should one test? And, when should one stop testing? In other words, how should one select a VVT strategy and how it be optimized? The book is organized in three parts: The first part provides introductory material about systems and VVT concepts. This part presents a comprehensive explanation of the role of VVT in the process of engineered systems (Chapter-1). The second part describes 40 systems' development VVT activities (Chapter-2) and 27 systems' post-development activities (Chapter-3). Corresponding to these activities, this part also describes 17 non-testing systems' VVT methods (Chapter-4) and 33 testing systems' methods (Chapter-5). The third part of the book describes ways to model systems' quality cost, time and risk (Chapter-6), as well as ways to acquire quality data and optimize the VVT strategy in the face of funding, time and other resource limitations as well as different business objectives (Chapter-7). Finally, this part describes the methodology used to validate the quality model along with a case study describing a system's quality improvements (Chapter-8). Fundamentally, this book is written with two categories of audience in mind. The first category is composed of VVT practitioners, including Systems, Test, Production and Maintenance engineers as well as first and second line managers. The second category is composed of students and faculties of Systems, Electrical, Aerospace, Mechanical and Industrial Engineering schools. This book may be fully covered in two to three graduate level semesters; although parts of the book may be covered in one semester. University instructors will most likely use the book to provide engineering students with knowledge about VVT, as well as to give students an introduction to formal modeling and optimization of VVT strategy. © 2010 John Wiley & Sons, Inc.",,Engel A.,2010,Book,"Verification, Validation, and Testing of Engineered Systems",10.1002/9780470618851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891584518&doi=10.1002%2f9780470618851&partnerID=40&md5=25e33b89758fbd3b9253df25d5c947cf,,John Wiley and Sons,English,,9780470527511
Scopus,Applying moving windows to software effort estimation,"Models for estimating software development effort are commonly built and evaluated using a set of historical projects. An important question is which projects to use as training data to build the model: should it be all of them, or a subset that seems particularly relevant? One factor to consider is project age: is it best to use the entire history of past projects, or is it more appropriate in a rapidly changing world to use a window of recent projects? We investigate the effect on estimation accuracy of using a moving window, using projects from the ISBSG data set. We find that using a moving window can improve accuracy, and we make some observations about factors that influence the range of possible window sizes and the best window size. © 2009 IEEE.",,"Lokan C., Mendes E.",2009,Conference,"2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",10.1109/ESEM.2009.5316019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449121808&doi=10.1109%2fESEM.2009.5316019&partnerID=40&md5=8933596e9513711b1bb0e5cea7eed039,"School of Engineering and IT, UNSW and ADFA, ACT, Australia; Department of Computer Science, University of Auckland, Auckland, New Zealand",,English,,9781424448418
Scopus,Assessing effort estimation models for corrective maintenance through empirical studies,"We present an empirical assessment and improvement of the effort estimation model for corrective maintenance adopted in a major international software enterprise. Our study was composed of two phases. In the first phase we used multiple linear regression analysis to construct effort estimation models validated against real data collected from five corrective maintenance projects. The model previously adopted by the subject company used as predictors the size of the system being maintained and the number of maintenance tasks. While this model was not linear, we show that a linear model including the same variables achieved better performances. Also we show that greater improvements in the model performances can be achieved if the types of the different maintenance tasks is taken into account. In the second phase we performed a replicated assessment of the effort prediction models built in the previous phase on a new corrective maintenance project conducted by the subject company on a software system of the same type as the systems of the previous maintenance projects. The data available for the new project were finer grained, according to the indications devised in the first study. This allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made. The new data also provided other useful indications to better understand the maintenance process of the company in a quantitative way. © 2004 Elsevier B.V. All rights reserved.",Corrective software maintenance; Cost estimation models; Experimentation; Management; Measurement,"De Lucia A., Pompella E., Stefanucci S.",2005,Journal,Information and Software Technology,10.1016/j.infsof.2004.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444284317&doi=10.1016%2fj.infsof.2004.05.002&partnerID=40&md5=f3ce1171e45763b3c51d5e78a9deb234,"Dipartimento di Matematica, University of Salerno, Via Ponte don Melillo, 84084 Fisciano (SA), Italy; EDS Italla Software S.p.A., Viale Edison, Loc. Lo Uttaro, Caserta 81100, Italy; Department of Engineering, University of Sannio, Palazzo Bosco Lucarelli, Piazza Roma, 82100 Benevento, Italy",,English,09505849,
Scopus,Using tabu search to configure support vector regression for effort estimation,"Recent studies have reported that Support Vector Regression (SVR) has the potential as a technique for software development effort estimation. However, its prediction accuracy is heavily influenced by the setting of parameters that needs to be done when employing it. No general guidelines are available to select these parameters, whose choice also depends on the characteristics of the dataset being used. This motivated the work described in (Corazza et al. 2010), extended herein. In order to automatically select suitable SVR parameters we proposed an approach based on the use of the meta-heuristics Tabu Search (TS). We designed TS to search for the parameters of both the support vector algorithm and of the employed kernel function, namely RBF. We empirically assessed the effectiveness of the approach using different types of datasets (single and cross-company datasets, Web and not Web projects) from the PROMISE repository and from the Tukutuku database. A total of 21 datasets were employed to perform a 10-fold or a leave-one-out cross-validation, depending on the size of the dataset. Several benchmarks were taken into account to assess both the effectiveness of TS to set SVR parameters and the prediction accuracy of the proposed approach with respect to widely used effort estimation techniques. The use of TS allowed us to automatically obtain suitable parameters' choices required to run SVR. Moreover, the combination of TS and SVR significantly outperformed all the other techniques. The proposed approach represents a suitable technique for software development effort estimation. © 2011 Springer Science+Business Media, LLC.",Effort estimation; Search based techniques; Support vector regression; Tabu search,"Corazza A., Di Martino S., Ferrucci F., Gravino C., Sarro F., Mendes E.",2013,Journal,Empirical Software Engineering,10.1007/s10664-011-9187-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878701385&doi=10.1007%2fs10664-011-9187-3&partnerID=40&md5=713b28779f2a4b21e5f1884155d0259a,"University of Napoli Federico II, Via Cintia, 80126 Naples, Italy; University of Salerno, Via Ponte don Melillo, 84084 Fisciano (SA), Italy; Zayed University, P.O. Box 18292, Dubai, United Arab Emirates",,English,13823256,
Scopus,Evaluating the learning effectiveness of using simulations in software project management education: Results from a twice replicated experiment,"The increasing demand for software project managers in industry requires strategies for the development of management-related knowledge and skills of the current and future software workforce. Although several educational approaches help to develop the necessary skills in a university setting, few empirical studies are currently available to characterise and compare their effects. This paper presents the results of a twice replicated experiment that evaluates the learning effectiveness of using a process simulation model for educating computer science students in software project management. While the experimental group applied a System Dynamics simulation model, the control group used the well-known COCOMO model as a predictive tool for project planning. The results of each empirical study indicate that students using the simulation model gain a better understanding about typical behaviour patterns of software development projects. The combination of the results from the initial experiment and the two replications with meta-analysis techniques corroborates this finding. Additional analysis shows that the observed effect can mainly be attributed to the use of the simulation model in combination with a web-based role-play scenario. This finding is strongly supported by information gathered from the debriefing questionnaires of subjects in the experimental group. They consistently rated the simulation-based role-play scenario as a very useful approach for learning about issues in software project management. © 2003 Elsevier B.V. All rights reserved.",COCOMO; Learning effectiveness; Replicated experiment; Software project management education; System dynamics simulation,"Pfahl D., Laitenberger O., Ruhe G., Dorsch J., Krivobokova T.",2004,Journal,Information and Software Technology,10.1016/S0950-5849(03)00115-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346973020&doi=10.1016%2fS0950-5849%2803%2900115-0&partnerID=40&md5=8154bd1b4ad6be655f2cc1617c1be162,"Fraunhofer IESE, Sauerwiesen 6, 67661 Kaiserslautern, Germany; Droege and Comp., Praterinsel 3-4, 80538 München, Germany; University of Calgary, 2500 University Drive NW, Calgary, Alta., Canada; Accenture, Campus Kronberg 1, 61476 Kronberg, Germany; University of Bielefeld, Universitätsstraße 25, 33615 Bielefeld, Germany",,English,09505849,
Scopus,Organizational benchmarking using the ISBSG data repository,"The International Software Bencharking Standards Group (ISBSG) maintains a repository of data from numerous organizations' completed software projects. In particular, the repository has provided research data on several topics, including function points structure, project duration, and cost estimation. A software benhmarking experiment performed by the ISBSG determined whether using anonymous data provides any valuable information to an organization. The organization's completed projects are compared to similar projects in a public data repository to establish averages for the organization and the industry as a whole.",,"Lokan C., Wright T., Hill P.R., Stringer M.",2001,Journal,IEEE Software,10.1109/52.951491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035448030&doi=10.1109%2f52.951491&partnerID=40&md5=bc0b96cf14927a3189c55461242f9b42,"University of New South Wales, Australia; International Software Benchmarking Standards Group; Software Engineering Management, United States",,English,07407459,
Scopus,Performance evaluation of general and company specific models in software development effort estimation,"In this paper we present the results of our effort estimation analysis of a European Space Agency database consisting of 108 software development projects. We develop and evaluate simple empirical effort estimation models that include only those productivity factors found to be significant for these projects and determine if models based on a multicompany database can be successfully used to make effort estimations within a specific company. This was accomplished by developing company specific effort estimation models based on the significant productivity factors of a particular company and by comparing the results with those from general ESA models on a holdout sample of the company. To our knowledge, no other published research has yet developed and analysed software development effort estimation models in this way. Effort predictions made on a holdout sample of the individual company's projects using general models were less accurate than the company specific model. However, it is likely that in the absence of enough resources and data for a company to develop its own model, the application of general models may be more accurate than the use of guessing and intuition.",,"Maxwell K., Van Wassenhove L., Dutta S.",1999,Journal,Management Science,10.1287/mnsc.45.6.787,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032664427&doi=10.1287%2fmnsc.45.6.787&partnerID=40&md5=a1af0a56f89b5fdedc4179eddba0356e,"INSEAD, Boulevard de Constance, 77305 Fontainebleau Cedex, France","INFORMS, Linthicum",English,00251909,
Scopus,Software effort estimation as a multiobjective learning problem,"Ensembles of learning machines are promising for software effort estimation (SEE), but need to be tailored for this task to have their potential exploited. A key issue when creating ensembles is to produce diverse and accurate base models. Depending on how differently different performance measures behave for SEE, they could be used as a natural way of creating SEE ensembles. We propose to view SEE model creation as a multiobjective learning problem. A multiobjective evolutionary algorithm (MOEA) is used to better understand the tradeoff among different performancemeasures by creating SEE models through the simultaneous optimisation of these measures.We show that the performance measures behave very differently, presenting sometimes even opposite trends. They are then used as a source of diversity for creating SEE ensembles. A good tradeoff among different measures can be obtained by using an ensemble of MOEA solutions. This ensemble performs similarly or better than a model that does not consider these measures explicitly. Besides, MOEA is also flexible, allowing emphasis of a particular measure if desired. In conclusion, MOEA can be used to better understand the relationship among performance measures and has shown to be very effective in creating SEE models. © 2013 ACM.",Ensembles of learning machines; Multiobjective evolutionary algorithms; Software effort estimation,"Minku L.L., Yao X.",2013,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/2522920.2522928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884946446&doi=10.1145%2f2522920.2522928&partnerID=40&md5=798b2a715c8c74332266007ab4bb7c95,"Centre of Excellence for Research in Computational Intelligence and Applications, School of Computer Science, University of Birmingham, Edgbaston, Birmingham, B15 2TT, United Kingdom",,English,1049331X,
Scopus,Categorical missing data imputation for software cost estimation by multinomial logistic regression,"A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate the possibility of using such a method for estimating categorical missing values in software cost databases. Specifically, the method known as multinomial logistic regression (MLR) is suggested for imputation and is applied on projects of the ISBSG multi-organizational software database. Comparisons of MLR with other techniques for handling missing data, such as listwise deletion (LD), mean imputation (MI), expectation maximization (EM) and regression imputation (RI) under different patterns and percentages of missing data, show the high efficiency of the proposed method. © 2005 Elsevier Inc. All rights reserved.",Cost estimation; Imputation; Missing data; Multinomial logistic regression; Software effort prediction,"Sentas P., Angelis L.",2006,Journal,Journal of Systems and Software,10.1016/j.jss.2005.02.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644682829&doi=10.1016%2fj.jss.2005.02.026&partnerID=40&md5=b9a94450744808c87dcfe2d897268bd2,"Department of Informatics, Aristotle University of Thesaloniki, Thesaloniki 54124, Greece",,English,01641212,
Scopus,Validation of an approach for improving existing measurement frameworks,"Software organizations are in need of methods to understand, structure, and improve the data they are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization 1, 2. The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto Laboratory. The top-down method was applied to improve the customer satisfaction (CUSTSAT) measurement from the point of view of three data user groups. It identified several new metrics for the interviewed groups, and also contributed to better understanding the data user needs. The bottom-up method was used to gain new insights into the existing CUSTSAT data. Unexpected associations between key variables prompted new business insights, and revealed problems with the process used to collect and analyze the CUSTSAT data. This paper uses the case study and its results to qualitatively compare our approach against current ad hoc practices used to improve existing measurement frameworks.",,"Mendonça M.G., Basili V.R.",2000,Journal,IEEE Transactions on Software Engineering,10.1109/32.852739,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034206537&doi=10.1109%2f32.852739&partnerID=40&md5=35d612624b639d4b29d32cd8ac93ad56,"Computer Networks Research Group, Salvador University (UNIFACS), Avenue Cardeal da Suva 747, Salvador, Ba, Brazil; Fraunhofer Center for Experimental Software Engineering and the Department of Computer Science, University of Maryland, College Park, MD 20742, United States",,English,00985589,
Scopus,Improving the Reliability of Function Point Measurement: An Empirical Study,"Information Systems development has operated for virtually its entire history without the quantitative measurement capability of other business functional areas such as marketing or manufacturing. Today, managers of information systems organizations are increasingly taken to task to measure and report, in quantitative terms, the effectiveness and efficiency of their internal operations. In addition, measurement of information systems development products is also an issue of increasing importance due to the growing costs associated with information systems development and maintenance. One measure of the size and complexity of information systems that is growing in acceptance and adoption is Function Points, a user-oriented, nonsource line of code metric of the systems development product. Recent previous research has documented the degree of reliability of Function Points as a metric. This research extends that work by a) identifying the major sources of variation through a survey of current practice, and b) estimating the magnitude of the effect of these sources of variation using detailed case study data from actual commercial systems. The results of this research show that a relatively small number of factors has the greatest potential for affecting reliability, and recommendations are made for using these results to improve the reliability of Function Point counting in organizations. © 1992 IEEE",Estimation; function points; management; measurement; performance; productivity evaluation; project control; project planning; reliability,"Kemerer C.F., Porter B.S.",1992,Journal,IEEE Transactions on Software Engineering,10.1109/32.177370,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026943261&doi=10.1109%2f32.177370&partnerID=40&md5=cf2a809d76007e35fc092c5db5bbe242,"MIT, Cambridge, MA, 02139, United States; Andersen Consulting, Boston, MA, 0210, United States",,English,00985589,
Scopus,From Origami to software development: A review of studies on judgment-based predictions of performance time,"This article provides an integrative review of the literature on judgment-based predictions of performance time, often described as task duration predictions in psychology and as expert-based effort estimation in engineering and management science. We summarize results on the characteristics of performance time predictions, processes and strategies, the influence of task characteristics and contextual factors, and the relations between estimates and characteristics of the estimator. Although dependent on the type of study and the level of analysis, underestimation was more frequently reported than overestimation in studies from the engineering and management literature. However, this was not the case in studies from the psychology literature. Our summaries challenge earlier results regarding the effects of factors such as complexity/difficulty and experience. We also question the recurrent finding that small tasks are overestimated and large tasks are underestimated, as this to some extent can be a statistical artifact caused by random error. Several other influences on predictions are identified and discussed. These include various types of anchoring effects, performance and accuracy incentives, task decomposition, request formats, group estimation, revisions of initial ideal or incomplete estimates, level of abstraction, and superficial cues. We summarize similarities and differences between performance time predictions (e.g., number of work hours) and completion time predictions (e.g., delivery dates) because many studies fail to distinguish between these 2 types of predictions. Finally, we discuss methodological issues in time prediction research and implications for research and application. © 2011 American Psychological Association.",Effort estimation; Performance time; Task duration; Time prediction,"Halkjelsvik T., Jørgensen M.",2012,Journal,Psychological Bulletin,10.1037/a0025996,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860568975&doi=10.1037%2fa0025996&partnerID=40&md5=514c67c09b1b211bbb5ba21c54da7301,"Department of Psychology, University of Oslo, Oslo, Norway; Simula Research Laboratory, Department of Informatics, University of Oslo, Lysaker, Norway",,English,00332909,
Scopus,Software effort estimation terminology: The tower of Babel,"It is well documented that the software industry suffers from frequent cost overruns. A contributing factor is, we believe, the imprecise estimation terminology in use. A lack of clarity and precision in the use of estimation terms reduces the interpretability of estimation accuracy results, makes the communication of estimates difficult, and lowers the learning possibilities. This paper reports on a structured review of typical software effort estimation terminology in software engineering textbooks and software estimation research papers. The review provides evidence that the term 'effort estimate' is frequently used without sufficient clarification of its meaning, and that estimation accuracy is often evaluated without ensuring that the estimated and the actual effort are comparable. Guidelines are suggested on how to reduce this lack of clarity and precision in terminology. © 2005 Elsevier B.V. All rights reserved.",Software effort estimation; Software estimation guidelines; Structured review; Terminology,"Grimstad S., Jørgensen M., Moløkken-Østvold K.",2006,Review,Information and Software Technology,10.1016/j.infsof.2005.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33344478479&doi=10.1016%2fj.infsof.2005.04.004&partnerID=40&md5=16d67414d41893aa15fa6242c9ad37e3,"Simula Research Laboratory, P.O. Box 134, NO 1325 Lysaker, Norway",Elsevier,English,09505849,
Scopus,A metrics-based software maintenance Effort Model,"We derive a model for estimating adaptive software maintenance effort in person hours, the Adaptive Maintenance Effort Model (AMEffMo). A number of metrics such as lines of code changed and number of operators changed were found to be strongly correlated to maintenance effort. The regression models performed well in predicting adaptive maintenance effort as well as provide useful information for managers and maintainers.",,"Hayes J.H., Patel S.C., Zhao L.",2004,Conference,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042735332&partnerID=40&md5=151591ea5f6f2799d4c394d70d9bce0e,"Computer Science Department, Lab. for Advanced Networking, University of Kentucky, United States; Computer Science Department, University of Louisville, United States; Computer Science Department, University of Kentucky, United States",,English,15345351,
Scopus,Systematic literature review of ensemble effort estimation,"The need to overcome the weaknesses of single estimation techniques for prediction tasks has given rise to ensemble methods in software development effort estimation (SDEE). An ensemble effort estimation (EEE) technique combines several of the single/classical models found in the SDEE literature. However, to the best of our knowledge, no systematic review has yet been performed with a focus on the use of EEE techniques in SDEE. The purpose of this review is to analyze EEE techniques from six viewpoints: single models used to construct ensembles, ensemble estimation accuracy, rules used to combine single estimates, accuracy comparison of EEE techniques with single models, accuracy comparison between EEE techniques and methodologies used to construct ensemble methods. We performed a systematic review of EEE studies published between 2000 and 2016, and we selected 24 of them to address the questions raised in this review. We found that EEE techniques may be separated into two types: homogeneous and heterogeneous, and that the machine learning single models are the most frequently employed in constructing EEE techniques. We also found that EEE techniques usually yield acceptable estimation accuracy, and in fact are more accurate than single models. © 2016 Elsevier Inc. All rights reserved.",Ensemble effort estimation; Software development effort estimation; Systematic literature review,"Idri A., Hosni M., Abran A.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2016.05.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969531836&doi=10.1016%2fj.jss.2016.05.016&partnerID=40&md5=4217c2107911348615a692540959501e,"Software Projects Management Research Team, ENSIAS, Mohammed v University in Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",Elsevier Inc.,English,01641212,
Scopus,On the application of genetic programming for software engineering predictive modeling: A systematic review,"The objective of this paper is to investigate the evidence for symbolic regression using genetic programming (GP) being an effective method for prediction and estimation in software engineering, when compared with regression/machine learning models and other comparison groups (including comparisons with different improvements over the standard GP algorithm). We performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables. A total of 23 primary studies were obtained after searching different information sources in the time span 1995-2008. The results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling: (i) Software quality classification (eight primary studies). (ii) Software cost/effort/size estimation (seven primary studies). (iii) Software fault prediction/software reliability growth modeling (eight primary studies). While there is evidence in support of using genetic programming for software quality classification, software fault prediction and software reliability growth modeling; the results are inconclusive for software cost/effort/size estimation. © 2011 Elsevier Ltd. All rights reserved.",Genetic programming; Modeling; Symbolic regression; Systematic review,"Afzal W., Torkar R.",2011,Review,Expert Systems with Applications,10.1016/j.eswa.2011.03.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955628180&doi=10.1016%2fj.eswa.2011.03.041&partnerID=40&md5=94df3dda632c91f545ac18da0d394d6e,"Blekinge Institute of Technology, S-371 79 Karlskrona, Sweden",,English,09574174,
Scopus,Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review,"Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies. © 2015 Elsevier Inc. All rights reserved.",Feature extractions; Natural language requirements; Requirements reuse; Software product lines; Systematic literature review,"Bakar N.H., Kasirun Z.M., Salleh N.",2015,Journal,Journal of Systems and Software,10.1016/j.jss.2015.05.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930795477&doi=10.1016%2fj.jss.2015.05.006&partnerID=40&md5=d3a7c41e22f6cad85abd068fc4b4feaf,"Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; Department of ICT, Centre for Foundation Studies, International Islamic University Malaysia, Petaling Jaya Selangor, 46350, Malaysia; Department of Computer Science, Kulliyyah of Information and Communication Technology, International Islamic University Malaysia, Jalan Gombak, Kuala Lumpur, 53100, Malaysia",Elsevier Inc.,English,01641212,
Scopus,An empirical analysis of data preprocessing for machine learning-based software cost estimation,"Context Due to the complex nature of software development process, traditional parametric models and statistical methods often appear to be inadequate to model the increasingly complicated relationship between project development cost and the project features (or cost drivers). Machine learning (ML) methods, with several reported successful applications, have gained popularity for software cost estimation in recent years. Data preprocessing has been claimed by many researchers as a fundamental stage of ML methods; however, very few works have been focused on the effects of data preprocessing techniques. Objective This study aims for an empirical assessment of the effectiveness of data preprocessing techniques on ML methods in the context of software cost estimation. Method In this work, we first conduct a literature survey of the recent publications using data preprocessing techniques, followed by a systematic empirical study to analyze the strengths and weaknesses of individual data preprocessing techniques as well as their combinations. Results Our results indicate that data preprocessing techniques may significantly influence the final prediction. They sometimes might have negative impacts on prediction performance of ML methods. Conclusion In order to reduce prediction errors and improve efficiency, a careful selection is necessary according to the characteristics of machine learning methods, as well as the datasets used for software cost estimation. © 2015 Elsevier B.V.",Case selection; Data preprocessing; Feature selection; Missing-data treatments; Scaling; Software cost estimation,"Huang J., Li Y.-F., Xie M.",2015,Conference,Information and Software Technology,10.1016/j.infsof.2015.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942029324&doi=10.1016%2fj.infsof.2015.07.004&partnerID=40&md5=72d28a260c241425df88a0df0548598a,"Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, Hong Kong; Department of Industrial Engineering, CentraleSupelec, Paris, France",Elsevier,English,09505849,
Scopus,Refining the systematic literature review process-two participant-observer case studies,"Systematic literature reviews (SLRs) are a major tool for supporting evidence-based software engineering. Adapting the procedures involved in such a review to meet the needs of software engineering and its literature remains an ongoing process. As part of this process of refinement, we undertook two case studies which aimed 1) to compare the use of targeted manual searches with broad automated searches and 2) to compare different methods of reaching a consensus on quality. For Case 1, we compared a tertiary study of systematic literature reviews published between January 1, 2004 and June 30, 2007 which used a manual search of selected journals and conferences and a replication of that study based on a broad automated search. We found that broad automated searches find more studies than manual restricted searches, but they may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers, or they are assessing research trends in research methodologies. For Case 2, we analyzed the process used to evaluate the quality of SLRs. We conclude that if quality evaluation of primary studies is a critical component of a specific SLR, assessments should be based on three independent evaluators incorporating at least two rounds of discussion. © 2010 Springer Science+Business Media, LLC.",Automated search; Broad search; Case study; Manual search; Mapping studies; Quality evaluation process; Systematic literature review; Targeted search,"Kitchenham B.A., Brereton P., Turner M., Niazi M.K., Linkman S., Pretorius R., Budgen D.",2010,Journal,Empirical Software Engineering,10.1007/s10664-010-9134-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956766628&doi=10.1007%2fs10664-010-9134-8&partnerID=40&md5=0a36377a6a3c91dda500a0314afbf3d5,"School of Computing and Mathematics, Keele University, Stoke-on-Trent ST5 5BG, United Kingdom; School of Engineering and Computing Sciences, Durham University, Durham DH1 3LE, United Kingdom",,English,13823256,
Scopus,Predicting project velocity in XP using a learning dynamic Bayesian network model,"Bayesian networks, which can combine sparse data, prior assumptions and expert judgment into a single causal model, have already been used to build software effort prediction models. We present such a model of an Extreme Programming environment and show how it can learn from project data in order to make quantitative effort predictions and risk assessments without requiring any additional metrics collection program. The model's predictions are validated against a real world industrial project, with which they are in good agreement. © 2009 IEEE.",Bayesian nets; Causal models; Extreme programming; Risk assessment,"Hearty P., Fenton N., Marquez D., Neil M.",2009,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2008.76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449107898&doi=10.1109%2fTSE.2008.76&partnerID=40&md5=616aab1fc71627a5d30b79894d6ec1e5,"Queen Mary University of London-Computer Science, Mile End Road, London E1 4NS, United Kingdom",,English,00985589,
Scopus,State of the practice: An exploratory analysis of schedule estimation and software project success prediction,"During discussions with a group of U.S. software developers we explored the effect of schedule estimation practices and their implications for software project success. Our objective is not only to explore the direct effects of cost and schedule estimation on the perceived success or failure of a software development project, but also to quantitatively examine a host of factors surrounding the estimation issue that may impinge on project outcomes. We later asked our initial group of practitioners to respond to a questionnaire that covered some important cost and schedule estimation topics. Then, in order to determine if the results are generalizable, two other groups from the US and Australia, completed the questionnaire. Based on these convenience samples, we conducted exploratory statistical analyses to identify determinants of project success and used logistic regression to predict project success for the entire sample, as well as for each of the groups separately. From the developer point of view, our overall results suggest that success is more likely if the project manager is involved in schedule negotiations, adequate requirements information is available when the estimates are made, initial effort estimates are good, take staff leave into account, and staff are not added late to meet an aggressive schedule. For these organizations we found that developer input to the estimates did not improve the chances of project success or improve the estimates. We then used the logistic regression results from each single group to predict project success for the other two remaining groups combined. The results show that there is a reasonable degree of generalizability among the different groups. © 2006 Elsevier B.V. All rights reserved.",Software effort estimation; Software project staffing; Software project success; Software schedule estimation,"Verner J.M., Evanco W.M., Cerpa N.",2007,Journal,Information and Software Technology,10.1016/j.infsof.2006.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751195713&doi=10.1016%2fj.infsof.2006.05.001&partnerID=40&md5=921deeea736e3a9ab9b74faa2ce88cb0,"National ICT Australia, Australian Technology Park, Alexandria, Sydney, NSW 1430, Australia; College of Information Science and Technology, Drexel University Philadelphia, PA 19104, United States; Department of Systems Engineering, Faculty of Engineering University of Talca, Talca, Chile",,English,09505849,
Scopus,Bagging predictors for estimation of software project effort,"This paper proposes and investigates the use of bagging predictors to improve performance of regression methods for estimation of the effort to develop software projects. We have applied bagging to M5P/regression trees, M5P/model trees, multi-layer perceptron (MLP), linear regression and support vector regression (SVR). This article reports on the influence of bagging on the performance of each of these regression methods in the estimation of the effort of software projects. Experiments carried out using a dataset of software projects from NASA show that bagging is able to significantly improve performance of regression methods in this task. Moreover, we show that bagging with M5P/model trees considerably outperforms previous results reported in the literature obtained by both linear regression and RBF networks. It is also shown that bagging with M5P/model trees obtains results comparable to those of SVR, with the advantage of producing more interpretable results. ©2007 IEEE.",,"Braga P.L., Oliveira A.L.I., Ribeiro G.H.T., Meira S.R.L.",2007,Conference,IEEE International Conference on Neural Networks - Conference Proceedings,10.1109/IJCNN.2007.4371196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48949110120&doi=10.1109%2fIJCNN.2007.4371196&partnerID=40&md5=2e6e38f0f51af46c598802ef61129db7,"IEEE; Department of Computing Systems, Polytechnic School of Engineering, Pernambuco State University, Rua Benfica, 455, Madalena, 50.750-410 Recife - PE, Brazil; Center of Informatics, Federal University of Pernambuco, Cidade Universitária, P.O. Box 7851, 50.732-970 Recife - PE, Brazil",Institute of Electrical and Electronics Engineers Inc.,English,10987576,142441380X; 9781424413805
Scopus,A replicated assessment of the use of adaptation rules to improve Web cost estimation,"Analogy-based estimation has, over the last 15 years, and particularly over the last 7 years, emerged as a promising approach with comparable accuracy to, or better than, algorithmic methods. In addition, it is potentially easier to both understand and apply; these two important factors can contribute to the successful adoption of estimation methods within Web development companies. We believe therefore, analogy-based estimation should be examined further. This paper replicates previous work that investigated the use of two types of adaptation rules as a contributing factor to better estimation accuracy. In addition, it also investigates the use of feature subset selection, in addition to adaptation rules. Two datasets are used in the analysis; results show that adaptation rules improved estimation accuracy for the less ""messy"" dataset. Feature subset selection also seems to help improve the adaptation results. © 2003 IEEE.",Accuracy; Calibration; Costs; Educational institutions; Frequency selective surfaces; Graphics; Industrial relations; Software algorithms; Software engineering; Web pages,"Mendes E., Mosley N., Counsell S.",2003,Conference,"Proceedings - 2003 International Symposium on Empirical Software Engineering, ISESE 2003",10.1109/ISESE.2003.1237969,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844302144&doi=10.1109%2fISESE.2003.1237969&partnerID=40&md5=b81bdc374f2f83777954dcb6d107f06c,"University of Auckland, Private Bag 92019, Auckland, New Zealand; OKKI Software, P.O. Box 3139 Shortland Street, Auckland, New Zealand; Birkbeck College, Malet Street, London, WC1E 7HX, United Kingdom",Institute of Electrical and Electronics Engineers Inc.,English,,0769520022; 9780769520025
Scopus,An empirical evaluation of ensemble adjustment methods for analogy-based effort estimation,"Context Effort adjustment is an essential part of analogy-based effort estimation, used to tune and adapt nearest analogies in order to produce more accurate estimations. Currently, there are plenty of adjustment methods proposed in literature, but there is no consensus on which method produces more accurate estimates and under which settings. Objective This paper investigates the potential of ensemble learning for variants of adjustment methods used in analogy-based effort estimation. The number k of analogies to be used is also investigated. Method We perform a large scale comparison study where many ensembles constructed from n out of 40 possible valid variants of adjustment methods are applied to eight datasets. The performance of each method was evaluated based on standardized accuracy and effect size. Results The results have been subjected to statistical significance testing, and show reasonable significant improvements on the predictive performance where ensemble methods are applied. Conclusion Our conclusions suggest that ensembles of adjustment methods can work well and achieve good performance, even though they are not always superior to single methods. We also recommend constructing ensembles from only linear adjustment methods, as they have shown better performance and were frequently ranked higher. © 2015 Elsevier Inc. All rights reserved.",Adjustment methods; Analogy based estimation; Ensemble learning,"Azzeh M., Nassif A.B., Minku L.L.",2015,Conference,Journal of Systems and Software,10.1016/j.jss.2015.01.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924891262&doi=10.1016%2fj.jss.2015.01.028&partnerID=40&md5=b8416096ad857641f93cd9f3f5d34a1f,"Department of Software Engineering, Applied Science University, Amman, 166, Jordan; Department of Electrical and Computer Engineering, University of Western Ontario, 1151 Richmond St, London, ON  N6A 3K7, Canada; School of Computer Science, University of Birmingham, Office 244, Edgbaston, Birmingham, B15 2TT, United Kingdom",Elsevier Inc.,English,01641212,
Scopus,The relationship between search based software engineering and predictive modeling,"Search Based Software Engineering (SBSE) is an approach to software engineering in which search based optimization algorithms are used to identify optimal or near optimal solutions and to yield insight. SBSE techniques can cater for multiple, possibly competing objectives and/or constraints and applications where the potential solution space is large and complex. This paper will provide a brief overview of SBSE, explaining some of the ways in which it has already been applied to construction of predictive models. There is a mutually beneficial relationship between predictive models and SBSE. The paper sets out eleven open problem areas for Search Based Predictive Modeling and describes how predictive models also have role to play in improving SBSE.",,Harman M.,2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649767913&doi=10.1145%2f1868328.1868330&partnerID=40&md5=5706154d48b514ab13ec2c028aa2aec3,"Department of Computer Science, CREST Centre, University College London, Malet Place, London, WC1E 6BT, United Kingdom",,English,,9781450304047
Scopus,The IT measurement compendium: Estimating and benchmarking success with functional size measurement,"The first step towards success in a software project is to ensure a professional setup. This includes a metrics-based formal estimation process to ensure a solid foundation for project planning. Accurate estimates require quantitative measurements, ideally tool based. In addition, software project managers must also monitor and update these estimates during the project's lifecycle to control progress and assess possible risks. Based on their many years of practical experience as software managers and consultants, Manfred Bundschuh and Carol Dekkers present a framework of value to anyone involved with software project management. They present all five ISO/IEC-acknowledged Functional Sizing Methods, with variants, experiences, counting rules and case studies, and they use numerous practical examples to show how to use functional size measurement to produce realistic estimates. Written in a highly practical style, including checklists, templates, and hands-on advice, and backed up with many pointers to both national and international metrics and standards organizations, this book is the ideal companion for the busy software project manager or quality assurance manager. © 2008 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,"Bundschuh M., Dekkers C.",2008,Book,The IT Measurement Compendium: Estimating and Benchmarking Success with Functional Size Measurement,10.1007/978-3-540-68188-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892126672&doi=10.1007%2f978-3-540-68188-5&partnerID=40&md5=bd59546aab0b1a0d4f6bef0d234750d0,"Sander Höhe 5, 51465 Bergisch Gladbach, Germany; 8430 Egret Lane, Seminole, FL 33776, United States",Springer Berlin Heidelberg,English,,9783540681878
Scopus,Effort prediction in iterative software development processes - incremental versus global prediction models,"Estimation of development effort without imposing overhead on the project and the development team is of paramount importance for any software company. This study proposes a new effort estimation methodology aimed at agile and iterative development environments not suitable for description by traditional prediction methods. We propose a detailed development methodology, discuss a number of architectures of such models (including a wealth of augmented regression models and neural networks) and include a thorough case study of Extreme Programming (XP) in two semi-industrial projects. The results of this research evidence that in the XP environment under study the proposed incremental model outperforms traditional estimation techniques most notably in early phases of development. Moreover, when dealing with new projects, the incremental model can be developed from scratch without resorting itself to historic data. © 2007 IEEE.",,"Abrahamsson P., Moser R., Pedrycz W., Sillitti A., Succi G.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949083731&doi=10.1109%2fESEM.2007.40&partnerID=40&md5=def2358ec9f3921353986b07b54b96aa,"VTT Electronics, Oulu, Finland; T P.O. Box 1100, FIN-90571 Oulu, Finland; Center for Applied Software Engineering, Free University of Bolzano-Bozen, Italy; Piazza Domenicani 3, I-39100 Bolzano, Italy; Department of Electrical and Computer Engineering, University of Alberta, Canada; Civil/Electrical Engineering, Building, 238, Edmonton, Canada",,English,,0769528864; 9780769528861
Scopus,Group processes in software effort estimation,"The effort required to complete software projects is often estimated, completely or partially, using the judgment of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment in which experts with different backgrounds combined their estimates in group discussion. First, 20 software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate via the pooling of knowledge in discussion. We found that the groups submitted less optimistic estimates than the individuals. Interestingly, the group discussion-based estimates were closer to the effort expended on the actual project than the average of the individual expert estimates were, i.e., the group discussions led to better estimates than a mechanical averaging of the individual estimates. The groups' ability to identify a greater number of the activities required by the project is among the possible explanations for this reduction of bias.",Effort estimation; Expert bias; Expert judgment; Group processes; Software development,"Moløkken-Østvold K., Jørgensen M.",2004,Journal,Empirical Software Engineering,10.1023/B:EMSE.0000039882.39206.5a,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444332798&doi=10.1023%2fB%3aEMSE.0000039882.39206.5a&partnerID=40&md5=18d80f67f870bced72845ba0785d143e,"Software Engineering Department, Simula Research Laboratory, 1325 Lysaker, Norway",,English,13823256,
Scopus,Using Web objects for estimating software development effort for Web applications,"Web development projects are certainly different from traditional software development projects and hence require differently tailored measures for accurate effort estimation. We investigate the suitability of a newly proposed size measure for Web development projects: Web objects. Web objects have been specifically developed for sizing Web applications and used for estimating effort in a COCOMO Il-like estimation model called WEBMO. However, no empirical validation has yet been published. We apply and validate the proposed Web object approach in the context of a small Australian Web development company, for the first time. Besides Web objects, we apply traditional function points as an effort predictor for Web applications. Effort estimation models based on Web objects are compared with models based on traditional function points using ordinary least squares regression (OLS). Tested on data from twelve Web applications, the estimates derived from estimation models using Web objects significantly outperformed models using function points, with a mean magnitude of relative error of 0.24 versus 0.33, respectively. Based on the results, it seems that Web objects are more suitable for effort estimation purposes of Web applications than traditional function points. © 2003 IEEE.",Application software; Australia; Business; Costs; Educational technology; Object oriented modeling; Predictive models; Programming; Size measurement; Software measurement,"Ruhe M., Jeffery R., Wieczorek I.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943140351&doi=10.1109%2fMETRIC.2003.1232453&partnerID=40&md5=92c6a966f462ee5c871c8e98a92e44e2,"Siemens AG, Corporate Technology SE3, Munich, 81730, Germany; University of New South Wales, School of Computer Science and Engineering, NICTA, Sydney, NSW  2052, Australia; Federal Ministry of Education and Research, Bonn, 53175, Germany",IEEE Computer Society,English,15301435,0769519873
Scopus,The Impact of Feature Selection on Defect Prediction Performance: An Empirical Comparison,"Software defect prediction aims to determine whether a software module is defect-prone by constructing prediction models. The performance of such models is susceptible to the high dimensionality of the datasets that may include irrelevant and redundant features. Feature selection is applied to alleviate this issue. Because many feature selection methods have been proposed, there is an imperative need to analyze and compare these methods. Prior empirical studies may have potential controversies and limitations, such as the contradictory results, usage of private datasets and inappropriate statistical test techniques. This observation leads us to conduct a careful empirical study to reinforce the confidence of the experimental conclusions by considering several potential source of bias, such as the noise in the dataset and the dataset types. In this paper, we investigate the impact of 32 feature selection methods on the defect prediction performance over two versions of the NASA dataset (i.e., the noisy and clean NASA datasets) and one open source AEEEM dataset. We use a state-of-the-art double Scott-Knott test technique to analyze these methods. Experimental results show that the effectiveness of these feature selection methods on defect prediction performance varies significantly over all the datasets. © 2016 IEEE.",defect prediction; feature selection; Scott-Knott test,"Xu Z., Liu J., Yang Z., An G., Jia X.",2016,Conference,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",10.1109/ISSRE.2016.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013347514&doi=10.1109%2fISSRE.2016.13&partnerID=40&md5=7da2913d6ff560edb3155b8db94caa80,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, Hubei, China; Department of Computer Science, Western Michigan University, Kalamazoo, Michigan, United States",IEEE Computer Society,English,10719458,9781467390019
Scopus,Software effort models for early estimation of process control applications,"Existing software effort models have severe limitations for early use in the software development lifecycle. We develop models to estimate lines of code and function counts directly from user application features of process control systems. Since the application features are known with reasonable degree of confidence during early stages of development, we are able to extend the use of the Constructive Cost Model (COCOMO) and function points based approach for early software cost estimation. We also develop alternative feature-based models that estimate size and effort using application features and productivity factors. At our research site, the feature-based models estimate software effort with the least error. © 1992 IEEE",Constructive cost model; early estimation; func-; process control applications; software cost; tion points,"Mukhopadhya> T., Kekre S.",1992,Journal,IEEE Transactions on Software Engineering,10.1109/32.163607,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026938355&doi=10.1109%2f32.163607&partnerID=40&md5=328af9037e5a1390b18cf129c963e213,"Graduate School of Industrial Administration Carnegie Mellon University, Pittsburgh, PA, 15213-3890, United States",,English,00985589,
Scopus,"Architecture analysis of enterprise systems modifiability - Models, analysis, and validation","Enterprise architecture (EA) models can be used in order to increase the general understanding of enterprise systems and to perform various kinds of analysis. This paper presents instantiated architectural models based on a metamodel for enterprise systems modifiability analysis, i.e. for assessing the cost of making changes to enterprise-wide systems. The instantiated architectural models detailed are based on 21 software change projects conducted at four large Nordic companies. Probabilistic relational models (PRMs) are used for formalizing the EA analysis approach. PRMs enable the combination of regular entity-relationship modeling aspects with means to perform enterprise architecture analysis under uncertainty. The modifiability metamodel employed in the analysis is validated with survey and workshop data (in total 110 experts were surveyed) and with the data collected in the 21 software change projects. Validation indicates that the modifiability metamodel contains the appropriate set of elements. It also indicates that the metamodel produces estimates within a 75% accuracy in 87% of the time and has a mean accuracy of 88% (when considering projects of 2000 man-hours or more). © 2010 Elsevier Inc. All rights reserved.",Enterprise architecture; Metamodel; Probabilistic relational models; Software change cost estimation; Software modifiability,"Lagerström R., Johnson P., Höök D.",2010,Journal,Journal of Systems and Software,10.1016/j.jss.2010.02.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953139934&doi=10.1016%2fj.jss.2010.02.019&partnerID=40&md5=b31ad57c5b745a591f0867d73b67a543,"Industrial Information and Control Systems, The Royal Institute of Technology, Osquldas väg 12, 100 44 Stockholm, Sweden",,English,01641212,
Scopus,An improved fuzzy approach for COCOMO's effort estimation using Gaussian membership function,"In software industry Constructive Cost Model (COCOMO) is considered to be the most widely used model for effort estimation. Cost drivers have significant influence on the COCOMO and this research investigates the role of cost drivers in improving the precision of effort estimation. It is important to stress that uncertainty at the input level of the COCOMO yields uncertainty at the output, which leads to gross estimation error in the effort estimation. Fuzzy logic has been applied to the COCOMO using the symmetrical triangles and trapezoidal membership functions to represent the cost drivers. Using Trapezoidal Membership Function (TMF), a few attributes are assigned the maximum degree of compatibility when they should be assigned lower degrees. To overcome the above limitation, in this paper, it is proposed to use Gaussian Membership Function (GMF) for the cost drivers by studying the behavior of COCOMO cost drivers. The present work is based on COCOMO dataset and the experimental part of the study illustrates the approach and compares it with the standard version of the COCOMO. It has been found that Gaussian function is performing better than the trapezoidal function, as it demonstrates a smoother transition in its intervals, and the achieved results were closer to the actual effort. © 2009 ACADEMY PUBLISHER.",COCOMO; Fuzzy based effort estimation; Gaussian membership function; Software cost estimation; Software effort estimation and Project management,"Satyananda Reddy C., Raju K.V.S.V.N.",2009,Journal,Journal of Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651585350&partnerID=40&md5=fe1c4072cd3ac94f230196efca834af5,"DENSE Research Group, Department of Computer Science and Systems Engineering, College of Engineering, Andhra University, Visakhapatnam, India",,English,1796217X,
Scopus,A complexity metrics set for large-scale object-oriented software systems,"Although traditional software metrics have widely been applied to practical software projects, they have insufficient abilities to measure a large-scale system's complexity at high level so as to provide an overview of the system for developers. So, an adequate metrics set for large-scale software systems that can comprehensively measure the complexity at various levels is still challengeable. First, we summarize universal properties and implicit limitations of recognized object-oriented metric sets in the face of ever-increasing complexities of modern software systems. Large-scale software systems represent an important class of artificial complex networks. Then, from the perspective of software engineering, the main parameters of complex networks are introduced in detail. Furthermore, we integrate these metrics and parameters into a hierarchical complexity metrics set, which can measure the complexity at different levels of a large-scale software system. Eventually, we prove the feasibility of our metrics set through analyzing the data from a software project. © 2006 IEEE.",,"Ma Y., He K., Du D., Liu J., Yan Y.",2006,Conference,"Proceedings - Sixth IEEE International Conference on Computer and Information Technology, CIT 2006",10.1109/CIT.2006.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547253210&doi=10.1109%2fCIT.2006.3&partnerID=40&md5=48c9d975f930001bee9f72044e18d157,,,English,,076952687X; 9780769526874
Scopus,Modeling software bidding risks,"We discuss a method of developing a software bidding model that allows users to visualize the uncertainty involved in pricing decisions and make appropriate bid/no bid decisions. We present a generic bidding model developed using the modeling method. The model elements were identified after a review of bidding research in software and other industries. We describe the method we developed to validate our model and report the main results of our model validation, including the results of applying the model to four bidding scenarios.",Bidding; Contingency costs; Model validation; Monte Carlo simulation; Risk; Software bidding model; Software cost estimation; Software projects,"Kitchenham B., Pickard L.M., Linkman S.G., Jones P.W.",2003,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2003.1205181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041848256&doi=10.1109%2fTSE.2003.1205181&partnerID=40&md5=5dac41165e4289e9902ff25df809feac,"Computer Science Department, Keele University, Keele, Staffordshire, ST5 5BG, United Kingdom; Mathematics Department, Keele University, Keele, Staffordshire, ST5 5BG, United Kingdom",,English,00985589,
Scopus,An empirical validation of the relationship between the magnitude of relative error and project size,"Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate measure of the expected MRE of small and large projects. We recommend therefore that the data set be partitioned into two or more subsamples and that MMRE is reported per subsample. In the long term, we should consider using other evaluation criteria. © 2002 IEEE.",Accuracy; Artificial intelligence; Costs; Electrical capacitance tomography; Investments; Measurement uncertainty; Project management; Q measurement; Size measurement; Software engineering,"Stensrud E., Foss T., Kitchenham B., Myrtveit I.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930742043&doi=10.1109%2fMETRIC.2002.1011320&partnerID=40&md5=dc843c73e190c226094cb5b9e3eaeac2,"Norwegian School of Management, Norway; Keele University, United Kingdom",IEEE Computer Society,English,15301435,0769513395
Scopus,Case-based software quality prediction,"Highly reliable software is becoming an essential ingredient in many systems. However, assuring reliability often entails time-consuming costly development processes. One cost-effective strategy is to target reliability-enhancement activities to those modules that are likely to have the most problems. Software quality prediction models can predict the number of faults expected in each module early enough for reliability enhancement to be effective. This paper introduces a case-based reasoning technique for the prediction of software quality factors. Case-based reasoning is a technique that seeks to answer new problems by identifying similar 'cases' from the past. A case-based reasoning system can function as a software quality prediction model. To our knowledge, this study is the first to use case-based reasoning systems for predicting quantitative measures of software quality. A case study applied case-based reasoning to software quality modeling of a family of full-scale industrial software systems. The case-based reasoning system's accuracy was much better than a corresponding multiple linear regression model in predicting the number of design faults. When predicting faults in code, its accuracy was significantly better than a corresponding multiple linear regression model for two of three test data sets and statistically equivalent for the third.",,"Ganesan K., Khoshgoftaar T.M., Allen E.B.",2000,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1016/S0218-1940(00)00009-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034165967&doi=10.1016%2fS0218-1940%2800%2900009-2&partnerID=40&md5=2c44cdbfdb4e710eb6e35708222ca69f,"Dept. of Comp. Sci. and Engineering, Florida Atlantic University, Boca Raton, FL 33431, United States","World Sci Publ Co Pte Ltd, Singapore, Singapore",English,02181940,
Scopus,The role of artificial intelligence in software engineering,"There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE. © 2012 IEEE.",,Harman M.,2012,Conference,"2012 1st International Workshop on Realizing AI Synergies in Software Engineering, RAISE 2012 - Proceedings",10.1109/RAISE.2012.6227961,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864213804&doi=10.1109%2fRAISE.2012.6227961&partnerID=40&md5=0e48a5acb960fc08418de3092decbbed,"CREST Centre, University College London, Malet Place, London, WC1E 6BT, United Kingdom",,English,,9781467317535
Scopus,Research on software defect prediction,"Software defect prediction is one of the active parts of software engineering since it has been developed from 1970's. It plays a very important role in the analysis of software quality and balance of software cost. This paper investigates and discusses the motivation, evolvement, solutions and challenges of software defect prediction technologies, and it also categorizes, analyzes and compares the representatives of these prediction technologies. Some case studies for software defect distribution models were given to help understanding.",Classification technology; Defect model; Defect prediction; Metric; Software defect,"Wang Q., Wu S.-J., Li M.-S.",2008,Journal,Ruan Jian Xue Bao/Journal of Software,10.3724/SP.J.1001.2008.01565,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48549084964&doi=10.3724%2fSP.J.1001.2008.01565&partnerID=40&md5=3906432edc38b34ea2b8715059ca9934,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Acad. of Sci., Beijing 100190, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Acad. of Sci., Beijing 100190, China; Graduate University, Chinese Acad. of Sci., Beijing 100049, China",,Chinese,10009825,
Scopus,Incomplete-case nearest neighbor imputation in software measurement data,"k nearest neighbor imputation (kNNI) is one of the most popular methods in empirical software engineering for imputing missing values. kNNI typically uses only complete cases as possible donors for imputation (called complete case kNNI or CCkNNI). Though it often produces reasonable results, CCkNNI is severely limited when the amount of missing data is large (and hence the number of complete cases is small). In response, a variant of CCkNNI called incomplete case k nearest neighbor imputation (ICkNNI) has been proposed as an attractive alternative. This work presents a detailed simulation comparing CCkNNI and ICkNNI using two different software measurement datasets. The empirical results show that using incomplete cases often increases the effectiveness of nearest neighbor imputation (especially at higher missingness levels), regardless of the type of missingness (i.e., the distribution of missing values in the data). © 2013 Elsevier Inc. All rights reserved.",Complete-case; Incomplete-case; Nearest neighbor imputation; Software measurement data,"Van Hulse J., Khoshgoftaar T.M.",2014,Journal,Information Sciences,10.1016/j.ins.2010.12.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889688873&doi=10.1016%2fj.ins.2010.12.017&partnerID=40&md5=fcac20fff58177c4cb255fb09d22761a,"Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, FL 33431, United States",,English,00200255,
Scopus,A systematic review of software robustness,"Context: With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view. Objective: To address this issue, we have conducted a literature review in the field of robustness. Method: This review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field. Results: From 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems. Conclusion: We conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering. © 2012 Elsevier B.V. All rights reserved.",Robustness; Software robustness; Systematic review,"Shahrokni A., Feldt R.",2013,Conference,Information and Software Technology,10.1016/j.infsof.2012.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867401202&doi=10.1016%2fj.infsof.2012.06.002&partnerID=40&md5=6ef747c551016eb85422e818c8bb0fed,"Department of Computer Science and Engineering, Chalmers University of Technology, 412 96 Gothenburg, Sweden",,English,09505849,
Scopus,Application migration to cloud: A taxonomy of critical factors,"Cloud computing has attracted attention as an important platform for software deployment, with perceived benefits such as elasticity to fluctuating load, and reduced operational costs compared to running in enterprise data centers. While some software is written from scratch specially for the cloud, many organizations also wish to migrate existing applications to a cloud platform. Such a migration exercise to a cloud platform is not easy: some changes need to be made to deal with differences in software environment, such as programming model and data storage APIs, as well as varying performance qualities. We report here on experiences in doing a number of sample migrations. We propose a taxonomy of the migration tasks involved, and we show the breakdown of costs among categories of task, for a case-study which migrated a .NET n-tier application to run on Windows Azure. We also indicate important factors that impact on the cost of various migration tasks. This work contributes towards our future direction of building a framework for cost-benefit tradeoff analysis that would apply to migrating applications to cloud platforms, and could help decision-makers evaluate proposals for using cloud computing. © 2011 ACM.",cloud computing; cost factors and overheads; cost-benefit analysis; deployment strategy; software metrics; taxonomy; utility computing,"Tran V., Keung J., Liu A., Fekete A.",2011,Conference,Proceedings - International Conference on Software Engineering,10.1145/1985500.1985505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960617830&doi=10.1145%2f1985500.1985505&partnerID=40&md5=04b3391976873121d0b68da7c18ecc81,"CSE, University of New South Wales, Australia; Hong Kong Polytechnic University, Hong Kong; National ICT Australia Ltd., Sydney, NSW, Australia; School of Information Technologies, University of Sydney, Sydney, NSW, Australia",,English,02705257,9781450305822
Scopus,Genetic programming for effort estimation: An analysis of the impact of different fitness functions,"Context: The use of search-based methods has been recently proposed for software development effort estimation and some case studies have been carried out to assess the effectiveness of Genetic Programming (GP). The results reported in the literature showed that GP can provide an estimation accuracy comparable or slightly better than some widely used techniques and encouraged further research to investigate whether varying the fitness function the estimation accuracy can be improved. Aim: Starting from these considerations, in this paper we report on a case study aiming to analyse the role played by some fitness functions for the accuracy of the estimates. Method: We performed a case study based on a publicly available dataset, i.e., Desharnais, by applying a 3-fold cross validation and employing summary measures and statistical tests for the analysis of the results. Moreover, we compared the accuracy of the obtained estimates with those achieved using some widely used estimation methods, namely Case-Based Reasoning (CBR) and Manual StepWise Regression (MSWR). Results: The obtained results highlight that the fitness function choice significantly affected the estimation accuracy. The results also revealed that GP provided significantly better estimates than CBR and comparable with those of MSWR for the considered dataset. © 2010 IEEE.",Empirical studies; Genetic programming; Software development effort estimation,"Ferrucci F., Gravino C., Oliveto R., Sarro F.",2010,Conference,"Proceedings - 2nd International Symposium on Search Based Software Engineering, SSBSE 2010",10.1109/SSBSE.2010.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952059191&doi=10.1109%2fSSBSE.2010.20&partnerID=40&md5=37aa37dd16097e76db55674e76fcec97,"DMI, University of Salerno, via Ponte don Melillo, 84084 Fisciano (SA), Italy",,English,,9780769541952
Scopus,Applying fuzzy neural network to estimate software development effort,"The ability to accurately and consistently estimate software development efforts is required by the project managers in planning and conducting software development activities. Since software effort drivers are vague and uncertain, software effort estimates, especially in the early stages of the development life cycle, are prone to a certain degree of estimation errors. A software effort estimation model which adopts a fuzzy inference method provides a solution to fit the uncertain and vague properties of software effort drivers. The present paper proposes a fuzzy neural network (FNN) approach for embedding artificial neural network into fuzzy inference processes in order to derive the software effort estimates. Artificial neural network is utilized to determine the significant fuzzy rules in fuzzy inference processes. We demonstrated our approach by using the 63 historical project data in the well-known COCOMO model. Empirical results showed that applying FNN for software effort estimates resulted in slightly smaller mean magnitude of relative error (MMRE) and probability of a project having a relative error of less than or equal to 0.25 (Pred(0.25)) as compared with the results obtained by just using artificial neural network and the original model. The proposed model can also provide objective fuzzy effort estimation rule sets by adopting the learning mechanism of the artificial neural network. © 2007 Springer Science+Business Media, LLC.",Artificial neural network; Fuzzy logic; Fuzzy neural network; Software effort estimation,"Huang S.-J., Chiu N.-H.",2009,Journal,Applied Intelligence,10.1007/s10489-007-0097-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949189200&doi=10.1007%2fs10489-007-0097-4&partnerID=40&md5=9f67cab0f28ef83b6e776a26c8119e96,"Department of Information Management, National Taiwan University of Science and Technology, 43, Keelung Rd., Taipei 106, Taiwan; Department of Information Management, Ching Yun University, 229, Chien-Hsin Rd., Jung-Li, Taoyuan, Taiwan",,English,0924669X,
Scopus,Handling imprecision and uncertainty in software development effort prediction: A type-2 fuzzy logic based framework,"Traditional approaches for software projects effort prediction such as the use of mathematical formulae derived from historical data, or the use of experts judgments are plagued with issues pertaining to effectiveness and robustness in their results. These issues are more pronounced when these effort prediction approaches are used during the early phases of the software development lifecycle, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Recent works have demonstrated promising results using approaches based on fuzzy logic. Effort prediction systems that use fuzzy logic can deal with imprecision; they, however, can not deal with uncertainty. This paper presents an effort prediction framework that is based on type-2 fuzzy logic to allow handling imprecision and uncertainty inherent in the information available for effort prediction. Evaluation experiments have shown the framework to be promising. © 2008 Elsevier B.V. All rights reserved.",COCOMO; Effort prediction; Imprecision; Type-2 fuzzy logic; Uncertainty,"Ahmed M.A., Muzaffar Z.",2009,Journal,Information and Software Technology,10.1016/j.infsof.2008.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849099133&doi=10.1016%2fj.infsof.2008.09.004&partnerID=40&md5=339a8965175745bf0a739fa26e9aac01,"LEROS Technologies Corporation, Fairfax, VA 22030, United States; Department of Computer Science, University of Western Ontario, Canada",,English,09505849,
Scopus,Software effort estimation using machine learning methods,"In software engineering, the main aim is to develop projects that produce the desired results within limited schedule and budget. The most important factor affecting the budget of a project is the effort. Therefore, estimating effort is crucial because hiring people more than needed leads to a loss of income and hiring people less than needed leads to an extension of schedule. The main objective of this research is making an analysis of software effort estimation to overcome problems related to it: budget and schedule extension. To accomplish this, we propose a model that uses machine learning methods. We evaluate these models on public datasets and data gathered from software organizations in Turkey. It is found out in the experiments that the best method for a dataset may change and this proves the point that the usage of one model cannot always produce the best results.",,"Başkeleş B., Turhan B., Bener A.",2007,Conference,"22nd International Symposium on Computer and Information Sciences, ISCIS 2007 - Proceedings",10.1109/ISCIS.2007.4456863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649104763&doi=10.1109%2fISCIS.2007.4456863&partnerID=40&md5=e7d17e14aecca94150573ed389b8f55d,"Department of Computer Engineering, Boǧaziçi University",,English,,1424413648; 9781424413645
Scopus,The Problem with Function Points,[No abstract available],,Kitchenha B.,1997,Journal,IEEE Software,10.1109/MS.1997.582972,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008033744&doi=10.1109%2fMS.1997.582972&partnerID=40&md5=deb1a473ce37e6f6ced9d2e959c657b9,"Keele University, United Kingdom",,English,07407459,
Scopus,LACE2: Better privacy-preserving data sharing for cross project defect prediction,"Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute ""interesting"" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multiparty approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors). © 2015 IEEE.",,"Peters F., Menzies T., Layman L.",2015,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2015.92,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951868301&doi=10.1109%2fICSE.2015.92&partnerID=40&md5=a927b15f1141a8c455dcdf3280b172e8,"Lero - The Irish Software Research Centre, University of Limerick, Ireland; Computer Science, North Carolina State University, United States; Fraunhofer Center for Experimental SE, College Park, United States",IEEE Computer Society,English,02705257,9781479919345
Scopus,The art of testing less without sacrificing quality,"Testing is a key element of software development processes for the management and assessment of product quality. In most development environments, the software engineers are responsible for ensuring the functional correctness of code. However, for large complex software products, there is an additional need to check that changes do not negatively impact other parts of the software and they comply with system constraints such as backward compatibility, performance, security etc. Ensuring these system constraints may require complex verification infrastructure and test procedures. Although such tests are time consuming and expensive and rarely find defects they act as an insurance process to ensure the software is compliant. However, long lasting tests increasingly conflict with strategic aims to shorten release cycles. To decrease production costs and to improve development agility, we created a generic test selection strategy called THEO that accelerates test processes without sacrificing product quality. THEO is based on a cost model, which dynamically skips tests when the expected cost of running the test exceeds the expected cost of removing it. We replayed past development periods of three major Microsoft products resulting in a reduction of 50% of test executions, saving millions of dollars per year, while maintaining product quality. © 2015 IEEE.",Cost estimation; Measurement; Test improvement,"Herzig K., Greiler M., Czerwonka J., Murphy B.",2015,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2015.66,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951766869&doi=10.1109%2fICSE.2015.66&partnerID=40&md5=7879f9d7ceb674ef3f062a98b8ebbb6d,"Microsoft Research, United Kingdom; Microsoft Corporation, Redmond, United States",IEEE Computer Society,English,02705257,9781479919345
Scopus,A replicated assessment and comparison of adaptation techniques for analogy-based effort estimation,"Variants of adaptation techniques have been proposed in previous studies to improve the performance of analogy-based effort estimation. The results of these studies are often contradictory and cannot simply be generalized because there are many uncontrollable source of variations between adaptation studies. The study presented in this paper has been carried out in order to replicate the assessment and comparison of different adaptation techniques utilised in analogy-based software effort prediction. Empirical evaluation of variants of adaptation techniques with Jack-knifing procedure have been carried out. Seven datasets come from PROMISE data repository were used for benchmarking. The results are also investigated within the presence/absence of feature subset selection algorithm. The current study permitted us to discover that linear adjustment approaches are more accurate than nonlinear adjustment because of the nature of the employed datasets that have, in most cases, normality characteristics. © 2011 Springer Science+Business Media, LLC.",Adaptation techniques; Analogy-based software effort estimation; Feature subset selection,Azzeh M.,2012,Journal,Empirical Software Engineering,10.1007/s10664-011-9176-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857312718&doi=10.1007%2fs10664-011-9176-6&partnerID=40&md5=98bd6f056406d5604af2cba53067d3c2,"Department of Software Engineering, Applied Science University, PO BOX 166, Amman, Jordan",,English,13823256,
Scopus,A systematic survey of games used for software engineering education,"Simsoft is a serious game- one that trains or educates- at the centre of a research project designed to see if and how games can contribute to better software engineering management education by helping software engineers and project managers explore some of the dynamic complexities of the field in a safe and inexpensive environment. A necessary precursor for this project was to establish what games already existed in the field and how effective they had been. To this end a systematic review of the literature was conducted using a collection of online science, engineering, education, and business databases looking for games or simulations used for educational or training purposes in software engineering or software project management across any of the SWEBOK knowledge areas. The initial search returned 243 results, which was filtered to 36 papers by applying some simple quality and relevance inclusion/exclusion criteria. These remaining papers were then analysed in more depth to see if and how they promoted education in the field of software engineering management. The results showed that games were mainly used in the SWEBOK knowledge areas of software engineering management and development processes, and most game activity was in Europe and the Americas. The results also showed that most games in the field have learning objectives pitched at the first rung of Bloom's taxonomy (knowledge), most studies followed a non-experimental design, and many had very small sample sizes. This suggests that more rigorous research is needed into the efficacy of games in teaching software engineering management, but enough evidence exists to say that educators could include serious games in their courses as a useful and interesting supplement to other teaching methods.",Project management education; Serious games; Software engineering,"Caulfield C., Xia J.C., Veal D., Paul Maj S.",2011,Journal,Modern Applied Science,10.5539/mas.v5n6p28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856249018&doi=10.5539%2fmas.v5n6p28&partnerID=40&md5=38a64ee5bd26dc57e347c6df6db9b19f,"School of Computer Science and Security Science, Edith Cowan University, 2 Bradford Street, Mount Lawley, WA 6050, Australia; Department of Spatial Sciences, Curtin University, Kent Street, Bentley, WA 6102, Australia",,English,19131844,
Scopus,The relationship between software development team size and software development cost,"A relationship between team size and software development cost is studied in a bid to provide advanced software coordination and communication capabilities for large sized teams. The International Software Benchmarking Standards Group (ISBSG) provides production economics theory as a guideline for testing the relationship between team size and the associated software cost. The models such as linear regression, log-linear models to test non-linear relationships, and non-parametric data envelopment analysis (DEA) model tests relationship between team size and software effort. The results suggest that the DEA approach do not impose a particular form on the production function and assumes a monotonically increasing and convex relationship between inputs and outputs. The constant returns to scale (CRS) means that proportional change in team size mean same proportional change in software effort.",,"Pendharkar P.C., Rodger J.A.",2009,Journal,Communications of the ACM,10.1145/1435417.1435449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849120645&doi=10.1145%2f1435417.1435449&partnerID=40&md5=350390878b035559f8b70c58bcd66091,"Department of Information Systems, Harrisburg, PA, United States; MIS, Indiana University of Pennsylvania, PA, United States",,English,00010782,
Scopus,Can k-NN imputation improve the performance of C4.5 with small software project data sets? A comparative evaluation,"Missing data is a widespread problem that can affect the ability to use data to construct effective prediction systems. We investigate a common machine learning technique that can tolerate missing values, namely C4.5, to predict cost using six real world software project databases. We analyze the predictive performance after using the k-NN missing data imputation technique to see if it is better to tolerate missing data or to try to impute missing values and then apply the C4.5 algorithm. For the investigation, we simulated three missingness mechanisms, three missing data patterns, and five missing data percentages. We found that the k-NN imputation can improve the prediction accuracy of C4.5. At the same time, both C4.5 and k-NN are little affected by the missingness mechanism, but that the missing data pattern and the missing data percentage have a strong negative impact upon prediction (or imputation) accuracy particularly if the missing data percentage exceeds 40%. © 2008 Elsevier Inc. All rights reserved.",C4.5; Data imputation; Missing data; Missing data toleration; Software project cost prediction,"Song Q., Shepperd M., Chen X., Liu J.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2008.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949118951&doi=10.1016%2fj.jss.2008.05.008&partnerID=40&md5=b9836e4c738111a34b804b85a39dbd23,"Department of Computer Science and Technology, Xi'an Jiaotong University, 28 Xian-Ning West Road, Xi'an, Shaanxi 710049, China; School of IS, Computing and Maths, Brunel University, Uxbridge, UB8 3PH, United Kingdom; 21 Dian-Chang East Road, Xi'an, Shaanxi 710038, China",,English,01641212,
Scopus,Effort estimation modeling techniques: A case study for web applications,"A reliable effort estimation is crucial for a successful web application development planning. Several approaches exist to address this issue. Among them, the algorithmic approach is one of the most widely used and investigated methods. It is based on suitable effort prediction models which relate the development effort with project characteristics. The size represents one of the most interesting characteristics of software products and several measures can be defined in order to estimate the size of web systems. Moreover, several techniques have been proposed in the literature to build the effort prediction models. Thus, of special interest should be to establish the most effective size measures to be employed in effort prediction models and the most suitable techniques for the model construction. To this aim some empirical studies have been undertaken so far. Since it is widely recognized that several investigations should be performed to verify/confirm empirical results, in the paper we will report on an empirical analysis we have carried out by exploiting data coming from 15 web projects developed by a software company. In particular, for the analysis we have considered two sets of size measures: Length Measures (e.g. number of pages, number of medias, number of client and server side scripts) and Functional Measures (e.g. external input, external output, external query). Moreover, we have employed different techniques, such as Linear Regression, Regression Tree, and Analogy-Based Estimation, in order to determine the one that provides the best prediction.",Effort prediction models; Empirical validation; Size metrics; Web applications,"Costagliola G., Di Martino S., Ferrucci F., Gravino C., Tortora G., Vitiello G.",2006,Conference,ICWE'06: The Sixth International Conference on Web Engineering,10.1145/1145581.1145584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250646633&doi=10.1145%2f1145581.1145584&partnerID=40&md5=c9c75630ed90bcf7b1a516d9989a7902,"Dipartimento di Matematica e Informatica, University of Salerno, Via Ponte Don Melillo, 84084, Fisciano (SA), Italy",,English,,1595933522; 9781595933522
Scopus,Cross-company and single-company effort models using the ISBSG database: A further replicated study,"Five years ago the ISBSG database was used by Jeffery et al. [6] (51) to compare the effort prediction accuracy between cross- and single-company effort models. Given that more than 2,000 projects were later volunteered to this database, in 2005 Mendes et al. [17] (52) replicated SI but obtained different results. The difference in results between both studies could have resulted from legitimate differences in data set patterns but also could have been influenced by differences in experimental procedure. S2 was unable to employ exactly the same experimental procedure used in S1, as S1's procedure was not fully documented. Therefore this paper aimed to apply S2's experimental procedure to the ISBSG database version used in SI (release 6) to assess if differences in experimental procedure would have contributed towards different results. Our resulte corroborated those from S1: we found that predictions based on a single-company model were significantly more accurate than those based on a cross-company model. Copyright 2006 ACM.",Cross-company estimation models; Effort estimation; Experimental procedure; Regression-based estimation models; Replication study; Single-company estimation model; Software projects,"Lokan C., Mendes E.",2006,Conference,ISESE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering,10.1145/1159733.1159747,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247387994&doi=10.1145%2f1159733.1159747&partnerID=40&md5=7a4b55b5fc9bf9e84f1c6025c0b3a05a,"School of IT and EE, UNSW at ADFA, Canberra, ACT 2600, Australia; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,1595932186; 9781595932181
Scopus,Estimating the design effort of Web applications,"We focus on the effort needed for designing Web applications. The effort required for the design phase is an important part of the total development effort of a Web application, whose implementation can be (partially) automated by tools. We carried out an empirical study with the students of an advanced university class that used W2000, as the special-purpose object-oriented design notation for the design of Web applications. We investigated the impact of a number of attributes (e.g., size, complexity) of the W2000 design artifacts built during the design phase on the total effort needed to design Web applications and we identified a few attributes that may be related to the total design effort. In addition, we carried out a finer-grain analysis, by studying which of these attributes have an impact on the effort devoted to the steps of the design phase that are followed when using W2000. © 2003 IEEE.",Application software; Companies; Costs; Internet; Navigation; Object oriented modeling; Phase estimation; Software engineering; Stock markets; Stress,"Baresi L., Morasca S., Paolini P.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749107390&doi=10.1109%2fMETRIC.2003.1232456&partnerID=40&md5=12022c40698cd02197ef302f97a65f09,"Dipartimento di Elettronica e Informazione, Politecnico di Milano, piazza Leonardo da Vinci, 32, Milan, 20133, Italy; Dipartimento di Scienze Chimiche, Fisiche e Matematiche, Università degli Studi dell'Insubria, via Valleggio, 11, Como, 22100, Italy",IEEE Computer Society,English,15301435,0769519873
Scopus,Software economics: status and prospects,"An overview is given on the current status of selected parts of software economics, highlighting the gaps both between practice and theory and between the current understanding and what is needed. The information given is meant to help researchers and managers.",,"Boehm B., Sullivan K.",1999,Journal,Information and Software Technology,10.1016/S0950-5849(99)00091-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033351898&doi=10.1016%2fS0950-5849%2899%2900091-9&partnerID=40&md5=35504cdabe31d9399a14a5b656d8f12a,"Center for Software Engineering, University of Southern California, Los Angeles, CA 90089-0781, United States","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,Function point-like measure for object-oriented software,"We present a method for estimating the size, and consequently effort and duration, of object oriented software development projects. Different estimates may be made in different phases of the development process, according to the available information. We define an adaptation of traditional function points, called `Object Oriented Function Points', to enable the measurement of object oriented analysis and design specifications. Tools have been constructed to automate the counting method. The novel aspect of our method is its flexibility. An organization can experiment with different counting policies, to find the most accurate predictors of size, effort, etc. in its environment. The method and preliminary results of its application in an industrial environment are presented and discussed.",,"Antoniol G., Lokan C., Caldiera G., Fiutem R.",1999,Journal,Empirical Software Engineering,10.1023/A:1009834811663,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033185465&doi=10.1023%2fA%3a1009834811663&partnerID=40&md5=dcbe52d9183725807bba0feaf1cc5737,"University of Sannio, Faculty of Engineering, Palazzo Bosco Lucarelli, Piazza Roma, I-82100 Benevento, Italy; School of Computer Science, Australian Defence Force Academy, UNSW, Canberra, ACT 2600, Australia; PricewaterhouseCoopers, 12902 Federal Systems Park Dr, Fairfax, VA 22033, United States; Sodalia Spa, Viale Zambra 1, Trento, Italy; Artificial Intelligence Division, Inst. Ric. Scientifica T., Trento, Italy; Ist. Ric. Scientifica T., Trento, Italy","Kluwer Academic Publishers, Dordrecht, Netherlands",English,13823256,
Scopus,Measuring productivity of software projects: A data envelopment analysis approach,"Current economic conditions are forcing information system departments to focus simultaneously on decreasing costs while increasing software productivity. Improving software productivity is becoming critical because software costs of large in-house software companies have been increasing rapidly. For many organizations, however, measuring software productivity has been a difficult task. Using Data Envelopment Analysis (DEA), this research study investigates the productivity of 78 commercial system projects. The results of this study have practical implications for software project managers undertaking software development. The results showed that the DEA technology can be successfully used to identify efficient and inefficient software projects. Furthermore, within the inefficient group, DEA can also identify factors that affect software productivity in a positive or negative manner, allowing managers to take corrective actions. Based on the findings of this study, the manuscript also provides some practical guidelines for managers to follow in software development.",Data Envelopment Analysis; Information Systems Implementation; Systems Development Methodologies,"Mahmood M.A., Pettingell K.J., Shaskevich A.I.",1996,Journal,Decision Sciences,10.1111/j.1540-5915.1996.tb00843.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041050969&doi=10.1111%2fj.1540-5915.1996.tb00843.x&partnerID=40&md5=6aaf008f1837ac3327d4c53874069d54,"Information and Decision Sciences, University of Texas-El Paso, El Paso, TX 79968, United States; 950 Camino Del Rex, Las Cruces, NM 88001, United States",Decision Sciences Institute,English,00117315,
Scopus,Missing data in software engineering,The collection of valid software engineering data involves substantial effort and is not a priority in most software production environments. This often leads to missing or otherwise invalid data. This fact tends to be overlooked by most software engineering researchers and may lead to a biased analysis. This chapter reviews missing data methods and applies them on a software engineering data set to illustrate a variety of practical contexts where such techniques are needed and to highlight the pitfalls of ignoring the missing data problem. © 2008 Springer-Verlag London.,,Mockus A.,2008,Book Chapter,Guide to Advanced Empirical Software Engineering,10.1007/978-1-84800-044-5_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73349092923&doi=10.1007%2f978-1-84800-044-5_7&partnerID=40&md5=ffb0b1fafb102f272e4f28672baa2d4f,"Software Technology Research Department, Avaya Labs Research, Basking Ridge, NJ 07920, United States",Springer London,English,,9781848000438
Scopus,How valuable is company-specific data compared to multi-company data for software cost estimation?,"This paper investigates the pertinent question whether multi-organizational data is valuable for software project cost estimation. Local, company-specific data is widely believed to provide a better basis for accurate estimates. On the other hand, multi-organizational databases provide an opportunity for fast data accumulation and shared. information benefits. Therefore, this paper trades off the potential advantages and drawbacks of using local data as compared to multi-organizational data. Motivated by the results from previous investigations, we further analyzed a large cost database from Finland that collects standard cost factors and includes information on six individual companies. Each of these companies provided data for more than ten projects. This information was used to compare the accuracy between company-specific (local) and company-external (global) cost models. They show that company-specific models seem not to yield better results than the company external models. Our results are based on applying two standard statistical estimation methods (OLS-regression, analysis of variance) and analogy-based estimation. © 2002 IEEE.",Analysis of variance; Computer science; Costs; Databases; Electrical capacitance tomography; Monitoring; Read only memory; Software engineering; Software quality; Statistical analysis,"Wieczorek I., Ruhe M.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948451781&doi=10.1109%2fMETRIC.2002.1011342&partnerID=40&md5=0dd7d7d7e650c0f9fe1f9de573d8ca5a,"Fraunhofer Institute for Experimental Software Engineering, Kaiserslautern, 67663, Germany; University of Kaiserslautern, Department of Computer Science, Kaiserslautern, 67661, Germany",IEEE Computer Society,English,15301435,0769513395
Scopus,A comprehensive empirical evaluation of missing value imputation in noisy software measurement data,"The handling of missing values is a topic of growing interest in the software quality modeling domain. Data values may be absent from a dataset for numerous reasons, for example, the inability to measure certain attributes. As software engineering datasets are sometimes small in size, discarding observations (or program modules) with incomplete data is usually not desirable. Deleting data from a dataset can result in a significant loss of potentially valuable information. This is especially true when the missing data is located in an attribute that measures the quality of the program module, such as the number of faults observed in the program module during testing and after release. We present a comprehensive experimental analysis of five commonly used imputation techniques. This work also considers three different mechanisms governing the distribution of missing values in a dataset, and examines the impact of noise on the imputation process. To our knowledge, this is the first study to thoroughly evaluate the relationship between data quality and imputation. Further, our work is unique in that it employs a software engineering expert to oversee the evaluation of all of the procedures and to ensure that the results are not inadvertently influenced by poor quality data. Based on a comprehensive set of carefully controlled experiments, we conclude that Bayesian multiple imputation and regression imputation are the most effective techniques, while mean imputation performs extremely poorly. Although a preliminary evaluation has been conducted using Bayesian multiple imputation in the empirical software engineering domain, this is the first work to provide a thorough and detailed analysis of this technique. Our studies also demonstrate conclusively that the presence of noisy data has a dramatic impact on the effectiveness of imputation techniques. © 2007.",Bayesian multiple imputation; Data quality; Imputation; Missing data; Software quality,"Van Hulse J., Khoshgoftaar T.M.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2007.07.043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749151020&doi=10.1016%2fj.jss.2007.07.043&partnerID=40&md5=9afa6bbef3561085547f08c8c6991c5e,"Empirical Software Engineering Laboratory, Department of Computer Science and Engineering, Florida Atlantic University, Boca Raton, FL 33431, United States",,English,01641212,
Scopus,Method and tool for assessing object-oriented projects and metrics management,"The number of metrics and tools for the assessment/control of object-oriented project is increasing. In the last years, the effort spent in defining new metrics has not been followed by a comparable effort in establishing methods and procedures for their systematic application. To make the investment on project assessment effective, specific methods and tools for product and process control have to be applied and customized on the basis of specific needs. In this paper, the experience of the authors cumulated in interpreting assessment results and defining a tool and a method for the assessment of object-oriented systems is reported. The tool architecture and method for system assessment provide support for: (1) customizing the assessment process to satisfy company needs, project typology, product profile, etc.; (2) visualizing results in an understandable way; (3) suggesting actions for tackling with problems; (4) avoiding unuseful interventions and shortening the assessment analysis; (5) supporting metrics validation and tuning. The tool and method have been defined in years of work in identifying tool features and general guidelines to define a modus operandi with metrics, with a special care to detect analysis and design problems as soon as possible, and for effort estimation and prediction. In this line, a specific assessment tool has been built and used as a research prototype in several projects.",,"Fioravanti F., Nesi P.",2000,Journal,Journal of Systems and Software,10.1016/S0164-1212(00)00050-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034250157&doi=10.1016%2fS0164-1212%2800%2900050-9&partnerID=40&md5=96d24bfadf028456c0d4fd18bbbafc23,"Dept. of Systems and Informatics, Fac. Eng., Univ. Florence, Via S. M., Florence, Italy","Elsevier Science Inc, New York",English,01641212,
Scopus,An empirical test of object-based output measurement metrics in a computer aided software engineering (CASE) environment,"Existing output measurement metrics for cost estimation and development productivity need to be reexamined to determine their performance in computer-aided software engineering (CASE) development environments. This paper critiques and empirically evaluates four approaches to the measurement of outputs. Two of the metrics, raw function counts and function points, are based on the function point analysis methodology pioneered by Albrecht and Gaffney at IBM. The second two, object counts and object points, are based on a new approach-object points analysis-that is introduced here for the first time. The latter metrics are specialized for output measurement in object-based CASE environments that include a centralized object repository. Estimation results for nineteen large-scale CASE projects show that the new metrics have the potential to yield equally accurate, yet easier to obtain estimates than function points-based measures. © M.E. Sharpe, Inc., 1992.",Computer-aided software engineering; Cost estimation; Function point analysis; Object point analysis; Object-based metrics; Productivity measurement; Reuse; Software development; Software economics; Software metrics,"Banker R.D., Kauffman R.J., Kumar R.",1991,Journal,Journal of Management Information Systems,10.1080/07421222.1991.11517933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450073151&doi=10.1080%2f07421222.1991.11517933&partnerID=40&md5=6c147e3b2390eba5edabe53247836438,"Accounting and Information Systems, Carlson School of Management, University of Minnesota; Stern School of Business, New York University",M.E. Sharpe Inc.,English,07421222,
Scopus,Comprehensible software fault and effort prediction: A data mining approach,"Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. © 2014 Elsevier Inc. All rights reserved.",Comprehensibility; Rule extraction; Software fault and effort prediction,"Moeyersoms J., Junqué De Fortuny E., Dejaeger K., Baesens B., Martens D.",2015,Journal,Journal of Systems and Software,10.1016/j.jss.2014.10.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919361139&doi=10.1016%2fj.jss.2014.10.032&partnerID=40&md5=2eab3043a0a68f06cf9b0418d024f57f,"Faculty of Applied Economics, University of Antwerp, Prinsstraat 13, Antwerp, B-2000, Belgium; Department of Decision Sciences and Information Management, Katholieke Universiteit Leuven, Naamsestraat 69, Leuven, B-3000, Belgium",Elsevier Inc.,English,01641212,
Scopus,Code smells as system-level indicators of maintainability: An empirical study,"Context Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations. Aim The research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability. Method We evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches. Results Most code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios. Conclusion Code smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack. © 2013 Elsevier Inc.",Code smells; Empirical study; Maintainability; System evaluation,"Yamashita A., Counsell S.",2013,Journal,Journal of Systems and Software,10.1016/j.jss.2013.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882716230&doi=10.1016%2fj.jss.2013.05.007&partnerID=40&md5=0032e0f1876939cca348e7a7be80fd5c,"Simula Research Laboratory, P.O. Box 134, Lysaker, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Brunel University, Kingston Lane, Uxbridge, Middlesex, United Kingdom",,English,01641212,
Scopus,Privacy and utility for defect prediction: Experiments with MORPH,"Ideally, we can learn lessons from software projects across multiple organizations. However, a major impediment to such knowledge sharing are the privacy concerns of software development organizations. This paper aims to provide defect data-set owners with an effective means of privatizing their data prior to release. We explore MORPH which understands how to maintain class boundaries in a data-set. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. The value of training on this MORPHed data is tested via a 10-way within learning study and a cross learning study using Random Forests, Naive Bayes, and Logistic Regression for ten object-oriented defect datasets from the PROMISE data repository. Measured in terms of exposure of sensitive attributes, the MORPHed data was four times more private than the unMORPHed data. Also, in terms of the f-measures, there was little difference between the MORPHed and unMORPHed data (original data and data privatized by data-swapping) for both the cross and within study. We conclude that at least for the kinds of OO defect data studied in this project, data can be privatized without concerns for inference efficacy. © 2012 IEEE.",data mining; defect prediction; privacy,"Peters F., Menzies T.",2012,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2012.6227194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864237116&doi=10.1109%2fICSE.2012.6227194&partnerID=40&md5=4f60c3abd7d08f15aa058427364b18f7,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, United States",,English,02705257,9781467310673
Scopus,On failure propagation in component-based software systems,"Ensuring reliability in Component-Based Software Systems (CBSSs) is important for their effective applications in large scale and safety critical systems. However, only few techniques consider failure propagation in system architectures for system reliability assessment. Those techniques focus only on content failure propagation through component interfaces. Therefore, the evaluation of CBSS architectures based on the current techniques fails to consider the impacts of all failure types on system reliability. In this paper, we present a failure propagation analysis technique for CBSSs. We analyze failure propagation based on architectural service routes (ASRs). An ASR is a sequence of components that are connected through interfaces. We discuss the attributes of ASRs with respect to system components and present their impacts on failure propagation and consequently on the reliability of CBSSs. Further analysis determines upper and lower bounds of failure propagation among components and shows some relationships between system reliability and architectural attributes. Our technique is not limited to any failure type, and it considers failure scattering and masking. Therefore, unlike other works, the proposed technique demonstrates more accurate representation of the practical aspect of failure propagation in CBSSs. The technique can also be used to achieve reliable designs in the early design stages of CBSSs and to localize component faults in the operational stage. We compare different example architectures based on their impacts on system reliability. © 2008 IEEE.",,"Mohamed A., Zulkernine M.",2008,Conference,Proceedings - International Conference on Quality Software,10.1109/QSIC.2008.46,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52449121443&doi=10.1109%2fQSIC.2008.46&partnerID=40&md5=5536fc626aa3eaf0711d247a881bfde6,"School of Computing, Queen's University, Kingston, ON K7L 3N6, Canada",,English,15506002,9780769533124
Scopus,Some theoretical considerations for a suite of metrics for the integration of software components,"This paper defines two suites of metrics, which address static and dynamic aspects of component assembly. The static metrics measure complexity and criticality of component assembly, wherein complexity is measured using Component Packing Density and Component Interaction Density metrics. Further, four criticality conditions namely, Link, Bridge, Inheritance and Size criticalities have been identified and quantified. The complexity and criticality metrics are combined to form a Triangular Metric, which can be used to classify the type and nature of applications. Dynamic metrics are collected during the runtime of a complete application. Dynamic metrics are useful to identify super-component and to evaluate the degree of utilization of various components. In this paper both static and dynamic metrics are evaluated using Weyuker's set of properties. The result shows that the metrics provide a valid means to measure issues in component assembly. We relate our metrics suite with McCall's Quality Model and illustrate their impact on product quality and to the management of component-based product development. © 2006 Elsevier Inc. All rights reserved.",Component assembly; Component-Based Software Engineering; CORBA Component Model; Software component metrics,"Lakshmi Narasimhan V., Hendradjaya B.",2007,Journal,Information Sciences,10.1016/j.ins.2006.07.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750708203&doi=10.1016%2fj.ins.2006.07.010&partnerID=40&md5=98d32b8be3e9e7e8c205b6208033f03f,"Faculty of Electrical Engineering and Computer Science, University of Newcastle, NSW 2308, Australia",,English,00200255,
Scopus,Metrics-based software reliability models using non-homogeneous poisson processes,"The traditional software reliability models aim to describe the temporal behavior of software fault-detection processes with only the fault data, but fail to incorporate some significant test-metrics data observed in software testing. In this paper we develop a useful modeling framework to assess the quantitative software reliability with time-dependent covariate as well as software-fault data. The basic ideas employed here are to introduce the discrete proportional hazard model on a cumulative Bernoulli trial process, and to represent a generalized fault-detection processes having time-dependent covariate structure. The resulting stochastic models are regarded as combinations of the proportional hazard models and the familiar non-homogeneous Poisson processes. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, and evaluate quantitatively both goodness-of-fit and predictive performances from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating time-dependent metrics data in modeling. © 2006 IEEE.",,"Shibata K., Rinsaka K., Dohi T.",2006,Conference,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",10.1109/ISSRE.2006.28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547668728&doi=10.1109%2fISSRE.2006.28&partnerID=40&md5=8e871cc80509b8efbd2553f21be5559c,"Department of Information Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi-Hiroshima 739-8527, Japan",,English,10719458,0769526845; 9780769526843
Scopus,Comparing uniform and flexible policies for software maintenance and replacement,"The importance of software maintenance in managing the life-cycle costs of a system cannot be overemphasized. Beyond a point, however, it is better to replace a system rather than maintain it. We derive model and operating policy that reduces the sum of maintenance and replacement costs in the useful life of a software system. The main goal is to compare uniform (occurring at fixed time intervals) versus flexible (occurring at varying, planned time intervals) polices for maintenance and replacement. The model draws from the empirical works of earlier researchers to consider 1) inclusion of user requests for maintenance, 2) scale economies in software maintenance, 3) efficiencies derived from replacing old software technology with new software technology, and 4) the impact of software reuse on replacement and maintenance. Results from our model show that the traditional practice of maintaining or replacing a software system at uniform time intervals may not be optimal. We also find that an increase in software reuse leads to more frequent replacement, but the number of maintenance activities is not significantly impacted. © 2005 IEEE.",Cost models; Optimal scheduling; Software maintenance and replacement,"Tan Y., Mookerjee V.S.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21244451607&doi=10.1109%2fTSE.2005.30&partnerID=40&md5=316666e923ca42d5d54a816287b8d786,"University of Washington Business School, Box 353200, Seattle, WA 98195, United States; University of Texas at Dallas, School of Management, Richardson, TX 75083-0688, United States",,English,00985589,
Scopus,Using prior-phase effort records for re-estimation during software projects,"Estimating the effort required for software process activities continues to present difficulties for software engineers, particularly given the uncertainty and subjectivity associated with the many factors that can influence effort. It is therefore advisable that managers review their estimates and plans on an ongoing basis during each project so that growing certainty can be harnessed in order to improve their management of future project tasks. We investigate the potential of using effort data recorded for completed project tasks to predict the effort needed for subsequent activities. Our approach is tested against data collected from sixteen projects undertaken by a single organization over a period of eighteen months. Our findings suggest that, at least in this case, the idea that there are 'standard proportions' of effort for particular development activities does not apply. Estimating effort on this basis would not have improved the management of these projects. We did find, however, that in most cases simple linear regression enabled us to produce better estimates than those provided by the project managers. Moreover, combining the managers' estimates with those produced by regression modeling also led to improvements in predictive accuracy. These results indicate that, in this organization, prior-phase effort data could be used to augment the estimation process already in place in order to improve the management of subsequent process tasks. This provides further confirmation of the value of local data and the benefits of quite simple quantitative analysis methods. © 2003 IEEE.",Accuracy; Design engineering; Linear regression; Predictive models; Project management; Size measurement; Software engineering; Software measurement; Testing; Uncertainty,"MacDonell S.G., Shepperd M.J.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943195178&doi=10.1109%2fMETRIC.2003.1232457&partnerID=40&md5=46a0110666ecc188fc9b4292d0e49e1c,"School of Computer and Information Sciences, Auckland University of Technology, Private Bag 92006, Auckland, New Zealand; Empirical Software Engineering Research Group, School of Design, Engineering and Computing, Bournemouth University, Bournemouth, BH1 3LT, United Kingdom",IEEE Computer Society,English,15301435,0769519873
Scopus,Managing uncertainty in project portfolio cost estimation,"Although typically a software development organisation is involved in more than one project simultaneously, the available tools in the area of software cost estimation deal mostly with single software projects. In order to calculate the possible cost of the entire project portfolio, one must combine the single project estimates taking into account the uncertainty involved. In this paper, statistical simulation techniques are used to calculate confidence intervals for the effort needed for a project portfolio. The overall approach is illustrated through the adaptation of the analogy-based method for software cost estimation to cover multiple projects. © 2001 Elsevier Science B.V. All rights reserved.",Confidence interval; Project portfolio; Software cost estimation; Statistical simulation; Uncertainty management,"Stamelos I., Angelis L.",2001,Journal,Information and Software Technology,10.1016/S0950-5849(01)00183-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035889328&doi=10.1016%2fS0950-5849%2801%2900183-5&partnerID=40&md5=734a938d920df9bfdbfe6c18141e2343,"Department of Informatics, Aristotle University, Thessaloniki, Greece",,English,09505849,
Scopus,The impact of parameter tuning on software effort estimation using learning machines,"Background: The use of machine learning approaches for software effort estimation (SEE) has been studied for more than a decade. Most studies performed comparisons of different learning machines on a number of data sets. However, most learning machines have more than one parameter that needs to be tuned, and it is unknown to what extent parameter settings may affect their performance in SEE. Many works seem to make an implicit assumption that parameter settings would not change the outcomes significantly. Aims: To investigate to what extent parameter settings affect the performance of learning machines in SEE, and what learning machines are more sensitive to their parameters. Method: Considering an online learning scenario where learning machines are updated with new projects as they become available, systematic experiments were performed using five learning machines under several different parameter settings on three data sets. Results: While some learning machines such as bagging using regression trees were not so sensitive to parameter settings, others such as multilayer perceptrons were affected dramatically. Combining learning machines into bagging ensembles helped making them more robust against different parameter settings. The average performance of k-NN across different projects was not so much affected by different parameter settings, but the parameter settings that obtained the best average performance across time steps were not so consistently the best throughout time steps as in the other approaches. Conclusions: Learning machines that are more/less sensitive to different parameter settings were identified. The different sensitivity obtained by different learning machines shows that sensitivity to parameters should be considered as one of the criteria for evaluation of SEE approaches. A good learning machine for SEE is not only one which is able to achieve superior performance, but also one that is either less dependent on parameter settings or to which good parameter choices are easy to make.",Ensembles; Machine learning; Online learning; Sensitivity to parameters; Software effort estimation,"Song L., Minku L.L., Yao X.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2499393.2499396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924914270&doi=10.1145%2f2499393.2499396&partnerID=40&md5=2f3c7dae1d620883db867df316e02f1e,"CERCIA, School of Computer Science, University of Birmingham, Edgbaston, Birmingham, B15 2TT, United Kingdom",Association for Computing Machinery,English,,9781450320160
Scopus,Reducing test effort: A systematic mapping study on existing approaches,"Context: Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective: The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method: Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results: In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion: The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected. © 2012 Elsevier B.V. All rights reserved.",Efficiency improvement; Mapping study; Quality assurance; Software testing; Test effort reduction,"Elberzhager F., Rosbach A., Münch J., Eschbach R.",2012,Journal,Information and Software Technology,10.1016/j.infsof.2012.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863453536&doi=10.1016%2fj.infsof.2012.04.007&partnerID=40&md5=86254d505217390ca5ea640033ece969,"Fraunhofer Institute for Experimental Software Engineering (IESE), Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; P.O. Box 68 (Gustaf Hällströmin katu 2b), 00014 Helsinki, Finland; ITK Engineering AG, Luitpoldstraße 59, 76863 Herxheim, Germany",,English,09505849,
Scopus,The use of search-based optimization techniques to schedule and staff software projects: An approach and an empirical study,"Allocating resources to a software project and assigning tasks to teams constitute crucial activities that affect project cost and completion time. Finding a solution for such a problem is NP-hard; this requires managers to be supported by proper tools for performing such an allocation. This paper shows how search-based optimization techniques can be combined with a queuing simulation model to address these problems. The obtained staff and task allocations aim to minimize the completion time and reduce schedule fragmentation. The proposed approach allows project managers to run multiple simulations, compare results and consider trade-offs between increasing the staffing level and anticipating the project completion date and between reducing the fragmentation and accepting project delays. The paper presents results from the application of the proposed search-based project planning approach to data obtained from two large-scale commercial software maintenance projects. Copyright © 2011 John Wiley & Sons, Ltd.",genetic algorithms in software engineering; search-based software engineering; software project management,"Di Penta M., Harman M., Antoniol G.",2011,Journal,Software - Practice and Experience,10.1002/spe.1001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952821836&doi=10.1002%2fspe.1001&partnerID=40&md5=c4e9b86588ce86971fae11a52f54ed4f,"Department of Engineering, University of Sannio-Palazzo Ex Poste, Via Traiano, 82100 Benevento, Italy; Department of Computer Science, King's College London-Strand, London WC2R 2LS, United Kingdom; Department of Génie Informatique, École Polytechnique de Montréal, Montréal, QC, Canada",,English,00380644,
Scopus,Comparison of weighted grey relational analysis for software effort estimation,"In recent years, grey relational analysis (GRA), a similarity-based method, has been proposed and used in many applications. However, we found that most traditional GRA methods only consider nonweighted similarity for predicting software development effort. In fact, nonweighted similarity may cause biased predictions, because each feature of a project may have a different degree of relevance to the development effort. Therefore, this paper proposes six weighted methods, including nonweighted, distance-based, correlative, linear, nonlinear, and maximal weights, to be integrated into GRA for software effort estimation. Numerical examples and sensitivity analyses based on four public datasets are used to show the performance of the proposed methods. The experimental results indicate that the weighted GRA can improve estimation accuracy and reliability from the nonweighted GRA. The results also demonstrate that the weighted GRA performs better than other estimation techniques and published results. In summary, we can conclude that weighted GRA can be a viable and alternative method for predicting software development effort. © 2010 Springer Science+Business Media, LLC.",Grey relational analysis (GRA); Software cost; Software development effort; Software effort estimation; Weighted assignment,"Hsu C.-J., Huang C.-Y.",2011,Journal,Software Quality Journal,10.1007/s11219-010-9110-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751648605&doi=10.1007%2fs11219-010-9110-y&partnerID=40&md5=ab74ebc4dc4b872db0d609f7fb10a03d,"Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",Kluwer Academic Publishers,English,09639314,
Scopus,A replicated comparison of cross-company and within-company effort estimation models using the ISBSG database,"Four years ago was the last time the ISBSG database was used to compare the effort prediction accuracy between cross-company and within-company cost models. Since then more than 2,000 projects have been volunteered to this database, which may have changed the trends previously observed. This paper therefore replicates a previous study by investigating how successful a cross-company cost model is: i) to estimate effort for projects that belong to a single company and were not used to build the cross-company model; ii) compared to a within-company cost model. Our within-company data set had data on 184 software projects from a single company and our cross-company data set employed data on 672 software projects. Our results did not corroborate those from the previous study, showing that predictions based on the within-company model were not significantly more accurate than those based on the cross-company model. We analysed the data using forward stepwise regression. © 2005 IEEE.",Cross-company estimation models; Effort estimation; Regression-based estimation models; Replication study; Software projects; Within-company estimation model,"Mendes E., Lokan C., Harrison R., Triggs C.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749066964&doi=10.1109%2fMETRICS.2005.4&partnerID=40&md5=7ac115e5b5978907931e5c86a7308bec,"Computer Science Department, University of Auckland, Private Bag, 92019, Auckland, New Zealand; Statistics Department, University of Auckland, Private Bag, 92019, Auckland, New Zealand; School of Information Technology and Electrical Engineering, UNSW at ADFA, Australian Defence Force Academy, Canberra, ACT 2600, Australia",,English,15301435,0769523714; 9780769523712
Scopus,Disaggregating and calibrating the CASE tool variable in COCOMO II,"CASE (Computer Aided Software Engineering) tools are believed to have played a critical role in improving software productivity and quality by assisting tasks in software development processes since 1970s, Several parametric software cost models adopt ""use of software tools"" as one of the environmental factors that affects software development productivity. Several software cost models assess the productivity impacts of CASE tools based just on breadth of tool coverage without considering other productivity dimensions such as degree of integration, tool maturity, and user support. This paper provides an extended set of tool rating scales based on the completeness of tool coverage, the degree of tool integration, and tool maturity/user support. Those scales are used to refine the way in which CASE tools are effectively evaluated within COCOMO (COnstructive COst MOdel) II. In order to find the best fit of weighting values for the extended set of tool rating scales in the extended research model, a Bayesian approach is adopted to combine two sources of (expert-judged and data-determined) information to increase prediction accuracy. The extended model using the three TOOL rating scales is validated by using the cross-validation methodologies, data splitting, and bootstrapping. This approach can be used to disaggregate other parameters that have significant impacts on software development productivity and to calibrate the best-fit weight values based on data-determined and expert-judged distributions. It results in an increase in the prediction accuracy in software parametric cost estimation models and an improvement in insights on software productivity investments.",Bayesian analysis; CASE (Computer Aided Software Engineering); Empirical studies; Prediction accuracy; Software cost models; Software metrics; Software productivity; Software tools,"Baik J., Boehm B., Steece B.M.",2002,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2002.1049401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036857032&doi=10.1109%2fTSE.2002.1049401&partnerID=40&md5=927dddc011b993b8c8785554103b99b5,"Motorola Labs, Software and Systems Eng. Res. Lab., 1303 E. Algonquin Rd., Schaumburg, IL 60196, United States; Center for Software Engineering, University of Southern California, Los Angeles, CA 90089, United States; Marshall School of Business, University of Southern California, Los Angeles, CA 90089, United States",,English,00985589,
Scopus,Modeling clones evolution through time series,"The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive models. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2.0.10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4%. An higher prediction error was observed only in correspondence of major system redesign.",Clone analysis; Software evolution; Time series,"Antoniol G., Casazza G., Di Penta M., Merlo E.",2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956616360&doi=10.1109%2fICSM.2001.972740&partnerID=40&md5=1dd71aac569c1bce06d886debd69c15d,"University of Sannio, Faculty of Engineering, Benevento, Italy; University of Naples Federico II, DIS, Naples, Italy; Ecole Polytechnique de Montreal, Montreal, Que., Canada",,English,,
Scopus,A systematic literature review on the industrial use of software process simulation,"Context Software process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry. Objective To assess the conflicting standpoints on the usefulness of SPSM. Method A systematic literature review was performed to identify, assess and aggregate empirical evidence on the usefulness of SPSM. Results In the primary studies, to date, the persistent trend is that of proof-of-concept applications of software process simulation for various purposes (e.g. estimation, training, process improvement, etc.). They score poorly on the stated quality criteria. Also only a few studies report some initial evaluation of the simulation models for the intended purposes. Conclusion There is a lack of conclusive evidence to substantiate the claimed usefulness of SPSM for any of the intended purposes. A few studies that report the cost of applying simulation do not support the claim that it is an inexpensive method. Furthermore, there is a paramount need for improvement in conducting and reporting simulation studies with an emphasis on evaluation against the intended purpose. © 2014 Elsevier Inc. All rights reserved.",Evidence based software engineering; Software process simulation; Systematic literature review,"Ali N.B., Petersen K., Wohlin C.",2014,Journal,Journal of Systems and Software,10.1016/j.jss.2014.06.059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908160600&doi=10.1016%2fj.jss.2014.06.059&partnerID=40&md5=62c4c0af6fd2596cfc83de88d7820919,"Blekinge Institute of Technology, Karlskrona, Sweden",Elsevier Inc.,English,01641212,
Scopus,A systematic mapping study on dynamic metrics and software quality,"Several important aspects of software product quality can be evaluated using dynamic metrics that effectively capture and reflect the software's true runtime behavior. While the extent of research in this field is still relatively limited, particularly when compared to research on static metrics, the field is growing, given the inherent advantages of dynamic metrics. The aim of this work is to systematically investigate the body of research on dynamic software metrics to identify issues associated with their selection, design and implementation. Mapping studies are being increasingly used in software engineering to characterize an emerging body of research and to identify gaps in the field under investigation. In this study we identified and evaluated 60 works based on a set of defined selection criteria. These studies were further classified and analyzed to identify their relativity to future dynamic metrics research. The classification was based on three different facets: research focus, research type and contribution type. We found a strong body of research related to dynamic coupling and cohesion metrics, with most works also addressing the abstract notion of software complexity. Specific opportunities for future work relate to a much broader range of quality dimensions. © 2012 IEEE.",dynamic analysis; dynamic metrics; mapping study; software metrics; software quality,"Tahir A., MacDonell S.G.",2012,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2012.6405289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873100927&doi=10.1109%2fICSM.2012.6405289&partnerID=40&md5=5e8bbccc9fcdbe846733e44db96126af,"SERL, School of Computing and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand",,English,,9781467323123
Scopus,An estimation model for test execution effort,"Testing is an important activity to ensure software quality. Big organizations can have several development teams with their products being tested by overloaded test teams. In such situations, test team managers must be able to properly plan their schedules and resources. Also, estimates for the required test execution effort can be an additional criterion for test selection, since effort may be restrictive in practice. Nevertheless, this information is usually not available for test cases never executed before. This paper proposes an estimation model for test execution effort based on the test specifications. For that, we define and validate a measure of size and execution complexity of test cases. This measure is obtained from test specifications written in a controlled natural language. We evaluated the model through an empirical study on the mobile application domain, which results suggested an accuracy improvement when compared with estimations based only on historical test productivity. © 2007 IEEE.",,"Aranha E., Borba P.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949095132&doi=10.1109%2fESEM.2007.19&partnerID=40&md5=a3c509026b81c14d71975088b0025940,"Informatics Center, Federal University of Pernambuco, PO Box 7851, Recife, PE, Brazil; Mobile Devices R and D, Motorola Industrial Ltda, Rod SP 340 - Km 128, 7A - 13820 000 Jaguariuna/SP, Brazil",,English,,0769528864; 9780769528861
Scopus,Usability evaluation based on Web design perspectives,"Given the growth in the number and size of Web Applications worldwide, Web quality assurance, and more specifically Web usability have become key success factors. Therefore, this work proposes a usability evaluation technique based on the combination of Web design perspectives adapted from existing literature, and heuristics. This new technique is assessed using a controlled experiment aimed at measuring the efficiency and effectiveness of our technique, in comparison to Nielsen's heuristic evaluation. Results indicated that our technique was significantly more effective than and as efficient as Nielsen's heuristic evaluation. © 2007 IEEE.",,"Conte T., Massollar J., Mendes E., Travassos G.H.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.84,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949094621&doi=10.1109%2fESEM.2007.84&partnerID=40&md5=670c412679aed9809c36ad0275dd9ff0,"PESC-COPPE, UFRJ, Cx. Postal 68.511, CEP 21945-970, Rio de Janeiro, RJ, Brazil; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,0769528864; 9780769528861
Scopus,Fuzzy decision tree approach for embedding risk assessment information into software cost estimation model,"As software cost drivers are fuzzy and uncertain, software cost estimates are prone to a certain degree of estimation errors especially in their early stages of software development life cycle. However, most of the existing software cost estimation models in present literature only generate a single point estimate and do not explicitly reveal the degree of risks caused by their inaccuracies. This paper proposes a fuzzy decision tree approach for embedding risk assessment information into a software cost estimation model. Using this model, one may be able to determine the software cost estimate as well as the estimation error in the form of a fuzzy set. In verifying the merits of this model, we have used the 63 historical project data in the COCOMO model. The validation result shows that our proposed model reveals the risk assessment of the generated software cost estimate, and at the same time yields an even more accurate result as compared to the original COCOMO model.",Fuzzy decision tree; Risk assessment; Software cost estimation model; Software measurement and analysis; Software project planning,"Huang S.-J., Lin C.-Y., Chiu N.-H.",2006,Conference,Journal of Information Science and Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645666666&partnerID=40&md5=8ca1611758d91f74717a863bed8c9ce4,"Department of Information Management, National Taiwan University of Science and Technology, Taipei, 106, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan; Software Engineering and Management Laboratory (SEML), Taiwan; Software Quality Promotion Committee, Chinese Society for Quality, China; Network Attached Storage System in Linux Platform, Software Department, Wistron Corporation, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taiwan, Taiwan",,English,10162364,
Scopus,Missing data techniques in analogy-based software development effort estimation,"Missing Data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort prediction systems. This paper investigates the use of missing data (MD) techniques with two analogy-based software development effort estimation techniques: Classical Analogy and Fuzzy Analogy. More specifically, we analyze the predictive performance of these two analogy-based techniques when using toleration, deletion or k-nearest neighbors (KNN) imputation techniques. A total of 1512 experiments were conducted involving seven data sets, three MD techniques (toleration, deletion and KNN imputation), three missingness mechanisms (MCAR: missing completely at random, MAR: missing at random, NIM: non-ignorable missing), and MD percentages from 10 percent to 90 percent. The results suggest that Fuzzy Analogy generates more accurate estimates in terms of the Standardized Accuracy measure (SA) than Classical Analogy regardless of the MD technique, the data set used, the missingness mechanism or the MD percentage. Moreover, this study found that the use of KNN imputation, rather than toleration or deletion, may improve the prediction accuracy of both analogy-based techniques. However, toleration, deletion and KNN imputation are affected by the missingness mechanism and the MD percentage, both of which have a strong negative impact upon effort prediction accuracy. © 2016 Elsevier Inc. All rights reserved.",Analogy-based software development effort estimation; Imputation; Missing data,"Idri A., Abnane I., Abran A.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2016.04.058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966340503&doi=10.1016%2fj.jss.2016.04.058&partnerID=40&md5=014ecb6ca07ef9fbb58c0cbfb8c1dc3a,"Software Project Management Research Team, ENSIAS, Mohamed v University of Rabat, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, Montréal, Canada",Elsevier Inc.,English,01641212,
Scopus,A mapping study to investigate component-based software system metrics,"A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future. © 2012 Elsevier Inc.",Component-based software system; Software components; Software metrics; Software quality; Systematic mapping study,"Abdellatief M., Sultan A.B.M., Ghani A.A.A., Jabar M.A.",2013,Journal,Journal of Systems and Software,10.1016/j.jss.2012.10.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872676227&doi=10.1016%2fj.jss.2012.10.001&partnerID=40&md5=231182a8ef6c90d93b9416e72d49c7b4,"Department of Information System, Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400 Serdang, Selangor, Malaysia",,English,01641212,
Scopus,Searching for rules to detect defective modules: A subgroup discovery approach,"Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases. © 2011 Elsevier Inc. All rights reserved.",Defect prediction; Imbalanced datasets; Rules; Subgroup discovery,"Rodríguez D., Ruiz R., Riquelme J.C., Aguilar-Ruiz J.S.",2012,Journal,Information Sciences,10.1016/j.ins.2011.01.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857597379&doi=10.1016%2fj.ins.2011.01.039&partnerID=40&md5=c1046e12a33ccc68513c038b31f99bb8,"Department of Computer Science, University of Alcalá, Ctra. Barcelona, Km. 31.6, 28871 Alcalá de Henares, Madrid, Spain; School of Engineering, Pablo de Olavide University, Ctra. Utrera km. 1, 41013 Seville, Spain; Department of Computer Science, University of Seville, Avda. Reina Mercedes s/n, 41012 Seville, Spain",,English,00200255,
Scopus,How to find relevant data for effort estimation?,"Background: Building effort estimators requires the training data. How can we find that data? It is tempting to cross the boundaries of development type, location, language, application and hardware to use existing datasets of other organizations. However, prior results caution that using such cross data may not be useful. Aim: We test two conjectures: (1) instance selection can automatically prune irrelevant instances and (2) retrieval from the remaining examples is useful for effort estimation, regardless of their source. Method: We selected 8 cross-within divisions (21 pairs of withincross subsets) out of 19 datasets and evaluated these divisions under different analogy-based estimation (ABE) methods. Results: Between the within & cross experiments, there were few statistically significant differences in (i) the performance of effort estimators; or (ii) the amount of instances retrieved for estimation. Conclusion: For the purposes of effort estimation, there is little practical difference between cross and within data. After applying instance selection, the remaining examples (be they from within or from cross source divisions) can be used for effort estimation. © 2011 IEEE.",Cross resource; k-NN; Software cost estimation; Within resource,"Kocaguneli E., Menzies T.",2011,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/esem.2011.34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858715036&doi=10.1109%2fesem.2011.34&partnerID=40&md5=4acb0dc8657cb1b8e0a6b90569f7f8ab,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, United States",IEEE Computer Society,English,19493770,
Scopus,An empirical study of the impact of team size on software development effort,"In this paper, we investigate the impact of team size on the software development effort. Using field data of over 200 software projects from various industries, we empirically test the impact of team size and other variables - such as software size in function points, ICASE tool and programming language type - on software development effort. Our results indicate that software size in function points significantly impacts the software development effort. The two-way interactions between function points and use of ICASE tool, and function points and language type are significant as well. Additionally, the interactions between team size and programming language type, and team size and use of ICASE tool were all significant. © Springer Science+Business Media, LLC 2007.",CASE tools; Cost estimation; Programming teams; Software engineering,"Pendharkar P.C., Rodger J.A.",2007,Journal,Information Technology and Management,10.1007/s10799-006-0005-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36148989647&doi=10.1007%2fs10799-006-0005-3&partnerID=40&md5=2e9130547123a7709ad88bd8eee0d987,"School of Business Administration, Capital College, Pennsylvania State University, 777 W. Harrisburg Pike, Middletown, PA 17057, United States; MIS and Decision Sciences, Eberly College of Business and Information Technology, Indiana University of Pennsylvania, Indiana, PA 15705, United States",,English,1385951X,
Scopus,Software effort estimation based on use cases,"Software effort and cost estimation is a very important activity that includes very uncertain elements. In the context of object oriented software, traditional methods and metrics were extended to help managers in this activity. The metric Use Case Points (UCP) is an example of metric that can be used. UCP considers functional aspects of the Use Case (UC) Model, widely used in most organizations in the early phases of the development. However, the metric UCP presents some limitations mainly related to the granularity of the UC. To overcome these limitations, this paper introduces two metrics, also based on UCs. The first one, named USP (Use Case Size Points), considers the internal structures of the UC and better captures its functionality. The second one, named FUSP (Fuzzy Use Case Size Points), considers concepts of the Fuzzy Set Theory to create gradual classifications that better deal with uncertainty. Results from an empirical evaluation show the applicability and some advantages of the proposed metrics. © 2006 IEEE.",Function points; Fuzzy Theory; Prediction; Software Size; Use Case Points,"Braz M.R., Vergilio S.R.",2006,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/COMPSAC.2006.77,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247533401&doi=10.1109%2fCOMPSAC.2006.77&partnerID=40&md5=c81d836427f0c0bb88769543173cdf76,"Computer Science Department, Federal University of Parana - UFPR, CP: 19081, CEP: 81531-970, Curitiba, Brazil",,English,07303157,0769526551; 9780769526553
Scopus,A survey of component based system quality assurance and assessment,"Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems. © 2005 Elsevier B.V. All rights reserved.",,"Mahmood S., Lai R., Kim Y.S., Kim J.H., Park S.C., Oh H.S.",2005,Journal,Information and Software Technology,10.1016/j.infsof.2005.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21244433139&doi=10.1016%2fj.infsof.2005.03.007&partnerID=40&md5=5990febfcfd3c89dd9067fd34358f02e,"Department of Computer Science and Computer Engineering, La Trobe University, Melbourne, Vic. 3086, Australia; College of Software, Kyungwon University, Songnam, Kyunggi-Do, 461-701, South Korea",Elsevier,English,09505849,
Scopus,Early Web size measures and effort prediction for Web costimation,"Size measures for Web costimation proposed in the literature are invariably related to implemented Web applications. Even when targeted at measuring functionality based on function point analysis, researchers only considered the final Web application, rather than requirements documentation generated using any existing Web development methods. This makes their usefulness as early effort predictors questionable. In addition, it is believed that company-specific data provide a better basis for accurate estimates. Many software engineering researchers have compared the accuracy of company-specific data with multiorganisation databases. However the datasets employed were comprised of data from conventional applications. To date no similar comparison has been adopted for Web project datasets. It has two objectives: The first is to present a survey where early size measures for Web costimation were identified using data collected from 133 Web companies worldwide. All companies included in the survey used Web forms to give quotes on Web development projects, based on gathered size measures. The second is to compare the prediction accuracy of a Web company-specific data with data from a multiorganisation database. Both datasets were obtained via Web forms, used as part of a research project called Tukutuku. Our results show that best predictions were obtained for company-specific dataset, for the two estimation techniques employed. © 2003 IEEE.",Application software; Computer science; Costs; Databases; HTML; Java; Predictive models; Size measurement; Software engineering; XML,"Mendes E., Mosley N., Counsell S.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943163408&doi=10.1109%2fMETRIC.2003.1232452&partnerID=40&md5=6f4276130a19a0e030bf94cf81074294,"Computer Science Department, University of Auckland, Auckland, New Zealand; MxM Technology, P.O. Box 3539 Shortland Street, Auckland, New Zealand; Computer Science Department, Birkbeck College, University of London, London, United Kingdom",IEEE Computer Society,English,15301435,0769519873
Scopus,An empirical study using task assignment patterns to improve the accuracy of software effort estimation,"In most software development organizations, there is seldom a one-to-one mapping between software developers and development tasks. It is frequently necessary to concurrently assign individuals to multiple tasks and to assign more than one individual to work cooperatively on a single task. A principal goal in making such assignments should be to minimize the effort required to complete each task. But what impact does the manner in which developers are assigned to tasks have on the effort requirements? This paper identifies four task assignment factors: team size, concurrency, intensity, and fragmentation. These four factors are shown to improve the predictive ability of the well-known Intermediate COCOMO cost estimation model. A parsimonious effort estimation model is also derived that utilizes a subset of the task assignment factors and Unadjusted Function Points. For the data examined, this parsimonious model is shown to have goodness of fit and quality of estimation superior to that of the COCOMO model, while utilizing fewer cost factors.",COCOMO; Project task assignment; Software effort estimation,"Smith R.K., Hale J.E., Parrish A.S.",2001,Journal,IEEE Transactions on Software Engineering,10.1109/32.910861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035278719&doi=10.1109%2f32.910861&partnerID=40&md5=50aad7f6f9cc430e115f5bd9ea80f525,"IEEE Computer Society, United States; Mathematical, Computing, and Information Science Department, Jacksonville State University, 700 Pelham Rd., N. Jacksonville, AL 36265, United States; Area of MIS, Culverhouse College of Commerce and Business Administration, University of Alabama, Tuscaloosa, AL 35487, United States; Department of Computer Science, University of Alabama, Tuscaloosa, AL 35487-0290, United States",,English,00985589,
Scopus,An empirical analysis of function point adjustment factors,"In function point analysis, fourteen ""general systems characteristics"" (GSCs) are used to construct a ""value adjustment factor"" (VAF), with which a basic function point count is adjusted. Although the GSCs and VAF have been criticized on both theoretical and practical grounds, they are used by many practitioners. This paper reports on an empirical investigation into their use and practical value. We conclude that recording the GSCs may be useful for understanding project cost drivers and for comparing similar projects, but the VAF should not be used: doubts about its construction are not balanced by any practical benefit. A new formulation is needed for using the GSCs to explain effort; factors identified here could guide further research. © 2000 Elsevier Science B.V. All rights reserved.",Function points; Software cost estimation; Software metrics,Lokan C.J.,2000,Journal,Information and Software Technology,10.1016/s0950-5849(00)00108-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003650759&doi=10.1016%2fs0950-5849%2800%2900108-7&partnerID=40&md5=093dc32245a8d351ab02ad890ffa1690,"School of Computer Science, Australian Defence Force Academy, University of New South Wales, Canberra, ACT 2600, Australia",Elsevier,English,09505849,
Scopus,Quantifying the effects of process improvement on effort,[No abstract available],,Clark B.K.,2000,Journal,IEEE Software,10.1109/52.895170,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034317410&doi=10.1109%2f52.895170&partnerID=40&md5=2979f16fbb33af89eca53beb620e043c,"Software Metrics, Inc., University of Southern California, United States",,English,07407459,
Scopus,On building prediction systems for software engineers,"Building and evaluating prediction systems is an important activity for software engineering researchers. Increasing numbers of techniques and datasets are now being made available. Unfortunately systematic comparison is hindered by the use of different accuracy indicators and evaluation processes. We argue that these indicators are statistics that describes properties of the estimation errors or residuals and that the sensible choice of indicator is largely governed by the goals of the estimator. For this reason it may be helpful for researchers to provide a range of indicators. We also argue that it is useful to formally test for significant differences between competing prediction systems and note that where only a few cases are available this can be problematic, in other words the research instrument may have insufficient power. We demonstrate that this is the case for a well known empirical study of cost models. Simulation, however, could be one means of overcoming this difficulty.",,"Shepperd M., Cartwright M., Kadoda G.",2000,Journal,Empirical Software Engineering,10.1023/A:1026582314146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034314869&doi=10.1023%2fA%3a1026582314146&partnerID=40&md5=1b5c2a3e0fcd39f8b2f5ea3b2c8a8a23,"Empirical Software Engineering Research Group, School of Design, Engineering and Computing, Bournemouth Univ, Talbot Campus, BH12 5BB, United Kingdom",,English,13823256,
Scopus,"Function point sizing: Structure, validity and applicability","This paper reports on a study carried out within a software development organization to evaluate the use of function points as a measure of early lifecycle software size. There were three major aims to the research: firstly to determine the extent to which the component elements of function points were independent of each other and thus appropriate for an additive model of size; secondly to investigate the relationship between effort and (1) the function point components, (2) unadjusted function points, and (3) adjusted function points, to determine whether the complexity weightings and technology adjustments were adding to the effort explanation power of the metric; and thirdly to investigate the suitability of function points for sizing in client server developments. The results show that the component parts are not independent of each other which supports an earlier study in this area. In addition the complexity weights and technology factors do not improve the effort/size model, suggesting that a simplified sizing metric may be appropriate. With respect to the third aim it was found that the function point metric revealed a much lower productivity in the client server environment. This likely is a reflection of cost of the introduction of newer technologies but is in need of further research. © 1996 Kluwer Academic Publishers.",Client-server; Complexity adjustment; Effort model; Function points; Software size,"Jeffery R., Stathis J.",1996,Journal,Empirical Software Engineering,10.1007/BF00125809,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029746756&doi=10.1007%2fBF00125809&partnerID=40&md5=6ef4b25c363a22fc82f2b2fb3e61530a,"Centre for Advanced Empirical Software Research (CAESAR), School of Information Systems, UNSW, Sydney, NSW 2052, Australia",Springer Netherlands,English,13823256,
Scopus,Search-based software project management,"Project management presents the manager with a complex set of related optimisation problems. Decisions made can more profoundly affect the outcome of a project than any other activity. In the chapter, we provide an overview of Search-Based Software Project Management, in which search-based software engineering (SBSE) is applied to problems in software project management. We show how SBSE has been used to attack the problems of staffing, scheduling, risk, and effort estimation. SBSE can help to solve the optimisation problems the manager faces, but it can also yield insight. SBSE therefore provides both decision making and decision support. We provide a comprehensive survey of search-based software project management and give directions for the development of this subfield of SBSE. © 2014 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,"Ferrucci F., Harman M., Sarro F.",2014,Book Chapter,Software Project Management in a Changing World,10.1007/978-3-642-55035-5_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908677680&doi=10.1007%2f978-3-642-55035-5_15&partnerID=40&md5=c906fcb77687a7e2618f5be0e8f4d24d,"DISTRA, University of Salerno, Salerno, Italy; CREST, Department of Computer Science, University College London, London, United Kingdom",Springer-Verlag Berlin Heidelberg,English,,9783642550355; 3642550347; 9783642550348
Scopus,Class level fault prediction using software clustering,"Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system. © 2013 IEEE.",Empirical Study; Fault Prediction; Software Clustering,"Scanniello G., Gravino C., Marcus A., Menzies T.",2013,Conference,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",10.1109/ASE.2013.6693126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893553028&doi=10.1109%2fASE.2013.6693126&partnerID=40&md5=ac10724f14f0f8caa0bf9d498cde6b11,"University of Basilicata, Italy; University of Salerno, Italy; Wayne State University, United States; West Virginia University, United States",,English,,9781479902156
Scopus,A survey on software cost estimation in the Chinese software industry,"Although a lot of attention has been paid to software cost estimation since 1960, making accurate effort and schedule estimation is still a challenge. To collect evidence and identify potential areas of improvement in software cost estimation, it is important to investigate the estimation accuracy, the estimation method used, and the factors influencing the adoption of estimation methods in current industry. This paper analyzed 112 projects from the Chinese software project benchmarking dataset and conducted questionnaire survey on 116 organizations to investigate the above information. The paper presents the current situations related to software project estimation in China and provides evidence-based suggestions on how to improve software project estimation. Our survey results suggest, e.g., that large projects were more prone to cost and schedule overruns, that most computing managers and professionals were neither satisfied nor dissatisfied with the project estimation, that very few organizations (15%) used model-based methods, and that the high adoption cost and insignificant benefit after adoption were the main causes for low use of model-based methods. Copyright 2008 ACM.",Estimation method; Estimation model; Software cost estimation,"Yang D., Wang Q., Li M., Yang Y., Ye K., Du J.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949174268&doi=10.1145%2f1414004.1414045&partnerID=40&md5=e03faa4863591ae1542afa2e7178fd89,"Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China; Graduate University, Chinese Academy of Sciences, Beijing, 100190, China",,English,,9781595939715
Scopus,Employing use cases to early estimate effort with simpler metrics,"In order to calculate size and to estimate effort in applications, the standard method most usually used is function points, which has been used with good results in the development of industrial software for some time. However, some aspects should be improved, namely: the time at which the estimation of effort is performed and the margin of error in the effort estimation. Consequently, another size metric which could be used to obtain more accurate estimations should be found. This article presents two other size metrics for projects based on use cases: transactions and entity objects. Effort is estimated by using the technique mean productivity value. There is also a description of two case studies, one which involved four academic projects and the other one which involved four industrial projects. They were developed in order to compare the estimations obtained with each method. The result shows that the current way of estimating effort can be improved by using the number of transactions as a size metric and the technique mean productivity value. © Springer-Verlag London Limited 2008.",Early effort estimation; Metric; Usecases,"Robiolo G., Orosco R.",2008,Conference,Innovations in Systems and Software Engineering,10.1007/s11334-007-0043-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41649104840&doi=10.1007%2fs11334-007-0043-y&partnerID=40&md5=77a1e63c316e73bb7aef44fcd28cdf4a,"Universidad Austral, Buenos Aires, Argentina; Universidad Argentina de la Empresa, Buenos Aires, Argentina",,English,16145046,
Scopus,Replicating studies on cross- vs single-company effort models using the ISBSG Database,"In 2001 the ISBSG database was used by Jeffery et al. (Using public domain metrics to estimate software development effort. Proceedings Metrics'01, London, pp 16-27, 2001; S1) to compare the effort prediction accuracy between cross- and single-company effort models. Given that more than 2,000 projects were later volunteered to this database, in 2005 Mendes et al. (A replicated comparison of cross-company and within-company effort estimation models using the ISBSG Database, in Proceedings of Metrics'05, Como, 2005; S2) replicated S1 but obtained different results. The difference in results could have occurred due to legitimate differences in data set patterns; however, they could also have occurred due to differences in experimental procedure given that S2 was unable to employ exactly the same experimental procedure used in S1 because S1's procedure was not fully documented. Recently, we applied S2's experimental procedure to the ISBSG database version used in S1 (release 6) to assess if differences in experimental procedure would have contributed towards different results (Lokan and Mendes, Cross-company and single-company effort models using the ISBSG Database: a further replicated study, Proceedings of the ISESE'06, pp 75-84, 2006; S3). Our results corroborated those from S1, suggesting that differences in the results obtained by S2 were likely caused by legitimate differences in data set patterns. We have since been able to reconstruct the experimental procedure of S1 and therefore in this paper we present both S3 and also another study (S4), which applied the experimental procedure of S1 to the data set used in S2. By applying the experimental procedure of S2 to the data set used in S1 (study S3), and the experimental procedure of S1 to the data set used in S2 (study S4), we investigate the effect of all the variations between S1 and S2. Our results for S4 support those of S3, suggesting that differences in data preparation and analysis procedures did not affect the outcome of the analysis. Thus, the different results of S1 and S2 are very likely due to fundamental differences in the data sets. © 2007 Springer Science+Business Media, LLC.",Cross-company estimation models; Effort estimation; Experimental procedure; Regression-based estimation models; Replication study; Single-company estimation model; Software projects,"Mendes E., Lokan C.",2008,Conference,Empirical Software Engineering,10.1007/s10664-007-9045-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37649019561&doi=10.1007%2fs10664-007-9045-5&partnerID=40&md5=dbc20ff976bc3de93ba47559420045e2,"Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand; School of IT and EE, UNSW at ADFA, Canberra, ACT 2600, Australia",,English,13823256,
Scopus,"Management competences, not tools and techniques: A grounded examination of software project management at WM-data","Traditional software project management theory often focuses on desk-based development of software and algorithms, much in line with the traditions of the classical project management and software engineering. This can be described as a tools and techniques perspective, which assumes that software project management success is dependent on having the right instruments available, rather than on the individual qualities of the project manager or the cumulative qualities and skills of the software organisation. Surprisingly, little is known about how (or whether) these tools techniques are used in practice. This study, in contrast, uses a qualitative grounded theory approach to develop the basis for an alternative theoretical perspective: that of competence. A competence approach to understanding software project management places the responsibility for success firmly on the shoulders of the people involved, project members, project leaders, managers. The competence approach is developed through an investigation of the experiences of project managers in a medium sized software development company (WM-data) in Denmark. Starting with a simple model relating project conditions, project management competences and desired project outcomes, we collected data through interviews, focus groups and one large plenary meeting with most of the company's project managers. Data analysis employed content analysis for concept (variable) development and causal mapping to trace relationships between variables. In this way we were able to build up a picture of the competences project managers use in their daily work at WM-data, which we argue is also partly generalisable to theory. The discrepancy between the two perspectives is discussed, particularly in regard to the current orientation of the software engineering field. The study provides many methodological and theoretical starting points for researchers wishing to develop a more detailed competence perspective of software project managers' work. © 2007 Elsevier B.V. All rights reserved.",Competence; Software engineering; Software project management,"Rose J., Pedersen K., Hosbond J.H., Kræmmergaard P.",2007,Journal,Information and Software Technology,10.1016/j.infsof.2007.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247101878&doi=10.1016%2fj.infsof.2007.02.005&partnerID=40&md5=9f577fb399485b9c27d9afcd62e1be3e,"Department of Computing Science, Aalborg University, Denmark; Department of Business Studies, Aarhus Schools of Business, Aarhus University, Denmark",,English,09505849,
Scopus,Estimating the development cost of custom software,"In this paper an approach for the estimation of software development costs is presented. The method is based on the characterization of the software to be developed in terms of project and environment attributes and comparison with some similar completed project(s) recovered from a historical database. A case study is also presented, focusing on the calibration and application of the method on 59 information systems implementing supply chain functions in industry. Various strategies are explored, the best of which predicted effort quite effectively, with a mean estimation error of 24% with respect to the actual effort. © 2002 Elsevier Science B.V. All rights reserved.",Cost estimation; Information system; Software development effort; Supply chain,"Stamelos I., Angelis L., Morisio M., Sakellaris E., Bleris G.L.",2003,Journal,Information and Management,10.1016/S0378-7206(02)00099-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041529920&doi=10.1016%2fS0378-7206%2802%2900099-X&partnerID=40&md5=3d36779704c1b1fb0c46cacdbd1f652b,"Department of Informatics, Aristotle University, Thessaloniki 54124, Greece; Politecnico di Torino, Corso Duca degli Abruzzi 24, Torino 10129, Italy; Singular International, 10A Dodekanisou Street, Thessaloniki 54626, Greece",,English,03787206,
Scopus,The application of case-based reasoning to early web project cost estimation,"Literature shows that over the years numerous techniques for estimating development effort have been suggested, derived from late project measures. However, to the successful management of software projects, estimates are necessary throughout the whole development life cycle. The objective of this paper is twofold. First, we describe the application of case-based reasoning (CBR) for estimating Web hypermedia development effort using measures collected at different stages in the development cycle. Second, we compare the prediction accuracy of those measures, obtained using different CBR configurations. Contrary to the expected, late measures did not show statistically significant better predictions than early measures.",,"Mendes E., Mosley N., Counsell S.",2002,Journal,Proceedings-IEEE Computer Society's International Computer Software and Applications Conference,10.1109/CMPSAC.2002.1045034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036389489&doi=10.1109%2fCMPSAC.2002.1045034&partnerID=40&md5=11027eeabbf273cb5142950b9c5af533,"Computer Science Department, The University of Auckland, Auckland, New Zealand; MXM Technology, P.O. Box 3139, Auckland, NZ, New Zealand; Birkbeck College, University of London, Computer Science Department, United Kingdom",,English,07303157,
Scopus,Fuzzy modeling of software effort prediction,Software sizing is an important management activity for both customer and developer that is characterized by uncertainty. Fuzzy system modeling offers a means to capture and logically reason with uncertainty. The paper investigates the application of fuzzy modeling techniques to two of the most widely used software effort prediction models: the Constructive Cost Model and the Function Points model. © 1998 IEEE.,,Ryder J.,1998,Conference,"1998 IEEE Information Technology Conference: Information Environment for the Future, IT 1998",10.1109/IT.1998.713380,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997081123&doi=10.1109%2fIT.1998.713380&partnerID=40&md5=b34d266aef8a29012588e834e032e6e7,"Kean University, Computer Science UnionNJ  07083, United States",Institute of Electrical and Electronics Engineers Inc.,English,,0780399145; 9780780399143
Scopus,Improved estimation of software development effort using Classical and Fuzzy Analogy ensembles,"Delivering an accurate estimate of software development effort plays a decisive role in successful management of a software project. Therefore, several effort estimation techniques have been proposed including analogy based techniques. However, despite the large number of proposed techniques, none has outperformed the others in all circumstances and previous studies have recommended generating estimation from ensembles of various single techniques rather than using only one solo technique. Hence, this paper proposes two types of homogeneous ensembles based on single Classical Analogy or single Fuzzy Analogy for the first time. To evaluate this proposal, we conducted an empirical study with 100/60 variants of Classical/Fuzzy Analogy techniques respectively. These variants were assessed using standardized accuracy and effect size criteria over seven datasets. Thereafter, these variants were clustered using the Scott-Knott statistical test and ranked using four unbiased errors measures. Moreover, three linear combiners were used to combine the single estimates. The results show that there is no best single Classical/Fuzzy Analogy technique across all datasets, and the constructed ensembles (Classical/Fuzzy Analogy ensembles) are often ranked first and their performances are, in general, higher than the single techniques. Furthermore, Fuzzy Analogy ensembles achieve better performance than Classical Analogy ensembles and there is no best Classical/Fuzzy ensemble across all datasets and no evidence concerning the best combiner. © 2016 Elsevier B.V.",Analogy; Ensemble effort estimation; Fuzzy logic; Software development effort estimation,"Idri A., Hosni M., Abran A.",2016,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2016.08.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997417515&doi=10.1016%2fj.asoc.2016.08.012&partnerID=40&md5=b29d5f00b8c137b8fef1ba5d0c2a68a1,"Software Projects Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco; Department of Software Engineering, École d e Technologie Supérieure, Montréal, Canada",Elsevier Ltd,English,15684946,
Scopus,How to make best use of cross-company data in software effort estimation?,"Previous works using Cross-Company (CC) data for making Within-Company (WC) Software Effort Estimation (SEE) try to use CC data or models directly to provide predictions in the WC context. So, these data or models are only helpful when they match the WC context well. When they do not, a fair amount of WC training data, which are usually expensive to acquire, are still necessary to achieve good performance. We investigate how to make best use of CC data, so that we can reduce the amount of WC data while maintaining or improving performance in comparison to WC SEE models. This is done by proposing a new framework to learn the relationship between CC and WC projects explicitly, allowing CC models to be mapped to the WC context. Such mapped models can be useful even when the CC models themselves do not match the WC context directly. Our study shows that a new approach instantiating this framework is able not only to use substantially less WC data than a corresponding WC model, but also to achieve similar/better performance. This approach can also be used to provide insight into the behaviour of a company in comparison to others. © 2014 ACM.",cross-company learning; ensembles of learning machines; online learning; Software effort estimation; transfer learning,"Minku L.L., Yao X.",2014,Conference,Proceedings - International Conference on Software Engineering,10.1145/2568225.2568228,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994193527&doi=10.1145%2f2568225.2568228&partnerID=40&md5=eb4d5b86b6edf905dc71a1edf50a6253,"CERCIA, School of Computer Science, University of Birmingham Edgbaston, Birmingham, B15 2TT, United Kingdom",IEEE Computer Society,English,02705257,
Scopus,Linear combination of multiple case-based reasoning with optimized weight for software effort estimation,"Since software development has become an essential investment for many organizations recently, both the software industry and academic communities are more and more concerned about a reliable and accurate estimation of the software development effort. This study puts forward six widely used case-based reasoning (CBR) methods with optimized weights derived from the particle swarm optimization (PSO) method to estimate the software effort. Meanwhile, four combination methods are adopted to assemble the results of independent CBR methods. The experiments are carried out using two datasets of software projects from Desharnais dataset and Miyazaki dataset. Experimental results show that different CBR methods can get the best results in different parameters settings, and there is not a best method for the software effort estimation among the six different CBR methods. Currently, combination methods proposed in this study outperform independent methods, and the weighted mean combination (WMC) method can get the better result. © 2010 Springer Science+Business Media, LLC.",Case-based reasoning; Linear combination forecasting; Particle swarm optimization; Software effort estimation; Weight optimization,"Wu D., Li J., Liang Y.",2013,Journal,Journal of Supercomputing,10.1007/s11227-010-0525-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886730173&doi=10.1007%2fs11227-010-0525-9&partnerID=40&md5=a8551b4d4198813555c147755c00037e,"Institute of Policy and Management, Chinese Academy of Sciences, Beijing 100190, China; Graduate University of Chinese Academy of Sciences, Beijing 100190, China; China Electronics Standardization Institute, Beijing 100007, China",,English,09208542,
Scopus,The optimization of success probability for software projects using genetic algorithms,"The software development process is usually affected by many risk factors that may cause the loss of control and failure, thus which need to be identified and mitigated by project managers. Software development companies are currently improving their process by adopting internationally accepted practices, with the aim of avoiding risks and demonstrating the quality of their work. This paper aims to develop a method to identify which risk factors are more influential in determining project outcome. This method must also propose a cost effective investment of project resources to improve the probability of project success. To achieve these aims, we use the probability of success relative to cost to calculate the efficiency of the probable project outcome. The definition of efficiency used in this paper was proposed by researchers in the field of education. We then use this efficiency as the fitness function in an optimization technique based on genetic algorithms. This method maximizes the success probability output of a prediction model relative to cost. The optimization method was tested with several software risk prediction models that have been developed based on the literature and using data from a survey which collected information from in-house and outsourced software development projects in the Chilean software industry. These models predict the probability of success of a project based on the activities undertaken by the project manager and development team. The results show that the proposed method is very useful to identify those activities needing greater allocation of resources, and which of these will have a higher impact on the projects success probability. Therefore using the measure of efficiency has allowed a modular approach to identify those activities in software development on which to focus the project's limited resources to improve its probability of success. The genetic algorithm and the measure of efficiency presented in this paper permit model independence, in both prediction of success and cost evaluation. © 2010 Elsevier Inc. All rights reserved.",Efficiency; Genetic algorithm; Optimization; Prediction model; Software project outcome; Software project success,"Reyes F., Cerpa N., Candia-Véjar A., Bardeen M.",2011,Journal,Journal of Systems and Software,10.1016/j.jss.2010.12.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952448497&doi=10.1016%2fj.jss.2010.12.036&partnerID=40&md5=3da4a4f43cce4372b40bd2126259eb17,"Facultad de Ingeniería, Universidad de Talca, Camino Los Niches Km. 1, Curicó, Chile",,English,01641212,
Scopus,"Systems lifecycle costing: Economic analysis, estimation, and management","Although technology and productivity has changed much of engineering, many topics are still taught in very similarly to how they were taught in the 70s. Using a new approach to engineering economics, Systems Life Cycle Costing: Economic Analysis, Estimation, and Management presents the material that a modern engineer must understand to work as a pr. © 2011 by Taylor & Francis Group, LLC.",,Farr J.V.,2011,Book,"Systems Life Cycle Costing: Economic Analysis, Estimation, and Management",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879874549&partnerID=40&md5=cb5b61070ecf82b5fff59411f02bc74b,"Center for Nation Reconstruction and Capacity Development, U.S. Military Academy, West Point, United States",CRC Press,English,,9781439828922; 9781439828915
Scopus,Adaptive ridge regression system for software cost estimating on multi-collinear datasets,"Cost estimation is one of the most critical activities in software life cycle. In past decades, a number of techniques have been proposed for cost estimation. Linear regression is yet the most frequently applied method in the literature. However, a number of studies point out that linear regression is prone to low prediction accuracy. The low prediction accuracy is due to a number of reasons such as non-linearity and non-normality. One less addressed reason is the multi-collinearities which may lead to unstable regression coefficients. On the other hand, it has been reported that multi-collinearity spreads widely across the software engineering datasets. To tackle this problem and improve regression's accuracy, we propose a holistic problem-solving approach (named adaptive ridge regression system) integrating data transformation, multi-collinearity diagnosis, ridge regression technique and multi-objective optimization. The proposed system is tested on two real world datasets with the comparisons with OLS regression, stepwise regression and other machine learning methods. The results indicate that adaptive ridge regression system can significantly improve the performance of regressions on multi-collinear datasets and produce more explainable results than machine learning methods. © 2010 Elsevier Inc. All rights reserved.",Machine learning; Multi-collinearity; Ridge regression; Software cost estimation,"Li Y.-F., Xie M., Goh T.-N.",2010,Journal,Journal of Systems and Software,10.1016/j.jss.2010.07.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957334050&doi=10.1016%2fj.jss.2010.07.032&partnerID=40&md5=03111faeba0f2270afc432c7b1ffc9e6,"Department of Industrial and Information Engineering, University of Tennessee, United States; National University of Singapore, Singapore",,English,01641212,
Scopus,LSEbA: Least squares regression and estimation by analogy in a semi-parametric model for Software Cost Estimation,"The importance of Software Cost Estimation at the early stages of the development life cycle is clearly portrayed by the utilization of several models and methods, appeared so far in the literature. The researchers' interest has been focused on two well known techniques, namely the parametric Regression Analysis and the non-parametric Estimation by Analogy. Despite the several comparison studies, there seems to be a discrepancy in choosing the best prediction technique between them. In this paper, we introduce a semi-parametric technique, called LSEbA that achieves to combine the aforementioned methods retaining the advantages of both approaches. Furthermore, the proposed method is consistent with the mixed nature of Software Cost Estimation data and takes advantage of the whole pure information of the dataset even if there is a large amount of missing values. The paper analytically illustrates the process of building such a model and presents the experimentation on three representative datasets verifying the benefits of the proposed model in terms of accuracy, bias and spread. Comparisons of LSEbA with linear regression, estimation by analogy and a combination of them, based on the average of their outcomes are made through accuracy metrics, statistical tests and a graphical tool, the Regression Error Characteristic curves. © Springer Science+Business Media, LLC 2010.",Estimation by analogy; Regression analysis; Semi-parametric model; Software cost estimation,"Mittas N., Angelis L.",2010,Journal,Empirical Software Engineering,10.1007/s10664-010-9128-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954759852&doi=10.1007%2fs10664-010-9128-6&partnerID=40&md5=327d0cb60295f12f97034f64a73abec9,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,13823256,
Scopus,A soft computing framework for software effort estimation,"Accurate software estimation such as cost estimation, quality estimation and risk analysis is a major issue in software project management. In this paper, we present a soft computing framework to tackle this challenging problem. We first use a preprocessing neuro-fuzzy inference system to handle the dependencies among contributing factors and decouple the effects of the contributing factors into individuals. Then we use a neuro-fuzzy bank to calibrate the parameters of contributing factors. In order to extend our framework into fields that lack of an appropriate algorithmic model of their own, we propose a default algorithmic model that can be replaced when a better model is available. One feature of this framework is that the architecture is inherently independent of the choice of algorithmic models or the nature of the estimation problems. By integrating neural networks, fuzzy logic and algorithmic models into one scheme, this framework has learning ability, integration capability of both expert knowledge and project data, good interpretability, and robustness to imprecise and uncertain inputs. Validation using industry project data shows that the framework produces good results when used to predict software cost. © Springer-Verlag 2005.",,"Huang X., Ho D., Ren J., Capretz L.F.",2006,Journal,Soft Computing,10.1007/s00500-004-0442-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30344459697&doi=10.1007%2fs00500-004-0442-z&partnerID=40&md5=275e4c4de23d3669f813b5e17d953b0a,"Department of Electrical and Computer Engineering, University of Western Ontario, London, Ont. N6A 5B9, Canada; Toronto Design Center, Motorola Canada Ltd., Markham, Ont. L6G 1B3, Canada",,English,14327643,
Scopus,Bayesian networks for evidence-based decision-making in software engineering,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. © 2014 IEEE.",Bayesian networks; Bayesian statistics; Evidence-based decision-making; post-release defects; software metrics; software reliability,"Misirli A.T., Bener A.B.",2014,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2014.2321179,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903132737&doi=10.1109%2fTSE.2014.2321179&partnerID=40&md5=d926eb4ef770e0a75ea32f2988127a8c,"Department of Information Processing Science, University of Oulu, Finland; Mechanical and Industrial Engineering Department, Ryerson University, Toronto, CA, Canada",Institute of Electrical and Electronics Engineers Inc.,English,00985589,
Scopus,Understanding developer and manager perceptions of function points and source lines of code,"Although function points (FP) are considered superior to source lines of code (SLOC) for estimating software size and monitoring developer productivity, practitioners still commonly use SLOC. One reason for this is that individuals who fill different roles on a development team, such as managers and developers, may perceive the benefits of FP differently. We conducted a survey to determine whether a perception gap exists between managers and developers for FP and SLOC across several desirable properties of software measures. Results suggest managers and developers perceive the benefits of FP differently and indicate that developers better understand the benefits of using FP than managers. © 2009 Elsevier Inc. All rights reserved.",Content analysis; Function points; Perception gap; Software measures; Source lines of code; Survey,"Sheetz S.D., Henderson D., Wallace L.",2009,Journal,Journal of Systems and Software,10.1016/j.jss.2009.04.038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949189558&doi=10.1016%2fj.jss.2009.04.038&partnerID=40&md5=4b087ce62ad6df9cd416fa1e4de1e831,"Department of Accounting and Information Systems, Pamplin College of Business, Virginia Tech, 3007 Pamplin Hall, (Mailcode 0101), Blacksburg, VA 24061, United States; Department of Accounting and Legal Studies, School of Business and Economics, College of Charleston, 5 Liberty Street, Suite 400, Charleston, SC 29401, United States",,English,01641212,
Scopus,A case study using web objects and COSMIC for effort estimation of web applications,"Some size measures have been proposed in the literature to be employed in estimating Web application development effort, but to date few empirical studies have been undertaken to validate them and compare their effectiveness. To address this issue we have performed an empirical study by using as size measures COSMIC and Web Objects, with the aim of verifying their effectiveness as indicators of development effort. In particular, we have built two effort estimation models by applying the Ordinary Least-Squares regression and by using a dataset of 15 Web applications developed by an italian software company. The performance of the obtained models have been evaluated by using a dataset of further 4 Web applications developed by the same software company some time after the first 15Web applications. The results reveal that both Web Objects and COSMIC are good indicators of the development effort. © 2008 IEEE.",,"Ferrucci F., Gravino C., Di Martino S.",2008,Conference,"EUROMICRO 2008 - Proceedings of the 34th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2008",10.1109/SEAA.2008.60,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349092671&doi=10.1109%2fSEAA.2008.60&partnerID=40&md5=9859f2c2995f1cb768aa03a4facb11df,"Dipartimento di Matematica e Informatica, University of Salerno, Italy; Dipartimento di Scienze Fisiche, Sezione Informatica University of Napoli, Italy",,English,,9780769532769
Scopus,The use of Bayesian networks for web effort estimation: Further investigation,"The objective of this paper is to further investigate the use of Bayesian Networks (BN) for Web effort estimation when using a cross-company dataset. Four BNs were built; two automatically using the Hugin tool with two training sets; two using a structure elicited by a domain expert, with parameters obtained from automatically fitting the network to the same training sets used in the automated elicitation (hybrid models). The accuracy of all four models was measured using two validation sets, and point estimates. As a benchmark, the BN-based predictions were also compared to predictions obtained using Manual StepWise Regression (MSWR), and Case-Based Reasoning (CBR). The BN model generated using Hugin presented similar accuracy to CBR and Mean effort-based predictions. Our results suggest that Hybrid BN models can provide significantly superior prediction accuracy. However, good results also seem to depend on characteristics of the training and validation sets used. © 2008 IEEE.",,Mendes E.,2008,Conference,"Proceedings - 8th International Conference on Web Engineering, ICWE 2008",10.1109/ICWE.2008.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52049107596&doi=10.1109%2fICWE.2008.16&partnerID=40&md5=206b84d41b5aff699dbe01d7a89e734f,University of Auckland,,English,,9780769532615
Scopus,A comparison of techniques for web effort estimation,"OBJECTIVE - The objective of this paper is to extend the work by Mende[15], and to compare four techniques for Web effort estimation to identify which one provides best prediction accuracy. METHOD - We employed four effort estimation techniques - Bayesian networks (BN), forward stepwise regression (SWR), case-based reasoning (CBR) and Classification and regression trees (CART) to obtain effort estimates. The dataset employed was of 150 Web projects from the Tukutuku dataset. RESULTS - Results showed that predictions obtained using a BN were significantly superior to those using other techniques. CONCLUSIONS - A model that incorporates the uncertainty inherent in effort estimation, can outperform other commonly used techniques, such as those used in this study. © 2007 IEEE.",,Mendes E.,2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949121581&doi=10.1109%2fESEM.2007.4&partnerID=40&md5=448e0d87085e76581c185f4134b1f04a,"Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,0769528864; 9780769528861
Scopus,Approximation of COSMIC functional size to support early effort estimation in Agile,"The demands in the software industry of estimating development effort in the early phases of development are met by measuring software size from user requirements. A large number of companies have adapted themselves with Agile processes, which, although, promise rapid software development, pose a huge burden on the development teams for continual decision making and expert judgement, when estimating the size of the software components to be developed at each iteration. COSMIC, on the other hand, is an ISO/IEC international standard that presents an objective method of measuring the functional size of the software from user requirements. However, its measurement process is not compatible with Agile processes, as COSMIC requires user requirements to be formalised and decomposed at a level of granularity where external interactions with the system are visible to the human measurer. This time-consuming task is avoided by agile processes, leaving it with the only option of quick subjective judgement by human measurers for size measurement that often tends to be erroneous. In this article, we address these issues by presenting an approach to approximate COSMIC functional size from informally written textual requirements demonstrating its applicability in popular agile processes. We also discuss the results of a preliminary experiment studying the feasibility of automating our approach using supervised text mining. © 2012 Elsevier B.V. All rights reserved.",Agile development processes; Functional size measurement; Natural language processing; Software requirements; Text mining,"Hussain I., Kosseim L., Ormandjieva O.",2013,Conference,Data and Knowledge Engineering,10.1016/j.datak.2012.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875247667&doi=10.1016%2fj.datak.2012.06.005&partnerID=40&md5=1c73eaa8ce65c9272177287ac4917f3f,"Department of Computer Science and Software Engineering, Concordia University, 1400 de Maisonneuve Blvd. West, Montreal, QC H3G 1M8, Canada",,English,0169023X,
Scopus,Software development cost estimation using analogy: A review,"Software project managers require reliable methods for estimating software project costs, and it is especially important at the early stage of software cycle. Analogy for software cost estimation has been considered as a suitable alternative to regression-based estimation method, and empirical studies have shown that it can be used successfully in many circumstances. It is important for project managers to understand the strengths and weaknesses of each useful software cost estimation method, more importantly when to use these methods and at which stage of the software development. This paper provides a comprehensive overview of the background and the underlying theory of analogy for software cost estimation, published in major software engineering journals and conferences over the past 15 years. Investigation on the dataset quality evaluation and its relevance to the target problem for analogy are further discussed, the result allows researchers and project managers to familiarize the underlying nature of the analogy-based approach. © 2009 Crown Copyright.",,Keung J.,2009,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2009.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349483008&doi=10.1109%2fASWEC.2009.32&partnerID=40&md5=64e9ca3e9ddce1cf06f66dbec74a74f5,"NICTA Ltd., CSE/UNSW, Sydney, Australia",,English,,9780769535999
Scopus,Confidence in software cost estimation results based on MMRE and PRED,"Bootstrapping is used to approximate the standard error and 95% confidence intervals of MMRE and PRED for a number of COCOMO I model variations applied to four PROMISE data sets. This is used to illustrate a lack of confidence in numerous published cost estimation research results based on MMRE and PRED comparisons such as model selection. We show that many such results are of questionable significance due to large possible variations resulting from population sampling error and suggest that a number of inconsistent and contradictory results may be explained by this. By using more standard statistical approaches that account for standard error, we may reduce the incidence of this and obtain greater confidence cost estimation in research results. Copyright 2008 ACM.",Bootstrapping; Calibration; Confidence; Cost estimation; Cost model; MMRE; Model selection; Parameters; PRED; Standard error,"Korte M., Port D.",2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370788.1370804,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049111199&doi=10.1145%2f1370788.1370804&partnerID=40&md5=58427aabba62990c65708611a320d557,"University of Applied Sciences and Arts Dortmund, Emil-Figge-Str. 42, 44227 Dortmund, Germany; University of Hawai'i, 2404 Maile Way, Manoa E303 Honolulu, HI 96822, United States",,English,02705257,9781605580364
Scopus,A constrained regression technique for COCOMO calibration,"Building cost estimation models is often considered a search problem in which the solver should return an optimal solution satisfying an objective function. This solution also needs to meet certain constraints. For example, a solution for the estimates coefficients of COCOMO models must be non-negative. In this research, we introduce a constrained regression technique that uses objective functions and constraints to estimate the coefficients of the COCOMO models. To access the performance of the proposed technique, we run a cross-validation procedure and compare the prediction accuracy from different approaches such as least squares, stepwise, Lasso, and Ridge regression. Our result suggests that the regression model that minimizes the sum of relative errors and imposes non-negative coefficients is a favorable technique for calibrating the COCOMO model parameters. Copyright 2008 ACM.",Calibration; COCOMO; Linear programming; Linear regression with constraints; Optimization; Quadratic programming; Software cost estimation,"Nguyen V., Steece B., Boehm B.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949214899&doi=10.1145%2f1414004.1414040&partnerID=40&md5=12444737090da60c7c6e1038fe116f02,"Computer Science Department, University of Southern California, 941 W. 37th Pl, SAL 332, Los Angeles, CA 90089, United States; IOM Department, University of Southern California, 3670 Trousdale Pkwy, Los Angeles, CA 90007, United States; Computer Science Department, University of Southern California, 941 W. 37th PI, SAL 326, Los Angeles, CA 90089, United States",,English,,9781595939715
Scopus,Estimating the costs of a reengineering project,"Accurate estimation of project costs is an essential prerequisite to making a reengineering project. Existing systems are usually reengineered because it is cheaper to reengineer them than to redevelop or to replace them. However, to make this decision, management must know what the reengineering will cost. This contribution describes an eight step tool supported process for calculating the time and the costs required to reengineer an existing system. The process is derived from the author's 20 year experience in estimating reengineering projects and has been validated by several real life field experiments in which it has been refined and calibrated. © 2005 IEEE.",Cost estimation; Gap analysis; Reengineering; Risk analysis; Software measurement,Sneed H.M.,2005,Conference,"Proceedings - Working Conference on Reverse Engineering, WCRE",10.1109/WCRE.2005.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846243852&doi=10.1109%2fWCRE.2005.18&partnerID=40&md5=63f27490e7a3dbcaf0da648bf6d233b9,"Anecon GmbH, Vienna, Austria; Institut für Wirtschaftsinformatik, University of Regensburg, Bavaria, Germany",,English,10951350,0769524745; 9780769524740
Scopus,Computational intelligence as an emerging paradigm of software engineering,"Software Engineering is inherently knowledge intensive. Software processes and products are human centered. The technology of Computational Intelligence (CI) intensively exploits various mechanisms of interaction with humans and processes domain knowledge with intent of building intelligent systems. As commonly perceived, CI dwells on three highly synergistic technologies of neural networks, fuzzy sets (or granular computing, in general) and evolutionary optimization. As the software complexity grows and the diversity of software systems skyrocket, it becomes apparent that there is a genuine need for a solid, efficient, designer-oriented vehicle to support software analysis, design, and implementation at various levels. The research agenda makes CI a highly compatible and appealing vehicle to address the needs of knowledge rich environment of Software Engineering. The objective of this study is to identify and discuss synergistic links emerging between Software Engineering and Computational Intelligence. We show how CI - based models contribute to the methodology of constructing models of software processes and products. Several selected examples (including software cost estimation, quality, and software measures) are included. Copyright 2002 ACM.",computational intelligence; data visualization; genetic optimization; granular computing; neural networks; software quality; synergy; uncertainty representation,Pedrycz W.,2002,Conference,ACM International Conference Proceeding Series,10.1145/568760.568763,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953739952&doi=10.1145%2f568760.568763&partnerID=40&md5=843315f71e49ca2b356341183e37ce07,"Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland",,English,,1581135564; 9781581135565
Scopus,Has twenty-five years of empirical software engineering made a difference?,"Our activities in software engineering typically fall into one of three categories, (1) to invent new phenomena, (2) to understand existing phenomena, and (3) to facilitate inspirational education. This paper explores the place of empirical software engineering in the first two of these activities. In this exploration evidence is drawn from the empirical literature in the areas of software inspections and software cost modelling and estimation. This research is then compared with the literature published in the Journal of Empirical Software Engineering. This evidence throws light on aspects of theory derivation, experimental methods and analysis, and also the challenges that we face as empirical software engineering evolves into the future. © 2002 IEEE.",Australia; Computer industry; Computer science; Computer science education; Costs; Educational products; Educational programs; Inspection; Software engineering; Software measurement,"Jeffery R., Scott L.",2002,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2002.1183076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942535501&doi=10.1109%2fAPSEC.2002.1183076&partnerID=40&md5=d25475ecd3a2748b85348bb0e20c24c7,"CAESER, School of Computer Science and Engineering, University of New South Wales, Sydney, NSW  2052, Australia",IEEE Computer Society,English,15301362,0769518508
Scopus,Software metrics data analysis - exploring the relative performance of some commonly used modeling techniques,"Whilst some software measurement research has been unquestionably successful, other research has struggled to enable expected advances in project and process management. Contributing to this lack of advancement has been the incidence of inappropriate or non-optimal application of various model-building procedures. This obviously raises questions over the validity and reliability of any results obtained as well as the conclusions that may have been drawn regarding the appropriateness of the techniques in question. In this paper we investigate the influence of various data set characteristics and the purpose of analysis on the effectiveness of four model-building techniques - three statistical methods and one neural network method. In order to illustrate the impact of data set characteristics, three separate data sets, drawn from the literature, are used in this analysis. In terms of predictive accuracy, it is shown that no one modeling method is best in every case. Some consideration of the characteristics of data sets should therefore occur before analysis begins, so that the most appropriate modeling method is then used. Moreover, issues other than predictive accuracy may have a significant influence on the selection of model-building methods. These issues are also addressed here and a series of guidelines for selecting among and implementing these and other modeling techniques is discussed.",,"Gray A.R., Macdonell S.G.",1999,Journal,Empirical Software Engineering,10.1023/A:1009849100780,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033324327&doi=10.1023%2fA%3a1009849100780&partnerID=40&md5=aef04da5c4ae31721bfcba73557a57f5,"Software Metrics Research Laboratory, Department of Information Science, University of Otago, Dunedin, New Zealand; Department of Information Science, University of Otago, Dunedin, New Zealand; NZCS, Dunedin, New Zealand; NAFIPS, Dunedin, New Zealand","Kluwer Academic Publishers, Dordrecht, Netherlands",English,13823256,
Scopus,AI tools for software development effort estimation,"Software development involves a number of interrelated factors which affect development effort and productivity. Since many of these relationships are not well understood, accurate estimation of software development time and effort is a difficult problem. Most estimation models in use or proposed in the literature are based on regression techniques. This paper examines the potential of two artificial intelligence approaches, viz. artificial neural networks and case-based reasoning, for creating development effort estimation models. Artificial neural networks can provide accurate estimates when there are complex relationships between variables and where the input data is distorted by high noise levels. Case-based reasoning solves problems by adapting solutions from old problems that are similar to the current problem. This research examines both the performance of backpropagation artificial neural networks in estimating software development effort and the potential of case-based reasoning for development estimation using the same dataset. © 1996 IEEE.",,"Finnie G.R., Wittig G.E.",1996,Conference,"Proceedings - 1996 International Conference Software Engineering: Education and Practice, SEEP 1996",10.1109/SEEP.1996.534020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058465671&doi=10.1109%2fSEEP.1996.534020&partnerID=40&md5=4551a20f2420c6f211b2e3da69416dd4,"Bond University, Gold Coast, QLD, Australia",Institute of Electrical and Electronics Engineers Inc.,English,,0818673796; 9780818673795
Scopus,Bayesian network model for task effort estimation in agile software development,"Even though the use of agile methods in software development is increasing, the problem of effort estimation remains quite a challenge, mostly due to the lack of many standard metrics to be used for effort prediction in plan-driven software development. The Bayesian network model presented in this paper is suitable for effort prediction in any agile method. Simple and small, with inputs that can be easily gathered, the suggested model has no practical impact on agility. This model can be used as early as possible, during the planning stage. The structure of the proposed model is defined by the authors, while the parameter estimation is automatically learned from a dataset. The data are elicited from completed agile projects of a single software company. This paper describes various statistics used to assess the precision of the model: mean magnitude of relative error, prediction at level m, accuracy (the percentage of successfully predicted instances over the total number of instances), mean absolute error, root mean squared error, relative absolute error and root relative squared error. The obtained results indicate very good prediction accuracy. © 2017 Elsevier Inc.",Agile software development; Bayesian network; Effort prediction,"Dragicevic S., Celar S., Turic M.",2017,Journal,Journal of Systems and Software,10.1016/j.jss.2017.01.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012069104&doi=10.1016%2fj.jss.2017.01.027&partnerID=40&md5=5fd38fedbacd309964550d5789606087,"Split Airport, Cesta dr. Franje Tudmana 1270, Kastel Stafilic, 21217, Croatia; Department of Electronics, FESB, University of Split, R. Boskovica 32, Split, 21000, Croatia; Venio indicium d.o.o., Doverska 19, Split, 21000, Croatia",Elsevier Inc.,English,01641212,
Scopus,Analysis of cost estimation function for facebook web click data,"Gephi is an open source tool, used for Data Analysis, Link Analysis, Social Network Analysis, Biological Network Analysis, Poster Creation. Gephi tool in this project is used for Social Network Analysis of Facebook Pages and is used to find metrics like Modularity, Average Weighted Degree, Average Degree etc. The Gephi tool gives different graphs as output which can be analyzed by the shape of the graph by applying layout algorithms. Social Media data that keeps increasing exponentially every second. This project analyses the Facebook pages based on the data retrieved from the Netvizz Application. © 2017 IEEE.",Control Chart; Cost Optimization; Degree; Kendall Method; Linear Regression Matrix; Modularity,"Thirumalai C., Sree K.S., Gannu H.",2017,Conference,"Proceedings of the International Conference on Electronics, Communication and Aerospace Technology, ICECA 2017",10.1109/ICECA.2017.8212788,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046655306&doi=10.1109%2fICECA.2017.8212788&partnerID=40&md5=9773276211b0ec63759ee73aa9fe4da0,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,
Scopus,Too much automation? the bellwether effect and its implications for transfer learning,"""Transfer learning"": is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple ""bellwether"" transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This ""bellwether"" data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) ""bellwethers"" are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives. © 2016 ACM.",Data Mining; Defect Prediction; Transfer learning,"Krishna R., Menzies T., Fu W.",2016,Conference,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,10.1145/2970276.2970339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989206867&doi=10.1145%2f2970276.2970339&partnerID=40&md5=959e16066254267048c55abd1fb21f06,"Computer Science, North Carolina State University, United States","Association for Computing Machinery, Inc",English,,9781450338455
Scopus,A cost model based on software maintainability,"In this paper we present a maintainability based model for estimating the costs of developing source code in its evolution phase. Our model adopts the concept of entropy in thermodynamics, which is used to measure the disorder of a system. In our model, we use maintainability for measuring disorder (i.e. entropy) of the source code of a software system. We evaluated our model on three proprietary and two open source real world software systems implemented in Java, and found that the maintainability of these evolving software is decreasing over time. Furthermore, maintainability and development costs are in exponential relationship with each other. We also found that our model is able to predict future development costs with high accuracy in these systems. © 2012 IEEE.",cost prediction model; development cost estimation; ISO/IEC 25000; ISO/IEC 9126; Software maintainability,"Bakota T., Hegedus P., Ladanyi G., Kortvelyesi P., Ferenc R., Gyimothy T.",2012,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2012.6405288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873113883&doi=10.1109%2fICSM.2012.6405288&partnerID=40&md5=4f910a7732951b959ca1caf04e3291a4,"University of Szeged, Department of Software Engineering, Árpád tér 2, H-6720 Szeged, Hungary",,English,,9781467323123
Scopus,A Systematic review of web resource estimation,"Background: Web development plays an important role in today's industry, so an in depth view into Web resource estimation would be valuable. However a systematic review (SR) on Web resource estimation in its entirety has not been done. Aim: The aim of this paper is to present a SR of Web resource estimation in order to define the current state of the art, and to identify any research gaps that may be present. Method: Research questions that would address the current state of the art in Web resource estimation were first identified. A comprehensive literature search was then executed resulting in the retrieval of 84 empirical studies that investigated any aspect of Web resource estimation. Data extraction and synthesis was performed on these studies with these research questions in mind. Results: We have found that there are no guidelines with regards to what resource estimation technique should be used in a particular estimation scenario, how it should be implemented, and how its effectiveness should be evaluated. Accuracy results vary widely and are dependent on numerous factors. Research has focused on development effort/cost estimation, neglecting other facets of resource estimation like quality and maintenance. Size measures have been used in all but one study as a resource predictor. Conclusions: Our results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique. Copyright © 2012 ACM.",Systematic review; Web resource estimation,"Azhar D., Mendes E., Riddle P.",2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867733071&doi=10.1145%2f2365324.2365332&partnerID=40&md5=4f12ec2bfe2c5eb7aa408a0e4d1763ff,"Department of Computer Science, University of Auckland, Auckland, New Zealand; College of Information Technology, Zayed University, United Arab Emirates",,English,,9781450312417
Scopus,Investigating the use of Support Vector Regression for web effort estimation,"Support Vector Regression (SVR) is a new generation of Machine Learning algorithms, suitable for predictive data modeling problems. The objective of this paper is twofold: first, to investigate the effectiveness of SVR for Web effort estimation using a cross-company dataset; second, to compare different SVR configurations looking at the one that presents the best performance. In particular, we took into account three variables' preprocessing strategies (no-preprocessing, normalization, and logarithmic), in combination with two different dependent variables (effort and inverse effort). As a result, SVR was applied using six different data configurations. Moreover, to understand the suitability of kernel functions to handle non-linear problems, SVR was applied without a kernel, and in combination with the Radial Basis Function (RBF) and the Polynomial kernels, thus obtaining 18 different SVR configurations. To identify, for each configuration, which were the best values for each of the parameters we defined a procedure based on a leave-one-out cross-validation approach. The dataset employed was the Tukutuku database, which has been adopted in many previous Web effort estimation studies. Three different training and test set splits were used, including respectively 130 and 65 projects. The SVR-based predictions were also benchmarked against predictions obtained using Manual StepWise Regression and Case-Based Reasoning. Our results showed that the configuration corresponding to the logarithmic features' preprocessing, in combination with the RBF kernel provided the best results for all three data splits. In addition, SVR provided significantly superior prediction accuracy than all the considered benchmarking techniques. © 2010 Springer Science+Business Media, LLC.",Effort estimation; Support Vector Regression,"Corazza A., Di Martino S., Ferrucci F., Gravino C., Mendes E.",2011,Journal,Empirical Software Engineering,10.1007/s10664-010-9138-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953191774&doi=10.1007%2fs10664-010-9138-4&partnerID=40&md5=fe37bb19d27cf5f63a0b53391ce5b065,"University of Napoli Federico II, Via Cinthia, Naples 80126, Italy; University of Salerno, Via Ponte Don Melillo, Fisciano, SA 84084, Italy; University of Auckland, Private Bag, Auckland 92019, New Zealand",,English,13823256,
Scopus,How effective is Tabu Search to configure Support Vector Regression for effort estimation?,"Background. Recent studies have shown that Support Vector Regression (SVR) has an interesting potential in the field of effort estimation. However applying SVR requires to carefully set some parameters that heavily affect the prediction accuracy. No general guidelines are available to select these parameters, whose choice also depends on the characteristics of the data set used. This motivates the work described in this paper. Aims. We have investigated the use of an optimization technique in combination with SVR to select a suitable subset of parameters to be used for effort estimation. This technique is named Tabu Search (TS), which is a meta-heuristic approach used to address several optimization problems. Method. We employed SVR with linear and RBF kernels, and used variables' preprocessing strategies (i.e., logarithmic). As for the data set, we employed the Tukutuku cross-company database, which is widely adopted in Web effort estimation studies, and performed a hold-out validation using two different splits of the data set. As benchmark, results are compared to those obtained with Manual StepWise Regression, Case-Based Reasoning, and Bayesian Networks. Results. Our results show that TS provides a good choice of parameters, so that the combination of TS and SVR outperforms any other technique applied on this data set. Conclusions. The use of the metaheuristic Tabu Search allowed us to obtain (I) an automatic choice of the parameters required to run SVR, and (II) a significant improvement on prediction accuracy for SVR. While we are not guaranteed that this is the global optimum, the results we are presenting are the best performance ever obtained on the problem at the hand, up to now. Of course, the experimental results here presented should be assessed on further data. However, they are surely interesting enough to suggest the use of SVR among the techniques that are suitable for effort estimation, especially when using a cross-company database.",Development effort estimation; Empirical studies; Support Vector machines; Support Vector Regression; Tabu Search,"Corazza A., Di Martino Martino S., Ferrucci F., Gravino C., Sarro F., Mendes E.",2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649803942&doi=10.1145%2f1868328.1868335&partnerID=40&md5=c479824e406a5d679d3ece40b4175b3e,"University of Napoli Federico II, via Cintia, 80126 Napoli, Italy; University of Salerno, via Ponte Don Melillo, 84084 Fisciano (SA), Italy; University of Auckland, Private Bag, 92019 Auckland, New Zealand",,English,,9781450304047
Scopus,Toward data mining engineering: A software engineering approach,"The number, variety and complexity of projects involving data mining or knowledge discovery in databases activities have increased just lately at such a pace that aspects related to their development process need to be standardized for results to be integrated, reused and interchanged in the future. Data mining projects are quickly becoming engineering projects, and current standard processes, like CRISP-DM, need to be revisited to incorporate this engineering viewpoint. This is the central motivation of this paper that makes the point that experience gained about the software development process over almost 40 years could be reused and integrated to improve data mining processes. Consequently, this paper proposes to reuse ideas and concepts underlying the IEEE Std 1074 and ISO 12207 software engineering model processes to redefine and add to the CRISP-DM process and make it a data mining engineering standard. © 2008 Elsevier B.V. All rights reserved.",Data mining; Knowledge engineering; Software engineering,"Marbán O., Segovia J., Menasalvas E., Fernández-Baizán C.",2009,Journal,Information Systems,10.1016/j.is.2008.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349148370&doi=10.1016%2fj.is.2008.04.003&partnerID=40&md5=6f7ea70339e7e25c2a2d6133d6e00a37,"Facultad de Informática, Universidad Politécnica de Madrid (U.P.M.), Spain",,English,03064379,
Scopus,Cross-company vs. single-company web effort models using the Tukutuku database: An extended study,"In 2004 [Kitchenham, B.A., Mendes, E., 2004a. Software productivity measurement using multiple size measures. IEEE Transactions on Software Engineering 30 (12), 1023-1035, Kitchenham, B.A., Mendes, E., 2004b. A comparison of cross-company and single-company effort estimation models for web applications. In: Proceedings Evaluation and Assessment in Software Engineering (EASE' 04), pp. 47-55] (S1) investigated, using data on 63 Web projects, to what extent a cross-company cost model could be successfully employed to estimate development effort for single-company Web projects. Their effort models were built using Forward Stepwise Regression (SWR) and they found that cross-company predictions were significantly worse than single-company predictions. This study S1 was extended by Mendes and Kitchenham [Mendes, E., Kitchenham, B.A., 2004. Further comparison of cross-company and within company effort estimation models for web applications. In: Proceedings International Software Metrics Symposium (METRICS'04), Chicago, Illinois, September 11-17th, 2004. IEEE Computer Society, pp. 348-357] (S2), who used SWR and Case-based reasoning (CBR), and data on 67 Web projects from the Tukutuku database. They built two cross-company and one single-company models and found that both SWR cross-company models and CBR cross-company data provided predictions significantly worse than single-company predictions. Since 2004 another 83 projects were volunteered to the Tukutuku database, and recently used by Mendes et al. [Mendes, E., Di Martino, S., Ferrucci, F., Gravino, C., in press. Effort estimation: How valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set? In: Proceedings of International World Wide Web Conference (WWW'07), Banff, Canada, 8-12 May] (S3), who partially replicated Mendes and Kitchenham's study (S2), using SWR and CBR. They corroborated some of S2's findings (SWR cross-company model and the CBR cross-company data provided predictions significantly worse than single-company predictions) however they replicated only part of S2. The objective of this paper (S4) is therefore to extend Mendes et al.'s work and fully replicate S2. We used the same dataset used in S3, and our results corroborated most of those obtained in S2. The main difference between S2 and our study was that one of our SWR cross-company models showed significantly similar predictions to the single-company model, which contradicts the findings from S2. © 2007 Elsevier Inc. All rights reserved.",,"Mendes E., Di Martino S., Ferrucci F., Gravino C.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2007.07.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40749124802&doi=10.1016%2fj.jss.2007.07.044&partnerID=40&md5=7553b1d295408b61aee92cfdf654d2fd,"Computer Science Department, The University of Auckland, Auckland, New Zealand; Dipartimento di Matematica e Informatica, Università degli Studi di Salerno, 84084 Fisciano, SA, Italy",,English,01641212,
Scopus,Ensemble of missing data techniques to improve software prediction accuracy,"Software engineers are commonly faced with the problem of incomplete data. Incomplete data can reduce system performance in terms of predictive accuracy. Unfortunately, rare research has been conducted to systematically explore the impact of missing values, especially from the missing data handling point of view. This has made various missing data techniques (MDTs) less significant. This paper describes a systematic comparison of seven MDTs using eight industrial datasets. Our findings from an empirical evaluation suggest listwise deletion as the least effective technique for handling incomplete data while multiple imputation achieves the highest accuracy rates. We further propose and show how a combination of MDTs by randomizing a decision tree building algorithm leads to a significant improvement in prediction performance for missing values up to 50%.",Decision trees; Ensemble; Incomplete data; Machine learning; Software prediction,"Twala B., Cartwright M., Shepperd M.",2006,Conference,Proceedings - International Conference on Software Engineering,10.1145/1134285.1134449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247112428&doi=10.1145%2f1134285.1134449&partnerID=40&md5=e1db76c0711d6aa6195bd2f03fa969e0,"Brunel University, UB8 3PH, United Kingdom",IEEE Computer Society,English,02705257,1595933751; 9781595933751
Scopus,An empirical approach to characterizing risky software projects based on logistic regression analysis,"During software development, projects often experience risky situations. If projects fail to detect such risks, they may exhibit confused behavior. In this paper, we propose a new scheme for characterization of the level of confusion exhibited by projects based on an empirical questionnaire. First, we designed a questionnaire from five project viewpoints, requirements, estimates, planning, team organization, and project management activities. Each of these viewpoints was assessed using questions in which experience and knowledge of software risks are determined. Secondly, we classify projects into ""confused"" and ""not confused,"" using the resulting metrics data. We thirdly analyzed the relationship between responses to the questionnaire and the degree of confusion of the projects using logistic regression analysis and constructing a model to characterize confused projects. The experimental result used actual project data shows that 28 projects out of 32 were characterized correctly. As a result, we concluded that the characterization of confused projects was successful. Furthermore, we applied the constructed model to data from other projects in order to detect risky projects. The result of the application of this concept showed that 7 out of 8 projects were classified correctly. Therefore, we concluded that the proposed scheme is also applicable to the detection of risky projects. © 2005 Springer Science + Business Media, Inc.",Logistic regression; Questionnaire; Risky project; Software risk management,"Takagi Y., Mizuno O., Kikuno T.",2005,Conference,Empirical Software Engineering,10.1007/s10664-005-3864-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26044464061&doi=10.1007%2fs10664-005-3864-z&partnerID=40&md5=d34c433ff74cf391da72f54a80df9ce6,"Social System Solutions Business Company, OMRON Corporation, Japan; OMRON Corporation, Japan; Graduate School of Information Science and Technology, Osaka University, Japan; IEEE, Japan; Department of Information and Computer Sciences, Osaka University, Japan; Osaka University, Nakanoshima Center, Japan; ACM, IEICE (the Institute of Electronics, Information and Communication Engineers), Japan; IPSJ (Information Processing Society of Japan), Japan; Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, Osaka 565-0871, Japan",,English,13823256,
Scopus,Measuring the functional size of web applications,"Today, more and more Web sites and Applications (WebApps) are becoming mission-critical systems. Measures of functional size are a prerequisite to successful quantitative management of software projects. However, the nature of the web has recently imposed new and challenging characteristics, which are not supported by the existing estimation metric and models. In order to avoid the Web crisis, urge the use of web engineering approaches for developing and estimating web projects in a systematic way. To achieve this goal, we present a novel size metric for accurately measuring the functional size of web applications early in the development life cycle. The main contribution of this work is the introduction of a measurement process that is embedded in the conceptual modelling phase of a model-driven approach for developing web applications. The results obtained from our experiments with this approach are outlined in this work.",Conceptual modelling; Function points; Functional size measurement; Object-oriented; Web engineering; Web metrics,"Abrahão S., Pastor O.",2003,Journal,International Journal of Web Engineering and Technology,10.1504/IJWET.2003.003265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942705961&doi=10.1504%2fIJWET.2003.003265&partnerID=40&md5=9fd711d88d633cd60831fbf53335b2b1,"Department of Information Systems, Valencia University of Technology, Camino de Vera s/n, 46071 Valencia, Spain",Inderscience Publishers,English,14761289,
Scopus,A comparison of case-based reasoning approaches,"Over the years software engineering researchers have suggested numerous techniques for estimating development effort. These techniques have been classified mainly as algorithmic, machine learning and expert judgement. Several studies have compared the prediction accuracy of those techniques, with emphasis placed on linear regression, stepwise regression, and Case-based Reasoning (CBR). To date no converging results have been obtained and we believe they may be influenced by the use of the same CBR configuration.The objective of this paper is twofold. First, to describe the application of case-based reasoning for estimating the effort for developing Web hypermedia applications. Second, comparing the prediction accuracy of different CBR configurations, using two Web hypermedia datasets.Results show that for both datasets the best estimations were obtained with weighted Euclidean distance, using either one analogy (dataset 1) or 3 analogies (dataset 2). We suggest therefore that case-based reasoning is a candidate technique for effort estimation and, with the aid of an automated environment, can be applied to Web hypermedia development effort prediction.",Case-based reasoning; Prediction models; Web effort prediction; Web hypermedia; Web hypermedia metrics,"Mendes E., Mosley N., Watson I.",2002,Conference,"Proceedings of the 11th International Conference on World Wide Web, WWW '02",10.1145/511446.511482,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953029927&doi=10.1145%2f511446.511482&partnerID=40&md5=300859d8220852121d618a056b0e88c8,"Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand; MxM Technology, Shortland Street, Auckland, New Zealand",,English,,1581134495; 9781581134490
Scopus,Estimating maintenance effort by analogy,"Effort estimation is a key step of any software project. This paper presents a method to estimate project effort using an improved version of analogy. Unlike estimation methods based on case-based reasoning, our method makes use of two nearest neighbors of the target project for estimation. An additional refinement based on the relative location of the target project is then applied to generate the effort estimate. We first identify the relationships between cost drivers and project effort, and then determine the number of past project data that should be used in the estimation to provide the best result. Our method is then applied to a set of maintenance projects. Based on a comparison of the estimation results from our estimation method and those of other estimation methods, we conclude that our method can provide more accurate results.",Analogy; Effort estimation; Maintenance projects,Leung H.K.N.,2002,Journal,Empirical Software Engineering,10.1023/A:1015202115651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036605452&doi=10.1023%2fA%3a1015202115651&partnerID=40&md5=9097babf115547d1e84e30d4abf4c93d,"Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong",,English,13823256,
Scopus,Applications of DEA to measure the efficiency of software production at two large Canadian banks,"This paper presents two empirical studies of software production conducted at two large Canadian banks. For this purpose, we introduce a new model of software production that considers more outputs than those previously cited in the literature. The first study analyses a group of software development projects and compares the ratio approach to performance measurement to the results of DEA. It is shown that the main deficiencies of the performance ratio method can be avoided with the latter. Two different approaches are employed to constrain the DEA multipliers with respect to subjective managerial goals. As is further shown, incorporating subjective values into efficiency measures must be done in a careful and rigorous manner, within a framework familiar to management. The second study investigates the effect of quality on software maintenance (enhancement) projects. Quality appears to have a significant impact on the efficiency and cost of software projects in the data set. We further show the problems that may result when quality is excluded from the production models for efficiency assessment. In particular, we show some of the misleading results that can be obtained when the simple, traditional, ratio definition of productivity is used for this purpose.",Data Envelopment Analysis; Efficiency measurement; Multiplier constraints; Software productivity; Software quality,"Paradi J.C., Reese D.N., Rosen D.",1997,Journal,Annals of Operations Research,10.1023/a:1018953900977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031480063&doi=10.1023%2fa%3a1018953900977&partnerID=40&md5=0eb2101796d71474ff9092c2f0091594,"Ctr. Mgmt. Technol. Entrepreneurship, Department of Industrial Engineering, University of Toronto, 4 Taddle Creek Road, Toronto, Ont. M5S 1A4, Canada",Kluwer Academic Publishers,English,02545330,
Scopus,Lean software development in action,"This book illustrates how goal-oriented, automated measurement can be used to create Lean organizations and to facilitate the development of Lean software, while also demonstrating the practical implementation of Lean software development by combining tried and trusted tools. In order to be successful, a Lean orientation of software development has to go hand in hand with a company's overall business strategy. To achieve this, two interrelated aspects require special attention: measurement and experience management. In this book, Janes and Succi provide the necessary knowledge to establish ""Lean software company thinking,"" while also exploiting the latest approaches to software measurement. A comprehensive, company-wide measurement approach is exactly what companies need in order to align their activities to the demands of their stakeholders, to their business strategy, etc. With the automatic, non-invasive measurement approach proposed in this book, even small and medium-sized enterprises that do not have the resources to introduce heavyweight processes will be able to make their software development processes considerably more Lean. The book is divided into three parts. Part I, ""Motivation for Lean Software Development,"" explains just what ""Lean Production"" means, why it can be advantageous to apply Lean concepts to software engineering, and which existing approaches are best suited to achieving this. Part II, ""The Pillars of Lean Software Development,"" presents the tools needed to achieve Lean software development: Non-invasive Measurement, the Goal Question Metric approach, and the Experience Factory. Finally, Part III, ""Lean Software Development in Action,"" shows how different tools can be combined to enable Lean Thinking in software development. The book primarily addresses the needs of all those working in the field of software engineering who want to understand how to establish an efficient and effective software development process. This group includes developers, managers, and students pursuing an M.Sc. degree in software engineering. © Springer-Verlag Berlin Heidelberg 2014. All rights are reserved.",,"Janes A., Succi G.",2014,Book,Lean Software Development in Action,10.1007/978-3-642-00503-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929922298&doi=10.1007%2f978-3-642-00503-9&partnerID=40&md5=3c58a765e1e73185c02010b7e9c55fee,"Libera Università di Bolzano, Bolzano, Italy",Springer-Verlag Berlin Heidelberg,English,,9783642005039; 3662441780; 9783662441787
Scopus,Using ensembles for web effort estimation,"Background: Despite the number of Web effort estimation techniques investigated, there is no consensus as to which technique produces the most accurate estimates, an issue shared by effort estimation in the general software estimation domain. A previous study in this domain has shown that using ensembles of estimation techniques can be used to address this issue. Aim: The aim of this paper is to investigate whether ensembles of effort estimation techniques will be similarly successful when used on Web project data. Method: The previous study built ensembles using solo effort estimation techniques that were deemed superior. In order to identify these superior techniques two approaches were investigated: The first involved replicating the methodology used in the previous study, while the second approach used the Scott-Knott algorithm. Both approaches were done using the same 90 solo estimation techniques on Web project data from the Tukutuku dataset. The replication identified 16 solo techniques that were deemed superior and were used to build 15 ensembles, while the Scott-Knott algorithm identified 19 superior solo techniques that were used to build two ensembles. Results: The ensembles produced by both approaches performed very well against solo effort estimation techniques. With the replication, the top 12 techniques were all ensembles, with the remaining 3 ensembles falling within the top 17 techniques. These 15 effort estimation ensembles, along with the 2 built by the second approach, were grouped into the best cluster of effort estimation techniques by the Scott-Knott algorithm. Conclusion: While it may not be possible to identify a single best technique, the results suggest that ensembles of estimation techniques consistently perform well even when using Web project data. © 2013 IEEE.",ensembles; replication; Scott-Knott algorithm; Web effort estimation,"Azhar D., Riddle P., Mendes E., Mittas N., Angelis L.",2013,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/ESEM.2013.25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893338357&doi=10.1109%2fESEM.2013.25&partnerID=40&md5=d740fe6df71bb26b5cb46afd5c8b230b,"Department of Computer Science, University of Auckland, Auckland, New Zealand; School of Computing, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece",,English,19493770,
Scopus,On the value of outlier elimination on software effort estimation research,"Producing accurate and reliable software effort estimation has always been a challenge for both academic research and software industries. Regarding this issue, data quality is an important factor that impacts the estimation accuracy of effort estimation methods. To assess the impact of data quality, we investigated the effect of eliminating outliers on the estimation accuracy of commonly used software effort estimation methods. Based on three research questions, we associatively analyzed the influence of outlier elimination on the accuracy of software effort estimation by applying five methods of outlier elimination (Least trimmed squares, Cook's distance, K-means clustering, Box plot, and Mantel leverage metric) and two methods of effort estimation (Least squares regression and Estimation by analogy with the variation of the parameters). Empirical experiments were performed using industrial data sets (ISBSG Release 9, Bank and Stock data sets that are collected from financial companies, and a Desharnais data set in the PROMISE repository). In addition, the effect of the outlier elimination methods is evaluated by the statistical tests (the Friedman test and the Wilcoxon signed rank test). The experimental results derived from the evaluation criteria showed that there was no substantial difference between the software effort estimation results with and without outlier elimination. However, statistical analysis indicated that outlier elimination leads to a significant improvement in the estimation accuracy on the Stock data set (in case of some combinations of outlier elimination and effort estimation methods). In addition, although outlier elimination did not lead to a significant improvement in the estimation accuracy on the other data sets, our graphical analysis of errors showed that outlier elimination can improve the likelihood to produce more accurate effort estimates for new software project data to be estimated. Therefore, from a practical point of view, it is necessary to consider the outlier elimination and to conduct a detailed analysis of the effort estimation results to improve the accuracy of software effort estimation in software organizations. © 2012 Springer Science+Business Media, LLC.",Outlier elimination; Software cost estimation; Software data quality; Software effort estimation,"Seo Y.-S., Bae D.-H.",2013,Journal,Empirical Software Engineering,10.1007/s10664-012-9207-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879838338&doi=10.1007%2fs10664-012-9207-y&partnerID=40&md5=bb5dedbb86c7592aef730665f16a2900,"Department of Computer Science, College of Information Science and Technology, KAIST, Daejeon, South Korea",,English,13823256,
Scopus,Kernel methods for software effort estimation: Effects of different kernel functions and bandwidths on estimation accuracy,"Analogy based estimation (ABE) generates an effort estimate for a new software project through adaptation of similar past projects (a.k.a. analogies). Majority of ABE methods follow uniform weighting in adaptation procedure. In this research we investigated non-uniform weighting through kernel density estimation. After an extensive experimentation of 19 datasets, 3 evaluation criteria, 5 kernels, 5 bandwidth values and a total of 2090 ABE variants, we found that: (1) non-uniform weighting through kernel methods cannot outperform uniform weighting ABE and (2) kernel type and bandwidth parameters do not produce a definite effect on estimation performance. In summary simple ABE approaches are able to perform better than much more complex approaches. Hence, - provided that similar experimental settings are adopted - we discourage the use of kernel methods as a weighting strategy in ABE. © 2011 Springer Science+Business Media, LLC.",Bandwidth; Data mining; Effort estimation; Kernel function,"Kocaguneli E., Menzies T., Keung J.W.",2013,Journal,Empirical Software Engineering,10.1007/s10664-011-9189-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872279818&doi=10.1007%2fs10664-011-9189-1&partnerID=40&md5=a398216796c4c978625b93b0d4baae52,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26505, United States; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong",,English,13823256,
Scopus,"Review of hardware cost estimation methods, models and tools applied to early phases of space mission planning","The primary purpose of this paper is to review currently existing cost estimation methods, models, tools and resources applicable to the space sector. While key space sector methods are outlined, a specific focus is placed on hardware cost estimation on a system level, particularly for early mission phases during which specifications and requirements are not yet crystallised, and information is limited. For the space industry, cost engineering within the systems engineering framework is an integral discipline. The cost of any space program now constitutes a stringent design criterion, which must be considered and carefully controlled during the entire program life cycle. A first step to any program budget is a representative cost estimate which usually hinges on a particular estimation approach, or methodology. Therefore appropriate selection of specific cost models, methods and tools is paramount, a difficult task given the highly variable nature, scope as well as scientific and technical requirements applicable to each program. Numerous methods, models and tools exist. However new ways are needed to address very early, pre-Phase 0 cost estimation during the initial program research and establishment phase when system specifications are limited, but the available research budget needs to be established and defined. Due to their specificity, for vehicles such as reusable launchers with a manned capability, a lack of historical data implies that using either the classic heuristic approach such as parametric cost estimation based on underlying CERs, or the analogy approach, is therefore, by definition, limited. This review identifies prominent cost estimation models applied to the space sector, and their underlying cost driving parameters and factors. Strengths, weaknesses, and suitability to specific mission types and classes are also highlighted. Current approaches which strategically amalgamate various cost estimation strategies both for formulation and validation of an estimate, and techniques and/or methods to attain representative and justifiable cost estimates are consequently discussed. Ultimately, the aim of the paper is to establish a baseline for development of a non-commercial, low cost, transparent cost estimation methodology to be applied during very early program research phases at a complete vehicle system level, for largely unprecedented manned launch vehicles in the future. This paper takes the first step to achieving this through the identification, analysis and understanding of established, existing techniques, models, tools and resources relevant within the space sector. © 2012 Elsevier Ltd.",Cost engineering; Cost estimation; Cost model; Early mission phase; Parametrics; Space hardware,"Trivailo O., Sippel M., Şekercioǧlu Y.A.",2012,Review,Progress in Aerospace Sciences,10.1016/j.paerosci.2012.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863306610&doi=10.1016%2fj.paerosci.2012.02.001&partnerID=40&md5=89172ecdccd664b335cd15d4f76bfc39,"Deutsches Zentrum für Luft- und Raumfahrt, Institute of Space Systems, Space Launcher Systems Analysis Department (SART), Robert-Hooke-Straße 7, 28359 Bremen, Germany; Department of Electrical and Computer Systems Engineering, Monash University, PO Box 35, Clayton, VIC 3800, Australia",,English,03760421,
Scopus,A Bayesian network approach to assess and predict software quality using activity-based quality models,"Context: Software quality is a complex concept. Therefore, assessing and predicting it is still challenging in practice as well as in research. Activity-based quality models break down this complex concept into concrete definitions, more precisely facts about the system, process, and environment as well as their impact on activities performed on and with the system. However, these models lack an operationalisation that would allow them to be used in assessment and prediction of quality. Bayesian networks have been shown to be a viable means for this task incorporating variables with uncertainty. Objective: The qualitative knowledge contained in activity-based quality models are an abundant basis for building Bayesian networks for quality assessment. This paper describes a four-step approach for deriving systematically a Bayesian network from an assessment goal and a quality model. Method: The four steps of the approach are explained in detail and with running examples. Furthermore, an initial evaluation is performed, in which data from NASA projects and an open source system is obtained. The approach is applied to this data and its applicability is analysed. Results: The approach is applicable to the data from the NASA projects and the open source system. However, the predictive results vary depending on the availability and quality of the data, especially the underlying general distributions. Conclusion: The approach is viable in a realistic context but needs further investigation in case studies in order to analyse its predictive validity. © 2010 Elsevier B.V. All rights reserved.",Activity-based quality model; Bayesian network; Quality assessment; Quality prediction,Wagner S.,2010,Conference,Information and Software Technology,10.1016/j.infsof.2010.03.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956403351&doi=10.1016%2fj.infsof.2010.03.016&partnerID=40&md5=f38371edbd186f593211648b0efb85f3,"Technische Universität München, Fakultät für Informatik, Boltzmannstr. 3, 85748 Garching, Germany",,English,09505849,
Scopus,An approach to modelling software evolution processes,"An Approach to Modelling Software Evolution Processes describes formal software processes that effectively support software evolution. The importance and popularity of software evolution increase as more and more successful software systems become legacy systems. For one thing, software evolution has become an important characteristic in the software life cycle; for another, software processes play an important role in increasing efficiency and quality of software evolution. Therefore, the software evolution process, the inter-discipline of software process and software evolution, becomes a key area in software engineering. The book is intended for software engineers and researchers in computer science. © 2008 Tsinghua University Press.",,Li T.,2009,Book,An Approach to Modelling Software Evolution Processes,10.1007/978-3-540-79464-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895224002&doi=10.1007%2f978-3-540-79464-6&partnerID=40&md5=37a1e255fa9160c6ed5cf68387e5080f,"School of Software, Yunnan University, Kunming 650091, China",Springer Berlin Heidelberg,English,,9783540794639
Scopus,BMR: Benchmarking metrics recommender for personnel issues in software development projects,"This paper presents an architecture which applies document similarity measures to the documentation produced during the phases of software development in order to generate recommendations of process and people metrics for similar projects. The application makes a judgment of similarity of the Service Provision Offer (SPO) document of a new proposed project to a collection of Project History Documents (PHD), stored in a repository of unstructured texts. The process is carried out in three stages: firstly, clustering of the Offer document with the set of PHDs which are most similar to it; this provides the initial indication of whether similar previous projects exist, and signifies similarity. Secondly, determination of which PHD in the set is most comparable with the Offer document, based on various parameters: project effort, project duration (time), project resources (members/size of team), costs, and sector(s) involved, indicating comparability of projects. The comparable parameters are extracted using the GATE Natural Language Processing architecture. Lastly, a recommendation of metrics for the new project is made, which is based on the transferability of the metrics of the most similar and comparable PHD extracted, here referred to as recommendation. © 2009 Taylor & Francis Group, LLC.",GATE; Natural Language Processing; Ontologies; Semantics; Software Metrics,"García-Crespo Á., Colomo-Palacios R., Gómez-Berbís J.M., Mencke M.",2009,Journal,International Journal of Computational Intelligence Systems,10.1080/18756891.2009.9727658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952866234&doi=10.1080%2f18756891.2009.9727658&partnerID=40&md5=7ef8023a35e0ccf8e0ec3dac486a3acc,"Computer Science Department, Universidad Carlos III de Madrid, Av. Universidad 30, Madrid, 28911, Spain",,English,18756891,
Scopus,Using sensitivity analysis to create simplified economic models for regression testing,"Software engineering methodologies are subject to complex cost-benefit tradeoffs. Economic models can help practitioners and researchers assess methodologies relative to these tradeoffs. Effective economic models, however, can be established only through an iterative process of refinement involving analytical and empirical methods. Sensitivity analysis provides one such method. By identifying the factors that are most important to models, sensitivity analysis can help simplify those models; it can also identify factors that must be measured with care, leading to guidelines for better test strategy definition and application. In prior work we presented the first comprehensive economic model for the regression testing process, that captures both cost and benefit factors relevant to that process while supporting evaluation of these processes across entire system lifetimes. In this work we use sensitivity analysis to examine our model analytically and assess the factors that are most important to the model. Based on the results of that analysis, we propose two new models of increasing simplicity. We assess these models empirically on data obtained by using regression testing techniques on several non-trivial software systems. Our results show that one of the simplified models assesses the relationships between techniques in the same way as the full model. © 2008 ACM.",Economic models; Empirical studies; Regression test selection; Regression testing; Sensitivity analysis; Test case prioritization,"Do H., Rothermel G.",2008,Conference,ISSTA'08: Proceedings of the 2008 International Symposium on Software Testing and Analysis 2008,10.1145/1390630.1390639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57449121576&doi=10.1145%2f1390630.1390639&partnerID=40&md5=87dd848f08f19c7292c12df23732a8ca,North Dakota State University; University of Nebraska - Lincoln,,English,,9781605580500
Scopus,A short note on safest default missingness mechanism assumptions,"A very common problem when building software engineering models is dealing with missing data. To address this there exist a range of imputation techniques. However, selecting the appropriate imputation technique can also be a difficult problem. One reason for this is that these techniques make assumptions about the underlying missingness mechanism, that is how the missing values are distributed within the data set. It is compounded by the fact that, for small data sets, it may be very difficult to determine what is the missingness mechanism. This means there is a danger of using an inappropriate imputation technique. Therefore, it is necessary to determine what is the safest default assumption about the missingness mechanism for imputation techniques when dealing with small data sets. We examine experimentally, two simple and commonly used techniques: Class Mean Imputation (CMI) and k Nearest Neighbors (k-NN) coupled with two missingness mechanisms: missing completely at random (MCAR) and missing at random (MAR). We draw two conclusions. First, that for our analysis CMI is the preferred technique since it is more accurate. Second, and more importantly, the impact of missingness mechanism on imputation accuracy is not statistically significant. This is a useful finding since it suggests that even for small data sets we can reasonably make a weaker assumption that the missingness mechanism is MAR. Thus both imputation techniques have practical application for small software engineering data sets with missing values. © 2005 Springer Science + Business Media, Inc.",Data imputation; Missing data; Missingness mechanism; Software effort prediction,"Song Q., Shepperd M., Cartwright M.",2005,Journal,Empirical Software Engineering,10.1007/s10664-004-6193-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444371705&doi=10.1007%2fs10664-004-6193-8&partnerID=40&md5=de48596569ff70cb7e87675be7bcecdd,"Empirical Software Eng. Res. Group, Sch. Des., Eng. Comp., B., United Kingdom",,English,13823256,
Scopus,Making inferences with small numbers of training sets,"A potential methodological problem with empirical studies that assess project effort prediction system is discussed. Frequently, a hold-out strategy is deployed so that the data set is split into a training and a validation set. Inferences are then made concerning the relative accuracy of the different prediction techniques under examination. This is typically done on very small numbers of sampled training sets. It is shown that such studies can lead to almost random results (particularly where relatively small effects are being studied). To illustrate this problem, two data sets are analysed using a configuration problem for case-based prediction and results generated from 100 training sets. This enables results to be produced with quantified confidence limits. From this it is concluded that in both cases using less than five training sets leads to untrustworthy results, and ideally more than 20 sets should be deployed. Unfortunately, this raises a question over a number of empirical validations of prediction techniques, and so it is suggested that further research is needed as a matter of urgency.",,"Kirsopp C., Shepperd M.",2002,Conference,IEE Proceedings: Software,10.1049/ip-sen:20020695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036807816&doi=10.1049%2fip-sen%3a20020695&partnerID=40&md5=995db4c1cd373b884aeab6accfcd556f,"Empirical Software Eng. Res. Group, Royal London House, Bournemouth University, Bournemouth BH1 3LT, United Kingdom",,English,14625970,
Scopus,Web development effort estimation using analogy,"Although estimating the effort required in developing Web applications is a difficult task, accurate estimates of development effort have an important role to play in the successful management of Web development projects. In software development work to date, emphasis has focused on algorithmic cost models such as COCOMO and function points. Two disadvantages of these models are firstly, the need for calibration of a model for each individual measurement environment and, secondly, the variable accuracy levels achieved even after calibration. The paper describes the use of estimation by analogy to calculate the development effort of Web applications. Two datasets containing empirical Web development data were used in the case study. One set contained data relating to forty-one novice developers, the other to twenty-nine experienced developers. The ANGEL tool supporting the automatic collection, storage and identification of the most analogous projects was used as a basis for estimating effort required for a new project. Results show estimation by analogy to be a promising alternative to algorithmic techniques. © 2000 IEEE.",Bismuth; Gold,"Mendes E., Counsell S.",2000,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2000.844577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244348334&doi=10.1109%2fASWEC.2000.844577&partnerID=40&md5=65a65539dc365b205b6c423ad59adb0d,"Department of Computer Science, University of Auckland, New Zealand; Department of Computer Science, Birkbeck College, United Kingdom",IEEE Computer Society,English,,0769506313
Scopus,Function points: A study of their measurement processes and scale transformations,"Function point metrics were initially designed through expert judgments. The underlying measurement model has not been clearly stated, and this has generated some confusion as to the true nature of these metrics and their usefulness in fields other than their initial Management Information System domain. When viewed without reference to implicit models hidden in the expert judgments, function points constitute a potpourri of measurement scales. This suggests that each step could represent a transcend the measurement scales and maintain or improve the desired relationship with development effort. © 1994.",,"Abran A., Robillard P.N.",1994,Journal,The Journal of Systems and Software,10.1016/0164-1212(94)90004-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028436237&doi=10.1016%2f0164-1212%2894%2990004-3&partnerID=40&md5=bff4260240b3c4219a67c1bc65618dcf,"Université du Québec à Montréal, Montréal, Que., Canada; École Polytechnique de Montréal, Montréal, Que., Canada",,English,01641212,
Scopus,A large-scale study of the impact of feature selection techniques on defect classification models,"The performance of a defect classification modeldepends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%-87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models. © 2017 IEEE.",Defect Classification Models; Feature Selection Techniques,"Ghotra B., McIntosh S., Hassan A.E.",2017,Conference,IEEE International Working Conference on Mining Software Repositories,10.1109/MSR.2017.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026553212&doi=10.1109%2fMSR.2017.18&partnerID=40&md5=569bcc4b6532b68cc266d59d345885a2,"Queen's University, Canada; McGill University, Canada",IEEE Computer Society,English,21601852,9781538615447
Scopus,Software development effort estimation using classical and fuzzy analogy: A cross-validation comparative study,"Software effort estimation is one of the most important tasks in software project management. Of several techniques suggested for estimating software development effort, the analogy-based reasoning, or Case-Based Reasoning (CBR), approaches stand out as promising techniques. In this paper, the benefits of using linguistic rather than numerical values in the analogy process for software effort estimation are investigated. The performance, in terms of accuracy and tolerance of imprecision, of two analogy-based software effort estimation models (Classical Analogy and Fuzzy Analogy, which use numerical and linguistic values respectively to describe software projects) is compared. Three research questions related to the performance of these two models are discussed and answered. This study uses the International Software Benchmarking Standards Group (ISBSG) dataset and confirms the usefulness of using linguistic instead of numerical values in analogy-based software effort estimation models. © Imperial College Press.",Case-based reasoning; Fuzzy clustering; Fuzzy logic; Real coded genetic algorithm; Software effort estimation,"Amazal F.A., Idri A., Abran A.",2014,Journal,International Journal of Computational Intelligence and Applications,10.1142/S1469026814500138,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908567208&doi=10.1142%2fS1469026814500138&partnerID=40&md5=7cc9c1aeaf80c1739df9a20546ac1885,"Software Project Management Research Team, ENSIAS, Mohammed V University, Madinate Al Irfane Rabat, 10100, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, University of Quebec, 1100 Notre-Dame West, Montreal, H3C 1K3, Canada",World Scientific Publishing Co.,English,14690268,
Scopus,Analytics for software development,"Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise. Copyright 2010 ACM.",Analytics; Project management,"Buse R.P.L., Zimmermann T.",2010,Conference,"Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010",10.1145/1882362.1882379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951654695&doi=10.1145%2f1882362.1882379&partnerID=40&md5=ffb74c1f1db346913115018d7c1b9a50,"University of Virginia, United States; Microsoft Research, United States",,English,,9781450304276
Scopus,Visual comparison of software cost estimation models by regression error characteristic analysis,"The well-balanced management of a software project is a critical task accomplished at the early stages of the development process. Due to this requirement, a wide variety of prediction methods has been introduced in order to identify the best strategy for software cost estimation. The selection of the best technique is usually based on measures of error whereas in more recent studies researchers use formal statistical procedures. The former approach can lead to unstable and erroneous results due to the existence of outlying points whereas the latter cannot be easily presented to non-experts and has to be carried out by an expert with statistical background. In this paper, we introduce the regression error characteristic (REC) analysis, a powerful visualization tool with interesting geometrical properties, in order to validate and compare different prediction models easily, by a simple inspection of a graph. Moreover, we propose a formal framework covering different aspects of the estimation process such as the calibration of the prediction methodology, the identification of factors that affect the error, the investigation of errors on certain ranges of the actual cost and the examination of the distribution of the cost for certain errors. Application of REC analysis to the ISBSG10 dataset for comparing estimation by analogy and linear regression illustrates the benefits and the significant information obtained. © 2009 Elsevier Inc. All rights reserved.",Estimation by analogy; Regression analysis; Regression error characteristic curves; Software cost estimation,"Mittas N., Angelis L.",2010,Journal,Journal of Systems and Software,10.1016/j.jss.2009.10.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749156201&doi=10.1016%2fj.jss.2009.10.044&partnerID=40&md5=bb789d9f714a2f1b5c349f955e8eb3b1,"Department of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece",,English,01641212,
Scopus,The impact of limited search procedures for systematic literature reviews - A participant-observer case study,"This study aims to compare the use of targeted manual searches with broad automated searches, and to assess the importance of grey literature and breadth of search on the outcomes of SLRs. We used a participant-observer multi-case embedded case study. Our two cases were a tertiary study of systematic literature reviews published between January 2004 and June 2007 based on a manual search of selected journals and conferences and a replication of that study based on a broad automated search. Broad searches find more papers than restricted searches, but the papers may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers; if publication bias is not an issue; or if they are assessing research trends in research methodologies. © 2009 IEEE.",,"Kitchenham B., Brereton P., Turner M., Niazi M., Linkman S., Pretorius R., Budgen D.",2009,Conference,"2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",10.1109/ESEM.2009.5314238,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72549119237&doi=10.1109%2fESEM.2009.5314238&partnerID=40&md5=ca670e390d8695171f3785938c1a62b3,"School of Computing and Mathematics, Keele University, Stoke-on-Trent ST5 5BG, United Kingdom; Dept. Computer Science, Durham University, Durham, DH1 3LE, United Kingdom",,English,,9781424448418
Scopus,Using an RBF neural network to locate program bugs,"We propose an RBF (radial basis function) neural network-based fault localization method to help programmers locate bugs in a more effective way. An RBF neural network with a three-layer feed-forward structure is employed to learn the relationship between the statement coverage of a test case and its corresponding execution result. The trained network is then given as input a set of virtual test cases, each covering only a single statement. The output of the network for each test case is considered to be the suspiciousness of the corresponding statement; a statement with a higher suspiciousness has a higher likelihood of containing a bug. The set of statements ranked in descending order by their suspiciousness are then examined by programmers one by one until a bug is located. Three case studies on different programs (space, grep and make) were conducted with each faulty version having exactly one bug. An additional program gcc was also used to demonstrate the concept of extending the proposed method to programs with multiple bugs. Our experimental data suggest that an RBF neural network-based fault localization method is more effective in locating a program bug (by xamining less code before the first faulty statement containing the bug s identified) than another popular method, Tarantula, which also uses the coverage and execution results to compute the suspiciousness of each statement. © 2008 IEEE.",EXAM score; Failed test; Fault localization; Program debugging; RBF (radial basis function) neural network; Successful test; Suspiciousness of code,"Wong W.E., Shi Y., Qi Y., Golden R.",2008,Conference,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",10.1109/ISSRE.2008.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249129946&doi=10.1109%2fISSRE.2008.15&partnerID=40&md5=61dab2f92229b2e49cf70caf4b869da8,"Department of Computer Science, University of Texas, Dallas, United States; School of Behavioral and Brain Sciences, University of Texas, Dallas, United States",,English,10719458,9780769534053
Scopus,The effect of communication overhead on software maintenance project staffing: A search-based approach,"Brooks' milestone 'Mythical Man Month' established the observation that there is no simple conversion between people and time in large scale software projects. Communication and training overheads yield a subtle and variable relationship between the person-months required for a project and the number of people needed to complete the task within a given timeframe. This paper formalises several instantiations of Brooks' law and uses these to construct project schedule and staffing instances - using a search-based project staffing and scheduling approach - on data from two large real world maintenance projects. The results reveal the impact of different formulations of Brooks' law on project completion time and on staff distribution across teams, and the influence of other factors such as the presence of dependencies between work packages on the effect of communication overhead. © 2007 IEEE.",Search-based software engineering; Software maintenance; Software project management,"Di Penta M., Harman M., Antoniol G., Qureshi F.",2007,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2007.4362644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349118781&doi=10.1109%2fICSM.2007.4362644&partnerID=40&md5=f26fb3b16481e5af94060c90dcaba21c,"RCOST, University of Sannio, Via Traiano, 82100 Benevento, Italy; Department of Computer Science, King's College London, Strand, London WC2R 2LS, United Kingdom; Department of Génie Informatique, École Polytechnique de Montréal, Canada",,English,,1424412560; 9781424412563
Scopus,Large-scale software engineering questions - Expert opinion or empirical evidence?,"A recent report on the state of the UK information technology (IT) industry based most of its findings and recommendations on expert opinion. It is surprising that the report was unable to incorporate more empirical evidence. This paper aims to assess whether it is necessary to base IT industry and academic policy on expert opinion rather than on empirical evidence. Current evidence related to the rate of project failure is identified and the methods used to accumulate that evidence discussed. This shows that the report failed to identify relevant evidence and most evidence related to project failure is based on convenience samples. The status of empirical research in the computing disciplines is reviewed showing that empirical evidence covers a restricted range of subjects and seldom addresses the 'Society' level of analysis. Other more robust designs that would address large-scale IT questions are discussed. We recommend adopting a more systematic approach to accumulating and reporting evidence. In addition, we propose using quasi-experimental designs developed and used in the social sciences to improve the methodology used for undertaking large-scale empirical studies in software engineering. © The Institution of Engineering and Technology 2007.",,"Kitchenham B., Budgen D., Brereton P., Turner M., Charters S., Linkman S.",2007,Journal,IET Software,10.1049/iet-sen:20060052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348996113&doi=10.1049%2fiet-sen%3a20060052&partnerID=40&md5=404046c69080b778bdf3fcb51b42f26f,"School of Computing and Mathematics, Keele University, Staffordshire, United Kingdom; Department of Computer Science, Durham University, Durham, United Kingdom; Applied Computing Group, Lincoln University, PO Box 84, Lincoln 7647, New Zealand",,English,17518806,
Scopus,Three empirical studies on estimating the design effort of Web applications,"Our research focuses on the effort needed for designing modern Web applications. The design effort is an important part of the total development effort, since the implementation can be partially automated by tools. We carried out three empirical studies with students of advanced university classes enrolled in engineering and communication sciences curricula. The empirical studies are based on the use of W2000, a special-purpose design notation for the design of Web applications, but the hypotheses and results may apply to a wider class of modeling notations (e.g., OOHDM, WebML, or UWE). We started by investigating the relative importance of each design activity. We then assessed the accuracy of a priori design effort predictions and the influence of a few process-related factors on the effort needed for each design activity. We also analyzed the impact of attributes like the size and complexity of W2000 design artifacts on the total effort needed to design the user experience of web applications. In addition, we carried out a finer-grain analysis, by studying which of these attributes impact the effort devoted to the steps of the design phase that are followed when using W2000. © 2007 ACM.",Effort estimation; Empirical study; W2000; Web application design,"Baresi L., Morasca S.",2007,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/1276933.1276936,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34648831105&doi=10.1145%2f1276933.1276936&partnerID=40&md5=5ba43abd05c55120e9b800f067a37ca6,"Politecnico di Milano; Università Degli Studi dell'Insubria; Dipartmento di Elettronica e Informazione, Politecnico di Milano, piazza Leonardo da Vinci, 32, I-20133 Milano, Italy; Dipartmento di Scienze della Cultura, Politiche e dell'Informazione, Università Degli Studi dell'Insubria, via Valleggio, 11, I-22100 Como, Italy",,English,1049331X,
Scopus,Using grey relational analysis to predict software effort with small data sets,"The inherent uncertainty of the software development process presents particular challenges for software effort prediction. We need to systematically address missing data values, feature subset selection and the continuous evolution of predictions as the project unfolds, and all of this in the context of data-starvation and noisy data. However, in this paper, we particularly focus on feature subset selection and effort prediction at an early stage of a project. We propose a novel approach of using Grey Relational Analysis (GRA) of Grey System Theory (GST), which is a recently developed system engineering theory based on the uncertainty of small samples. In this work we address some of the theoretical challenges in applying GRA to feature subset selection and effort prediction, and then evaluate our approach on five publicly available industrial data sets using stepwise regression as a benchmark. The results are very encouraging in the sense of being comparable or better than other machine learning techniques and thus indicate that the method has considerable potential. © 2005 IEEE.",Effort prediction; Empirical evaluation; Feature subset selection; Grey Relational Analysis; Grey System Theory; Software project estimation,"Song Q., Shepperd M., Mair C.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.51,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749060423&doi=10.1109%2fMETRICS.2005.51&partnerID=40&md5=2974c6213954acac8e32adba978fa83f,"Xi'an Jiaotong University, China; Brunel University, United Kingdom",,English,15301435,0769523714; 9780769523712
Scopus,The question of scale economies in software - Why cannot researchers agree?,"This paper investigates the different research results obtained when different researchers have investigated the issue of economies and diseconomies of scale in software projects. Although researchers have used broadly similar sets of software project data sets, the results of their analyses and the conclusions they have drawn have differed. The paper highlights methodological differences that have lead to the conflicting results and shows how in many cases the differing results can be reconciled. It discusses the application of econometric concepts such as production frontiers and data envelopment analysis (DEA) to software data sets. It concludes that the assumptions underlying DEA may make it unsuitable for most software datasets but stochastic production frontiers may be relevant. It also raises some statistical issues that suggest testing hypothesis about economies and diseconomies of scale may be much more difficult than has been appreciated. The paper concludes with a plea for agreed standards for research synthesis activities. © 2002 Elsevier Science B.V. All rights reserved.",Data envelopment analysis; Production functions; Research synthesis; Scale economies; Software estimation models,Kitchenham B.A.,2002,Journal,Information and Software Technology,10.1016/S0950-5849(01)00204-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037081208&doi=10.1016%2fS0950-5849%2801%2900204-X&partnerID=40&md5=f7166a834f433def1895419ba60c88cf,"Department of Computer Science, University of Keele, Staffordshire ST5 5BG, United Kingdom",,English,09505849,
Scopus,Effort estimation in Agile software development: A survey on the state of the practice,"Context: There are numerous studies on effort estimation in Agile Software Development (ASD) and the state of the art in this area has been recently documented in a Systematic Literature Review (SLR). However, to date there are no studies on the state of the practice in this area, focusing on similar issues to those investigated in the above-mentioned SLR. Objectives: The aim of this paper is to report on the state of the practice on effort estimation in ASD, focusing on a wide range of aspects such as the estimation techniques and effort predictors used, to name a few. Method: A survey was carried out using as instrument an on-line questionnaire answered by agile practitioners who have experience in effort estimation. Results: Data was collected from 60 agile practitioners from 16 different countries, and the main findings are: 1) Planning poker (63%), analogy (47%) and expert judgment (38%) are frequently practiced estimation techniques in ASD; 2) Story points is the most frequently (62%) employed size metric, used solo or in combination with other metrics (e.g., function points); 3) Team's expertise level and prior experience are most commonly used cost drivers; 4) 52% of the respondents believe that their effort estimates on average are under/over estimated by an error of 25% or more; 5) Most agile teams take into account implementation and testing activities during effort estimation; and 6) Estimation is mostly performed at sprint and release planning levels in ASD. Conclusions: Estimation techniques that rely on experts' subjective assessment are the ones used the most in ASD, with effort underestimation being the dominant trend. Further, the use of multiple techniques in combination and story points seem to present a positive association with estimation accuracy, and team-related cost drivers are the ones used by most agile teams. Finally, requirements and management related issues are perceived as the main reasons for inaccurate estimates. Copyright 2015 ACM.",,"Usman M., Mendes E., Börstler J.",2015,Conference,ACM International Conference Proceeding Series,10.1145/2745802.2745813,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961114932&doi=10.1145%2f2745802.2745813&partnerID=40&md5=6a8e6f26dbad202da67aac01a3db9bcc,"Blekinge Institute of Technology, Karlskrona, 371 79, Sweden",Association for Computing Machinery,English,,9781450333504
Scopus,DICB: Dynamic intelligent customizable benign pricing strategy for cloud computing,"As cloud services need a fair pricing for both service providers and customers. If the price is too high, the customer may not use it, if the price is too low, service providers have less incentive to develop services. This paper proposes a novel pricing framework for cloud services using game theory (Cournot Duopoly, Cartel, and Stackelberg models) and data mining techniques (clustering and classification, e.g., SVM (Support Vector Machine)) to determine optimal prices for cloud services. The framework is dynamic because the price is determined based on recent usage data and available resources, it is also intelligent as it takes into various economic models into consideration, it is benign because it considers two conflicting parties, service providers and consumers, into consideration at the same time, and it is customizable based on various pricing strategies proposed by service providers and usage patterns as exhibited by consumers. Linear regression is used in various game theory models to determine the optimal price. A global pricing union (GPU) framework is proposed to achieve the best practice of game theory models. Based on the proposed technique, this paper applies this pricing framework to a case study in cloud services, and demonstrates that the prices obtained meet the requirement of traditional supply-demand analysis. In other words, the price obtained is good enough. © 2012 IEEE.",Game Theory; Pricing Models; SaaS,"Tsai W.-T., Qi G.",2012,Conference,"Proceedings - 2012 IEEE 5th International Conference on Cloud Computing, CLOUD 2012",10.1109/CLOUD.2012.49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866749969&doi=10.1109%2fCLOUD.2012.49&partnerID=40&md5=f897fc186a3c61a4cae2f59b5bc255de,"School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, United States; Department of Computer Science and Technology, Tsinghua University, Beijing, China",,English,,9780769547558
Scopus,Cost estimation for cross-organizational ERP projects: Research perspectives,"There are many methods for estimating size, effort, schedule and other cost aspects of IS projects, but only one specifically developed for Enterprise Resource Planning (ERP) (Stensrud, Info Soft Technol 43(7):413-423, 2001) and none for simultaneous, interdependent ERP projects in a cross-organizational context. The objective of this paper is to sketch the problem domain of cross-organizational ERP cost estimation, to survey available solutions, and to propose a research program to improve those solutions. In it, we: (i) explain why knowledge in the cost estimation of cross-organizational ERP is fragmented, (ii) assess the need to integrate research perspectives, and (iii) propose research directions that an integrated view of estimating cross-organizational ERP project cost should include. © 2008 The Author(s).",Cost estimation models; Cross-company estimation; Enterprise resource planning,"Daneva M., Wieringa R.",2008,Conference,Software Quality Journal,10.1007/s11219-008-9045-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48449104377&doi=10.1007%2fs11219-008-9045-8&partnerID=40&md5=0017bf13a55dcd99a12a9cb9a6fcae0c,"Department of Computer Science, University of Twente, Drienerlolaan 5, 7500 AE Enschede, Netherlands",Kluwer Academic Publishers,English,09639314,
Scopus,Research Collaborations between Academia and Industry,"The rapid and complex research and innovation processes require high-tech companies to optimize their technology transfer processes. It is clearly not sufficient to solely rely on internal R&D strategic cooperations with external research centers of excellence are needed in order to compete in the global innovation market. Candidates for such strategic cooperations are universities, research institutions, and technology focused consulting companies. Key challenge is the effective integration of external competences into the company-internal innovation processes. In this paper we present a survey of the state-of-the-art in technology transfer, high-light promising success cases for the future, and derive success criteria for successful technology transfer in a global world. The cooperation between Siemens and Fraunhofer IESE is presented as a concrete example. © 2007 IEEE.",,"Rombach D., Achatz R.",2007,Conference,FoSE 2007: Future of Software Engineering,10.1109/FOSE.2007.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748812749&doi=10.1109%2fFOSE.2007.16&partnerID=40&md5=e08e788d464bf4e1ceda85edbdfe1445,"University of Kaiserslautern and Fraunhofer IESE, Kaiserslautern, Germany; Siemens Corp., Siemens, Munich, Germany",,English,,0769528295; 9780769528298
Scopus,Software development effort estimation using fuzzy logic: A case STudy,"Software Estimation has been identified as one of the three great challenges for half-century-old computer science. Developers should be able to achieve practices containing effort estimation based on their own programs. New paradigms as Fuzzy Logic may offer an alternative for software effort estimation. This paper describes an application whose results are compared with those of a multiple regression. A subset of 41 modules developed from ten programs are used as data. Result shows that the value of MMRE (an aggregation of Magnitude of Relative Error, MRE) applying fuzzy logic was slightly higher than MMRE applying multiple regression; while the value of Pred(20) applying fuzzy logic was slightly higher than Pred(20) applying multiple regression. Moreover, six of 41 MRE was equal to zero (without any deviation) when fuzzy logic was applied (not any similar case was presented when multiple regression was applied). © 2005 IEEE.",Coefficient of determination; Correlation; Fuzzy logic; Multiple regression; Software effort estimation,"Martín C.L., Pasquier J.L., Cornelio Yáñez M., Agustín Gutiérrez T.",2005,Conference,Proceedings of the Mexican International Conference on Computer Science,10.1109/ENC.2005.47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947240249&doi=10.1109%2fENC.2005.47&partnerID=40&md5=0af5e4a504158cd9c8c200f5019423d2,"Center for Computing Research, National Polytechnic Institute, Mexico; Centro Universitario de Ciencias Exactas e Ingenierías, University of Guadalajara, Mexico",,English,15504069,0769524540; 9780769524542
Scopus,The ROI from software quality,"The ROI from Software Quality provides the tools needed for software engineers and project managers to calculate how much they should invest in quality, what benefits the investment will reap, and just how quickly those benefits will be realized. This text provides the quantitative models necessary for making real and reasonable calculations and it shows how to perform ROI analysis before and after implementing a quality program. The book demonstrates how to collect the appropriate data and easily perform the appropriate ROI analysis. Taking an evidence-based approach, this book supports its methodology with large amounts of data and backs up its positioning with numerous case studies and straightforward return-on-investment calculations. By carefully substantiating arguments, this volume separates itself from other works on ROI. © 2005 by Taylor & Francis Group, LLC.",,El Emam K.,2005,Book,The ROI from Software Quality,10.1201/9780849332982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645894813&doi=10.1201%2f9780849332982&partnerID=40&md5=4f096e7ad59e47f10d2dab293ddb4602,"Children’s Hospital, Eastern Ontario Research Institute, University of Ottawa, Ottawa, Canada; Faculty of Medicine, Ottawa Software Quality Association, Center for Global eHealth Innovation, University of Toronto, Canada; School of Business, Korea University, South Korea",CRC Press,English,,9781420031201; 0849332982; 9780849332982
Scopus,Increasing the accuracy and reliability of analogy-based cost estimation with extensive project feature dimension weighting,"Accurate and reliable software cost estimation is a vital task in software project portfolio decisions like resource ' scheduling or bidding. A prominent and transparent method of supporting estimators is analogy-based cost estimation, which is based on finding similar projects in historical portfolio data. However, the various project feature dimensions used to determine project analogy represent project aspects differing widely in their relevance; they are known to have varying impact on the analogies - and in turn on the overall estimation accuracy and reliability - , which is not addressed by traditional approaches. This paper (a) proposes an improved analogy-based approach based on extensive dimension weighting, and (ii) empirically evaluates the accuracy and reliability improvements in the context of five real-world portfolio data sets. Main results are accuracy and reliability improvements for all analyzed portfolios and quality measures. Furthermore, the approach indicates a quality barrier for analogy-based estimation approaches using the same basic assumptions and quality measures.",,"Auer M., Biffl S.",2004,Conference,"Proceedings - 2004 International Symposium on Empirical Software Engineering, ISESE 2004",10.1109/ISESE.2004.1334902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244262511&doi=10.1109%2fISESE.2004.1334902&partnerID=40&md5=55fdb61ba574a7265b3f799cb6e63dd3,"Inst. Software Technol./I.S., Vienna University of Technology, Favoritenstr. 9-11, A-1040 Vienna, Austria",,English,,0769521657; 9780769521657
Scopus,"Software Cost Estimation: A Review of Models, Process, and Practice","This article presents a review of software cost estimation models, processes, and practice. A general prediction process and a framework for selecting predictive measures and methods are proposed and used to analyze and interpret the research that is reviewed in this article. The prediction process and selection framework highlight the importance of measurement within the context of system development and the need to package and reuse experience to improve the accuracy of software cost estimates. As a result of our analysis we identify the need for further work to expand the range of models that are the focus of software cost estimation research. We also recommend that practitioners adopt an estimation process that incorporates feedback on the accuracy of estimates and packaging of experience. © 1997 Academic Press Inc.",,"Walkerden F., Jeffery R.",1997,Book Chapter,Advances in Computers,10.1016/S0065-2458(08)60337-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21744448782&doi=10.1016%2fS0065-2458%2808%2960337-X&partnerID=40&md5=e4989c3dbe988da5a3d7dceb430f890e,"Centre for Advanced Empirical Software Research School, Information Systems University of New South Wales, Sydney, Australia",Academic Press Inc.,English,00652458,
Scopus,A systematic review of requirements change management,"Context Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement. Objective In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature. Method We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work. Results The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified. Conclusions This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success. © 2017 Elsevier B.V.",Agile; Requirements change management; Systematic review,"Jayatilleke S., Lai R.",2018,Review,Information and Software Technology,10.1016/j.infsof.2017.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029498987&doi=10.1016%2fj.infsof.2017.09.004&partnerID=40&md5=ca74a48d5dfa6a03ad7e17d6f0c377fc,"Department of Computer Science and Information Technology, La Trobe University, Bundoora, Victoria  3086, Australia",Elsevier B.V.,English,09505849,
Scopus,AREION: Software effort estimation based on multiple regressions with adaptive recursive data partitioning,"Context Along with expert judgment, analogy-based estimation, and algorithmic methods (such as Function point analysis and COCOMO), Least Squares Regression (LSR) has been one of the most commonly studied software effort estimation methods. However, an effort estimation model using LSR, a single LSR model, is highly affected by the data distribution. Specifically, if the data set is scattered and the data do not sit closely on the single LSR model line (do not closely map to a linear structure) then the model usually shows poor performance. In order to overcome this drawback of the LSR model, a data partitioning-based approach can be considered as one of the solutions to alleviate the effect of data distribution. Even though clustering-based approaches have been introduced, they still have potential problems to provide accurate and stable effort estimates. Objective In this paper, we propose a new data partitioning-based approach to achieve more accurate and stable effort estimates via LSR. This approach also provides an effort prediction interval that is useful to describe the uncertainty of the estimates. Method Empirical experiments are performed to evaluate the performance of the proposed approach by comparing with the basic LSR approach and clustering-based approaches, based on industrial data sets (two subsets of the ISBSG (Release 9) data set and one industrial data set collected from a banking institution). Results The experimental results show that the proposed approach not only improves the accuracy of effort estimation more significantly than that of other approaches, but it also achieves robust and stable results according to the degree of data partitioning. Conclusion Compared with the other considered approaches, the proposed approach shows a superior performance by alleviating the effect of data distribution that is a major practical issue in software effort estimation. © 2013 Elsevier B.V. All rights reserved.",Adaptive recursive data partitioning; Data distribution; Least squares regression; Software cost estimation; Software effort estimation; Software project management,"Seo Y.-S., Bae D.-H., Jeffery R.",2013,Journal,Information and Software Technology,10.1016/j.infsof.2013.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880788524&doi=10.1016%2fj.infsof.2013.03.007&partnerID=40&md5=876ac01f59c9955b09236efda483deeb,"Department of Computer Science, College of Information Science and Technology, KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon 305-701, South Korea; NICTA, Locked Bag 9013, Alexandria, NSW 1435, Australia",,English,09505849,
Scopus,Teaching software engineering and software project management: An integrated and practical approach,"We present a practical approach for teaching two different courses of Software Engineering (SE) and Software Project Management (SPM) in an integrated way. The two courses are taught in the same semester, thus allowing to build mixed project teams composed of five-eight Bachelor's students (with development roles) and one or two Master's students (with management roles). The main goal of our approach is to simulate a real-life development scenario giving to the students the possibility to deal with issues arising from typical project situations, such as working in a team, organising the division of work, and coping with time pressure and strict deadlines. © 2012 IEEE.",,"Bavota G., De Lucia A., Fasano F., Oliveto R., Zottoli C.",2012,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2012.6227027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864274174&doi=10.1109%2fICSE.2012.6227027&partnerID=40&md5=26671bdf4076ed15e34edc90824a6ab7,"School of Science, University of Salerno, 84084 Fisciano (SA), Italy; STAT Department, University of Molise, 86090 Pesche (IS), Italy",,English,02705257,9781467310673
Scopus,Software project effort estimation with voting rules,"Social choice deals with aggregating the preferences of a number of voters into a collective preference. We will use this idea for software project effort estimation, substituting the voters by project attributes. Therefore, instead of supplying numeric values for various project attributes that are then used in regression or similar methods, a new project only needs to be placed into one ranking per attribute, necessitating only ordinal values. Using the resulting aggregate ranking the new project is again placed between other projects whose actual expended effort can be used to derive an estimation. In this paper we will present this method and extensions using weightings derived from genetic algorithms. We detail a validation based on several well-known data sets and show that estimation accuracy similar to classic methods can be achieved with considerably lower demands on input data. © 2008 Elsevier B.V. All rights reserved.",Cost estimation; Evolutionary computing; Genetic algorithms; Product metrics; Social choice,"Koch S., Mitlöhner J.",2009,Journal,Decision Support Systems,10.1016/j.dss.2008.12.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349190350&doi=10.1016%2fj.dss.2008.12.002&partnerID=40&md5=1f565bda784b44e9c4f3cf59886974fb,"Bogazici University, Istanbul, Turkey; Vienna University of Economics and Business Administration, Vienna, Austria",,English,01679236,
Scopus,Model-based functional size measurement,"Function Point Analysis (FPA) is the most widely used method for measuring the size of software requirements, usually for the purpose of cost estimation. Unfortunately, FPA is affected by several drawbacks: it must be performed by specifically skilled personnel, it is expensive, and the resulting measures are subject to high variability. In order to solve -at least partially- these problems, researchers have proposed to base FP counting on UML models. However, models built without having FPA in mind hardly provide the required information at the proper detail level, so that the measures of the models tend to vary accordingly. On the contrary, building models that are suitable for FPA generally requires additional notations, skills and effort, thus partly spoiling the advantages of the approach. This paper illustrates a technique for building FPA-oriented UML models that do not need to include more information than usually required by the development process, and are easy to measure. As a result, FPA can be performed in a seamless way, while yielding reliable results. The proposed technique was validated by means of a controlled experiment and a set of pilot applications, which are also briefly described in the paper. Copyright 2008 ACM.",Function point analysis; Functional size measurement; Requirements modeling; UML,"Lavazza L.A., Bianco V.D., Garavaglia C.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949189583&doi=10.1145%2f1414004.1414021&partnerID=40&md5=01f62d1dd0943a0255d3fd3570eb0330,"CEFRIEL, University of Insubria, Via Mazzini, 5, 21100 Várese, Italy; University of Insubria, Via Mazzini, 5, 21100 Varese, Italy; Syrea - Intecs, Via Archimede, 10, 20100 Milano, Italy",,English,,9781595939715
Scopus,Software defect prediction: Heuristics for weighted naïve bayes,"Defect prediction is an important topic in software quality research. Statistical models for defect prediction can be built on project repositories. Project repositories store software metrics and defect information. This information is then matched with software modules. Naive Bayes is a well known, simple statistical technique that assumes the 'independence' and 'equal importance' of features, which are not true in many problems. However, Naive Bayes achieves high performances on a wide spectrum of prediction problems. This paper addresses the 'equal importance' of features assumption of Naive Bayes. We propose that by means of heuristics we can assign weights to features according to their importance and improve defect prediction performance. We compare the weighted Naive Bayes and the standard Naive Bayes predictors' performances on publicly available datasets. Our experimental results indicate that assigning weights to software metrics increases the prediction performance significantly.",Defect prediction; Empirical software engineering; Feature weighting; Naïve bayes; Software metrics; Software quality,"Turhan B., Bener A.",2007,Conference,"ICSOFT 2007 - 2nd International Conference on Software and Data Technologies, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049185645&partnerID=40&md5=f295053d506bef027672fdfcecf8c9ec,"Department of Computer Engineering, Bogazici University, 34342 Bebek, Istanbul, Turkey",,English,,
Scopus,"Effort estimation: How valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set?","Previous studies comparing the prediction accuracy of effort models built using Web cross- and single-company data sets have been inconclusive, and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a cross-company effort model. This paper therefore replicates a previous study by investigating how successful a cross-company effort model is: i) to estimate effort for Web projects that belong to a single company and were not used to build the cross-company model; ii) compared to a single-company effort model. Our single-company data set had data on 15 Web projects from a single company and our cross-company data set had data on 68 Web projects from 25 different companies. The effort estimates used in our analysis were obtained by means of two effort estimation techniques, namely forward stepwise regression and case-based reasoning. Our results were similar to those from the replicated study, showing that predictions based on the single-company model were significantly more accurate than those based on the cross-company model.",Case-based reasoning; Cost estimation; Cross-company effort model; Effort estimation; Single-company effort model; Stepwise regression; Web applications; Web projects,"Mendes E., Di Martino S., Ferrucci F., Gravino C.",2007,Conference,"16th International World Wide Web Conference, WWW2007",10.1145/1242572.1242702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348880970&doi=10.1145%2f1242572.1242702&partnerID=40&md5=73153adedc6d20199f1341f89fb41d4c,"University of Auckland, Private Bag 92019, Auckland, New Zealand; Università di Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy",,English,,1595936548; 9781595936547
Scopus,Cost estimation of a software product using COCOMO II.2000 model - A case study,"This paper discusses the estimation cost in terms of effort spent on a software product (project). COCOMO II.2000 Model has been employed for estimating the effort of an embedded system project. This study has been made in a software services company, which is involved in software development for an embedded system, client-server and Internet environment. The embedded systems group is involved in developing software for major car manufacturers. This study is based on a sample of ten projects, of which eight are development projects and two are porting projects. The actual effort on the projects has been collected from the metrics database of the company. The Lines of code for the various projects have been enumerated using ""Code Count"" tool to achieve the logical source lines of code for each project. The standard questionnaire has been used to collect the required data to arrive at the various scale factors and effort multipliers. The calibration of the COCOMO (Constructive Cost Model) has been done through Natural Log approach and curve fitting approach. Statistical tools like MS-EXCEL 2000, SPSSv10, Curve Expert 1.3 and data fit have been used for this purpose. The study shows that the curve fitting approach yields better estimates of the model parameters. The calibration of COCOMO Model helps the company estimate the effort that is to be spent on the software development projects. © 2004 Elsevier Ltd and IPMA. All rights reserved.",Calibration; Embedded systems; Scale factors and effort multipliers,"Dillibabu R., Krishnaiah K.",2005,Journal,International Journal of Project Management,10.1016/j.ijproman.2004.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18144368846&doi=10.1016%2fj.ijproman.2004.11.003&partnerID=40&md5=79f42ba429b6484e6e9f931546df848f,"Department of Industrial Engineering, College of Engineering, Anna University, Guindy, Chennai 600025 Tamilnadu, India",,English,02637863,
Scopus,Expert estimation of web-development projects: Are software professionals in technical roles more optimistic than those in non-technical roles?,"Estimating the effort required to complete web-development projects involves input from people in both technical (e.g., programming), and non-technical (e.g., user interaction design) roles. This paper examines how the employees' role and type of competence may affect their estimation strategy and performance. An analysis of actual web-development project data and results from an experiment suggest that people with technical competence provided less realistic project effort estimates than those with less technical competence. This means that more knowledge about how to implement a requirement specification does not always lead to better estimation performance. We discuss, amongst others, two possible reasons for this observation: (1) Technical competence induces a bottom-up, construction-based estimation strategy, while lack of this competence induces a more ""outside"" view of the project, using a top-down estimation strategy. An ""outside"" view may encourage greater use of the history of previous projects and reduce the bias towards over-optimism. (2) Software professionals in technical roles perceive that they are evaluated as more skilled when providing low effort estimates. A consequence of our findings is that the choice of estimation strategy, estimation evaluation criteria and feedback are important aspects to consider when seeking to improve estimation accuracy. © 2005 Springer Science + Business Media, Inc.",Bidding process; Effort estimation; Individual differences; Web development,"Moløkken-Østvold K., Jørgensen M.",2005,Journal,Empirical Software Engineering,10.1023/B:EMSE.0000048321.46871.2e,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21244451010&doi=10.1023%2fB%3aEMSE.0000048321.46871.2e&partnerID=40&md5=24955600a4db3444194d2d4f9ec23a17,"Software Engineering Department, Simula Research Laboratory, 1325 Lysaker, Norway",,English,13823256,
Scopus,Regression models of software development effort estimation accuracy and bias,"This paper describes models whose purpose is to explain the accuracy and bias variation of an organization's estimates of software development effort through regression analysis. We collected information about variables that we believed would affect the accuracy or bias of estimates of the performance of tasks completed by the organization. In total, information about 49 software development tasks was collected. We found that the following conditions led to inaccuracies in estimates: (1) Estimates were provided by a person in the role of ""software developer"" instead of ""project leader"", (2) The project had as its highest priority time-to-delivery instead of quality or cost, and (3) The estimator did not participate in the completion of the task. The following conditions led to an increased bias towards under-estimation: (1) Estimates were provided by a person with the role of ""project leader"" instead of ""software developer"". (2) The estimator assessed the accuracy of own estimates of similar, previously completed tasks to be low (more than 20% error). Although all variables included in the models were significant (p < 0.1), the explanatory and predictive power of both models was poor, that is, most of the variance in the accuracy and bias of estimates was not explained or predicted by our models. In addition, there were several important threats to the validity of the coefficients suggested by the models. An analysis of the estimators' own descriptions of the reasons for achieved estimation accuracy on each task suggests that it will be difficult to include all important estimation accuracy and bias factors in regression-based models. It is, for this reason, not realistic to expect such models to replace human judgment in estimation uncertainty assessments and as input to plans for the improvement of estimates. It is, nevertheless, possible that the type of formal analysis and regression-based models presented in this paper may, in some cases, be useful as support for human judgment.",,Jørgensen M.,2004,Journal,Empirical Software Engineering,10.1023/B:EMSE.0000039881.57613.cb,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444261102&doi=10.1023%2fB%3aEMSE.0000039881.57613.cb&partnerID=40&md5=65ceb990aa67944c133c4afdf39d4338,"Simula Research Laboratory, Norway",,English,13823256,
Scopus,Evaluating alternative software production functions,"Software development projects are notorious for cost overruns and schedule delays. While dozens of software cost models have been proposed, few of them seem to have any degree of consistent accuracy. One major factor contributing to this persistent and wide spread problem is an inadequate understanding of the real behavior of software development processes. We believe that software development could be studied as an economic production process and that established economic theories and methods could be used to develop and validate software production and cost models. Here, we present the results of evaluating four alternative software production models using the P-test, a statistical procedure developed specifically for testing the truth of a hypothesis in the presence of alternatives in econometric studies. We found that the truth of the widely used Cobb-Douglas type of software production and cost models (e.g., COCOMO) cannot be maintained in the presence of quadratic or translog models. Overall, the quadratic software production function is shown to be the most plausible model for representing software production processes. Limitations of this study and future directions are also discussed. © 1997 IEEE.",Production functions; Software cost models; Software development; Software engineering economics; Software process modeling,Hu Q.,1997,Journal,IEEE Transactions on Software Engineering,10.1109/32.601078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031170731&doi=10.1109%2f32.601078&partnerID=40&md5=432142f6981f60298e00f51d0fa43a01,"Department of Decision and Information Systems, Florida Atlantic University, Ft. Lauderdale, FL 33301, United States",,English,00985589,
Scopus,Research patterns and trends in software effort estimation,"Context Software effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. Objective To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. Method A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE. Results As many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. Conclusions This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas. © 2017 Elsevier B.V.",Latent Dirichlet allocation; Research trends; Software effort estimation,"Sehra S.K., Brar Y.S., Kaur N., Sehra S.S.",2017,Review,Information and Software Technology,10.1016/j.infsof.2017.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020853907&doi=10.1016%2fj.infsof.2017.06.002&partnerID=40&md5=d7bb2515ad81a0037a63628ac8372db5,"I.K.G. Punjab Technical University, Jalandhar, Punjab, India; Guru Nanak Dev Engineering College, Ludhiana, Punjab, India; Sri Guru Granth Sahib World University, Fatehgarh Sahib, Punjab, India",Elsevier B.V.,English,09505849,
Scopus,Search-based refactoring: Metrics are not enough,"Search-based Software Engineering (SBSE) techniques have been applied extensively to refactor software, often based on metrics that describe the object-oriented structure of an application. Recent work shows that in some cases applying popular SBSE tools to open-source software does not necessarily lead to an improved version of the software as assessed by some subjective criteria. Through a survey of professionals, we investigate the relationship between popular SBSE refactoring metrics and the subjective opinions of software engineers. We find little or no correlation between the two. Through qualitative analysis, we find that a simple static view of software is insufficient to assess software quality, and that software quality is dependent on factors that are not amenable to measurement via metrics. We recommend that future SBSE refactoring research should incorporate information about the dynamic behaviour of software, and conclude that a human-in-the-loop approach may be the only way to refactor software in a manner helpful to an engineer. © Springer International Publishing Switzerland 2015.",Metrics; Optimisation; Search-based software engineering; Software quality,"Simons C., Singer J., White D.R.",2015,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-22183-0_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951271742&doi=10.1007%2f978-3-319-22183-0_4&partnerID=40&md5=5c4fa4a98e910b9309d5aafba6a4edbb,"Department of Computer Science and Creative Technologies, University of West England, Bristol, BS16 1QY, United Kingdom; School of Computing Science, University of Glasgow, Glasgow, G12 8RZ, United Kingdom",Springer Verlag,English,03029743,9783319221823
Scopus,Can cross-company data improve performance in software effort estimation?,"Background: There has been a long debate in the software engineering literature concerning how useful cross-company (CC) data are for software effort estimation (SEE) in comparison to within-company (WC) data. Studies indicate that models trained on CC data obtain either similar or worse performance than models trained solely on WC data. Aims: We aim at investigating if CC data could help to increase performance and under what conditions. Method: The work concentrates on the fact that SEE is a class of online learning tasks which operate in changing environments, even though most work so far has neglected that. We conduct an analysis based on the performance of different approaches considering CC and WC data. These are: (1) an approach not designed for changing environments, (2) approaches designed for changing environments and (3) a new online learning approach able to identify when CC data are helpful or detrimental. Results: Interesting features of data sets commonly used in the SEE literature are revealed, showing that different subsets of CC data can be beneficial or detrimental depending on the moment in time. The newly proposed approach is able to benefit from that, successfully using CC data to improve performance over WC models. Conclusions: This work not only shows that CC data can help to increase performance for SEE tasks, but also demonstrates that the online nature of software prediction tasks should be exploited, being an important issue to be considered in the future. Copyright © 2012 ACM.",Chronological split; Concept drift; Cross-company estimation models; Ensembles of learning machines; Online learning; Software effort estimation,"Minku L.L., Yao X.",2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365334,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867704010&doi=10.1145%2f2365324.2365334&partnerID=40&md5=b1f0ecc576939b2104e68f800990c6df,"CERCIA, School of Computer Science, University of Birmingham Edgbaston, Birmingham B15 2TT, United Kingdom",,English,,9781450312417
Scopus,A review of productivity factors and strategies on software development,"Since the late seventies, efforts to catalog factors that influences productivity, as well as actions to improve it, has been a huge concern for both academy and software development industry. Despite numerous studies, software organizations still do not know which the most significant factors are and what to do with it. Several studies present the factors in a very superficial way, some others address only the related factors or there are those that describe only a single factor. Actions to deal with the factors are spread and frequently were not mapped. Through a literature review, this paper presents a consolidated view of the main factors that have affected productivity over the years, and the strategies to deal with these factors nowadays. This research aims to support software development industry on the selection of their strategies to improve productivity by maximizing the positive factors and minimizing or avoiding the impact of the negative ones. © 2010 IEEE.",Productivity factors; Software engineering and productivity improvement; Strategies,"De Barros Sampaio S.C., Barros E.A., De Aquino Jr. G.S., Carlos E Silva M.J., De Lemos Meira S.R.",2010,Conference,"Proceedings - 5th International Conference on Software Engineering Advances, ICSEA 2010",10.1109/ICSEA.2010.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649892581&doi=10.1109%2fICSEA.2010.37&partnerID=40&md5=e2c33f7d592ccc58d2b6d6e5e31ed905,"CIN - Informatics Center, Federal University of Pernambuco - UFPE, Recife-PE, Brazil; CESAR - Recife Center of Advanced Studies and Systems, Recife-PE, Brazil",,English,,9780769541440
Scopus,Exploring the effects of SourceForge.net coordination and communication tools on the efficiency of open source projects using data envelopment analysis,"In this paper we explore possible benefits of communication and coordination tools in open source projects using an efficiency score derived from data envelopment analysis (DEA) as dependent variable. DEA is a general non-parametric method for efficiency comparisons without asking the user to define any relations between different factors or a production function. The method can account for economies or diseconomies of scale, and is able to deal with multi-input, multi-output systems in which the factors have different scales. Using two different data sets, successful and random open source projects, retrieved from SourceForge.net, we analyze impacts on their efficiency from the usage of communication and coordination tools. The results were mixed with no clear positive effects being proven consistently: In the data set of successful projects, mostly negative influences were found. On the contrary, tool adoption showed positive relationships to efficiency in the random data set. This stresses the importance of development status as a moderating variable and might also hint at threshold values for tool benefits. In addition, adoption of tools outside the hosting platform may be more likely for successful projects. © 2008 Springer Science+Business Media, LLC.",Communication; Coordination; Data envelopment analysis; Efficiency; Open source software; Tools,Koch S.,2009,Journal,Empirical Software Engineering,10.1007/s10664-008-9086-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650963806&doi=10.1007%2fs10664-008-9086-4&partnerID=40&md5=437189e63d21bb465684c52ac47228c9,"Institute for Information Business, Vienna University of Economics and BA, Augasse 2-6, Vienna 1090, Austria",,English,13823256,
Scopus,Investigating the use of chronological splitting to compare software cross-company and single-company effort predictions: A replicated study,"CONTEXT: Three previous studies have investigated the use of chronological split to compare cross- to single-company effort predictions, where all used the ISBSG dataset release 10. Therefore there is a need for these studies to be replicated using different datasets such that the patterns previously observed can be compared and contrasted, and a better understanding with regard to the use of chronological splitting can be reached. OBJECTIVE: The aim of this study is to replicate [17] using the same chronological splitting; however a different database – the Finnish dataset. METHOD: Chronological splitting was compared with two forms of cross-validation. The chronological splitting used was the project-by-project chronological split, in which a validation set contains a single project, and a regression model is built from scratch using as training set the set of projects completed before the validation project’s start date. We used 201 single-company projects and 593 cross-company projects from the Finnish dataset. RESULTS: Single-company models presented significantly better prediction than cross-company models. Chronological splitting provided significantly worse accuracy than leave-one and leave-two out cross-validations when based on single-company data; and provided similar accuracy when based on cross-company data. CONCLUSIONS: Results did not seem promising when using project-by-project splitting; however in a real scenario companies that use their own data can only apply some sort of chronological splitting when obtaining effort estimates for their new projects. Therefore we urge the use of chronological splitting in effort estimation studies such that more realistic results can be provided to inform industry. © 2009 BCS Learning and Development Ltd. All rights reserved.",Chronological split; Cross-company estimation models; Effort estimation; Regression-based estimation models; Single-company estimation models; Software projects,"Mendes E., Lokan C.",2009,Conference,"13th International Conference on Evaluation and Assessment in Software Engineering, EASE 2009",10.14236/ewic/ease2009.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088074047&doi=10.14236%2fewic%2fease2009.2&partnerID=40&md5=3aa7d6601414a6d2b05cd8d8bae68133,"Computer Science Department, University of Auckland, Private Bag, Auckland, 92019, New Zealand; School of IT and EE, UNSW@ADFA, Canberra, ACT  2600, Australia",BCS Learning and Development Ltd.,English,,
Scopus,A new calibration for Function Point complexity weights,"Function Point (FP) is a useful software metric that was first proposed 25 years ago, since then, it has steadily evolved into a functional size metric consolidated in the well-accepted Standardized International Function Point Users Group (IFPUG) Counting Practices Manual - version 4.2. While software development industry has grown rapidly, the weight values assigned to count standard FP still remain same, which raise critical questions about the validity of the weight values. In this paper, we discuss the concepts of calibrating Function Point, whose aims are to estimate a more accurate software size that fits for specific software application, to reflect software industry trend, and to improve the cost estimation of software projects. A FP calibration model called Neuro-Fuzzy Function Point Calibration Model (NFFPCM) that integrates the learning ability from neural network and the ability to capture human knowledge from fuzzy logic is proposed. The empirical validation using International Software Benchmarking Standards Group (ISBSG) data repository release 8 shows a 22% accuracy improvement of mean magnitude relative error (MMRE) in software effort estimation after calibration. © 2007 Elsevier B.V. All rights reserved.",Function Point; Function point analysis; Fuzzy logic; Neural network; Software size estimation,"Xia W., Capretz L.F., Ho D., Ahmed F.",2008,Journal,Information and Software Technology,10.1016/j.infsof.2007.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42649134811&doi=10.1016%2fj.infsof.2007.07.004&partnerID=40&md5=219e8a23fe28c9d0da0f4981e214630a,"HSBC Bank Canada, IT Department, Vancouver, BC, Canada; University of Western Ontario, Department of Electrical and Computer Engineering, London, Ont., Canada; NFA Estimation Inc., London, Ont., Canada; United Arab Emirates University, College of Information Technology, Al-Ain, United Arab Emirates",,English,09505849,
Scopus,The relationship between customer collaboration and software project overruns,"Most agile projects rely heavily on good collaboration with the customer in order to achieve project goals and avoid overruns. However, the role of the customer in software projects is not fully understood. Often, successful projects are attributed to developer competence, while unsuccessful projects are attributed to customer incompetence. A study was conducted on eighteen of the latest projects of a software contractor. Quantitative project data was collected, and project managers interviewed, on several issues related to estimates, key project properties, and project outcome. It was found that in projects where collaboration was facilitated by daily communication between the contractor and the customer, they experienced a lesser magnitude of effort overruns. In addition, employing a contract that facilitates risk-sharing may also have a positive impact. © 2007 IEEE.",,"Moløkken-Østvold K., Furulund K.M.",2007,Conference,Proceedings - AGILE 2007,10.1109/AGILE.2007.57,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449123773&doi=10.1109%2fAGILE.2007.57&partnerID=40&md5=766e120b11f035c9705cdfd99c781dc3,"Simula Research Laboratory, P.O.Box 134, 1325 Lysaker, Norway; Department of Informatics, University of Oslo, P.O.Box 1080 Blindern, 0316 Oslo, Norway",,English,,0769528724; 9780769528724
Scopus,An empirical analysis of software productivity over time,"OBJECTIVE - the aim is to investigate how software project productivity has changed over time. Within this overall goal we also compare productivity between different business sectors and seek to identify major drivers. METHOD - we analysed a data set of more than 600 projects that have been collected from a number of Finnish companies since 1978. RESULTS - overall, we observed a quite pronounced improvement in productivity over the entire time period, though, this improvement is less marked since the 1990s. However, the trend is not smooth. We also observed productivity variability between company and business sector. CONCLUSIONS - whilst this data set is not a random sample so generalisation is somewhat problematic, we hope that it contributes to an overall body of knowledge about software productivity and thereby facilitates the construction of a bigger picture. © 2005 IEEE.",Empirical analysis; Project management; Projects; Software productivity; Trend analysis,"Premraj R., Kitchenham B., Shepperd M., Forselius P.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749058941&doi=10.1109%2fMETRICS.2005.8&partnerID=40&md5=469e0787bbddb1714bc6b0c8394578f9,"Bournemouth University, United Kingdom; National ICT, Australia; Brunel University, United Kingdom; STTF Oy, Finland; Keele University, United Kingdom",,English,15301435,0769523714; 9780769523712
Scopus,A multiple-case study of software effort estimation based on use case points,"Through industry collaboration we have experienced an increasing interest in software effort estimation based on use cases. We therefore investigated one promising method, the use case points method, which is inspired by function points analysis. Four companies developed equivalent functionality, but their development processes varied, ranging from a light, code-and-fix process with limited emphasis on code quality, to a heavy process with considerable emphasis on analysis, design and code quality. Our effort estimate, which was based on the use case points method, was close to the actual effort of the company with the lightest development process; the estimate was 413 hours while actual effort of the four companies ranged from 431 to 943 hours. These results show, that the use case points method needs modification to better handle effort related to the development process and the quality of the code. © 2005 IEEE.",,"Anda B., Benestad H.C., Hove S.E.",2005,Conference,"2005 International Symposium on Empirical Software Engineering, ISESE 2005",10.1109/ISESE.2005.1541849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749052907&doi=10.1109%2fISESE.2005.1541849&partnerID=40&md5=4b5098072d03a256beec10df6ec103ce,"Simula Research Laboratory, P.O. Box 134, NO-1325 Lysaker, Norway",,English,,0780395085; 9780780395084
Scopus,Further investigation into the use of CBR and stepwise regression to predict development effort for Web hypermedia applications,"To date studies using CBR for Web hypermedia effort prediction have not applied adaptation rules to adjust effort according to a given criterion. In addition, when applying n-fold cross-validation, their analysis has been limited to a maximum of three training sets, which according to recent studies, may lead to untrustworthy results. This paper has therefore two objectives. The first is to further investigate the use of CBR for Web hypermedia effort prediction by comparing the prediction accuracy of eight CBR techniques, of which three have previously been compared. The second objective is to compare the prediction accuracy of the best CBR technique against stepwise regression, using a twenty-fold cross-validation. All prediction accuracies were measured using Mean Magnitude of Relative Error (MMRE), Median Magnitude of Relative Error, Prediction at level 1 (1=25%), and boxplots of the residuals. One dataset was used in the estimation process and, according to all measures of prediction accuracy, stepwise regression showed the best prediction accuracy. © 2002 IEEE.",,"Mendes E., Mosley N.",2002,Conference,"ISESE 2002 - Proceedings, 2002 International Symposium on Empirical Software Engineering",10.1109/ISESE.2002.1166928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964691696&doi=10.1109%2fISESE.2002.1166928&partnerID=40&md5=f1324f1e88bddf7d5908752475aa60a4,"Dept. of Comput. Sci., Auckland Univ., New Zealand; MXM Technology, Auckland, New Zealand",Institute of Electrical and Electronics Engineers Inc.,English,,076951796X; 9780769517964
Scopus,An approach to modelling long-term growth trends in software systems,"Three related models of growth, consistent with the view that complexity constrains system growth, were fitted to empirical data relating to four software systems. Predictive accuracy of the models, measured in Mean Magnitude of Relative Error, ranges from approximately 2 to 17 percent of the quantitative data. The modelling approach emphasises simple models that, nevertheless, provide a basis for evolution planning and management tools. These and previous results suggest that it is meaningful to search for models of this kind, though the presence of discontinuities in the trends and the extent to which the latter are restricted to individual processes or specific domains need to be further investigated. The work presented here has been pursued as part of the FEAST/2 (Feedback, Evolution And Software Technology) project.",,"Lehman M.M., Ramil J.F., Sandler U.",2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956612679&doi=10.1109%2fICSM.2001.972735&partnerID=40&md5=8c632e38f43416b509e04226c68d1877,"Department of Computing, Imperial Coll. of Sci. Technol. Med., 180 Queen's Gate, London SW7 2BZ, United Kingdom; Department of Mathematics, Jerusalem College of Technology, P.O. Box 16031, Jerusalem 91160, Israel",,English,,
Scopus,A method for estimating maintenance cost in a software project: A case study,"Various research works indicate that the maintenance stage consumes most of the resources needed for a software project. Thus, this stage must be especially considered in productivity studies. Maintainability is the quality factor including all those software characteristics designed to make the product easier to maintain towards the end of achieving greater productivity in the maintenance stage. This paper proposes an empirical model for estimating maintenance cost based on this quality factor, as well as the method of using it. Finally, a practical case will be considered which reinforces the validity of this model. © 1997 by John Wiley & Sons, Ltd.",Case study; Cost estimation; Maintainability; Productivity; Software maintenance; Software metrics,"Granja-Alvarez J.C., Barranco-García M.J.",1997,Journal,Journal of Software Maintenance and Evolution,10.1002/(sici)1096-908x(199705)9:3<161::aid-smr148>3.0.co;2-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031142653&doi=10.1002%2f%28sici%291096-908x%28199705%299%3a3%3c161%3a%3aaid-smr148%3e3.0.co%3b2-8&partnerID=40&md5=11007e11ef3ca47d69f2eeadea62b601,"Department of Computer Science, University of Granada, Avda. de Andalucía 38, E18071 Granada, Spain; Microjisa Inc., Calle Pintor Manuel Angeles Ortiz, 2, Bajo, E23006 Jaén, Spain",John Wiley and Sons Ltd,English,1532060X,
Scopus,A review of existing models for project planning and estimation and the need for a new approach,"Although planning is a crucial part of the system development process, it is often neglected by project managers. The problem being addressed by this paper is that of inadequate models for planning the requirements capture and analysis stage (RCA) of a software development project. It is stressed that there is a need for a new model because the existing models give inaccurate, inconsistent or unreliable predictions. Additionally, they are based on either inappropriate variables or variables that can not be measured at the beginning of the development process. Finally, existing models do not support the planning of individual stages of the development process but only try to make predictions about the project development process as a whole. This paper examines existing models and provides evidence about their inadequacy and lack of accuracy, and then introduces a new model and presents the approach followed for its development. Copyright © 1996 Elsevier Science Ltd and IPMA.",Estimating; IS planning; IS project management; Software metrics,"Chatzoglou P.D., Macaulay L.A.",1996,Journal,International Journal of Project Management,10.1016/0263-7863(95)00074-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030165699&doi=10.1016%2f0263-7863%2895%2900074-7&partnerID=40&md5=60022d7fb0b100cdb9c79ceee26a9b0d,"Department of Computation, UMIST, P.O. Box 88, Manchester, M60 1QD, United Kingdom",Elsevier BV,English,02637863,
Scopus,A model for measuring information system size,"Management of the software development process requires a thorough understanding of the environment in which development takes place. Ability to estimate, plan, and manage resource consumption is limited by the central problem of determining the size of system specifications. To address this issue, a general strategy for measurement and evaluation of system development environments needs to be established. This article presents a research model that will help managers and researchers understand and establish the linkages between units of systems requirements specification, design, and source code. Initial validation of the model was performed by reverse engineering systems written in a fourth generation language from source code to design metrics. Results indicate that the model may provide reliable measures of system size in terms of both design metrics and lines of code.",Field study; FOCUS; Fourth generation languages; Models and principles; Reverse engineering; Software management; Software metrics; Tools and techniques,"Wrigley C.D., Dexter A.S.",1991,Journal,MIS Quarterly: Management Information Systems,10.2307/249386,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003122971&doi=10.2307%2f249386&partnerID=40&md5=5f33b3d8c2d8b7c542fe11c2076e150d,"Faculty of Management, McGill University, Montreal, Que. H3A 1G5, Canada; Faculty of Commerce and Business Administration, University of British Columbia, 2053 Main Mall, Vancouver, BC V6T 1Y8, Canada",Management Information Systems Research Center,English,02767783,
Scopus,What recommendation systems for software engineering recommend: A systematic literature review,"A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs, research gaps, and possible research directions. We evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more efficient and less expensive, lower developer's cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Context-adapted and proactive behavior could improve the acceptance of RSSE systems in practice. © 2015 Elsevier Inc. Allrights reserved.",Recommendation system for software engineering; Systematic literature review,"Gasparic M., Janes A.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2015.11.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962376126&doi=10.1016%2fj.jss.2015.11.036&partnerID=40&md5=949412aeaf85932acff3fbdef86ec0cc,"Free University of Bolzano, Italy",Elsevier Inc.,English,01641212,
Scopus,Potential and limitations of the ISBSG dataset in enhancing software engineering research: A mapping review,"Context The International Software Benchmarking Standards Group (ISBSG) maintains a software development repository with over 6000 software projects. This dataset makes it possible to estimate a project's size, effort, duration, and cost. Objective The aim of this study was to determine how and to what extent, ISBSG has been used by researchers from 2000, when the first papers were published, until June of 2012. Method A systematic mapping review was used as the research method, which was applied to over 129 papers obtained after the filtering process. Results The papers were published in 19 journals and 40 conferences. Thirty-five percent of the papers published between years 2000 and 2011 have received at least one citation in journals and only five papers have received six or more citations. Effort variable is the focus of 70.5% of the papers, 22.5% center their research in a variable different from effort and 7% do not consider any target variable. Additionally, in as many as 70.5% of papers, effort estimation is the research topic, followed by dataset properties (36.4%). The more frequent methods are Regression (61.2%), Machine Learning (35.7%), and Estimation by Analogy (22.5%). ISBSG is used as the only support in 55% of the papers while the remaining papers use complementary datasets. The ISBSG release 10 is used most frequently with 32 references. Finally, some benefits and drawbacks of the usage of ISBSG have been highlighted. Conclusion This work presents a snapshot of the existing usage of ISBSG in software development research. ISBSG offers a wealth of information regarding practices from a wide range of organizations, applications, and development types, which constitutes its main potential. However, a data preparation process is required before any analysis. Lastly, the potential of ISBSG to develop new research is also outlined. © 2014 Elsevier B.V. All rights reserved.",ISBSG; Research methods; Software cost prediction; Software effort estimation; Software engineering; Systematic mapping study,"Fernández-Diego M., González-Ladrón-De-Guevara F.",2014,Review,Information and Software Technology,10.1016/j.infsof.2014.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897099090&doi=10.1016%2fj.infsof.2014.01.003&partnerID=40&md5=363473f46f0e2b3a35d52378cf5a2c36,"Business and Economics Department, Universitat Politècnica de València, Camino de Vera, s/n, 46022 Valencia, Spain",Elsevier B.V.,English,09505849,
Scopus,Who are we doing Global Software Engineering research for?,"Twelve years ago a group of practitioners and researchers came together to try to solve problems relating specifically to Global Software Engineering (GSE) practice. This paper aims to assess whether the many hundreds of GSE research papers written over this period have had an impact on practice. We conducted semi-structured interviews with senior managers and project managers from ten companies, four of which are large multinationals (three in Fortune 100); four are medium sized enterprises, and two are small startups. GSE research is perceived as useful by industry with all participants stating that studying the subject would improve GSE performance; but all were unanimous in saying they did not read articles on GSE. Practitioners go to books, blogs, colleagues, forums, experience reports of 1-2 pages in length, or depend on their own experience to solve problems in GSE. Controversially, many didn't see GSE as separate from general project management. Practitioners don't want frameworks; they want patterns of context specific help. While dissemination techniques need to be improved, that is not sufficient. Experience-based advice is just as important. © 2013 IEEE.",Empirical research; Global software development; Global software engineering; Practitioner experience; Research dissemination; Theory and practice,"Beecham S., O'Leary P., Richardson I., Baker S., Noll J.",2013,Conference,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",10.1109/ICGSE.2013.14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887456304&doi=10.1109%2fICGSE.2013.14&partnerID=40&md5=6034aa9525ea31fdf5edcdd29f55a250,"Lero - the Irish Software Engineering Research Centre, University of Limerick, Ireland; Incaplex Limited Dublin, Ireland",IEEE Computer Society,English,,9780768550572
Scopus,A fuzzy logic model for predicting the development effort of short scale programs based upon two independent variables,"Fuzzy models have been recently used for estimating the development effort of software projects and this practice could start with short scale programs. In this paper, new and changed (N&C) as well as reused code were gathered from small programs developed by 74 programmers using practices of the Personal Software Process; these data were used as input for a fuzzy model for estimating the development effort. Accuracy of this fuzzy model was compared with the accuracy of a statistical regression model. Two samples of 163 and 68 programs were used for verifying and validating respectively the models; the comparison criterion was the Mean Magnitude of Error Relative to the estimate (MMER). In verification and validation stages, fuzzy model kept a MMER lower or equal than that regression model and an accuracies comparison of the models based on ANOVA, did not show a statistically significant difference amongst their means. This result suggests that fuzzy logic could be used for predicting the effort of small programs based upon these two kinds of lines of code. © 2010 Elsevier B.V. All rights reserved.",Fuzzy logic; Multiple linear regression; Personal Software Process; Software effort estimation; Software engineering education and training,Lopez-Martin C.,2011,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2009.12.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957927850&doi=10.1016%2fj.asoc.2009.12.034&partnerID=40&md5=d595cba42b60ec422ce9d47b6fa5b8cd,"Information Systems Department, CUCEA, Guadalajara University, P.O. Box 45100, Jalisco, Mexico",,English,15684946,
Scopus,Evaluating logistic regression models to estimate software project outcomes,"Context: Software has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome. Objective: In this research, we identify aspects which could influence project success, build prediction models based on the aspects using data collected from multiple companies, and then test their performance on data from a single organization. Method: A survey-based empirical investigation was used to examine variables and factors that contribute to project outcome. Variables that were highly correlated to project success were selected and the set of variables was reduced to three factors by using principal components analysis. A logistic regression model was built for both the set of variables and the set of factors, using heterogeneous data collected from two different countries and a variety of organizations. We tested these models by using a homogeneous hold-out dataset from one organization. We used the receiver operating characteristic (ROC) analysis to compare the performance of the variable and factor-based models when applied to the homogeneous dataset. Results: We found that using raw variables or factors in the logistic regression models did not make any significant difference in predictive capability. The prediction accuracy of these models is more balanced when the cut-off is set to the ratio of success to failures in the datasets used to build the models. We found that the raw variable and factor-based models predict significantly better than random chance. Conclusion: We conclude that an organization wishing to estimate whether a project will succeed or fail may use a model created from heterogeneous data derived from multiple organizations. © 2010 Elsevier B.V.",Classifier evaluation; Cross-company data; Project outcome; ROC analysis; Single-company data; Tailored cut-off,"Cerpa N., Bardeen M., Kitchenham B., Verner J.",2010,Journal,Information and Software Technology,10.1016/j.infsof.2010.03.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953683193&doi=10.1016%2fj.infsof.2010.03.011&partnerID=40&md5=842d722707b8ac9585d62c3b76c81043,"Facultad de Ingeniería, Universidad de Talca, Curicó, Chile; School of Computing and Mathematics, Keele University, Staffordshire ST5 5BG, United Kingdom; Computer Science and Engineering, University of New South Wales, Sydney, Australia",,English,09505849,
Scopus,A new perspective on data homogeneity in software cost estimation: A study in the embedded systems domain,"Cost estimation and effort allocation are the key challenges for successful project planning and management in software development. Therefore, both industry and the research community have been working on various models and techniques to accurately predict the cost of projects. Recently, researchers have started debating whether the prediction performance depends on the structure of data rather than the models used. In this article, we focus on a new aspect of data homogeneity, ""cross- versus within-application domain"", and investigate what kind of training data should be used for software cost estimation in the embedded systems domain. In addition, we try to find out the effect of training dataset size on the prediction performance. Based on our empirical results, we conclude that it is better to use cross-domain data for embedded software cost estimation and the optimum training data size depends on the method used. © Springer Science+Business Media, LLC 2009.",Application domain; Cost estimation; Data homogeneity; Embedded software; Machine learning,"Bakır A., Turhan B., Bener A.B.",2010,Journal,Software Quality Journal,10.1007/s11219-009-9081-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72649093790&doi=10.1007%2fs11219-009-9081-z&partnerID=40&md5=2ea4cabbd5c44fe30367afb5cf6503ed,"Department of Computer Engineering, Boǧaziçi University, 34342 Bebek, Istanbul, Turkey; Software Engineering Group, Institute for Information Technology, National Research Council of Canada, 1200 Montreal Road, Building M50, Ottawa, ON K1A0R6, Canada",Kluwer Academic Publishers,English,09639314,
Scopus,CQML scheme: A classification scheme for comprehensive quality model landscapes,"Managing quality during the development, operation, and maintenance of software(-intensive) systems and services is a challenging task. Although many organizations need to define, control, measure, and improve various quality aspects of their development artifacts and processes, nearly no guidance is available on how to select, adapt, define, combine, use, and evolve quality models. Catalogs of models as well as selection and tailoring processes are widely missing. A first step towards better support for selecting and adapting quality models can be seen in a classification of existing quality models, especially with respect to their suitability for different purposes and contexts. This article presents so-called comprehensive quality model landscapes (CQMLs), which provide such a classification scheme and help to get an overview of existing models and their relationships. The article focuses on the description and justification of essential concepts needed to define quality models and landscapes. In addition, the article describes typical usage scenarios, illustrates the concept with examples, and sketches open questions and future work. © 2009 IEEE.",Classification sheme; Quality assurance; Quality improvement; Software quality management,"Kläs M., Heidrich J., Münch J., Trendowicz A.",2009,Conference,Conference Proceedings of the EUROMICRO,10.1109/SEAA.2009.88,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549195616&doi=10.1109%2fSEAA.2009.88&partnerID=40&md5=8290aac7a566a3c4764b41f83f5dd4f8,"Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany",,English,10896503,9780769537849
Scopus,"Integrate the GM(1,1) and Verhulst models to predict Software stage effort","Background: Software effort prediction clearly plays a crucial role in software project management. Problem: In keeping with more dynamic approaches to software development, it is not sufficient to only predict the whole-project effort at an early stage. Rather, the project manager must also dynamically predict the effort of different stages or activities during the software development process. This can assist the project manager to reestimate effort and adjust the project plan, thus avoiding effort or schedule overruns. Method: This paper presents a method for software physical time stage-effort prediction basedongrey models GM(1,1) and Verhulst. This method establishes models dynamically according to particular types of stage-effort sequences, and can adapt to particular development methodologies automatically by using a novel grey feedback mechanism. Result: We evaluate the proposed method with a large-scale real-world software engineering dataset, and compare it with the linear regression method and the Kalman filter method, revealing that accuracy has been improved by at least 28% and 50%, respectively. Conclusion: The results indicate that the method can be effective and has considerable potential. We believe that stage predictions could be a useful complement to whole-project effort prediction methods. © 2009 IEEE.",Grey prediction; Software project management; Software project stage-effort prediction,"Wang Y., Song Q., MacDonell S., Shepperd M., Shen J.",2009,Journal,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",10.1109/TSMCC.2009.2020690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955085642&doi=10.1109%2fTSMCC.2009.2020690&partnerID=40&md5=d0621cd1ea3a07a6dff3b20babdc6261,"Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an 710049, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan 430072, China; School of Computing and Mathematical Sciences, Auckland University of Technology, Auckland 1142, New Zealand; School of IS, Computing and Maths, Brunel University, UB8 London, United Kingdom",,English,10946977,
Scopus,Software quality analysis by combining multiple projects and learners,"When building software quality models, the approach often consists of training data mining learners on a single fit dataset. Typically, this fit dataset contains software metrics collected during a past release of the software project that we want to predict the quality of. In order to improve the predictive accuracy of such quality models, it is common practice to combine the predictive results of multiple learners to take advantage of their respective biases. Although multi-learner classifiers have been proven to be successful in some cases, the improvement is not always significant because the information in the fit dataset sometimes can be insufficient. We present an innovative method to build software quality models using majority voting to combine the predictions of multiple learners induced on multiple training datasets. To our knowledge, no previous study in software quality has attempted to take advantage of multiple software project data repositories which are generally spread across the organization. In a large scale empirical study involving seven real-world datasets and seventeen learners, we show that, on average, combining the predictions of one learner trained on multiple datasets significantly improves the predictive performance compared to one learner induced on a single fit dataset. We also demonstrate empirically that combining multiple learners trained on a single training dataset does not significantly improve the average predictive accuracy compared to the use of a single learner induced on a single fit dataset. © 2008 Springer Science+Business Media, LLC.",Cost of misclassification; Data mining; Majority voting; Multiple learners; Multiple software metrics repositories; Software quality classification model,"Khoshgoftaar T.M., Rebours P., Seliya N.",2009,Journal,Software Quality Journal,10.1007/s11219-008-9058-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65049088406&doi=10.1007%2fs11219-008-9058-3&partnerID=40&md5=06eba0a38d1ff8b43ab27b16cb75fa0e,"Computer Science and Engineering, Florida Atlantic University, 777 Glades Road, Boca Raton, FL 33431, United States; Computer and Information Science, University of Michigan-Dearborn, 4901 Evergreen Road, Dearborn, MI 48128, United States",Kluwer Academic Publishers,English,09639314,
Scopus,Estimating software size with UML models,"Function points have been used for more than 25 years for estimating software size and building productivity models. Today, three methods to do it are accepted as ISO standards. The theory behind this type of measurement is more explicit but none of these methods have yet been fully automated. All of them still require the involvement of an expert in order to be used correctly. Why is it so difficult to implement those methods? In our opinion, the main reason lies in the fact that each method has its own vocabulary and its own way of modeling software. The research work presented here has been realised mainly through two doctoral theses, one trying to automate the measure from a UML perspective and the other to add an objective measure of complexity to a standard measure in COSMIC-FFP in order to reach a higher level of confidence with those measures. So far, it can be concluded that, from UML use-cases and Actor-Object sequence diagrams of a system application, the number of messages exchanged correspond to the number of function points according to the COSMIC-FFP method. Going farther and adding the number of conditions or decisions to be applied according to UML version 2.0 would add more precision taking into account the complexity of the processes. Copyright © 2008 ACM.",Function point; Productivity model; Software size estimation; UML sequence diagram; UML use case,"Levesque G., Bevo V., Cao T.D.",2008,Conference,ACM International Conference Proceeding Series,10.1145/1370256.1370268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049091734&doi=10.1145%2f1370256.1370268&partnerID=40&md5=03e5120c8d27045aab207438ac9a1c03,"University of Quebec at Montreal, Dept.of Computer Science, Station C.V., P.O. Box 8888, Montreal, Canada; University of Cantho, CICT, 1 Ly Tu Trong, Cantho, Viet Nam",,English,,9781605581019
Scopus,Impact of base functional component types on software functional size based effort estimation,"Software effort estimation is still a significant challenge for software management. Although Functional Size Measurement (FSM) methods have been standardized and have become widely used by the software organizations, the relationship between functional size and development effort still needs further investigation. Most of the studies focus on the project cost drivers and consider total software functional size as the primary input to estimation models. In this study, we investigate whether using the functional sizes of different functionality types, represented by the Base Functional Component (BFC) types; instead of using the total single size figure have a significant impact on estimation reliability. For the empirical study, we used the projects data in the International Software Benchmarking Standards Group (ISBSG) Release 10 dataset, which were sized by the COSMIC FSM method. © 2008 Springer-Verlag Berlin Heidelberg.",Base Functional Component; COSMIC; Effort Estimation; Functional Size Measurement; International Software Benchmarking Standards Group (ISBSG),"Buglione L., Gencel C.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249120718&doi=10.1007%2f978-3-540-69566-0_9&partnerID=40&md5=8ca93f544684028f85092864f0d812ef,"École de Technologie Supérieure (ETS) / Engineering.it; Department of Systems and Software Engineering, Blekinge Institute of Technology",,English,03029743,3540695648; 9783540695646
Scopus,Cost estimation techniques for Web projects,"Having realistic estimates of effort at an early stage in a Web project's life is vital to the successful management of resources. The principles of the prediction process are identifying the influencing factors, gathering past project data, generating an effort prediction model, and assessing the effectiveness of such prediction model. Cost Estimation Techniques for Web Projects provides a step-by-step methodology to improving cost estimation practices for Web projects. Utilizing such techniques as stepwise regression modeling, case-base reasoning, classification and regression trees, and expert opinion, this book is a powerful tool for scholars, researchers, and practitioners in the areas of Web development, Web engineering, project management, and software engineering. © 2008 by IGI Global. All rights reserved.",,Mendes E.,2007,Book,Cost Estimation Techniques for Web Projects,10.4018/978-1-59904-135-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898399777&doi=10.4018%2f978-1-59904-135-3&partnerID=40&md5=f70b91d1d96d03fbe2cb6bf6e6628f6b,"University of Auckland, New Zealand",IGI Global,English,,9781599041353
Scopus,A functional size measurement method for object-oriented conceptual schemas: Design and evaluation issues,"Functional Size Measurement (FSM) methods are intended to measure the size of software by quantifying the functional user requirements of the software. The capability to accurately quantify the size of software in an early stage of the development lifecycle is critical to software project managers for evaluating risks, developing project estimates and having early project indicators. In this paper, we present OO-Method Function Points (OOmFP), which is a new FSM method for object-oriented systems that is based on measuring conceptual schemas. OOmFP is presented following the steps of a process model for software measurement. Using this process model, we present the design of the measurement method, its application in a case study, and the analysis of different evaluation types that can be carried out to validate the method and to verify its application and results. © Springer-Verlag 2005.",Conceptual modeling; Functional size measurement; Measure validation; Measurement verification; Object orientation,"Abrahão S., Poels G., Pastor O.",2006,Journal,Software and Systems Modeling,10.1007/s10270-005-0098-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645223327&doi=10.1007%2fs10270-005-0098-x&partnerID=40&md5=3497e8bf6edf0e2bcfc0f633619bc40f,"Department of Computer Science and Computation, Valencia University of Technology, Camino de Vera s/n, 46022 Valencia, Spain; Faculty of Economics and Business Administration, Ghent University, Hoveniersberg 24, 9000 Gent, Belgium",,English,16191366,
Scopus,An empirical study of process-related attributes in segmented software cost-estimation relationships,"Parametric software effort estimation models consisting on a single mathematical relationship suffer from poor adjustment and predictive characteristics in cases in which the historical database considered contains data coming from projects of a heterogeneous nature. The segmentation of the input domain according to clusters obtained from the database of historical projects serves as a tool for more realistic models that use several local estimation relationships. Nonetheless, it may be hypothesized that using clustering algorithms without previous consideration of the influence of well-known project attributes misses the opportunity to obtain more realistic segments. In this paper, we describe the results of an empirical study using the ISBSG-8 database and the EM clustering algorithm that studies the influence of the consideration of two process-related attributes as drivers of the clustering process: the use of engineering methodologies and the use of CASE tools. The results provide evidence that such consideration conditions significantly the final model obtained, even though the resulting predictive quality is of a similar magnitude. © 2005 Elsevier Inc. All rights reserved.",Clustering algorithms; EM algorithm; Parametric software effort estimation; Software cost drivers,"Cuadrado-Gallego J.J., Sicilia M.-A., Garre M., Rodríguez D.",2006,Journal,Journal of Systems and Software,10.1016/j.jss.2005.04.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644665874&doi=10.1016%2fj.jss.2005.04.040&partnerID=40&md5=4417868e94382f82c59e2bec2a69c460,"Computer Science Department, Polytechnic School, University of Alcalá, Ctra. Barcelona km. 33.6, 28871 Alcala de Henares, Madrid, Spain; Computer Science Department, University of Reading, Reading RG6 6AY, United Kingdom",,English,01641212,
Scopus,Software source code sizing using fuzzy logic modeling,"Knowing the likely size of a software product before it has been constructed is potentially beneficial in project management: for instance, size can be an important factor in determining an appropriate development/integration schedule, and it can be a significant input in terms of the allocation of personnel and other resources. In this study we consider the applicability of fuzzy logic modeling methods to the task of software source code sizing, using a previously published data set. Our results suggest that, particularly with refinement using data and knowledge, fuzzy predictive models can outperform their traditional regression-based counterparts. © 2003 Elsevier Science B.V. All rights reserved.",Fuzzy logic; Prediction; SLOC; Software size,MacDonell S.G.,2003,Journal,Information and Software Technology,10.1016/S0950-5849(03)00011-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038219574&doi=10.1016%2fS0950-5849%2803%2900011-9&partnerID=40&md5=795bdc549d5f4ba626dc244a0010c342,"School of Information Technology, Auckland University of Technology, Private Bag 92006, Auckland, New Zealand",Elsevier,English,09505849,
Scopus,Field studies using functional size measurement in building estimation models for software maintenance,"Even though a significant number of estimation models have been proposed for development projects, few have been proposed for software maintenance. This paper reports on two field studies carried out on the use of functional size measures in building estimation models for sets of maintenance projects implementing small functional enhancements in existing software. The first field-study reports on models built with 15 projects making functional enhancements to an internet-based software program for linguistic applications. The second field study analyses 19 maintenance projects on a single real-time embedded software program in the defense industry. Both field studies collected functional size measures using version 2.0 of the COSMIC-FFP functional size measurement method. Also both field studies classified projects into two classes of project difficulty in order to aid identifying subsets of projects with greater homogeneity in the relationship of project effort to functional size. This paper is the first published paper reporting on the use this second generation of functional size-measurement methods in a maintenance-estimation context. Copyright © 2002 John Wiley & Sons, Ltd.",COSMIC-FFP; Estimating projects; Maintenance effort; Project effort; Project size; Software size,"Abran A., Silva I., Primera L.",2002,Journal,Journal of Software Maintenance and Evolution,10.1002/smr.245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036298665&doi=10.1002%2fsmr.245&partnerID=40&md5=831f17059a9b3b62e3cf6cc42bb4b8a9,"́cole de Technologie Suṕrieure, Montreal, Que., Canada; ́cole de Technologie Suṕrieure, 1100 Notre-Dame West St, Montreal, Que., Canada",,English,1532060X,
Scopus,Measuring functionality and productivity in Web-based applications: A case study,"In this paper we explore the variation of the cost of writing code when object-oriented framework based development of web applications is encountered for the first time. Managers need such information to justify their investments in innovative development strategies. Size measurements are essential in this task and a number of metrics, namely Lines of Code, classical Function Points and Object-Oriented Function Points, are employed. It is argued that lines of code and object-oriented function points are more suitable in this case. Data analysis reveals that learning influences mainly the cost of writing new code, consisting of continuous calls to components provided by the framework. We also explore the applicability of an already proposed effort prediction model that is based on different reuse types. A cost estimation model is the by-product of this study, providing a helpful tool for managing the first projects in which the framework is employed.",,"Morisio Maurizio, Stamelos Ioannis, Spahos Vasilis, Romano Daniele",1999,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033355544&partnerID=40&md5=8e91e7c39f59b3b78369b0778c6fff57,"Univ of Maryland, College Park, United States","IEEE, Los Alamitos, CA, United States",English,,
Scopus,Software quality assurance: An analytical survey and research prioritization,"We present an overview of the Software Quality Assurance (SQA) research domain. An extensive review of the literature was conducted to identify areas that are being currently investigated or have received attention from the research community. Articles appearing in outlets appropriate for software and information engineering were considered. Our categorization scheme includes four key dimensions: technical, managerial, organizational, and economic. These primary dimensions were deduced from the literature, and sub-dimensions were induced to lead to a finer categorization scheme. We present a summary of the content and methodological orientation of present research. In its present state, the SQA domain has largely drawn upon principles and theories from other reference areas but their integration with the actual task and technology context is still in a rudimentary stage. This is especially true for research dealing with organizational and managerial issues. Further, there is limited examination of related thematic issues across the technical, managerial, organizational and economic strands. We conclude by recommending directions for future research. © 1998 Elsevier Science Inc.",,"Rai A., Song H., Troutt M.",1998,Short Survey,Journal of Systems and Software,10.1016/S0164-1212(97)00146-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031649797&doi=10.1016%2fS0164-1212%2897%2900146-5&partnerID=40&md5=eb126ae1ac18c666399886cd29866d1a,"Department of Decision Sciences, College of Business Administration, Georgia State University, Atlanta, GA, United States; Software Quality Department, San Francisco, CA, United States; Department of Management, 214 Rehn Hall, S. Illinois University at Carbondale, Carbondale, IL, United States; Georgia State University, College of Business Administration, Department of Decision Sciences, Atlanta, GA 30303, United States",Elsevier Inc.,English,01641212,
Scopus,An effective approach for software project effort and duration estimation with machine learning algorithms,"During the last two decades, there has been substantial research performed in the field of software estimation using machine learning algorithms that aimed to tackle deficiencies of traditional and parametric estimation techniques, increase project success rates and align with modern development and project management approaches. Nevertheless, mostly due to inconclusive results and vague model building approaches, there are few or none deployments in practice. The purpose of this article is to narrow the gap between up-to-date research results and implementations within organisations by proposing effective and practical machine learning deployment and maintenance approaches by utilization of research findings and industry best practices. This was achieved by applying ISBSG dataset, smart data preparation, an ensemble averaging of three machine learning algorithms (Support Vector Machines, Neural Networks and Generalized Linear Models) and cross validation. The obtained models for effort and duration estimation are intended to provide a decision support tool for organisations that develop or implement software systems. © 2017 Elsevier Inc.",Effort and duration estimation; Ensemble models; ISBSG; Machine learning; Software project estimation,"Pospieszny P., Czarnacka-Chrobot B., Kobylinski A.",2018,Journal,Journal of Systems and Software,10.1016/j.jss.2017.11.066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037534472&doi=10.1016%2fj.jss.2017.11.066&partnerID=40&md5=9db2ffd867c320e60b56603483a60755,"Institute of Information Systems and Digital Economy, Warsaw School of Economics, Warsaw, Poland",Elsevier Inc.,English,01641212,
Scopus,Logic pluralism in mobile platform ecosystems: A study of indie app developers on the iOS App Store,"The software development field has inherently been identified with two fieldlevel institutional logics, i.e., logic of the profession and logic of the markets. Traditionally, information systems development methodologies have been used to reconcile the competing demands from these two logics. In this paper, we study how these two logics manifest in a platform-based software ecosystem where significant entrepreneurial opportunities are created for independent third-party app developers (indies). Specifically,we study how indie developers manage the two logics on the iOS platformecosystem under the influence of the platformowner Apple.We first identify practices of indie developers consistent with the two field logics across three entrepreneurial domains, i.e., app ideation, execution, and marketing. Second, we identify areas where the two field logics may be in conflict as well as in coexistence.We showthat indie developers enact logic conflict through disagreement and denunciation of the opposing logic. When logics that are inherently opposed appear to coexist, we investigate how app developers manage these opposing demands through a process we call logic synthesis. Using a grounded theory approach, we propose a model of field-level market and professional logics operating in the mobile app platform ecosystem and the practices reflecting such logics in the indie developer community. Our work contributes to the growing literature on platform ecosystems and the processes adopted by third-party app developers in such ecosystems, in addition to furthering the study of institutional logics in technology contexts. © 2017 INFORMS.",Grounded theory; Independent software developers; Institutional logics; IOS App Store; Platform ecosystems; Qualitative research,"Qiu Y., Gopal A., Hann I.-H.",2017,Journal,Information Systems Research,10.1287/isre.2016.0664,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021162196&doi=10.1287%2fisre.2016.0664&partnerID=40&md5=c4f4fa236f87a76e7319e896296cd4f7,"Health Integrity LLC, Easton, MD  21601, United States; Robert H. Smith School of Business, University of Maryland, College Park, MD  20742, United States",INFORMS Inst.for Operations Res.and the Management Sciences,English,10477047,
Scopus,Dynamic Software Project Scheduling through a Proactive-Rescheduling Method,"Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development. Yet most studies schedule software projects by considering static and deterministic scenarios only, which may cause performance deterioration or even infeasibility when facing disruptions. In order to capture more dynamic features of software project scheduling than the previous work, this paper formulates the project scheduling problem by considering uncertainties and dynamic events that often occur during software project development, and constructs a mathematical model for the resulting multi-objective dynamic project scheduling problem (MODPSP), where the four objectives of project cost, duration, robustness and stability are considered simultaneously under a variety of practical constraints. In order to solve MODPSP appropriately, a multi-objective evolutionary algorithm based proactive-rescheduling method is proposed, which generates a robust schedule predictively and adapts the previous schedule in response to critical dynamic events during the project execution. Extensive experimental results on 21 problem instances, including three instances derived from real-world software projects, show that our novel method is very effective. By introducing the robustness and stability objectives, and incorporating the dynamic optimization strategies specifically designed for MODPSP, our proactive-rescheduling method achieves a very good overall performance in a dynamic environment. © 2016 IEEE.",dynamic software project scheduling; mathematical modeling; multi-objective evolutionary algorithms; Schedule and organizational issues; search-based software engineering,"Shen X., Minku L.L., Bahsoon R., Yao X.",2016,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2015.2512266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979243475&doi=10.1109%2fTSE.2015.2512266&partnerID=40&md5=b88110846c55069cba9ee7d2e0f9bb58,"B-DAT, CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, 210044, China; Department of Computer Science, University of Leicester, University Road, Leicester, LE17RH, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, B15 2TT, United Kingdom",Institute of Electrical and Electronics Engineers Inc.,English,00985589,
Scopus,From Aristotle to Ringelmann: a large-scale analysis of team productivity and coordination in Open Source Software projects,"Complex software development projects rely on the contribution of teams of developers, who are required to collaborate and coordinate their efforts. The productivity of such development teams, i.e., how their size is related to the produced output, is an important consideration for project and schedule management as well as for cost estimation. The majority of studies in empirical software engineering suggest that - due to coordination overhead - teams of collaborating developers become less productive as they grow in size. This phenomenon is commonly paraphrased as Brooks’ law of software project management, which states that “adding manpower to a software project makes it later”. Outside software engineering, the non-additive scaling of productivity in teams is often referred to as the Ringelmann effect, which is studied extensively in social psychology and organizational theory. Conversely, a recent study suggested that in Open Source Software (OSS) projects, the productivity of developers increases as the team grows in size. Attributing it to collective synergetic effects, this surprising finding was linked to the Aristotelian quote that “the whole is more than the sum of its parts”. Using a data set of 58 OSS projects with more than 580,000 commits contributed by more than 30,000 developers, in this article we provide a large-scale analysis of the relation between size and productivity of software development teams. Our findings confirm the negative relation between team size and productivity previously suggested by empirical software engineering research, thus providing quantitative evidence for the presence of a strong Ringelmann effect. Using fine-grained data on the association between developers and source code files, we investigate possible explanations for the observed relations between team size and productivity. In particular, we take a network perspective on developer-code associations in software development teams and show that the magnitude of the decrease in productivity is likely to be related to the growth dynamics of co-editing networks which can be interpreted as a first-order approximation of coordination requirements. © 2015, Springer Science+Business Media New York.",Coordination; Open source software; Productivity factors; Repository mining; Social aspects of software engineering; Software engineering,"Scholtes I., Mavrodiev P., Schweitzer F.",2016,Journal,Empirical Software Engineering,10.1007/s10664-015-9406-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950257604&doi=10.1007%2fs10664-015-9406-4&partnerID=40&md5=2b9f96df7857068da67b680731155cf5,"ETH Zürich, Chair of Systems Design, Weinbergstrasse 56/58, Zurich, 8092, Switzerland",Springer New York LLC,English,13823256,
Scopus,"The Impacts of Test Automation on Software's Cost, Quality and Time to Market","In spite of the availability of most proficient quality assurance teams and tools, software testing has always been a time-consuming task. Thus test automation is being profoundly practiced in most of the software industries to leverage the total development time. Although the test automation has its own advantages and disadvantages and it influences various other development phases, the higher management is particularly interested in reckoning its effects on total software's cost, quality and time. In this paper, we have tried to ascertain some of the critical factors related to test automation and cost/return of/from automation. As automation is itself a pricy activity, it requires development effort and significant time, we have attempted to enumerate test automation's impacts on software's cost, time and quality on three different softwares. The results of our experiments clearly show the positive effects of test automation on cost, quality and time to market of the software. © 2016 The Authors.",Quality Factors; Return on Investment; Software Development Cost; Software's Time to Market; Test Automation,"Kumar D., Mishra K.K.",2016,Conference,Procedia Computer Science,10.1016/j.procs.2016.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964813779&doi=10.1016%2fj.procs.2016.03.003&partnerID=40&md5=fcb8031eed3592b751b63b5cfbd28e5b,"Motilal Nehru National Institute of Technology Allahabad, Allahabad, 244001, India",Elsevier B.V.,English,18770509,
Scopus,How does project size affect cost estimation error? Statistical artifacts and methodological challenges,"Empirical studies differ in what they report as the underlying relation between project size and percent cost overrun. As a consequence, the studies also differ in their project management recommendations. We show that studies with a project size measure based on the actual cost systematically report an increase in percent cost overrun with increased project size, whereas studies with a project size measure based on the estimated cost report a decrease or no change in percent cost overrun with increased project size. The observed pattern is, we argue, to some extent a statistical artifact caused by imperfect correlation between the estimated and the actual cost. We conclude that the previous observational studies cannot be considered to provide reliable evidence in favor of an underlying project size related cost estimation bias. We discuss the limited, statistically robust evidence and the need for other types of studies. © 2012 Elsevier Ltd.",Cost estimation; Magnitude bias; Non-random samples; Random error; Regression analysis; Statistical artifacts,"Jørgensen M., Halkjelsvik T., Kitchenham B.",2012,Journal,International Journal of Project Management,10.1016/j.ijproman.2012.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865400212&doi=10.1016%2fj.ijproman.2012.01.007&partnerID=40&md5=b13a3b953c2ed282e407ea5b04580ea8,"Simula Research Laboratory, P.O. Box 134, Lysaker, Norway; University of Oslo, Norway; University of Keele, England, United Kingdom",,English,02637863,
Scopus,Single and multi objective genetic programming for software development effort estimation,"The idea of exploiting Genetic Programming (GP) to estimate software development effort is based on the observation that the effort estimation problem can be formulated as an optimization problem. Indeed, among the possible models, we have to identify the one providing the most accurate estimates. To this end a suitable measure to evaluate and compare different models is needed. However, in the context of effort estimation there does not exist a unique measure that allows us to compare different models but several different criteria (e.g., MMRE, Pred(25), MdMRE) have been proposed. Aiming at getting an insight on the effects of using different measures as fitness function, in this paper we analyzed the performance of GP using each of the five most used evaluation criteria. Moreover, we designed a Multi-Objective Genetic Programming (MOGP) based on Pareto optimality to simultaneously optimize the five evaluation measures and analyzed whether MOGP is able to build estimation models more accurate than those obtained using GP. The results of the empirical analysis, carried out using three publicly available datasets, showed that the choice of the fitness function significantly affects the estimation accuracy of the models built with GP and the use of some fitness functions allowed GP to get estimation accuracy comparable with the ones provided by MOGP. © 2012 ACM.",effort estimation; empirical study; genetic programming; multi objective search,"Sarro F., Ferrucci F., Gravino C.",2012,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/2245276.2231968,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863575220&doi=10.1145%2f2245276.2231968&partnerID=40&md5=8f053e40832243a00e32556e32ac0175,"University of Salerno, Via Ponte Don Melillo, 84084 Fisciano (SA), Italy",,English,,9781450308571
Scopus,An empirical study of the Cobb-Douglas production function properties of software development effort,In this paper we study whether software development effort exhibits Cobb-Douglas functional form with respect to team size and software size. We empirically test this relationship using real-world software engineering data set containing over 500 software projects. The results of our experiments indicate that the hypothesized Cobb-Douglas function form for software development effort with respect to team size and software size is true. We also find increasing returns to scale relationship between software size and team size with software development effort. © 2007 Elsevier B.V. All rights reserved.,Cost models; Forecasting; Project management,"Pendharkar P.C., Rodger J.A., Subramanian G.H.",2008,Journal,Information and Software Technology,10.1016/j.infsof.2007.10.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449121471&doi=10.1016%2fj.infsof.2007.10.019&partnerID=40&md5=680c135bf58e08ef31c88aecaebb7d1c,"Department of Information Systems, School of Business Administration, Penn State Harrisburg, 777 West Harrisburg Pike, Middletown, PA 17057, United States; Department of MIS and Decision Sciences, Eberly College of Business and Information Technology, Indiana University of Pennsylvania, Indiana, PA 15705, United States",,English,09505849,
Scopus,A neuro-fuzzy model for function point calibration,"The need to update the calibration of Function Point (FP) complexity weights is discussed, whose aims are to fit specific software appliqation, to reflect software industry trend, and to improve cost estimation. Neuro-Fuzzy is a technique that incorporates the learning ability from neural network and the ability to capture human knowledge from fuzzy logic. The empirical validation using ISBSG data repository Release 8 shows a 22% improvement in software effort estimation after calibration using Neuro-Fuzzy technique.",Fuzzy logic; Neural networks; Neuro-fuzzy; Software cost estimation,"Xia W., Capretz L.F., Ho D.",2008,Journal,WSEAS Transactions on Information Science and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149133296&partnerID=40&md5=314844fbca846dfc7713c995ff6a3b94,"HSBC Bank Canada, IT Department, Vancouver, BC, Canada; University of Western Ontario, Dept. Electrical and Computer Engineering, London, ON, Canada; NFA Estimation Inc., London, ON, Canada",,English,17900832,
Scopus,Impact analysis of missing values on the prediction accuracy of analogy-based software effort estimation method AQUA,"Effort estimation by analogy (EBA) is often confronted with missing values. Our former analogy-based method AUQA is able to tolerate missing values in the data set, but it is unclear how the percentage of missing values impacts the prediction accuracy and if there is an upper bound for how big this percentage might become in order to guarantee the applicability of AQUA. This paper investigates these questions through an impact analysis. The impact analysis is conducted for seven data sets being of different size and having different initial percentages of missing values. The major results are that (i) we confirm the intuition that the more missing values, the poorer the prediction accuracy of AQUA; (ii) there is a quadratic dependency between the prediction accuracy and the percentage of missing values; and (Hi) the upper limit of missing values for the applicability of AQUA is determined as 40%. These results are obtained in the context of AQUA. Further analysis is necessary for other ways of applying EBA, such as using different similarity measures or analogy adaptation methods from those used in AQUA. For that purpose, the experimental design in this study can be adapted. © 2007 IEEE.",,"Li J., Al-Emran A., Ruhe G.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.59,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949104709&doi=10.1109%2fESEM.2007.59&partnerID=40&md5=5b4d09096379fc2b432ac8cc4cd3b158,"Software Engineering, Decision Support Laboratory, University of Calgary, Calgary AB, T2N 1N4, Canada",,English,,0769528864; 9780769528861
Scopus,Benchmarking k-nearest neighbour imputation with homogeneous Likert data,"Missing data are common in surveys regardless of research field, undermining statistical analyses and biasing results. One solution is to use an imputation method, which recovers missing data by estimating replacement values. Previously, we have evaluated the hot-deck k-Nearest Neighbour (k-NN) method with Likert data in a software engineering context. In this paper, we extend the evaluation by benchmarking the method against four other imputation methods: Random Draw Substitution, Random Imputation, Median Imputation and Mode Imputation. By simulating both non-response and imputation, we obtain comparable performance measures for all methods. We discuss the performance of k-NN in the light of the other methods, but also for different values of k, different proportions of missing data, different neighbour selection strategies and different numbers of data attributes. Our results show that the k-NN method performs well, even when much data are missing, but has strong competition from both Median Imputation and Mode Imputation for our particular data. However, unlike these methods, k-NN has better performance with more data attributes. We suggest that a suitable value of k is approximately the square root of the number of complete cases, and that letting certain incomplete cases qualify as neighbours boosts the imputation ability of the method.",Imputation; K-Nearest Neighbour; Likert data; Missing data,"Jönsson P., Wohlin C.",2006,Conference,Empirical Software Engineering,10.1007/s10664-006-9001-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746034603&doi=10.1007%2fs10664-006-9001-9&partnerID=40&md5=d27cd3e58995eda04ea3a268322390ac,"School of Engineering, Blekinge Institute of Technology, PO-Box 520, SE-372 25 Ronneby, Sweden",,English,13823256,
Scopus,A further empirical investigation of the relationship between MRE and project size,"The mean magnitude of relative error, MMRE, is the de facto standard evaluation criterion to assess the accuracy of software project prediction models. The fundamental metric of MMRE is MRE, a relative residual error. For MMRE to be a meaningful summary statistic, it is a necessary, but not sufficient, condition that MRE and project size are uncorrelated. Except for two previous conference studies done by the same authors, it has never been empirically validated that MRE and project size really are uncorrelated. In this paper, we extend the previous studies using the same data sets as before: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. Unlike the previous studies, we plot MRE against the predicted effort rather than against the actual effort and. in so doing, we obtain very different results from the previous studies. The results of this study suggest that MRE and project size are uncorrelated, which apparently is contradictory to the previous results where we found a negative correlation. The explanation for these seemingly contradictory results is presented in this study.",Empirical validation; Evaluation metrics; Magnitude of relative error; Project cost estimation; Software engineering,"Stensrud E., Foss T., Kitchenham B., Myrtveit I.",2003,Journal,Empirical Software Engineering,10.1023/A:1023010612345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038287473&doi=10.1023%2fA%3a1023010612345&partnerID=40&md5=987aa1858c9429bb2755d377d321d301,"Keele University, Keele, United Kingdom",,English,13823256,
Scopus,Investigating soft computing in case-based reasoning for software cost estimation,"Software cost estimation has been the subject of intensive investigations in the field of software engineering. As a result, numerous software cost estimation techniques have been proposed and investigated. To our knowledge, currently there are no cost estimation techniques that can incorporate and/or tolerate the aspects of imprecision, vagueness, and uncertainty into their predictions. However, software projects are often described by vague information. Furthermore, an estimate is only a probabilistic assessment of a future condition. Consequently, cost estimation models must be able to deal with imprecision and uncertainty, the two principal components of soft computing. To estimate the cost of software projects when they are described by vague and imprecise attributes, in an earlier study we have proposed an innovative approach referred to as Fuzzy Analogy. In this paper, we investigate the uncertainty of cost estimates generated by the Fuzzy Analogy approach. The primary aim is to generate a set of possible values for the actual software development cost. This set then be used to deduce, for practical purposes, a point estimate for the cost, and for analyzing the risks associated with all possible estimates.",Case-based reasoning; Soft computing; Software cost estimation,"Idri A., Khoshgoftaar T.M., Abran A.",2002,Journal,International Journal of Engineering Intelligent Systems for Electrical Engineering and Communications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036766970&partnerID=40&md5=864acaacd237b649cfef48cac335cc2b,"ENSIAS, University Mohamed V, BP. 713, Agdal, Rabat, Morocco; Empirical Software Engineering Lab., Florida Atlantic University, Boca Raton, FL, United States; Ecole de Technologie Superieure, Montreal, Que., 1100 Notre-dame, Canada",,English,09691170,
Scopus,A decisional framework for legacy system management,"Making a decision about how to evolve a legacy system cannot be made spontaneously; rather, it requires a decisional framework that takes into account several factors including software value, risk analysis, and cost estimation. We present a decisional framework to manage legacy systems that exploits an assessment model and a taxonomy of maintenance interventions a legacy system can undergo during its life-cycle. The decisional framework has been defined within a pilot project involving a major international software enterprise. The project aims at assessing and improving the current practices of the organization and at experimenting software maintenance processes conducted by teams distributed at different sites in a cooperative networking environment.",,"De Lucia A., Fasolino A.R., Pompella E.",2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956614556&doi=10.1109%2fICSM.2001.972781&partnerID=40&md5=944caa6539b7ba44219b7213372d1673,"Faculty of Engineering, University of Sannio, Palazzo Bosco Lucarelli, Piazza Roma, 82110 Benevento, Italy; Dept. of Informatica e Sistemistica, University of Naples Federico II, Via Claudio, 21, 80125 Naples, Italy; EDS Italia Software S.p.A, Viale Edison - Loc. Lo Uttaro, 81100 Caserta, Italy",,English,,
Scopus,Understanding and improving effort estimation in agile software development- An industrial case study,"Effort estimation is more challenging in an agile context, as instead of exerting strict control over changes in requirements, dynamism is embraced. Current practice relies on expert judgment, where the accuracy of estimates is sensitive to the expertise of practitioners and prone to bias. In order to improve the effectiveness of the effort estimation process, the goal of this research is to investigate and understand the estimation process with respect to its accuracy in the context of agile software development from the perspective of agile development teams. Using case study research, two observations and eleven interviews were conducted with three agile development teams at SAP SE, a German multinational software corporation. The results reveal that factors such as the developer's knowledge and experience and the complexity and impact of changes on the underlying system affect the magnitude as well as the accuracy of estimation. Moreover, if certain aspects of the estimation process, such as the potential impact of a change on the underlying system, are supported by a tool can help improve estimation accuracy. We conclude that explicit consideration of these factors in the estimation process can support experts in making accurate and informed estimates. Furthermore, there is a need for a tool that incorporates expert knowledge, enables explicit consideration of cost drivers by experts and visualizes this information in order to improve the effectiveness of the effort estimation process. © 2016 ACM.",Agile development; Change impact; Effort estimation,"Tanveer B., Guzmán L., Engel U.M.",2016,Conference,"Proceedings - International Conference on Software and System Process, ICSSP 2016",10.1145/2904354.2904373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974529280&doi=10.1145%2f2904354.2904373&partnerID=40&md5=c6684ed21f5570fd8d37779694e8d046,"Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer Platz-1, Kaiserslautern, 67663, Germany; SAP SE, Dietmar-Hopp-Allee 16, Walldorf, 69190, Germany; Institute for Theoretical Physics, University of Münster, Germany","Association for Computing Machinery, Inc",English,,9781450341882
Scopus,The usage of ISBSG data fields in software effort estimation: A systematic mapping study,"The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models. © 2015 Elsevier Inc. All rights reserved.",ISBSG data field; Software effort estimation; Systematic mapping study,"González-Ladrón-de-Guevara F., Fernández-Diego M., Lokan C.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2015.11.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962343712&doi=10.1016%2fj.jss.2015.11.040&partnerID=40&md5=2e17a7b891f2c30d95c2fe597c7a526d,"Department of Business Organisation, Universitat Politècnica de València, Camino de Vera, s/n, Valencia, 46022, Spain; School of Engineering and Information Technology, UNSW Canberra, Northcott Drive, Canberra, ACT  2600, Australia",Elsevier Inc.,English,01641212,
Scopus,Architecture-based assessment and planning of change requests,"Software architecture reflects important decisions on structure, used technology and resources. Architecture decisions influence to a large extent requirements on software quality. During software evolution change requests have to be implemented in a way that the software maintains its quality, as various potential implementations of a specific change request influence the quality properties differently. Software development processes involve various organisational and technical roles. Thus, for sound decision making it is important to understand the consequences of the decisions on the various software engineering artefacts (e.g. architecture, code, test cases, build, or deployments) when analysing the impact of a change request. However, existing approaches do not use sufficient architecture descriptions or are limited to software development without taking management tasks into account. In this paper, we present the tool-supported approach Karlsruhe Architectural Maintainability Prediction (KAMP) to analyse the change propagation caused by a change request in a software system based on the architecture model. Using context information annotated on the architecture KAMP enables project members to assess the effects of a change request on various technical and organisational artefacts and tasks during software life cycle. We evaluate KAMP in an empirical study, which showed that it improves scalability of analysis for information systems due to automatically generated task lists containing more complete and precise context annotations than manually created ones. Copyright © 2015 ACM.",Change request; Impact analysis; Software evolution,"Rostami K., Stammel J., Reussner R., Heinrich R.",2015,Conference,"QoSA 2015 - Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures, Part of CompArch 2015",10.1145/2737182.2737198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990874034&doi=10.1145%2f2737182.2737198&partnerID=40&md5=5fa63c57f042f792238c4433651b7275,"Karlsruhe Institute of Technology (KIT), Am Fasanengarten 5, Karlsruhe, 76131, Germany; andrena objects ag Albert-Nestler-Str. 9, Karlsruhe, 76131, Germany","Association for Computing Machinery, Inc",English,,9781450334709
Scopus,Software engineering job productivity-a systematic review,"Productivity is a key element in organizational management. Although it can be measured at different levels (country, sector, organization ...) this research focuses on productivity at the job level. The aim of this paper is to obtain an overview of the state of the art in productivity measurement in software engineering, including the inputs and outputs of the production process used for this measurement at the job level in the workplace. To do so, a systematic literature review protocol was adapted from literature review protocol standards, and subsequently carried out. The objective is to assess the current inputs and outputs present in the literature in order to create new productivity measures for software practitioners. This paper reveals that two different measures are used to assess software engineering professionals: traditional SLOC/Time and planning projects units per time unit. © 2013 World Scientific Publishing Company.",job productivity; software economics; Software engineering; systematic review,"Hernández-López A., Colomo-Palacios R., García-Crespo A.",2013,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194013500125,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880544566&doi=10.1142%2fS0218194013500125&partnerID=40&md5=87097d27f63fbafadc067a6507ad1aea,"Computer Science Department, University Carlos III of Madrid, Av. Universidad 30, Leganés, Madrid 28911, Spain",,English,02181940,
Scopus,Availability of enterprise it systems: An expert-based bayesian framework,"Ensuring the availability of enterprise IT systems is a challenging task. The factors that can bring systems down are numerous, and their impact on various system architectures is difficult to predict. At the same time, maintaining high availability is crucial in many applications, ranging from control systems in the electric power grid, over electronic trading systems on the stock market to specialized command and control systems for military and civilian purposes. This paper describes a Bayesian decision support model, designed to help enterprise IT system decision-makers evaluate the consequences of their decisions by analyzing various scenarios. The model is based on expert elicitation from 50 experts on IT systems availability, obtained through an electronic survey. The Bayesian model uses a leaky Noisy-OR method to weigh together the expert opinions on 16 factors affecting systems availability. Using this model, the effect of changes to a system can be estimated beforehand, providing decision support for improvement of enterprise IT systems availability. The Bayesian model thus obtained is then integrated within a standard, reliability block diagram-style, mathematical model for assessing availability on the architecture level. In this model, the IT systems play the role of building blocks. The overall assessment framework thus addresses measures to ensure high availability both on the level of individual systems and on the level of the entire enterprise architecture. Examples are presented to illustrate how the framework can be used by practitioners aiming to ensure high availability. © Springer Science+Business Media, LLC 2011.",Bayesian networks; Downtime; Expert elicitation; High availability; Noisy-or; Systems availability,"Franke U., Johnson P., König J., Marcks von Würtemberg L.",2012,Journal,Software Quality Journal,10.1007/s11219-011-9141-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860493993&doi=10.1007%2fs11219-011-9141-z&partnerID=40&md5=00108813e2499986373ffcf43991081d,"Industrial Information and Control Systems, Royal Institute of Technology, Stockholm, Sweden",Kluwer Academic Publishers,English,09639314,
Scopus,A case-based classifier for hypertension detection,The exploration of three-dimensional (3D) anthropometry scanning data along with other existing subject medical profiles using data mining techniques becomes an important research issue for medical decision support. This research attempts to construct a classification approach based on the hybrid use of case-based reasoning (CBR) and genetic algorithms (GAs) for hypertension detection using anthropometric body surface scanning data. The obtained result reveals the relationship between a subject's 3D scanning data and hypertension disease. The GA is adopted to determine the appropriate feature weights for CBR. The proposed approaches were experimented and compared with a regular CBR and other widely used approaches including neural nets and decision trees. The experiment showed that applying GA to determine the suitable weights in CBR is a feasible approach to improving the effectiveness of case matching of hypertension disease. It also demonstrated that different weighted CBR approach presents better classification accuracy over the results obtained from other approaches. © 2010 Elsevier B.V. All rights reserved.,Anthropometric data; Case-based reasoning; Decision support systems; Genetic algorithms; Hypertension detection,"Hsu K.-H., Chiu C., Chiu N.-H., Lee P.-C., Chiu W.-K., Liu T.-H., Hwang C.-J.",2011,Journal,Knowledge-Based Systems,10.1016/j.knosys.2010.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957980038&doi=10.1016%2fj.knosys.2010.07.002&partnerID=40&md5=6e3e373af071fcc12d75fb98a22de25f,"Dept. of Health Care Management, Chang Gung University, Taiwan; Dept. of Information Management, Yuan Ze University, Taiwan; Dept. of Information Management, Ching Yun University, Taiwan; Dept. of Industrial Design, Chang Gung University, Taiwan",,English,09507051,
Scopus,Applying radial basis function neural networks based on fuzzy clustering to estimate web applications effort,"An important research issue in software project management is how to predict accurately an effort for the software project to develop at an early stage. Achieving high accuracy when estimating the effort of the software under development would supply effectively the project managers in leading to projects finished on time and within budget. Unfortunately, existing software effort estimation techniques are still unable to provide acceptable estimates. This paper investigates the application of Radial Basis Function Neural Networks (RBFN) based on fuzzy clustering in web development effort estimation. The proposed model is designed by integrating the principles of RBFN and the fuzzy C-means clustering algorithm. The architecture of the network is suitably modified at the hidden layer to realize a novel neural implementation of the fuzzy clustering algorithm. Fuzzy set-theoretic concepts are incorporated at the hidden layer, enabling the model to handle uncertain and imprecise data, which can significantly improve the accuracy of the obtained estimates. MMRE and Pred are used as measures of prediction accuracy for the undertaken study. The results show that an RBFN using fuzzy C means performs better than an RBFN using hard K-means. This study uses data on web applications from the Tukutuku database. © 2010 Praise Worthy Prize S.r.l.",Fuzzy C-means; Rbf neural networks; Tukutuku dataset; Web effort estimation,"Zakrani A., Idri A.",2010,Journal,International Review on Computers and Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78951478126&partnerID=40&md5=d80672acda377940a046a22117b6b033,"Department of Software Engineering, SI2M Laboratory, ENSIAS, Mohammed Vth -Souissi University, BP. 713, Madinat Al Irfane, Rabat, Morocco",,English,18286003,
Scopus,The role of outcome feedback in improving the uncertainty assessment of software development effort estimates,"Previous studies report that software developers are over-confident in the accuracy of their effort estimates. Aim: This study investigates the role of outcome feedback, that is, feedback about the discrepancy between the estimated and the actual effort, in improving the uncertainty assessments. Method: We conducted two in-depth empirical studies on uncertainty assessment learning. Study 1 included five student developers and Study 2, 10 software professionals. In each study the developers repeatedly assessed the uncertainty of their effort estimates of a programming task, solved the task, and received estimation accuracy outcome feedback. Results: We found that most, but not all, developers were initially over-confident in the accuracy of their effort estimates and remained over-confident in spite of repeated and timely outcome feedback. One important, but not sufficient, condition for improvement based on outcome feedback seems to be the use of explicitly formulated, instead of purely intuition-based, uncertainty assessment strategies. © 2008 ACM.",Cost estimation; Effort prediction intervals; Judgment-based uncertainty assessment; Overconfidence; Software cost estimation; Software development management,"Gruschke T.M., Jørgensen M.",2008,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/13487689.13487693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649116352&doi=10.1145%2f13487689.13487693&partnerID=40&md5=9bb78d7c7805fcb2f074f7890d047dce,"Simula Research Laboratory; University of Oslo; Software Engineering Department, Simula Research Laboratory, Fornebu, N-1325, Norway; Departement of Informatics, University of Oslo, Blindern N-0371, Norway",,English,1049331X,
Scopus,An investigation of software development productivity in China,"Software productivity conveys fundamental information for many decision making processes, such as in-house development benchmarking and outsourcing strategic planning. However, there is a lack of statistical results on this matter with respect to Chinese software industry. In this paper, through the analysis of 999 industry projects in China, we seek to develop in-depth and comprehensive understanding about software productivity status in China, by identifying significant influential factors and examining their true effects based on our dataset. As a result, Organization is identified as the most significant factor, followed by Development type, Business area, Region, Language, Project size and Team size. Further assessment and findings are also presented with relevant recommendations to increase productivity and improve software processes. © 2008 Springer-Verlag Berlin Heidelberg.",Chinese software industry; Empirical analysis; Globalization of software development; Software process improvement; Software productivity,"He M., Li M., Wang Q., Yang Y., Ye K.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-79588-9_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649199827&doi=10.1007%2f978-3-540-79588-9_33&partnerID=40&md5=9f149abec6c0108d44afa930e24575a3,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China; Graduate University, Chinese Academy of Sciences, Beijing 100039, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China",,English,03029743,3540795871; 9783540795872
Scopus,An intelligent early warning system for software quality improvement and project management,"There are three major problems with software projects: over budget, behind schedule, and poor quality. It is often too late to correct these problems by the time they are detected in failed software projects. In this paper, we discuss design, implementation, and evaluation of an experimental intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It consists of the following components: software metrics database, risk knowledge base, intelligent risk assessment, and risk tracing. It helps to assess risks associated with the three problems from perspectives of product, process, and organization in the early phases of the software development process. It is capable of aggregating various partial risk assessment results into an overall risk indicator even if they may be conflicting. In addition, it can be used to analyze a risk by identifying its root causes through its risk tracing utility. © 2006 Elsevier Inc. All rights reserved.",Early warning; Fuzzy logic; Metrics; Process improvement; Quality management,"Liu X.(F.), Kane G., Bambroo M.",2006,Journal,Journal of Systems and Software,10.1016/j.jss.2006.01.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748202986&doi=10.1016%2fj.jss.2006.01.024&partnerID=40&md5=80ae08d2f7b47d9547ba6427b0341959,"Department of Computer Science, University of Missouri-Rolla, 1870 Miner Circle, Rolla, 65401, United States",,English,01641212,
Scopus,Comparison of various methods for handling incomplete data in software engineering databases,"Objective: Increasing the awareness of how missing data affects software predictive accuracy has led to increasing numbers of missing data techniques (MDTs). This paper investigates the robustness and accuracy of eight popular techniques for tolerating incomplete training and test data using tree-based models. Method: MDTs were compared by artificially simulating different proportions, patterns, and mechanisms of missing data. A 4-way repeated measures design was employed to analyze the data. Results: The simulation results suggest important differences. Listwise deletion is substantially inferior while multiple imputation (MI) represents a superior approach to handling missing data. Decision tree single imputation and surrogate variables splitting are more severely impacted by missing values distributed among all attributes. Conclusion: MI should be used if the data contain many missing values. If few values are missing, any of the MDTs might be considered. Choice of technique should be guided by pattern and mechanisms of missing data. © 2005 IEEE.",,"Twala B., Cartwright M., Shepperd M.",2005,Conference,"2005 International Symposium on Empirical Software Engineering, ISESE 2005",10.1109/ISESE.2005.1541819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749052909&doi=10.1109%2fISESE.2005.1541819&partnerID=40&md5=c53023874ceaef16d6a1ee62b9dfcc48,"Brunel University, Uxbridge, Middlesex UB8 3PH, United Kingdom",,English,,0780395085; 9780780395084
Scopus,Using simulated data sets to compare data analysis techniques used for software cost modelling,"The goals of this study were to compare different data analysis methods and to demonstrate the viability of simulation as a mechanism to allow such comparisons. Simulation was used to create data sets with a known underlying model and with non-Normal characteristics that are frequently found in software data sets: skewness, unstable variance, and outliers and combinations of these characteristics. Three data analysis approaches were investigated: residual analysis; multiple regression; classification and regression trees (CART). In addition to the standard statistical 'least squares' version of each method, robust and non-parametric versions of the techniques were also investigated. It was found that standard multiple regression techniques were best if the data only exhibited moderate non-Normality. As might be expected, under more extreme conditions such as severe heteroscedasticity, the non-parametric techniques performed best. It was more surprising to find that under strongly non-Normal conditions the robust and non-parametric residual analysis techniques performed as well as the conventional robust and non-parametric versions of multiple regression. However, the most important result of this study is to demonstrate the value of simulation as a technique for evaluating different data analysis techniques under controlled conditions.",,"Pickard L., Kitchenham B., Linkman S.J.",2001,Journal,IEE Proceedings: Software,10.1049/ip-sen:20010621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035726598&doi=10.1049%2fip-sen%3a20010621&partnerID=40&md5=dab7fe8dbba8b64c17330c23117d11f1,"Department of Computer Science, Keele University, Staffordshire ST5 5BG, United Kingdom",,English,14625970,
Scopus,A DEA framework to assess the efficiency of the software requirements capture and analysis process,"In this paper a theoretical framework to assess the efficiency of the Requirements Capture and Analysis (RCA) process in software development is introduced. Although it is widely recognized that successful implementation of the first stages of the software development process is critical for the overall development process, RCA efficiency assessments have not been given much attention. The presented theoretical framework to assess RCA efficiency follows a production approach to model the early stages of a software project. An approach based on Data Envelopment Analysis that utilizes the proposed framework to isolate the effects of exogenous factors, such as the environment or the type of project, on the project's RCA efficiency is also presented. Finally, the applicability of the methodology through an exploratory empirical study is demonstrated, and managerial implications are discussed.",Data envelopment analysis; Requirements capture and analysis; Software development,"Chatzoglou P.D., Soteriou A.C.",1999,Journal,Decision Sciences,10.1111/j.1540-5915.1999.tb01620.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039523252&doi=10.1111%2fj.1540-5915.1999.tb01620.x&partnerID=40&md5=ac850bf58f8246bc5d1e2ee8d84806f1,"Business Administration Department, School of Business and Economics, T.E.I. of Kavala, Agios Loukas, P.O. Box 1194, 65404 Kavala, Greece; Dept. of Pub. and Bus. Admin., University of Cyprus, P.O. Box 537, Nicosia, Cyprus",Decision Sciences Institute,English,00117315,
Scopus,A project effort estimation study,"This paper outlines a four step effort estimation study and focuses on the first and second step. The four steps are formulated to successively introduce a more formal effort experience base. The objective of the study is to evaluate the needed formalism to improve effort estimation and to study different approaches to record and reuse experiences from effort planning in software projects. In the first step (including seven projects), the objective is to compare estimation of effort based on a rough figure (indicating approximate size of the projects) with an informal experience base. The objective of the second step is on reuse of experiences from an effort experience base, where the outcomes of seven previous projects were stored. Seven new projects are planned based on the previous experiences. The plans are, after project completion, compared with the initial plans and with the data from six out of the seven new projects, to plan the seventh. It is clear from the studies that effort estimation is difficult and that the mean estimation error is in the range of 14%-19% independent of the approach used. Further, it is concluded that the best estimates are obtained when the projects use the previous experience and complement this information with their own thoughts and opinions. Finally, it is concluded that data collection is not enough in itself, the data collected must be processed, i.e. interpreted, generalized and synthesized into a reusable form. © 1998 Elsevier Science B.V. All rights reserved.",Data collection; Education; Effort; Empirical study; Experience base; Measurements,"Ohlsson M.C., Wohlin C., Regnell B.",1998,Journal,Information and Software Technology,10.1016/S0950-5849(98)00097-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032302278&doi=10.1016%2fS0950-5849%2898%2900097-4&partnerID=40&md5=d54dff682274f1cb352681144bb97301,"Department of Communication Systems, Lund University, Box 118, S-221 00, Lund, Sweden",Elsevier,English,09505849,
Scopus,Probabilistic neural network based categorical data imputation,"Real world datasets contain both numerical and categorical attributes. Very often missing values are present in both numerical and categorical attributes. The missing data has to be imputed as the inferences made from complete data are often more accurate and reliable than those made from incomplete data [15]. Also, most of the data mining algorithms cannot work with incomplete datasets. The paper proposes a novel soft computing architecture for categorical data imputation. The proposed imputation technique employs Probabilistic Neural Network (PNN) preceded by mode for imputing the missing categorical data. The effectiveness of the proposed imputation technique is tested on 4 benchmark datasets under the 10 fold-cross validation framework. In all datasets, except Mushroom, which are complete, some values, which are randomly removed, are treated as missing values. The performance of the proposed imputation technique is compared with that of 3 statistical and 3 machine learning methods for data imputation. The comparison of the mode+PNN imputation technique with mode, K-Nearest Neighbor (K-NN), Hot Deck (HD), Naive Bayes, Random Forest (RF) and J48 (Decision Tree) imputation techniques demonstrates that the proposed method is efficient, especially when the percentage of missing values is high, for records having more than one missing value and for records having a large number of categories for each categorical variable. © 2016 Elsevier B.V.",Categorical data imputation; Decision Tree (DT); Probabilistic Neural Network (PNN); Random Forest (RF),"Nishanth K.J., Ravi V.",2016,Journal,Neurocomputing,10.1016/j.neucom.2016.08.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994176946&doi=10.1016%2fj.neucom.2016.08.044&partnerID=40&md5=7cb7b718fb30a2262a59971114c58056,"Center of Excellence in CRM and Analytics, Institute for Development and Research in Banking Technology (IDRBT), Castle Hills Road #1, Masab Tank, Hyderabad, 500057, India",Elsevier B.V.,English,09252312,
Scopus,Web Effort Estimation: Function Point Analysis vs. COSMIC,"Context: software development effort estimation is a crucial management task that critically depends on the adopted size measure. Several Functional Size Measurement (FSM) methods have been proposed. COSMIC is considered a 2nd generation FSM method, to differentiate it from Function Point Analysis (FPA) and its variants, considered as 1st generation ones. In the context of Web applications, few investigations have been performed to compare the effectiveness of the two generations. Software companies could benefit from this analysis to evaluate if it is worth to migrate from a 1st generation method to a 2nd one. Objective: the main goal of the paper is to empirically investigate if COSMIC is more effective than FPA for Web effort estimation. Since software companies using FPA cannot build an estimation model based on COSMIC as long as they do not have enough COSMIC data, the second goal of the paper is to investigate if conversion equations can be exploited to support the migration from FPA to COSMIC. Method: two empirical studies have been carried out by employing an industrial data set. The first one compared the effort prediction accuracy obtained with Function Points (FPs) and COSMIC, using two estimation techniques (Simple Linear Regression and Case-Based Reasoning). The second study assessed the effectiveness of a two-step strategy that first exploits a conversion equation to transform historical FPs data into COSMIC, and then builds a new prediction model based on those estimated COSMIC sizes. Results: the first study revealed that, on our data set, COSMIC was significantly more accurate than FPs in estimating the development effort. The second study revealed that the effectiveness of the analyzed two-step process critically depends on the employed conversion equation. Conclusion: for Web effort estimation COSMIC can be significantly more effective than FPA. Nevertheless, additional research must be conducted to identify suitable conversion equations so that the two-step strategy can be effectively employed for a smooth migration from FPA to COSMIC. © 2015 Elsevier B.V. All rights reserved.",COSMIC; Functional Size Measures; IFPUG Function Point Analysis; Web Effort Estimation,"Di Martino S., Ferrucci F., Gravino C., Sarro F.",2016,Journal,Information and Software Technology,10.1016/j.infsof.2015.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958604486&doi=10.1016%2fj.infsof.2015.12.001&partnerID=40&md5=51ffaf3fc1e230c41b4174c3960e725b,"Dipartimento di Ingegneria Elettrica e Delle Tecnologie dell'Informazione, University of Napoli Federico II, Italy; Department of Computer Science, University of Salerno, Italy; CREST, Department of Computer Science, University College London, United Kingdom",Elsevier,English,09505849,
Scopus,Statistical comparison of modelling methods for software maintainability prediction,"The objective of this paper is statistical comparison of modelling methods for software maintainability prediction. The statistical comparison is performed by building software maintainability prediction models using 27 dierent regression and machine learning based algorithms. For this purpose, software metrics datasets of two dierent commercial object-oriented systems are used. These systems were developed using an object oriented programming language Ada. These systems are User Interface Management System (UIMS) and Quality Evaluation System (QUES). It is shown that dierent measures like MMRE, RMSE, Pred(0.25) and Pred(0.30) calculated on predicted values obtained from leave one out (LOO) cross validation produce very divergent results regarding accuracy of modelling methods. Therefore the 27 modelling methods are evaluated on the basis of statistical signicance tests. The Friedman test is used to rank various modelling methods in terms of absolute residual error. Six out of the ten top ranked modelling methods are common to both UIMS and QUES. This indicates that modelling methods for software maintainability predicton are solid and scalable. After obtaining ranks, pair wise Wilcoxon Signed rank test is performed.Wilcoxon Sign rank test indicates that the top ranking method in UIMS data set is significantly superior to only four other modelling methods whereas the top tanking method in QUES data set is significantly superior to 11 other modelling methods. The performance of instance based learning algorithms IBk and Kstar is comparable to modelling methods used in earlier studies. #.c World Scientific Publishing Company.",Machine learning; Significance tests; Software maintainability prediction,"Kaur A., Kaur K.",2013,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194013500198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887071983&doi=10.1142%2fS0218194013500198&partnerID=40&md5=9c47f87a23b19f3c3669bba2bf117730,"USICT, GGS Indraprastha University, Dwarka Sector 16-C, Delhi 110075, India",,English,02181940,
Scopus,Ensemble missing data techniques for software effort prediction,"Constructing an accurate effort prediction model is a challenge in software engineering. The development and validation of models that are used for prediction tasks require good quality data. Unfortunately, software engineering datasets tend to suffer from the incompleteness which could result to inaccurate decision making and project management and implementation. Recently, the use of machine learning algorithms has proven to be of great practical value in solving a variety of software engineering problems including software prediction, including the use of ensemble (combining) classifiers. Research indicates that ensemble individual classifiers lead to a significant improvement in classification performance by having them vote for the most popular class. This paper proposes a method for improving software effort prediction accuracy produced by a decision tree learning algorithm and by generating the ensemble using two imputation methods as elements. Benchmarking results on ten industrial datasets show that the proposed ensemble strategy has the potential to improve prediction accuracy compared to an individual imputation method, especially if ultiple imputation is a component of the ensemble. © 2010 - IOS Press and the authors.",Decision tree; Ensemble; Imputation; Incomplete data; Machine learning; Missing data techniques; Software prediction; Supervised learning,"Twala B., Cartwright M.",2010,Journal,Intelligent Data Analysis,10.3233/IDA-2010-0423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953511815&doi=10.3233%2fIDA-2010-0423&partnerID=40&md5=b8c97018f2eb1b8f4136336c87845617,"Department of Electrical and Electronic Engineering Science, University of Johannesburg, Auckland Park, Johannesburg 2006, South Africa; Brunel Software Engineering Research Centre, School of Information Systems, Computing and Mathematics, Brunel University, Uxbridge, United Kingdom",,English,1088467X,
Scopus,Software development effort prediction: A study on the factors impacting the accuracy of fuzzy logic systems,"Reliable effort prediction remains an ongoing challenge to software engineers. Traditional approaches to effort prediction such as the use of models derived from historical data, or the use of expert opinion are plagued with issues pertaining to their effectiveness and robustness. These issues are more pronounced when the effort prediction is used during the early phases of the software development lifecycle. Recent works have demonstrated promising results obtained with the use of fuzzy logic. Fuzzy logic based effort prediction systems can deal better with imprecision, which characterizes the early phases of most software development projects, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Fuzzy logic based prediction systems could produce further better estimates provided that various parameters and factors pertaining to fuzzy logic are carefully set. In this paper, we present an empirical study, which shows that the prediction accuracy of a fuzzy logic based effort prediction system is highly dependent on the system architecture, the corresponding parameters, and the training algorithms. © 2009 Elsevier B.V. All rights reserved.",COCOMO; Effort prediction; Fuzzy logic; Imprecision; Uncertainty,"Muzaffar Z., Ahmed M.A.",2010,Journal,Information and Software Technology,10.1016/j.infsof.2009.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350566636&doi=10.1016%2fj.infsof.2009.08.001&partnerID=40&md5=cb18ce5eb7479828f64b957e978a2635,"Department of Computer Science, University of Western Ontario, Canada; LEROS Technologies Corporation, Fairfax, VA 22030, United States",,English,09505849,
Scopus,Conceptual data model-based software size estimation for information systems,Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. Some information required by existing software sizing methods is difficult to predict in the early stage of software development. A conceptual data model is widely used in the early stage of requirements analysis for information systems. Lines of code (LOC) is a commonly used software size measure. This article proposes a novel LOC estimation method for information systems from their conceptual data models through using a multiple linear regression model. We have validated the proposed method using samples from both the software industry and open-source systems. © 2009 ACM.,Conceptual data model; Line of code (LOC); Multiple linear regression model; Software sizing,"Tan H.B.K., Zhao Y., Zhang H.",2009,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/1571629.1571630,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350214668&doi=10.1145%2f1571629.1571630&partnerID=40&md5=77ddeda747f588eb6edc7c646044b79f,"Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798, Singapore; School of Software, Tsinghua University, Beijing 100084, China",,English,1049331X,
Scopus,Software development effort estimation using fuzzy logic - A survey,"Most of the software estimates should be performed at the beginning of the life cycle, when we do not yet know the problem we are going to solve. Effort estimation is used to predict how many hours of work and how many workers are needed to develop a project. The effort invested in a software project is probably one of the most important and most analyzed variables in recent years in the process of project management. Estimating the effort with a high grade of reliability is a problem which has not yet been solved and even the project manager has to deal with it since the beginning. Fuzzy systems try to emulate cognitive processes of the brain with a rule base. The basic concept is inspired by the human processes where the decisional criteria are not clear cut, but blurred and it is difficult to find objective to make the decisions more precise and clear. Fuzzy decision systems are based on fuzzy logic that tries to reproduce the fuzzy human reasoning. The aim of this survey is to analyze the use of Fuzzy logic in the existing models and to provide in depth review of software and project estimation techniques existing in industry and literature, its strengths and weaknesses. © 2008 IEEE.",Cost models; Effort estimation; Fuzzy logic; Software engineering,"Nisar M.W., Wang Y.-J., Elahi M.",2008,Conference,"Proceedings - 5th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2008",10.1109/FSKD.2008.370,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149131573&doi=10.1109%2fFSKD.2008.370&partnerID=40&md5=a3524f2455f04f7d8b62e4e08fb875ab,"Laboratory for Internet Software Technologies, Graduate University, Chinese Academy of Sciences, Beijing 100190, China; State Key Laboratory of Computer Science, Chinese Academy of Sciences; COMSATS Institute of Information Technology, Wah 47000, Pakistan; Intelligence Engineering Laboratory, Graduate University, Chinese Academy of Sciences",,English,,9780769533056
Scopus,Comparing size measures for predicting Web application development effort: A case study,"Size represents one of the most important attribute of software products used to predict software development effort. In the past nine years, several measures have been proposed to estimate the size of Web applications, and it is important to determine which one is most effective to predict Web development effort. To this aim in this paper we report on an empirical analysis where, using data from 15 Web projects developed by a software company, we compare four sets of size measures, using two prediction techniques, namely Forward Stepwise Regression (SWR) and Case-Based Reasoning (CBR). All the measures provided good predictions in terms of MMRE, MdMRE, and Pred(0.25) statistics, for both SWR and CBR. Moreover, when using SWR, Length measures and Web Objects gave significant better results than Functional measures, however presented similar results to the Tukutuku measures. As for CBR, results did not show any significant differences amongst the four sets of size measures. © 2007 IEEE.",,"Di Martino S., Ferrucci F., Gravino C., Mendes E.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949113657&doi=10.1109%2fESEM.2007.33&partnerID=40&md5=57d94ee0546cb05547a3828e91155a0e,"Università di Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy; University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,0769528864; 9780769528861
Scopus,Developing project duration models in software engineering,"Based on the empirical analysis of data contained in the International Software Benchmarking Standards Group (ISBSG) repository, this paper presents software engineering project duration models based on project effort. Duration models are built for the entire dataset and for subsets of projects developed for personal computer, mid-range and mainframe platforms. Duration models are also constructed for projects requiring fewer than 400 person-hours of effort and for projects requiring more than 400 person-hours of effort. The usefulness of adding the maximum number of assigned resources as a second independent variable to explain duration is also analyzed. The opportunity to build duration models directly from project functional size in function points is investigated as well. © Science Press, Beijing, China and Springer Science + Business Media, LLC, USA 2007.",International software benchmarking standards group (ISBSG); Project duration models; Project scheduling models; Schedule and organizational issues; Software engineering; Time estimation,"Bourque P., Oligny S., Abran A., Fournier B.",2007,Journal,Journal of Computer Science and Technology,10.1007/s11390-007-9051-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249852712&doi=10.1007%2fs11390-007-9051-5&partnerID=40&md5=23fbb2fd184d269ab09e4698d960503d,"École de Technologie Supérieure, Montréal, Que., Canada; Bell Canada, Montréal, Que., Canada; Université du Québec à Montréal, Montréal, Que., Canada",,English,10009000,
Scopus,Evaluating a functional size measurement method for Web applications: An empirical analysis,"This paper describes a laboratory experiment which evaluates OO-Method Function Points for the Web. OOmFPWeb measures the functional size of Web applications using conceptual models that are developed with Object-Oriented Web Solutions (OOWS), an automated software production method for Web applications. OOmFPWeb is evaluated on a range of performance-based and perception-based variables, including efficiency, reproducibility, perceived ease of use, perceived usefulness and intention to use. The results show that OOmFPWeb is efficient when compared to current industry practices. Furthermore, the method produces consistent functional size assessments and is perceived to be easy to use and useful by its users. © 2004 IEEE.",Conceptual Modeling; Empirical Software Engineering; Functional Size Measurement; Method Evaluation; Validation; Web Applications,"Abrahão S., Poels G., Pastor O.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844296446&doi=10.1109%2fMETRIC.2004.1357921&partnerID=40&md5=aec15d4669f7a46eaf278198295cd80d,"Department of Information Systems, Valencia University of Technology, Camino de Vera, s/n, 46022, Valencia, Spain; Fac. of Econ. and Bus. Admin., Ghent University, Hoveniersberg 24, 9000 Ghent, Belgium",,English,15301435,0769521290
Scopus,Measuring Software Sustainability,"Planning and management of software sustainment is impaired by a lack of consistently applied, practical measures. Without these measures, it is impossible to determine the effect of efforts to improve sustainment practices. In this paper we provide a context for evaluating sustainability and discuss a set of measures developed at the Software Engineering Institute at Carnegie Mellon University.",,"Seacord R.C., Elm J., Goethert W., Lewis G.A., Plakosh D., Robert J., Wrage L., Lindvall M.",2003,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2003.1235455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956617309&doi=10.1109%2fICSM.2003.1235455&partnerID=40&md5=82632fb30f46c3c9475a4346d3ec4bb4,"Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, United States",IEEE Computer Society,English,,769519059
Scopus,Effort estimation for corrective software maintenance,"This paper reports on an empirical study aiming at constructing cost estimation models for corrective maintenance projects. Data available were collected from five maintenance projects currently carried out by a large software enterprise. The resulting models, constructed using multivariate linear regression techniques, allow to estimate the costs of a project conducted according to the adopted maintenance processes. Model performances on future observations were achieved by taking into account different corrective maintenance task typologies, each affecting the effort in a different way, and assessed by means of a cross validation which guarantees a nearly unbiased estimate of the prediction error. The constructed models are currently adopted by the subject company. Copyright 2002 ACM.","D.2.8 [Software Engineering]: Management - software maintenance, cost estimation; Experimentation; Management; Measurement","De Lucia A., Pompella E., Stefanucci S.",2002,Conference,ACM International Conference Proceeding Series,10.1145/568760.568831,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953732532&doi=10.1145%2f568760.568831&partnerID=40&md5=12423578292f6cdae56f2d6e198e6703,"RCOST - Department of Engineering, University of Sannio, Palazzo Bosco Lucarelli, Piazza Roma, 82100 Benevento, Italy; EDS Italia Software S.p.A., Viale Edison - Loc. Lo Uttaro, 81100 Caserta, Italy",,English,,1581135564; 9781581135565
Scopus,Software effort estimation based on the optimal Bayesian belief network,"In this paper, we present a model for software effort (person-month) estimation based on three levels Bayesian network and 15 components of COCOMO and software size. The Bayesian network works with discrete intervals for nodes. However, we consider the intervals of all nodes of network as fuzzy numbers. Also, we obtain the optimal updating coefficient of effort estimation based on the concept of optimal control using Genetic algorithm and Particle swarm optimization for the COCOMO NASA database. In the other words, estimated value of effort is modified by determining the optimal coefficient. Also, we estimate the software effort with considering software quality in terms of the number of defects which is detected and removed in three steps of requirements specification, design and coding. If the number of defects is more than the specified threshold then the model is returned to the current step and an additional effort is added to the estimated effort. The results of model indicate that optimal updating coefficient obtained by genetic algorithm increases the accuracy of estimation significantly. Also, results of comparing the proposed model with the other ones indicate that the accuracy of the model is more than the other models. © 2016 Elsevier B.V.",Bayesian belief network; Optimal control; Software effort estimation; Software quality,"Zare F., Khademi Zare H., Fallahnezhad M.S.",2016,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2016.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994803389&doi=10.1016%2fj.asoc.2016.08.004&partnerID=40&md5=9dd4f75f2a76d9fa4a26c19f29ada560,"Department of Industrial Engineering, Yazd University, Yazd, Iran",Elsevier Ltd,English,15684946,
Scopus,A survey on the characteristics of projects with success in delivering client benefits,"Context A large waste of resources in software development projects currently results from being unable to produce client benefits. Objective The main objective is to better understand the characteristics of successful software projects and contribute to software projects that are more likely to produce the planned client benefits. Method We asked 63 Norwegian software professionals, representing both the client and the provider role, to report information about their last completed project. In a follow-up survey with 64 Norwegian software professionals, we addressed selected findings from the first survey. Results The analysis of the project information showed the following: i) The project management triangle criteria of being on time, on budget, and having the specified functionality are poor correlates of the essential success dimension client benefits. ii) Benefit management planning before the project started and benefit management activities during project execution were connected with success in delivering client benefits. iii) Fixed-price projects and projects in which the selection of providers had a strong focus on low price were less successful in delivering project benefits than other projects. iv) Agile projects were in general more successful than other projects, but agile projects without flexible scope to reflect changed user needs and learning, or without frequent delivery to the client, had less than average success in delivering client benefits. Conclusions The software projects that were successful in delivering client benefits differed from the less successful ones in several ways. In particular, they applied benefit management practices during project execution, they avoided fixed-price contracts, they had less focus on low price in the selection of providers, and they applied the core agile practices - frequent delivery to the client and scope flexibility. © 2016 Elsevier B.V. All rights reserved.",Client benefits; Project success factors; Software projects; Survey,Jørgensen M.,2016,Journal,Information and Software Technology,10.1016/j.infsof.2016.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971643798&doi=10.1016%2fj.infsof.2016.05.008&partnerID=40&md5=8431616f5b108c14524aae8a0f929bc8,"Simula Research Laboratory, P.O. Box 134, Lysaker, Norway",Elsevier B.V.,English,09505849,
Scopus,A framework for comparing multiple cost estimation methods using an automated visualization toolkit,"Context: The importance of accurate predictions in Software Cost Estimation and the related challenging research problems, led to the introduction of a plethora of methodologies in literature. However, the wide variety of cost estimation methods, the techniques for improving them and the different measures of accuracy have caused new problems such as the inconsistent findings and the conclusion instability. Today, there is a confusion regarding the choice of the most appropriate method for a specific dataset and therefore a need for well-established statistical frameworks as well as for automated tools that will reinforce and lead a comprehensive experimentation and comparison process, based on the thorough study of the cost estimation errors. © 2014 Elsevier B.V. All rights reserved.",Automated tools; Graphical comparison; Prediction models; REC curves; Software Cost Estimation,"Mittas N., Mamalikidis I., Angelis L.",2015,Conference,Information and Software Technology,10.1016/j.infsof.2014.05.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922596154&doi=10.1016%2fj.infsof.2014.05.010&partnerID=40&md5=e7a63e81c5b572c0a63204a58a3f1432,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece; Department of Electrical Engineering, Technological Educational Institute of Kavala, Kavala, 65404, Greece",Elsevier B.V.,English,09505849,
Scopus,The value of software sizing,"Context: One of the difficulties faced by software development Project Managers is estimating the cost and schedule for new projects. Previous industry surveys have concluded that software size and cost estimation is a significant technical area of concern. In order to estimate cost and schedule it is important to have a good understanding of the size of the software product to be developed. There are a number of techniques used to derive software size, with function points being amongst the most documented. Objective: In this paper we explore the utility of function point software sizing techniques when applied to two levels of software requirements documentation in a commercial software development organisation. The goal of the research is to appraise the value (cost/benefit) which functional sizing techniques can bring to the project planning and management of software projects within a small-to-medium sized software development enterprise (SME). Method: Functional counts were made at the bid and detailed functional specification stages for each of five commercial projects used in the research. Three variants of the NESMA method were used to determine these function counts. Through a structured interview session, feedback on the sizing results was obtained to evaluate its feasibility and potential future contribution to the company. Results: The results of our research suggest there is value in performing size estimates at two appropriate stages in the software development lifecycle, with simplified methods providing the optimal return on effort expended. Conclusion: The 'Estimated NESMA' is the most appropriate tool for use in size estimation for the company studied. The use of software sizing provides a valuable contribution which would augment, but not replace, the company's existing cost estimation approach. © 2011 Elsevier B.V. All rights reserved.",Empirical software engineering; Function points; NESMA; Project planning; Size metrics; Software size estimation,"Wilkie F.G., McChesney I.R., Morrow P., Tuxworth C., Lester N.G.",2011,Journal,Information and Software Technology,10.1016/j.infsof.2011.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051669402&doi=10.1016%2fj.infsof.2011.05.008&partnerID=40&md5=04e0743d1ada12169024fd25590b5b4a,"School of Computing and Mathematics, University of Ulster, Newtownabbey, Co. Antrim BT37 0QB, United Kingdom; Equiniti ICS Limited, 205 Airport Road West, Belfast BT3 9ED, United Kingdom; Information Services Directorate, University of Ulster, Newtownabbey, Co. Antrim BT37 0QB, United Kingdom",,English,09505849,
Scopus,Improving the reliability of transaction identification in use cases,"Context: The concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement. Objective: The goal of this study was to evaluate reliability of transaction identification in use cases (with the methods mentioned in the literature), analyze their weaknesses, and propose some means for their improvement. Method: A controlled experiment on a group of 120 students was performed to investigate if the methods for transaction identification, known from the literature, provide similar results. In addition, a qualitative analysis of the experiment data was performed to investigate the potential problems related to transaction identification in use cases. During the experiment a use-case benchmark specification was used. The automatic methods for transaction identification, proposed in the paper have been validated using the same benchmark by comparing the outcomes provided by these methods to on-average number of transactions identified by the participants of the experiment. Results: A significant difference in the median number of transactions was observed between groups using different methods of transaction identification. The Kruskal-Wallis test was performed with the significance level α set to 0.05 and followed by the post-hoc analysis performed according to the procedure proposed by Conover. Also a large intra-method variability was observed. The ratios between the maximum and minimum number of transactions identified by the participants using the same method were equal to 1.96, 3.83, 2.03, and 2.21. The proposed automatic methods for transaction identification provided results consistent with those provided by the participants of the experiment and functional measurement experts. The relative error between the number of transaction identified by the tool and on-average number of transactions identified by the participants of the experiment ranged from 3% to 7%. Conclusions: Human-performed transaction identification is error prone and quite subjective. Its reliability can be improved by automating the process with the use of natural language processing techniques. © 2011 Elsevier B.V. All rights reserved.",Functional size measurement; Natural language processing; Use Case Points; Use-case transactions,"Ochodek M., Alchimowicz B., Jurkiewicz J., Nawrocki J.",2011,Conference,Information and Software Technology,10.1016/j.infsof.2011.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957479773&doi=10.1016%2fj.infsof.2011.02.004&partnerID=40&md5=9bdaf2575d3ffb4e323e8698580828f8,"Poznan University of Technology, Institute of Computing Science, ul. Piotrowo 2, 60-965 Poznań, Poland",,English,09505849,
Scopus,Effort estimation for enterprise resource planning implementation projects using social choice - a comparative study,"ERP implementation projects have received enormous attention in the last years, due to their importance for organisations, as well as the costs and risks involved. The estimation of effort and costs associated with new projects therefore is an important topic. Unfortunately, there is still a lack of models that can cope with the special characteristics of these projects. As the main focus lies in adapting and customising a complex system, and even changing the organisation, traditional models like COCOMO can not easily be applied. In this article, we will apply effort estimation based on social choice in this context. Social choice deals with aggregating the preferences of a number of voters into a collective preference, and we will apply this idea by substituting the voters by project attributes. Therefore, instead of supplying numeric values for various project attributes, a new project only needs to be placed into rankings per attribute, necessitating only ordinal values, and the resulting aggregate ranking can be used to derive an estimation. We will describe the estimation process using a data set of 39 projects, and compare the results to other approaches proposed in the literature. © 2010 Taylor & Francis.",cost estimation; DEA; ERP; product metrics; social choice,"Koch S., Mitlöhner J.",2010,Journal,Enterprise Information Systems,10.1080/17517575.2010.496494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955546000&doi=10.1080%2f17517575.2010.496494&partnerID=40&md5=ccfa0e701531b5869f1136d19aa45c6b,"Department of Management, Bogazici University, Istanbul, Turkey; Institute for Information Business, WU-Vienna University of Economics and Business, Vienna, Austria",,English,17517575,
Scopus,Transparent combination of expert and measurement data for defect prediction - An industrial case study,"Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE). © 2010 ACM.",defect content; effectiveness; hybrid estimation; HyDEEP,"Kläs M., Elberzhager F., Münch J., Hartjes K., Von Graevemeyer O.",2010,Conference,Proceedings - International Conference on Software Engineering,10.1145/1810295.1810313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954729204&doi=10.1145%2f1810295.1810313&partnerID=40&md5=edba6f4441b81b1a8d2b90839d676b24,"Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Deutsche Telekom AG, Kampstraße 106, 44137 Dortmund, Germany; Deutsche Telekom AG, Hamburger Allee 25a, 30161 Hannover, Germany",,English,02705257,9781605587196
Scopus,Software cost estimation using computational intelligence techniques,"This paper presents computational intelligence techniques for software cost estimation. We proposed a new recurrent architecture for Genetic Programming (GP) in the process. Three linear ensembles based on (i) arithmetic mean (ii) geometric mean and (iii) harmonic mean are implemented. We also performed GP based feature selection. The efficacy of these techniques viz Multiple Linear Regression, Polynomial Regression, Support Vector Regression, Classification and Regression Tree, Multivariate Adaptive Regression Splines, Multilayer FeedForward Neural Network, Radial Basis Function Neural Network, Counter Propagation Neural Network, Dynamic Evolving Neuro-Fuzzy Inference System, Tree Net, Group Method of Data Handling and Genetic Programming has been tested on the International Software Benchmarking Standards Group (ISBSG) release 10 dataset. Ten-fold cross validation is performed throughout the study. The results obtained from our experiments indicate that new recurrent architecture for Genetic Programming outperformed all the other techniques. ©2009 IEEE.",Classification and Regression Tree (CART); Counter Propagation Neural Network (CPNN); Dynamic Evolving Neuro-Fuzzy Inference System (DENFIS); Genetic Programming (GP); Group Method of Data Handling (GMDH); Multilayer FeedForward Neural Network (MPFF); Multiple Linear Regression (MLR); Multivariate Adaptive Regression Splines (MARS); Polynomial regression; Radial Basis Function Neural Network (RBF); Support Vector Regression (SVR); Tree net,"Pahariya J.S., Ravi V., Carr M.",2009,Conference,"2009 World Congress on Nature and Biologically Inspired Computing, NABIC 2009 - Proceedings",10.1109/NABIC.2009.5393534,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949583526&doi=10.1109%2fNABIC.2009.5393534&partnerID=40&md5=4a19e3bb225311c901ce334aafc46e74,"Institute for Development and Research in Banking Technology, Castle Hills Road #1, Masab Tank, Hyderabad 500 057, AP, India",,English,,9781424456123
Scopus,Best practices in software measurement: How to use metrics to improve project and process performance,"Manfred Bundschuh is IT quality manager with AXA Service AG, Cologne, Germany as well as president of DASMA e.V., the German metrics organization. © Springer-Verlag Berlin Heidelberg 2005. All rights are reserved.",,"Ebert C., Dumke R., Bundschuh M., Schmietendorf A.",2007,Book,Best Practices in Software Measurement: How to use Metrics to Improve Project and Process Performance,10.1007/b138013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891983644&doi=10.1007%2fb138013&partnerID=40&md5=352d71865b4cb8b853216702499614e9,"Alcatel, 54 rue la Boetie, 75008 Paris, France; Otto-von-Guericke-Universität Magdeburg, Postfach 4120, 39016 Magdeburg, Germany; Fachbereich Informatik, Fachhochschule Köln, Am Sandberg 1, 51643 Gummersbach, Germany; T-Systems Nova, Postfach 652, 13509 Berlin, Germany",Springer Berlin Heidelberg,English,,3540208674; 9783540716488
Scopus,Improving effort estimation accuracy by weighted grey relational analysis during software development,"Grey relational analysis (GRA), a similarity-based method, presents acceptable prediction performance in software effort estimation. However, we found that conventional GRA methods only consider non-weighted conditions while predicting effort. Essentially, each feature of a project may have a different degree of relevance in the process of comparing similarity. In this paper, we propose six weighted methods, namely, non-weight, distance-based weight, correlative weight, linear weight, nonlinear weight, and maximal weight, to be integrated into GRA. Three public dataseis are used to evaluate the accuracy of the weighted GRA methods. Experimental results show that the weighted GRA performs better precision than the non-weighted GRA. Specifically, the linearly weighted GRA greatly improves accuracy compared with the other weighted methods. To sum up, the weighted GRA not only can improve the accuracy of prediction but is an alternative method to be applied to software development life cycle. © 2007 IEEE.",Grey relational analysis (GRA); Software development; Software effort estimation; Software project management; Weighted GRA,"Hsu C.-J., Huang C.-Y.",2007,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2007.63,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949178376&doi=10.1109%2fAPSEC.2007.63&partnerID=40&md5=51e5e63c0d975c96bb8139f5ac49d406,"Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",,English,15301362,0769530575; 9780769530574
Scopus,The use of a bayesian network for web effort estimation,"The objective of this paper is to describe the use of a probabilistic approach to Web effort estimation by means of a Bayesian Network. A Bayesian Network is a model that embodies existing knowledge of a complex domain in a way that supports reasoning with uncertainty. Given that the causal system relative to Web effort estimation has an inherently uncertain nature the use of Bayesian model seemed a reasonable choice. We used a cross-company data set of 150 industrial Web projects volunteered from Web companies worldwide, which are part of the Tukutuku database. Results showed that the effort estimates obtained using a Bayesian Network were sound and significantly superior to the prediction based on two benchmark models, using the mean and median effort respectively. © Springer-Verlag Berlin Heidelberg 2007.",Bayesian networks; Effort accuracy; Web cost estimation; Web effort estimation,Mendes E.,2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-73597-7_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38348998900&doi=10.1007%2f978-3-540-73597-7_8&partnerID=40&md5=fda9f55620e9683200f6ce9183d257b4,"University of Auckland, Private Bag 92019, Auckland, New Zealand",Springer Verlag,English,03029743,3540735968; 9783540735960
Scopus,A comparative study of attribute weighting heuristics for effort estimation by analogy,"Five heuristics for attribute weighting in analogy-based effort estimation are evaluated in this paper. The baseline heuristic involves using all attributes with equal weights. We propose four additional heuristics that use rough set analysis for attribute weighting. These five heuristics are evaluated over five data sets related to software projects. Three of the data sets are publicly available, hence allowing comparison with other methods. The results indicate that three of the rough set analysis based heuristics perform better than the equal weights heuristic. This evaluation is based on an integrated measure of accuracy. Copyright 2006 ACM.",Attribute selection and weighting; Estimation by analogy; Missing values; Non-quantitative attributes; Rough set analysis; Software effort estimation,"Li J., Ruhe G.",2006,Conference,ISESE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering,10.1145/1159733.1159746,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247326538&doi=10.1145%2f1159733.1159746&partnerID=40&md5=9c916e13f6e7418ef27298b076cd7cdf,"Software Engineering Decision Support Laboratory, University of Calgary, 2500 University Dr. NW, Calgary, Alta. T2N1N4, Canada",,English,,1595932186; 9781595932181
Scopus,Size and effort estimation for applications written in Java,"The paper presents a methodology for estimation of software size and effort at early stages of software development. The research concentrates on the size estimation problem, which seems to be the weakest element of cost estimation. The methodology concerns the object-oriented technology and the Java language. Authors review current techniques of size and cost estimation to identify their strengths and weaknesses. The paper describes statistical characteristics of class and method sizes for programs written in Java. The analysis of nearly one million lines of code lead to the conclusion that the average class size and the average method size is independent from application size. This feature is useful in calculating the final application size if the number of classes is known or could be estimated during software development. The paper contains definitions of three simple models of size estimation that are based on class and method sizes. The presented approach may be easily applied as it uses data that is typically produced during early stages of software development. The experimental model was theoretically verified and analysed. Further, an independent set of applications written in Java was used to verify the correctness of the acquired equations and models. Statistical characteristics were acquired and analysed with the use of a dedicated tool that was implemented as a part of the research. © 2004 Elsevier B.V. All rights reserved.",Java; Object oriented; Software size estimation,"Kaczmarek J., Kucharski M.",2004,Journal,Information and Software Technology,10.1016/j.infsof.2003.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942536614&doi=10.1016%2fj.infsof.2003.11.001&partnerID=40&md5=0ece4413911b65f585094a7413041262,"Department of Applied Informatics, Gdansk University of Technology, Narutowicza 11/12, 80-952 Gdansk, Poland",Elsevier,English,09505849,
Scopus,Value creation and capture: A model of the software development process,"Software development process ranges form exploration and drilling to data management and decision analysis. A project dynamics model use with market sensitivity and economic analysis results profitability. By establishing the contract, maximize business value to assess uncertainty.",,Little T.,2004,Journal,IEEE Software,10.1109/MS.2004.1293072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042624869&doi=10.1109%2fMS.2004.1293072&partnerID=40&md5=7d9d453b2519a93e51ee2670aee1904e,Landmark Graphics,,English,07407459,
Scopus,"Software metrics: A guide to planning, analysis, and application","The modern field of software metrics emerged from the computer modeling and ""statistical thinking"" services of the 1980s. As the field evolved, metrics programs were integrated with project management, and metrics grew to be a major tool in the managerial decision-making process of software companies. Now practitioners in the software industry have a reference that validates software metrics as a crucial tool for efficient and successful project management and execution. Software Metrics: A Guide to Planning, Analysis, and Application simplifies software measurement and explains its value as a pragmatic tool for management. Ideas and techniques presented in this book are derived from best practices. The ideas are field-proven, down to earth, and straightforward, making this volume an invaluable resource for those striving for process improvement. This overview helps readers enrich their knowledge of measurements and analysis, best practices, and how ordinary analysis techniques can be applied to achieve extraordinary results. Easy-to-understand tools and methods are applied to demonstrate how metrics create models that are indispensable to decision-making in the software industry. © 2003 by Taylor & Francis Group, LLC.",,Pandian C.R.,2003,Book,"Software Metrics: A Guide to Planning, Analysis, and Application",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055149349&partnerID=40&md5=19cf3cc1434d4d1e41d011c118c4d204,"Quality Improvements Consultants, Hyderabad, India",CRC Press,English,,9780203496077; 9780849316616
Scopus,On the sensitivity of COCOMO II software cost estimation model,"Software cost estimation techniques predict the amount of effort required to develop a software system. Cost estimates are needed throughout the software lifecycle to determine feasibility of software projects and to provide for appropriate allocation or reallocation of available resources. To assess the effect of imprecise evaluations, a comprehensive sensitivity analysis was performed on a major cost estimation model, COCOMO II. Results of this analysis are described and explicated in this paper. To reduce risk of drawing biased conclusions, three different methods for sensitivity analysis were employed: the mathematical analysis of the estimating equation, Monte Carlo simulation, and error propagation. The results of the first two methods are very consistent and confirm expected highest sensitivity of the model to the imprecision of the size estimate. Error propagation allows determination of the combined impact of imprecision in multiple inputs and it is therefore most valuable from the practical point of view. The results obtained by this technique also indicate very strong sensitivity to the imprecision in size estimates. A possible way to cope with imprecise information in software cost estimation is also indicated. © 2002 IEEE.",Computer science; Costs; Electrical capacitance tomography; Performance evaluation; Programming; Resource management; Sensitivity analysis; Software systems; Sun; Uncertainty,"Musilek P., Pedrycz W., Sun N., Succi G.",2002,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2002.1011321,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948472509&doi=10.1109%2fMETRIC.2002.1011321&partnerID=40&md5=86e021606e078708ff2e43245fe4f8b1,"Dept. of Electrical and Computer Engineering, University of Alberta, ECERF, Edmonton, AB  T6G 2V4, Canada; Faculty of Computer Science, Free University of Bozen, Dominikanerplatz 3, Bozen, I-39100, Italy",IEEE Computer Society,English,15301435,0769513395
Scopus,A structured methodology for software development effort prediction using the analytic hierarchy process,Predicting the effort needed for software development projects has frustrated many project managers. A primary reason is that most estimation measures fail to observe environmental characteristics that vary significantly depending on the organization involved in the software development. The objective of this article is to propose an improved method for software estimation using the analytic hierarchy process (AHP) to capture such environmental factors as well as technical aspects of software in the estimation process. Development efforts of a software project are factored into manageable components through a hierarchical structure. This methodology can provide a structured approach in which the organization's characteristics are integrated with the technical requirements of the software project. A small sample software project demonstrates the applicability of the AHP in software estimation. © 1993.,,Lee H.,1993,Journal,The Journal of Systems and Software,10.1016/0164-1212(93)90040-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027593606&doi=10.1016%2f0164-1212%2893%2990040-5&partnerID=40&md5=7b35d00a1161cdb88869cc7581671a31,"Information Systems and Quantitative Analysis Department, College of Business Administration, University of Nebraska, Omaha, NebraskaU.S.A.",,English,01641212,
Scopus,Models for undergraduate project courses in software engineering,"The software engineering course provides undergraduates with an opportunity to learn something about real-world software development. Since software engineering is far from being a mature engineering discipline, it is not possible to define a completely satisfactory syllabus. Content with a sound basis is in short supply, and the material most often taught is at high risk of becoming obsolete within a few years. Undergraduate software engineering courses are now offered in more than 100 universities. Although three textbooks dominate the market, there is not yet consensus on the scope and form of the course. The two major decisions an instructor faces are the balance between technical and management topics and the relation between the lecture and project components. We discuss these two decisions, with support from sample syllabi and survey data on course offerings in the US and Canada. We also offer some advice on the management of a project-oriented course. © Springer-Verlag Berlin Heidelberg 1991.",,"Shaw M., Tomayko J.E.",1991,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/bfb0024284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029739615&doi=10.1007%2fbfb0024284&partnerID=40&md5=b21d3060bcd241523a3cb41aade8b147,"School of Computer Science, Software Engineering Institute, Carnegie Mellon University, United States; Software Engineering Institute, Carnegie Mellon University, United States",Springer Verlag,English,03029743,9783540545026
Scopus,A Review of Software Cost Estimation in Agile Software Development Using Soft Computing Techniques,"For a successful software project, accurate prediction of its overall effort and cost estimation is a very much essential task. Software projects have evolved through a number of development models over the last few decades. Hence, to cover an accurate measurement of the effort and cost for different software projects based on different development models having new and innovative phases of software development, is a crucial task to be done. An accurate prediction always leads to a successful software project within the budget with no delay, but any percentage of misconduct in the overall effort and cost estimate may lead to a project failure in terms of delivery time, budget or features. Software industries have adopted various development models based on the project requirements and organization's capabilities. Due to adaptability to changes in a software project, agile software development model has become a much successful and popular framework for development over the last decade. The customer is involved as an active participant in the development using an agile framework. Hence, changes can occur at any phase of development and they can be dynamic in nature. That is why an accurate prediction of effort and cost of such projects is a crucial task to be done as the complexity of overall development structure is increased with the time. Soft computing techniques have proven that they are one of the best problem solving techniques in such scenarios. Such techniques are more flexible and presence of bio-intelligence increases their accuracy. Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Artificial Neural Network (ANN), Fuzzy Inference Systems (FIS), etc. are applied successfully for estimation of cost and effort of agile based software projects. This paper deals with such soft computing techniques and provides a detailed and analytical overview of such methods. It also provides the future scope and possibilities to explore such techniques on the basis of survey provided by this paper. © 2016 IEEE.",Agile software development; cost estimation; effort estimation; neural network; soft computing; software project management (SPM),"Bilgaiyan S., Mishra S., Das M.",2016,Conference,Proceedings - International Conference on Computational Intelligence and Networks,10.1109/CINE.2016.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032355649&doi=10.1109%2fCINE.2016.27&partnerID=40&md5=220a647c48eb156cf1787c7574743e76,"School of Computer Engineering, KIIT University, Bhubaneswar, Odisha, India",Institute of Electrical and Electronics Engineers Inc.,English,23755822,0769557457
Scopus,Source code size estimation approaches for object-oriented systems from UML class diagrams: A comparative study,"Background Source code size in terms of SLOC (source lines of code) is the input of many parametric software effort estimation models. However, it is unavailable at the early phase of software development. Objective We investigate the accuracy of early SLOC estimation approaches for an object-oriented system using the information collected from its UML class diagram available at the early software development phase. Method We use different modeling techniques to build the prediction models for investigating the accuracy of six types of metrics to estimate SLOC. The used techniques include linear models, non-linear models, rule/tree-based models, and instance-based models. The investigated metrics are class diagram metrics, predictive object points, object-oriented project size metric, fast&&serious class points, objective class points, and object-oriented function points. Results Based on 100 open-source Java systems, we find that the prediction model built using object-oriented project size metric and ordinary least square regression with a logarithmic transformation achieves the highest accuracy (mean MMRE = 0.19 and mean Pred(25) = 0.74). Conclusion We should use object-oriented project size metric and ordinary least square regression with a logarithmic transformation to build a simple, accurate, and comprehensible SLOC estimation model. © 2013 Elsevier B.V. All rights reserved.",Class diagrams; Code size; Estimation; Object-oriented; UML,"Zhou Y., Yang Y., Xu B., Leung H., Zhou X.",2014,Journal,Information and Software Technology,10.1016/j.infsof.2013.09.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889878709&doi=10.1016%2fj.infsof.2013.09.003&partnerID=40&md5=06b42275b46f285644110fc74fbb1105,"State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Engineering, Southeast University, China",Elsevier B.V.,English,09505849,
Scopus,A fuzzy classifier approach to estimating software quality,"With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers. © 2013 Elsevier Inc. All rights reserved.",Computational intelligence; Fuzzy logic; Pattern classification; Software engineering; Software metric,Pizzi N.J.,2013,Journal,Information Sciences,10.1016/j.ins.2013.04.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878223411&doi=10.1016%2fj.ins.2013.04.027&partnerID=40&md5=f0dc312d50802e2bd135c3ca4e8af691,"Department of Computer Science, University of Manitoba, Winnipeg, MB R3T 2N2, Canada",,English,00200255,
Scopus,Empirical evaluation of mixed-project defect prediction models,"Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects. On the other hand, recent studies try to utilize data across projects for building defect prediction models. We combine both approaches and investigate the effects of using mixed (i.e. within and cross) project data on defect prediction performance, which has not been addressed in previous studies. We conduct experiments to analyze models learned from mixed project data using ten proprietary projects from two different organizations. We observe that code metric based mixed project models yield only minor improvements in the prediction performance for a limited number of cases that are difficult to characterize. Based on existing studies and our results, we conclude that using cross project data for defect prediction is still an open challenge that should only be considered in environments where there is no local data collection activity, and using data from other projects in addition to a project's own data does not pay off in terms of performance. © 2011 IEEE.",cross project; defect prediction; mixed project; product metrics; within project,"Turhan B., Tosun A., Bener A.",2011,Conference,"Proceedings - 37th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2011",10.1109/SEAA.2011.59,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955237725&doi=10.1109%2fSEAA.2011.59&partnerID=40&md5=28f7538f4a71f34bb59eee0ec393645e,"Department of Information Processing Science, University of Oulu, 90014, Oulu, Finland; Department of Computer Engineering, Boǧaziçi University, 34342, Istanbul, Turkey; Ted Rogers School of ITM, Ryerson University, Toronto, ON M5B-2K3, Canada",,English,,9780769544885
Scopus,Building an expert-based web effort estimation model using Bayesian networks,"OBJECTIVE – The objective of this paper is to describe a case study where Bayesian Networks (BNs) were used to construct an expert-based Web effort model. METHOD – We built a single-company BN model solely elicited from expert knowledge, where the domain expert was an experienced Web project manager from a small Web company in Auckland, New Zealand. This model was validated using data from eight past finished Web projects. RESULTS – The BN model has to date been successfully used to estimate effort for four Web projects, providing effort estimates superior to those based solely on expert opinion. CONCLUSIONS – Our results suggest that, at least for the Web Company that participated in this case study, the use of a model that allows the representation of uncertainty, inherent in effort estimation, can outperform expert-based estimates. Another five companies have also benefited from using Bayesian Networks, with very promising results. © 2009 BCS Learning and Development Ltd. All rights reserved.",Bayesian networks; Expert-based elicitation; Single-company model; Web effort estimation,"Mendes E., Pollino C., Mosley N.",2009,Conference,"13th International Conference on Evaluation and Assessment in Software Engineering, EASE 2009",10.14236/ewic/ease2009.5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087228576&doi=10.14236%2fewic%2fease2009.5&partnerID=40&md5=ae75ad7a16481ffac32419982624b688,"Computer Science department, University of Auckland, Private Bag, Auckland, 92019, New Zealand; Fenner School of Environment and Society, Australian National University, Canberra, Australia; MetriQ Ltd. (Research Division), P.O. Box 837, Oneroa, Auckland, New Zealand",BCS Learning and Development Ltd.,English,,
Scopus,Incorporating vital factors in agile estimation through algorithmic method,"Agile methods have become the mainstream of software development due to their enriched practices. Some commonly used practices include collaborative development, meeting evolving requirements with working software, simple design etc. These methods address the problem of volatile requirements by applying above practices. Thus, these practices reduce cost of change at later stage of software development. These methods do not hold big upfront for early estimation of size, cost and duration due to uncertainty in requirements. It has been observed that agile methods mostly rely on an expert opinion and historical data of project for estimation of cost, size and duration. It has been observed that these methods do not consider the vital factors affecting the cost, size and duration of project for estimation. In absence of historical data and experts, existing agile estimation methods such as analogy, planning poker become unpredictable. Therefore, there is a strong need to devise simple algorithmic method that incorporates the factors affecting the cost, size and duration of project. It also provides the basis for inexperienced practitioners to estimate more precisely. In this paper, we are presenting the study of both traditional and agile estimation methods with equivalence of terms and differences. We investigated some vital factors affecting the estimation of an agile project with scaling factor of low, medium and high. Also, an algorithm Constructive Agile Estimation Algorithm (CAEA) is proposed for incorporating vital factors. © 2009 Technomathematics Research Foundation.",Agile cost estimation; Function points; New object points; Object points; Story points; Traditional cost estimation; Velocity and Ideal time,"Bhalerao S., Ingle M.",2009,Journal,International Journal of Computer Science and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65549122110&partnerID=40&md5=94d8aa149b7be961b4eb72a2aeb78a36,"School of Computer Science, D.A.V.V., Khandwa Road, Indore, Madya Pradesh, India",Technomathematics Research Foundation,English,09729038,
Scopus,Theoretical maximum prediction accuracy for analogy-based software cost estimation,"Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignore the importance of the dataset quality. What is more, they implicitly assume the prediction model to be able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models only provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. It is therefore important to realize the theoretical maximum prediction accuracy (TMPA) for the given model with a given dataset. In this paper, we first discuss the practical importance of this notion, and propose a novel method for the determination of TMPA in the application of analogy-based software cost estimation. Specifically, we determine the TMPA of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models.","Software metrics and measurement, Software cost estimation, Analogy, K-NN, MMRE",Keung J.W.,2008,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650547019&partnerID=40&md5=bf57893cdb475b7e7c6174b9985b5165,"NICTA Ltd., Sydney, NSW, Australia; CSE, UNSW, Sydney, NSW, Australia",,English,15301362,9780769534466
Scopus,Software project similarity measurement based on fuzzy C-means,"A reliable and accurate similarity measurement between two software projects has always been a challenge for analogy-based software cost estimation. Since the effort for a new project is retrieved from similar historical projects, it is essentially to use the appropriate similarity measure that finds those close projects which in turn increases the estimation accuracy. In software engineering literature, there is a relatively little research addressed the issue of how to find out similarity between two software projects when they are described by numerical and categorical features. Despite simplicity of exiting similarity techniques such as: Euclidean distance, weighted Euclidean distance and maximum distance, it is hard to deal with categorical features. In this paper we present two approaches to measure similarity between two software projects based on fuzzy C-means clustering and fuzzy logic. The new approaches are suitable for both numerical and categorical features. © 2008 Springer-Verlag Berlin Heidelberg.",Fuzzy C-means; Fuzzy logic; Software project similarity,"Azzeh M., Neagu D., Cowling P.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-79588-9_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649116645&doi=10.1007%2f978-3-540-79588-9_12&partnerID=40&md5=4e7d4074c8bca4b060c2143b2ddb3b17,"Department of Computing, University of Bradford, Bradford, BD7 1DP, United Kingdom",,English,03029743,3540795871; 9783540795872
Scopus,Decision support analysis for software effort estimation by analogy,"Effort estimation by analogy (EBA) is an established method for software effort estimation. For this paper, we understand EBA as a meta-method which needs to be instantiated and customized at different stages and decision points regarding a specific context. Some example decision problems are related to the selection of the similarity measures, the selection of analogs for adaptation or the weighting and selection of attributes. This paper proposes a decision-centric process model for EBA by generalizing the existing EBA methods. Typical decision-making problems are identified at different stages of the process as part of the model. Some existing solution alternatives of the decision-making problems are then studied. The results of the decision support analysis can be used for better understanding of EBA related techniques and for providing guidelines for implementation and customization of general EBA. An example case of the process model is finally presented. © 2007 IEEE.",,"Li J., Ruhe G.",2007,Conference,"Proceedings - ICSE 2007 Workshops: Third International Workshop on Predictor Models in Software Engineering, PROMISE'07",10.1109/PROMISE.2007.5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949037149&doi=10.1109%2fPROMISE.2007.5&partnerID=40&md5=e22eace73048fcd27f8c862d7fc0d6fa,"Software Engineering Decision Support Laboratory, University of Calgary, Calgary Alta. T2N1N4, Canada",,English,,0769529542; 9780769529547
Scopus,"A model-driven measurement procedure for sizing web applications: Design, automation and validation","This paper introduces the Object-Oriented Hypermedia Function Points (OO-HFP), which is a functional size measurement procedure for Web projects developed using the Object-Oriented Hypermedia (OO-H) method. This method provides model-driven and transformation-based support for the development of Web applications. Using OO-HFP, a size measure is obtained once a Web application's conceptual model is completed. We follow the steps of a process model for software measurement in order to detail the design and automation of OO-HFP. Finally, we present the validation of OO-HFP for Web effort estimation by comparing the prediction accuracy that it provides to the accuracy provided by another set of validated size measures (the Tukutuku measures) that was found to be a good effort predictor. The results of a study using industrial data show that the effort estimates obtained for projects that are sized using OO-HFP were similar to those using the Tukutuku measures, thus suggesting that the OO-HFP is a suitable effort predictor. © Springer-Verlag Berlin Heidelberg 2007.",Functional size measurement; Model-driven development; OO-H; Web effort estimation; Web engineering,"Abrahão S., Mendes E., Gomez J., Insfran E.",2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-75209-7_32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049008392&doi=10.1007%2f978-3-540-75209-7_32&partnerID=40&md5=89039abbdb98d50fec38b90e9043f415,"Department of Computer Science and Computation, Valencia University of Technology, Camino de Vera, s/n, 46022, Valencia, Spain; Department of Computer Science, University of Auckland, Private Bag 92019, Auckland, New Zealand; Department of Information Systems and Languages, University of Alicante, Campus de San Vicente del Raspeig, Apartado 99, 03080 Alicante, Spain",Springer Verlag,English,03029743,9783540752080
Scopus,Using data envelopment analysis in software development productivity measurement,"The ever-increasing size and complexity of software systems make the cost of developing and maintaining software important. Unfortunately, the process of software production has not been particularly well understood. This article helps clarify the relationship between postimplementation function points (FP) and the corresponding development effort for software development projects in a large Canadian bank, knowledge of this relationship enables evaluations of the productivity of completed projects and, in particular, provides a predictive tool for future projects. The empirical analysis employs a combination of traditional regression models and Data Envelopment Analysis (DEA). The regression analyses show a log-linear relationship between project size and development effort, which is subsequently used in the DEA models. The DEA models identify best performers and use these as benchmarks, but are not limited to the constant returns to scale assumption of the regression analyses and are capable of including the delivery time as a nondiscretionary input. Finally, by including data from the International Software Benchmarking Standards Group (ISBSG) repository in the DEA models, the bank's projects are benchmarked not only against its own best performers but also against what is globally feasible. Copyright © 2006 John Wiley & Sons, Ltd.",Bank; Data envelopment analysis (DEA); Development effort; Function points; Productivity; Software development,"Asmild M., Paradi J.C., Kulkarni A.",2006,Journal,Software Process Improvement and Practice,10.1002/spip.298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845952862&doi=10.1002%2fspip.298&partnerID=40&md5=a3f9425bafa1fa8fd1f3555402581f32,"Nottingham University, Business School, Jubilee Campus, Wollaton Road, Nottingham NG8 1BB, United Kingdom; Centre for Management of Technology and Entrepreneurship, University of Toronto, Canada; Centre for Management of Technology and Entrepreneurship, Department of Chemical Engineering and Applied Chemistry, University of Toronto, 200 College Street, Toronto, Ont. M5S 3E5, Canada",,English,10774866,
Scopus,Determining inspection cost-effectiveness by combining project data and expert opinion,"There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed. © 2005 IEEE.",Case study; Cost-effectiveness model; Expert opinion; Monte Carlo simulation; Software inspection,"Freimut B., Briand L.C., Vollei F.",2005,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2005.136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947131690&doi=10.1109%2fTSE.2005.136&partnerID=40&md5=bbd171e8da428e468260636c0f4dd0d6,"IEEE, Norway; Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, D-67663 Kaiserslautern, Germany; Simula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway; Siemens AG, Rupert-Mayer-Str. 44, D-81359 Munich, Germany",,English,00985589,
Scopus,Preliminary data analysis methods in software estimation,"Software is quite often expensive to develop and can become a major cost factor in corporate information systems' budgets. With the variability of software characteristics and the continual emergence of new technologies the accurate prediction of software development costs is a critical problem within the project management context. In order to address this issue a large number of software cost prediction models have been proposed. Each model succeeds to some extent but they all encounter the same problem, i.e., the inconsistency and inadequacy of the historical data sets. Often a preliminary data analysis has not been performed and it is possible for the data to contain non-dominated or confounded variables. Moreover, some of the project attributes or their values are inappropriately out of date, for example the type of computer used for project development in the COCOMO 81 (Boehm, 1981) data set. This paper proposes a framework composed of a set of clearly identified steps that should be performed before a data set is used within a cost estimation model. This framework is based closely on a paradigm proposed by Maxwell (2002). Briefly, the framework applies a set of statistical approaches, that includes correlation coefficient analysis, Analysis of Variance and Chi-Square test, etc., to the data set in order to remove outliers and identify dominant variables. To ground the framework within a practical context the procedure is used to analyze the ISBSG (International Software Benchmarking Standards Group data-Release 8) data set. This is a frequently used accessible data collection containing information for 2,008 software projects. As a consequence of this analysis, 6 explanatory variables are extracted and evaluated. © 2005 Springer Science + Business Media, Inc.",Data analysis in software engineering; Software cost estimating models,"Liu Q., Mintram R.C.",2005,Conference,Software Quality Journal,10.1007/s11219-004-5262-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444422896&doi=10.1007%2fs11219-004-5262-y&partnerID=40&md5=86b6359ea1a14fd430f8d69e05b7d046,"School of Informatics, University of Northumbria, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Do Adaptation Rules Improve Web Cost Estimation?,"Analogy-based estimation has, over the last 15 years, and particularly over the last 7 years, emerged as a promising approach with comparable accuracy to, or better than, algorithmic methods in some studies. In addition, it is potentially easier to understand and apply; these two important factors can contribute to the successful adoption of estimation methods within Web development companies. We believe therefore, analogy-based estimation should be examined further. This paper compares several methods of analogy-based effort estimation. In particular, it investigates the use of adaptation rules as a contributing factor to better estimation accuracy. Two datasets are used in the analysis; results show that the best predictions are obtained for the dataset that first, presents a continuous ""cost"" function, translated as a strong linear relationship between size and effort, and second, is more ""unspoiled"" in terms of outliers and collinearity. Only one of the two types of adaptation rules employed generated good predictions.",Case-based reasoning; Prediction models; Web effort prediction; Web hypermedia; Web hypermedia metrics,"Mendes E., Mosley N., Counsell S.",2003,Conference,Proceedings of the ACM Conference on Hypertext,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142305197&partnerID=40&md5=7c78180827ce0d24e92b87c555b190ce,"University of Auckland, Private bag 92019, Auckland, New Zealand; Okki Software, P.O. Box 3139, Auckland, New Zealand; University of London, Mallet Street, London WC1E 7HX, United Kingdom",,English,,
Scopus,Prioritizing and assessing software project success factors and project characteristics using subjective data,"This paper presents a method for analyzing the impact software project factors have on project success as defined by project success factors that have been prioritized. It is relatively easy to collect measures of project attributes subjectively (i.e., based on expert judgment). Often Likert scales are used for that purpose. It is much harder to identify whether and how a large number of such ranked project factors influence project success, and to prioritize their influence on project success. At the same time, it is desirable to use the knowledge of project personnel effectively. Given a prioritization of project goals, it is shown how some key project characteristics can be related to project success. The method is applied in a case study consisting of 46 projects. For each project, six success factors and 27 project attributes were measured. Successful projects show common characteristics. Using this knowledge can lead to better control and software project management and to an increased likelihood of project success.",Project assessment; Project success; Subjective measures,"Wohlin C., Andrews A.A.",2003,Journal,Empirical Software Engineering,10.1023/A:1024476828183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042767588&doi=10.1023%2fA%3a1024476828183&partnerID=40&md5=3e2254da9a9f5ef4d58f1b06b6309755,"Dept. Software Eng. and Comp. Sci., Soft Center, Blekinge Institute of Technology, Box 520, SE-372 25 Ronneby, Sweden; School of Elec. Eng. and Comp. Sci., Washington State University, Pullman, WA 99164-2752, United States",,English,13823256,
Scopus,Subjective evaluation as a tool for learning from software project success,"This paper presents a method for using subjective factors to evaluate project success. The method is based on collection of subjective measures with respect to project characteristics and project success indicators. The paper introduces a new classification scheme for assessing software projects. Further, it is illustrated how the method may be used to predict software success using subjective measures of project characteristics. The classification scheme is illustrated in two case studies. The results are positive and encouraging for future development of the approach.",,"Wohlin C., Mayrhauser A.V., Höst M., Regnell B.",2000,Journal,Information and Software Technology,10.1016/S0950-5849(00)00150-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034319946&doi=10.1016%2fS0950-5849%2800%2900150-6&partnerID=40&md5=4dcc72d7a37c820566ef9de5b7d1f7f9,"Department of Communication Systems, Lund Inst. Technol., Lund Univ., P., Lund, Sweden; Computer Science Department, Colorado State University, 80523, Fort Collins, CO, United States","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,Systematic mapping study of ensemble effort estimation,"Ensemble methods have been used recently for prediction in data mining area in order to overcome the weaknesses of single estimation techniques. This approach consists on combining more than one single technique to predict a dependent variable and has attracted the attention of the software development effort estimation (SDEE) community. An ensemble effort estimation (EEE) technique combines several existing single/classical models. In this study, a systematic mapping study was carried out to identify the papers based on EEE techniques published in the period 2000-2015 and classified them according to five classification criteria: research type, research approach, EEE type, single models used to construct EEE techniques, and rule used the combine single estimates into an EEE technique. Publication channels and trends were also identified. Within the 16 studies selected, homogeneous EEE techniques were the most investigated. Furthermore, the machine learning single models were the most frequently employed to construct EEE techniques and two types of combiner (linear and non-linear) have been used to get the prediction value of an ensemble. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Ensemble effort estimation; Software development effort estimation; Systematic mapping study,"Idri A., Hosni M., Abran A.",2016,Conference,ENASE 2016 - Proceedings of the 11th International Conference on Evaluation of Novel Software Approaches to Software Engineering,10.5220/0005822701320139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979496940&doi=10.5220%2f0005822701320139&partnerID=40&md5=1f4747d6245cf107096850d8025b5af5,"Software Project Management Research Team, ENSIAS, Mohammed V University in Rabat, Rabat, Morocco; Department of Software Engineering, ETS, Montréal, H3C IK3, Canada",SciTePress,English,,9789897581892
Scopus,Analogy-based effort estimation: A new method to discover set of analogies from dataset characteristics,"Analogy-based effort estimation (ABE) is one of the efficient methods for software effort estimation because of its outstanding performance and capability of handling noisy datasets. Conventional ABE models usually use the same number of analogies for all projects in the datasets in order to make good estimates. The authors' claim is that using same number of analogies may produce overall best performance for the whole dataset but not necessarily best performance for each individual project. Therefore there is a need to better understand the dataset characteristics in order to discover the optimum set of analogies for each project rather than using a static k nearest projects. The authors propose a new technique based on bisecting k-medoids clustering algorithm to come up with the best set of analogies for each individual project before making the prediction. With bisecting k-medoids it is possible to better understand the dataset characteristic, and automatically find best set of analogies for each test project. Performance figures of the proposed estimation method are promising and better than those of other regular ABE models. © The Institution of Engineering and Technology 2015.",,"Azzeh M., Nassif A.B.",2015,Journal,IET Software,10.1049/iet-sen.2013.0165,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928012146&doi=10.1049%2fiet-sen.2013.0165&partnerID=40&md5=d5f8010892264a0bcdd1be5309fe59de,"Department of Software Engineering, Applied Science University, POBOX 166, Amman, Jordan; Department of Computer Science, University of Western Ontario, London, ON  N6A 5B9, Canada",Institution of Engineering and Technology,English,17518806,
Scopus,Empirical study of homogeneous and heterogeneous ensemble models for software development effort estimation,"Accurate estimation of software development effort is essential for effective management and control of software development projects. Many software effort estimation methods have been proposed in the literature including computational intelligence models. However, none of the existing models proved to be suitable under all circumstances; that is, their performance varies from one dataset to another. The goal of an ensemble model is to manage each of its individual models' strengths and weaknesses automatically, leading to the best possible decision being taken overall. In this paper, we have developed different homogeneous and heterogeneous ensembles of optimized hybrid computational intelligence models for software development effort estimation. Different linear and nonlinear combiners have been used to combine the base hybrid learners. We have conducted an empirical study to evaluate and compare the performance of these ensembles using five popular datasets. The results confirm that individual models are not reliable as their performance is inconsistent and unstable across different datasets. Although none of the ensemble models was consistently the best, many of them were frequently among the best models for each dataset. The homogeneous ensemble of support vector regression (SVR), with the nonlinear combiner adaptive neurofuzzy inference systems-subtractive clustering (ANFIS-SC), was the best model when considering the average rank of each model across the five datasets. © 2013 Mahmoud O. Elish et al.",,"Elish M.O., Helmy T., Hussain M.I.",2013,Journal,Mathematical Problems in Engineering,10.1155/2013/312067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880863420&doi=10.1155%2f2013%2f312067&partnerID=40&md5=ca42a910a758bc330478877eabdc0c3c,"Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia; College of Engineering, Tanta University, Tanta, Egypt",,English,1024123X,
Scopus,Web effort estimation: The value of cross-company data set compared to single-company data set,"This study investigates to what extent Web effort estimation models built using cross-company data sets can provide suitable effort estimates for Web projects belonging to another company, when compared to Web effort estimates obtained using that company's own data on their past projects (single-company data set). It extends a previous study (S3) where these same research questions were investigated using data on 67 Web projects from the Tukutuku database. Since S3 was carried out, data on other 128 Web projects was added to Tukutuku; therefore this study uses the entire set of 195 projects from the Tukutuku database, which now also includes new data from other single-company data sets. Predictions between cross-company and singlecompany models are compared using Manual Stepwise Regression+Linear Regression and Case-Based Reasoning. In addition, we also investigated to what extent applying a filtering mechanism to cross-company datasets prior to building prediction models can affect the accuracy of the effort estimates they provide. The present study corroborates the conclusions of S3 since the cross-company models provided much worse predictions than the single-company models. Moreover, the use of the filtering mechanism significantly improved the prediction accuracy of cross-company models when estimating singlecompany projects, making it comparable to that using singlecompany datasets. Copyright © 2012 ACM.",Case-based reasoning; Cost estimation; Cross-company effort model; Effort estimation; Filtering mechanism; Single-company effort model; Stepwise regression; Web projects,"Ferrucci F., Sarro F., Mendes E.",2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867703659&doi=10.1145%2f2365324.2365330&partnerID=40&md5=ea1a26076f6603ca8dffbee099870e11,"University of Salerno, via Ponte Don Melillo, Fisciano (SA), Italy; Zayed University Dubai Campus, P.O. Box 19282, Dubai, UAE, United Arab Emirates",,English,,9781450312417
Scopus,Investigating intentional distortions in software cost estimation - An exploratory study,"Cost estimation of software projects is an important activity that continues to be a source of problems for practitioners despite improvement efforts. Most of the research on estimation has focused on methodological issues while the research focused on human factors primarily has targeted cognitive biases or perceived inhibitors. This paper focuses on the complex organizational context of estimation and investigates whether estimates may be distorted, i.e. intentionally changed for reasons beyond legitimate changes due to changing prerequisites such as requirements or scope. An exploratory study was conducted with 15 interviewees at six large companies that develop software-intensive products. The interviewees represent five stakeholder roles in estimation, with a majority being project or line managers. Document analysis was used to complement the interviews and provided additional context. The results show that both estimate increase and estimate decrease exist and that some of these changes can be explained as intentional distortions. The direction of the distortion depends on the context and the stakeholders involved. The paper underlines that it is critical to consider also human and organizational factors when addressing estimation problems and that intentional estimate distortions should be given more and direct attention. © 2012 Elsevier Inc. All rights reserved.",Cost estimation; Distortion; Empirical study; Estimation inaccuracy; Human factors; Organizational factors; Organizational politics; Software engineering,"Magazinius A., Börjesson S., Feldt R.",2012,Journal,Journal of Systems and Software,10.1016/j.jss.2012.03.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861341260&doi=10.1016%2fj.jss.2012.03.026&partnerID=40&md5=821ed1badc262068705dc863756c1d93,"Department of Computer Science and Engineering, Chalmers University of Technology, Sweden; Department of Technology Management and Economics, Chalmers University of Technology, Sweden",,English,01641212,
Scopus,A controlled experiment in assessing and estimating software maintenance tasks,"Context: Software maintenance is an important software engineering activity that has been reported to account for the majority of the software total cost. Thus, understanding the factors that influence the cost of software maintenance tasks helps maintainers to make informed decisions about their work. Objective: This paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The objective of the study is to assess the maintenance size, effort, and effort distributions of three different maintenance types and to describe estimation models to predict the programmer's effort spent on maintenance tasks. Method: Twenty-three graduate students and a senior majoring in computer science participated in the experiment. Each student was asked to perform maintenance tasks required for one of the three task groups. The impact of different LOC metrics on maintenance effort was also evaluated by fitting the data collected into four estimation models. Results: The results indicate that corrective maintenance is much less productive than enhancive and reductive maintenance and program comprehension activities require as much as 50% of the total effort in corrective maintenance. Moreover, the best software effort model can estimate the time of 79% of the programmers with the error of or less than 30%. Conclusion: Our study suggests that the LOC added, modified, and deleted metrics are good predictors for estimating the cost of software maintenance. Effort estimation models for maintenance work may use the LOC added, modified, deleted metrics as the independent parameters instead of the simple sum of the three. Another implication is that reducing business rules of the software requires a sizable proportion of the software maintenance effort. Finally, the differences in effort distribution among the maintenance types suggest that assigning maintenance tasks properly is important to effectively and efficiently utilize human resources. © 2010 Elsevier B.V. All rights reserved.",COCOMO; Maintenance experiment; Maintenance size; Software estimation; Software maintenance,"Nguyen V., Boehm B., Danphitsanuphan P.",2011,Journal,Information and Software Technology,10.1016/j.infsof.2010.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953722810&doi=10.1016%2fj.infsof.2010.11.003&partnerID=40&md5=be2189da35208e9fac8f5e3cc6505e49,"Computer Science Department, University of Southern California, Los Angeles, United States; Computer Science Department, King Mongkut's University of Technology North Bangkok, Bangkok, Thailand",,English,09505849,
Scopus,Size estimation of cloud migration projects with Cloud Migration Point (CMP),"One major obstacle to enterprise adoption of cloud technologies has been the lack of visibility into migration effort and cost. In this paper, we present a methodology, called Cloud Migration Point (CMP), for estimating the size of cloud migration projects, by recasting a well-known software size estimation model called Function Point (FP) into the context of cloud migration. We empirically evaluate our CMP model by performing a cross-validation on six different small-scale cloud migration projects and show that our size estimation model can be used as a reliable predictor for effort estimation. Furthermore, we prove that our CMP model satisfies the fundamental properties of a software size measure. © 2011 IEEE.",Cloud computing; Effort estimation; Empirical validation; Migration; Size measures; Theoretical validation,"Tran V.T.K., Lee K., Fekete A., Liu A., Keung J.",2011,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/esem.2011.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863347280&doi=10.1109%2fesem.2011.35&partnerID=40&md5=960b6fdd0f69cdad15c2b445ae1c9814,"National ICT Australia Ltd., Australia; School of Computer Science and Engineering, University of New South Wales, Australia; School of Information Technologies, University of Sydney, Australia; Hong Kong Polytechnic University, Hong Kong, Hong Kong",IEEE Computer Society,English,19493770,
Scopus,Investigating Tabu search for web effort estimation,"Tabu Search is a meta-heuristic approach successfully used to address optimization problems in several contexts. This paper reports the results of an empirical study carried out to investigate the effectiveness of Tabu Search in estimating Web application development effort. The dataset employed in this investigation is part of the Tukutuku database. This database has been used in several studies to assess the effectiveness of various effort estimation techniques, such as Linear Regression and Case-Based Reasoning. Our results are encouraging given that Tabu Search outperformed all the other estimation techniques against which it has been compared. © 2010 IEEE.",,"Ferrucci F., Gravino C., Oliveto R., Sarro F., Mendes E.",2010,Conference,"Proceedings - 36th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2010",10.1109/SEAA.2010.59,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449268614&doi=10.1109%2fSEAA.2010.59&partnerID=40&md5=887fdaab09cb914ac0d2703370c36910,"University of Salerno, Via Ponte don Melillo, 84084s Fisciano (SA), Italy; University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,9780769541709
Scopus,Introducing the evaluation of complexity in functional size measurement: A UML-based approach,"Functional Size Measures are often used for effort estimation. However, these measures do not take into account the amount and complexity of elaboration required, concentrating instead on the amount of data accessed or moved. Methods for measuring the functional complexity have been proposed, but, being based on the textual description of requirements, are not very easy to apply. In this paper we show that measurement-oriented UML modeling can support the measurement of both functional size and functional complexity. We show, by means of a case study, that it is reasonably easy to derive different types of functional size measures, as well as complexity measures, from UML models. We show also that it is possible to build models for effort estimation that use the functional size and complexity measures as independent variables. © 2010 ACM.",COSMIC function points; effort estimation; function points; functional complexity measurement; functional size measurement; UML-based measurement; use case-based measurement,"Lavazza L., Robiolo G.",2010,Conference,ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1852786.1852820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149257332&doi=10.1145%2f1852786.1852820&partnerID=40&md5=2d72f0403a46973e56d36567ac7d1c7d,"Dipartimento di Informatica e Comunicazione, Università degli Studi dell'Insubria, Via Mazzini, 5, 21100 Varese, Italy; Facultad de Ingeniería, Universidad Austral, Av. Juan de Garay 125, 1063 Bs. As., Argentina",,English,,9781450300391
Scopus,Balancing uncertainty of context in ERP project estimation: An approach and a case study,"The increasing demand for Enterprise Resource Planning (ERP) solutions as well as the high rates of troubled ERP implementations and outright cancellations calls for developing effort estimation practices to systematically deal with uncertainties in ERP projects. This paper describes an approach-and a case study-to balancing uncertainties of context in the very early project stages, when an ERP adopter initiates a request-for-proposal process and when alternative bids are to be compared for the purpose of choosing an implementation partner. The proposed empirical approach leverages the complementary application of three techniques, an algorithmic estimation model, Monte Carlo simulation, and portfolio management. Our case study findings show how the ability of our approach to model uncertainty allows practitioners to address the challenging question of how to adjust project context factors so that chances of project success are increased. We also include a discussion on the implications of our approach for practice as well as on the possible validity threats and what the practitioner could do to counterpart them. Copyright © 2010 John Wiley & Sons, Ltd.",COCOMO; Enterprise resource planning implementation; Monte Carlo simulation; Portfolio management; Project effort estimation,Daneva M.,2010,Conference,Journal of Software Maintenance and Evolution,10.1002/smr.466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955438327&doi=10.1002%2fsmr.466&partnerID=40&md5=c1d19241b20b325e4d06ad23ed94cd1b,"Computer Science Department, University of Twente, Drienerlolaan 5, 7500 Enschede, Netherlands",,English,1532060X,
Scopus,Studying the impact of uncertainty in operational release planning - An integrated method and its initial evaluation,"Context: Uncertainty is an unavoidable issue in software engineering and an important area of investigation. This paper studies the impact of uncertainty on total duration (i.e., make-span) for implementing all features in operational release planning. Objective: The uncertainty factors under investigation are: (1) the number of new features arriving during release construction, (2) the estimated effort needed to implement features, (3) the availability of developers, and (4) the productivity of developers. Method: An integrated method is presented combining Monte-Carlo simulation (to model uncertainty in the operational release planning (ORP) process) with process simulation (to model the ORP process steps and their dependencies as well as an associated optimization heuristic representing an organization-specific staffing policy for make-span minimization). The method allows for evaluating the impact of uncertainty on make-span. The impact of uncertainty factors both in isolation and in combination are studied in three different pessimism levels through comparison with a baseline plan. Initial evaluation of the method is done by an explorative case study at Chartwell Technology Inc. to demonstrate its applicability and its usefulness. Results: The impact of uncertainty on release make-span increases - both in terms of magnitude and variance - with an increase of pessimism level as well as with an increase of the number of uncertainty factors. Among the four uncertainty factors, we found that the strongest impact stems from the number of new features arriving during release construction. We have also demonstrated that for any combination of uncertainty factors their combined (i.e., simultaneous) impact is bigger than the addition of their individual impacts. Conclusion: The added value of the presented method is that managers are able to study the impact of uncertainty on existing (i.e., baseline) operational release plans pro-actively. © 2009 Elsevier B.V. All rights reserved.",Discrete-event simulation; Explorative case study; Heuristic optimization; Impact analysis; Operational release planning; Uncertainty,"Al-Emran A., Kapur P., Pfahl D., Ruhe G.",2010,Journal,Information and Software Technology,10.1016/j.infsof.2009.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049128239&doi=10.1016%2fj.infsof.2009.11.003&partnerID=40&md5=28add461814d6353678710c12b2b2ca6,"Department of Electrical and Computer Engineering, Schulich School of Engineering, University of Calgary, 2500 University of Calgary NW, Calgary, AB T2N 1N4, Canada; Chartwell Technology Inc., Suite 400, 750, 11th ST SW, Calgary, AB T2P 3N7, Canada; Department of Informatics, University of Oslo, P.O. Box 1080 Blindern, 0316 Oslo, Norway; Simula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway; Department of Computer Science, University of Calgary, 2500 University of Calgary NW, Calgary, AB T2N 1N4, Canada",,English,09505849,
Scopus,Existing model metrics and relations to model quality,"This paper presents quality goals for models and provides a state-of-the-art analysis regarding model metrics. While model-based software development often requires assessing the quality of models at different abstraction and precision levels and developed for multiple purposes, existing work on model metrics do not reflect this need. Model size metrics are descriptive and may be used for comparing models but their relation to model quality is not well-defined. Code metrics are proposed to be applied on models for evaluating design quality while metrics related to other quality goals are few. Models often consist of a significant amount of elements, which allows a large amount of metrics to be defined on them. However, identifying useful model metrics, linking them to model quality goals, providing some baseline for interpretation of data, and combining metrics with other evaluation models such as inspections requires more theoretical and empirical work. © 2009 IEEE.",,"Mohagheghi P., Dehlen V.",2009,Conference,Proceedings - International Conference on Software Engineering,10.1109/WOSQ.2009.5071555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049455153&doi=10.1109%2fWOSQ.2009.5071555&partnerID=40&md5=5202660e451c844258cccea9381a0732,"SINTEF, Blindern, N-0314 Oslo, Norway",,English,02705257,9781424437238
Scopus,Identifying exogenous drivers and evolutionary stages in FLOSS projects,"The success of a Free/Libre/Open Source Software (FLOSS) project has been evaluated in the past through the number of commits made to its configuration management system, number of developers and number of users. Most studies, based on a popular FLOSS repository (SourceForge), have concluded that the vast majority of projects are failures. This study's empirical results confirm and expand conclusions from an earlier and more limited work. Not only do projects from different repositories display different process and product characteristics, but a more general pattern can be observed. Projects may be considered as early inceptors in highly visible repositories, or as established projects within desktop-wide projects, or finally as structured parts of FLOSS distributions. These three possibilities are formalized into a framework of transitions between repositories. The framework developed here provides a wider context in which results from FLOSS repository mining can be more effectively presented. Researchers can draw different conclusions based on the overall characteristics studied about an Open Source software project's potential for success, depending on the repository that they mine. These results also provide guidance to OSS developers when choosing where to host their project and how to distribute it to maximize its evolutionary success. © 2008 Elsevier Inc. All rights reserved.",Open Source software; Software evolution; Software repositories,"Beecher K., Capiluppi A., Boldyreff C.",2009,Journal,Journal of Systems and Software,10.1016/j.jss.2008.10.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62849106624&doi=10.1016%2fj.jss.2008.10.026&partnerID=40&md5=6967bf026f0c4a6c0a2da1448c320677,"Centre of Research on Open Source Software - CROSS, Department of Computing and Informatics, University of Lincoln, United Kingdom",,English,01641212,
Scopus,Software cost estimation models using radial basis function neural networks,"Radial Basis Function Neural Networks (RBFN) have been recently studied due to their qualification as an universal function approximation. This paper investigates the use of RBF neural networks for software cost estimation. The focus of this study is on the design of these networks, especially their middle layer composed of receptive fields, using two clustering techniques: the C-means and the APC-III algorithms. A comparison between a RBFN using C-means and a RBFN using APC-III, in terms of estimates accuracy, is hence presented. This study uses the COCOMO'81 dataset and data on Web applications from the Tukutuku database. © 2008 Springer-Verlag Berlin Heidelberg.",Neural Networks; Predictive accuracy; Radial basis function neural networks; Software effort estimation,"Idri A., Zahi A., Mendes E., Zakrani A.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85553-8_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249149525&doi=10.1007%2f978-3-540-85553-8_2&partnerID=40&md5=483abae3a29ef61079aab5da6a389786,"Department of Software Engineering, ENSIAS, Mohamed v University, Rabat, Morocco; Department of Computer Science FST, Sidi Mohamed Ben Abdellah Universit, Fez, Morocco; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,03029743,3540855521; 9783540855521
Scopus,Experiments with analogy-X for software cost estimation,"We developed a novel method called Analogy-X to provide statistical inference procedures for analogybased software effort estimation. Analogy-X is a method to statistically evaluate the relationship between useful project features and target features such as effort to be estimated, which ensures the dataset used is relevant to the prediction problem, and project features are selected based on their statistical contribution to the target variables. We hypothesize that this method can be (1) easily applied to a much larger dataset, and (2) also it can be used for incorporating joint effort and duration estimation into analogy, which was not previously possible with conventional analogy estimation. To test these two hypotheses, we conducted two experiments using different datasets. Our results show that Analogy-X is able to deal with ultra large datasets effectively and provides useful statistics to assess the quality of the dataset. In addition, our results show that feature selection for duration estimation differs from feature selection for joint-effort duration estimation. We conclude Analogy-X allows users to assess the best procedure for estimating duration given their specific requirements and dataset. © 2008 IEEE.",Analogy; Analogy-X; Case-based reasoning; Duration prediction; ISBSG; Mantel's correlation; Software effort prediction,"Keung J., Kitchenham B.",2008,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2008.4483211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249181100&doi=10.1109%2fASWEC.2008.4483211&partnerID=40&md5=4bacd4e884a1774ddd9f1064274e4bd0,"National ICT Australia Ltd., Australia; Keele University, United Kingdom",,English,,0769531008; 9780769531007
Scopus,Why and how can human-related measures support software development processes?,"In this paper we discuss why and how measures related to human aspects should be incorporated into software development processes. This perspective is based on the vast evidence that human aspects are the source of the majority of problems associated with software development projects. Having said that, we do not blame the humans involved in software development processes; rather, we suggest that human-related measures might be one means by which human aspects of software development processes can be supported. © 2008 Elsevier Inc. All rights reserved.",Human aspects; Process measurement; Software engineering; Software measures,"Hazzan O., Hadar I.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2008.01.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43849098005&doi=10.1016%2fj.jss.2008.01.037&partnerID=40&md5=e729e3a03d256d8595df70004a804694,"Department of Education in Technology and Science, Technion - Israel Institute of Technology, Haifa, Israel; Department of Management Information Systems, University of Haifa, Haifa, Israel",,English,01641212,
Scopus,Imputation techniques for multivariate missingness in software measurement data,"The problem of missing values in software measurement data used in empirical analysis has led to the proposal of numerous potential solutions. Imputation procedures, for example, have been proposed to 'fill-in' the missing values with plausible alternatives. We present a comprehensive study of imputation techniques using real-world software measurement datasets. Two different datasets with dramatically different properties were utilized in this study, with the injection of missing values according to three different missingness mechanisms (MCAR, MAR, and NI). We consider the occurrence of missing values in multiple attributes, and compare three procedures, Bayesian multiple imputation, k Nearest Neighbor imputation, and Mean imputation. We also examine the relationship between noise in the dataset and the performance of the imputation techniques, which has not been addressed previously. Our comprehensive experiments demonstrate conclusively that Bayesian multiple imputation is an extremely effective imputation technique. © 2008 Springer Science+Business Media, LLC.",Bayesian multiple imputation; Data quality; Imputation; Missing data; Software quality,"Khoshgoftaar T.M., Van Hulse J.",2008,Journal,Software Quality Journal,10.1007/s11219-008-9054-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349173555&doi=10.1007%2fs11219-008-9054-7&partnerID=40&md5=931ea873b340860fc7b3b3fd5dc70a6f,"Department of Computer Science and Engineering, Florida Atlantic University, Boca Raton, FL 33431, United States",Kluwer Academic Publishers,English,09639314,
Scopus,An Externally Replicated Experiment for Evaluating the Learning Effectiveness of Using Simulations in Software Project Management Education,"The increasing demand for software project managers in industry requires strategies for the development of the management-related knowledge and skills of the current and future software workforce. Although several approaches help teach the required skills in a university setting, few empirical studies are currently available to characterize and compare their effects. This paper presents results of an externally replicated controlled experiment that evaluates the learning effectiveness of using a process simulation model for educating computer science students in software project management. While the experimental group applies a system dynamics (SD) simulation model, the control group uses the well-known COCOMO model as a predictive tool for project planning. The results of the empirical study indicate that students using the simulation model gain a better understanding about typical behavior patterns of software development projects. The combination of the results from the initial experiment and the replication corroborates this finding. Additional analysis shows that the observed effect can mainly be attributed to the use of the simulation model in combination with a web-based role-play scenario. This finding is strongly supported by information gathered from the debriefing questionnaires of subjects in the experimental group. They consistently rated the simulation-based role-play scenario as a very useful approach for learning about issues in software project management.",COCOMO; Learning effectiveness; Replicated experiment; Software project management education; System dynamics simulation,"Pfahl D., Laitenberger O., Dorsch J., Ruhe G.",2003,Journal,Empirical Software Engineering,10.1023/A:1025320418915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141958728&doi=10.1023%2fA%3a1025320418915&partnerID=40&md5=134ed4cf91f28c8bf8a2216625581aca,"University of Calgary, Calgary, Alta., Canada",,English,13823256,
Scopus,Model-based tests of truisms,"Software engineering (SE) truisms capture broadly-applicable principles of software construction. The trouble with truisms is that such general principles may not apply in specific cases. This paper tests the specificity of two SE truisms: (a) increasing software process level is a desirable goal; and (b) it is best to remove errors during the early parts of a software lifecycle. Our tests are based on two well-established SE models: (1) Boehm et.al.'s COCOMO II cost estimation model; and (2) Raffo's discrete event software process model of a software project life cycle. After extensive simulations of these models, the TAR2 treatment learner was applied to find the model parameters that most improved the potential performance of the real-world systems being modelled. The case studies presented here showed that these truisms are clearly sub-optimal for certain projects since other factors proved to be far more critical. Hence, we advise against truism-based process improvement. This paper offers a general alternative framework for model-based assessment of methods to improve software quality: modelling + validation + simulation + sensitivity. That is, after recording what is known in a model, that model should be validated, explored using simulations, then summarized to find the key factors that most improve model behavior. © 2002 IEEE.",,"Menzies T., Raffo D., Setamanit S.-O., Hu Y., Tootoonian S.",2002,Conference,Proceedings - ASE 2002: 17th IEEE International Conference on Automated Software Engineering,10.1109/ASE.2002.1115012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982069310&doi=10.1109%2fASE.2002.1115012&partnerID=40&md5=11ce59fc3a49496217a05a3451abb32a,"Lane Dept. of Com. Sci., University of West Virginia, PO Box 6109, Morgantown, WV  26506-6109, United States; Portland State University, School of Business, PO Box 751, Portland, OR  97207, United States; Dept. Elec. and Comp. Eng., Vancouver, BC  V6T1Z4, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,0769517366; 9780769517360
Scopus,Empirical Measurement of the Effects of Cultural Diversity on Software Quality Management,"The difficulties of achieving social acceptance for Software Quality Management systems have been underestimated in the past, and they will be exacerbated in the future by the globalization of the software market and the increasing use of cross-cultural development teams within multinational companies. Management that can take account of the cultural context of their endeavours will improve understanding, minimize risk and ensure a higher degree of success in improvement programs within the software industry. This paper addresses cross-cultural issues in Software Quality Management. Qualitative and quantitative research was carried out in five European countries by using a postal questionnaire. Empirical measures of organizational culture, national culture and their interdependence, are presented together with interim instruments developed for the purpose of classifying organizations. Verification of the statistical results from the survey was carried out by triangulation, which included qualitative research methods in the form of interviews and observation. Cultural factors, which may have bearing on successful adoption and implementation of Software Quality Management were identified, and an assessment model, has been developed for use by organizations developing software in different parts of the world. The intention is that the recommendations following from the assessment will lead to greater cultural awareness in addressing quality, and will provide stimulus for improvement. The model's aims is to predict to what degree there is a fit between the organizational and the national culture, and to give recommendations and guidelines for software process improvement.",Cultural diversity; Empirical measurement; Software process improvement; Software quality management,"Siakas K.V., Georgiadou E.",2002,Journal,Software Quality Journal,10.1023/A:1020528024624,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842856193&doi=10.1023%2fA%3a1020528024624&partnerID=40&md5=b57d9bcaf695665006cda2cc42988471,"Technol. Educ. Inst. of Thessaloniki, Department of Informatics, P.O. Box 14561, GR-54101 Thessaloniki, Greece; School of Computing Science, Middlesex University, Trent Park, Bramley Road, London NI4 4YZ, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Calibration of software quality: Fuzzy neural and rough neural computing approaches,"This paper compares different neural computing approaches to estimating the number of changes needed in a software product based on assessments of software quality. Two forms of neural computation are considered: fuzzy neural computation based on fuzzy sets and rough neural computation based on rough sets. Both forms of neural computation are defined in the context of the McCall software quality evaluation framework, which is hierarchical. This hierarchy has three levels: factors (highest-level based user views of software quality), criteria (mid-level based on characteristics of software), and metrics (lowest level based on quantification of software quality). The introduction of a neural approach to estimating the number of changes needed to achieve software quality according to a project requirement is motivated by the need to harness the complexities inherent in the relationships between factors, criteria and metrics. The architecture of both types of networks is given. The results of calibrating both types of networks are also given. The performance of the two forms of neural computation is compared. © 2001 Elsevier Science B.V. All rights reserved.",Calibration; Fuzzy sets; Neural networks; Rough sets; Software engineering; Software quality,"Pedrycz W., Han L., Peters J.F., Ramanna S., Zhai R.",2001,Journal,Neurocomputing,10.1016/S0925-2312(00)00340-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034495149&doi=10.1016%2fS0925-2312%2800%2900340-4&partnerID=40&md5=38c38367cc01724701d90200e79fc356,"Dept. Electrical/Computer Engr., University of Manitoba, Winnipeg, Man. R3T 2N2, Canada",Elsevier,English,09252312,
Scopus,"A Study of the Relationships among Albrecht and Mark II Function Points, Lines of Code 4GL and Effort","There is a strong interest in finding metrics for replacing the common LOC measure of software size, with most of the interest focusing on the Function Point measures. Mark II Function Points were proposed as a better technique than the original of Albrecht Function Points. In this work, the results of a study comparing those measures are stated, and they are also compared against effort and LOC. Since other published results are based on samples generated randomly, it is interesting to see both methods working when applied to the same projects. The data collected comes from the measurements of academic projects. The fact that all projects have been developed in the same environment (mostly 4GL) and domain (accounting information systems) allows the value of the ""technical complexity adjustment"" variable to be set to constant and also allows us to examine the relationships among the variables. Several conclusions are reported. © 1997 by Elsevier Science Inc.",,Dolado J.J.,1997,Journal,Journal of Systems and Software,10.1016/S0164-1212(96)00111-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031141422&doi=10.1016%2fS0164-1212%2896%2900111-2&partnerID=40&md5=ba25ae1fc1e2d91833b82e1b2587121b,"Computer Science Department, Facultad de Informática, University of the Basque Country, 20.009, San Sebastián, Spain; Computer Science Department, Facultad de Informática, 20.009, San Sebastian, Spain",Elsevier Inc.,English,01641212,
Scopus,Establishing relationships between specification size and software process effort in CASE environments,"Advances in software process technology have rendered some existing methods of size assessment and effort estimation inapplicable. The use of automation in the software process, however, provides an opportunity for the development of more appropriate software size-based effort estimation models. A specification-based size assessment method has therefore been developed and tested in relation to process effort on a preliminary set of systems. The results of the analysis confirm the assertion that, within the automated environment class, specification size indicators (that may be automatically and objectively derived) are strongly related to process effort requirements.",CASE; Process effort; Software metrics,MacDonell S.G.,1997,Journal,Information and Software Technology,10.1016/0950-5849(96)01125-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030735131&doi=10.1016%2f0950-5849%2896%2901125-1&partnerID=40&md5=d8e40d75b21e8f96a4e96cc4d63ac6ea,"Computer and Information Science, University of Otago, P.O. Box 56, Dunedin, New Zealand",Elsevier,English,09505849,
Scopus,On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions,"Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations. © 2016 Elsevier B.V.",Change prediction; Defect prediction; Effort estimation; Maintainability prediction; Search-based techniques; Software quality,"Malhotra R., Khanna M., Raje R.R.",2017,Journal,Swarm and Evolutionary Computation,10.1016/j.swevo.2016.10.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002466321&doi=10.1016%2fj.swevo.2016.10.002&partnerID=40&md5=01a5d51962f9955f474e107a6c5e5e7e,"Department of Software Engineering, Delhi Technological University, India; Delhi Technological University, India; Sri Guru Gobind Singh College of Commerce, University of Delhi, India; Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indiana, IN, United States",Elsevier B.V.,English,22106502,
Scopus,A stability assessment of solution adaptation techniques for analogy-based software effort estimation,"Among numerous possible choices of effort estimation methods, analogy-based software effort estimation based on Case-based reasoning is one of the most adopted methods in both the industry and research communities. Solution adaptation is the final step of analogy-based estimation, employed to aggregate and adapt to solutions derived during the case-based reasoning process. Variants of solution adaptation techniques have been proposed in previous studies; however, the ranking of these techniques is not conclusive and shows conflicting results, since different studies rank these techniques in different ways. This paper aims to find a stable ranking of solution adaptation techniques for analogy-based estimation. Compared with the existing studies, we evaluate 8 commonly adopted solution techniques with more datasets (12), more feature selection techniques included (4), and more stable error measures (5) to a robust statistical test method based on the Brunner test. This comprehensive experimental procedure allows us to discover a stable ranking of the techniques applied, and to observe similar behaviors from techniques with similar adaptation mechanisms. In general, the linear adaptation techniques based on the functions of size and productivity (e.g., regression towards the mean technique) outperform the other techniques in a more robust experimental setting adopted in this study. Our empirical results show that project features with strong correlation to effort, such as software size or productivity, should be utilized in the solution adaptation step to achieve desirable performance. Designing a solution adaptation strategy in analogy-based software effort estimation requires careful consideration of those influential features to ensure its prediction is of relevant and accurate. © 2016, Springer Science+Business Media New York.",Analogy-based estimation; Ranking instability; Robust statistical method; Software effort estimation; Solution adaptation techniques,"Phannachitta P., Keung J., Monden A., Matsumoto K.",2017,Journal,Empirical Software Engineering,10.1007/s10664-016-9434-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973160570&doi=10.1007%2fs10664-016-9434-8&partnerID=40&md5=7137f3ea89fed0d651e73d103e04dfd1,"Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan",Springer New York LLC,English,13823256,
Scopus,Cost-effective regression testing through Adaptive Test Prioritization strategies,"Regression testing is an important part of the software development life cycle. It is also very expensive. Many different techniques have been proposed for reducing the cost of regression testing. However, research has shown that the effectiveness of different techniques varies under different testing environments and software change characteristics. In prior work, we developed strategies to investigate ways of choosing the most cost-effective regression testing technique for a particular regression testing session. In this work, we empirically study the existing strategies presented in prior work as well as develop two additional Adaptive Test Prioritization (ATP) strategies using fuzzy analytical hierarchy process (AHP) and the weighted sum model (WSM). We also provide a comparative study examining each of the ATP strategies presented to date. This research will provide researchers and practitioners with strategies to utilize in regression testing plans as well as provide data to use when deciding which of the strategies would best fit their testing needs. The empirical studies provided in this research show that utilizing these strategies can improve the cost-effectiveness of regression testing. © 2016 Elsevier Inc. All rights reserved.",Adaptive regression testing strategy; Regression testing; Test case prioritization,"Schwartz A., Do H.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2016.01.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960881560&doi=10.1016%2fj.jss.2016.01.018&partnerID=40&md5=9b0cfcfba1c7ed8b6ab89a52e5660c8c,"University of South Carolina Upstate, Chesnee, SC  29323, United States; University of North Texas, Denton, TX  76203, United States",Elsevier Inc.,English,01641212,
Scopus,Accuracy Comparison of Analogy-Based Software Development Effort Estimation Techniques,"Estimation by analogy is a commonly used software effort estimation technique and a suitable alternative to other conventional estimation techniques: It predicts the effort of the target project using information from former similar projects. While it is relatively easy to handle numerical attributes, dealing with categorical attributes is one of the most difficult issues for analogy-based estimation techniques. Therefore, we propose, in this paper, a novel analogy-based approach, called 2FA-kprototypes, to predict effort when software projects are described by a mix of numerical and categorical attributes. To this aim, the well-known fuzzy k-prototypes algorithm is integrated into the process of estimation by analogy. The estimation accuracy of 2FA-kprototypes was evaluated and compared with that of two techniques: (1) classical analogy-based technique and (2) 2FA-kmodes, which is a technique that we have developed recently. The comparison was performed using four data sets that are quite diverse and have different sizes: ISBSG, COCOMO, USP05-FT, and USP05-RQ. The results obtained showed that both 2FA-kprototypes and 2FA-kmodes perform better than classical analogy. © 2015 Wiley Periodicals, Inc.",,"Idri A., Amazal F.A., Abran A.",2016,Journal,International Journal of Intelligent Systems,10.1002/int.21748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938886103&doi=10.1002%2fint.21748&partnerID=40&md5=d6d3bf98e8678fff671b7bc18b1febd5,"Software Projects Management Research Team, ENSIAS, Mohamed v University of Rabat, Madinate Al Irfane, Rabat, 10100, Morocco; Department of Software Engineering and Information Technology, Ecole de Technologie Supérieure, Montréal, H3 C IK3, Canada",John Wiley and Sons Ltd,English,08848173,
Scopus,How to Make Best Use of Cross-Company Data for Web Effort Estimation?,"[Context]: The numerous challenges that can hinder software companies from gathering their own data have motivated over the past 15 years research on the use of cross-company (CC) datasets for software effort prediction. Part of this research focused on Web effort prediction, given the large increase worldwide in the development of Web applications. Some of these studies indicate that it may be possible to achieve better performance using CC models if some strategy to make the CC data more similar to the within-company (WC) data is adopted. [Goal]: This study investigates the use of a recently proposed approach called Dycom to assess to what extent Web effort predictions obtained using CC datasets are effective in relation to the predictions obtained using WC data when explicitly mapping the CC models to the WC context. [Method]: Data on 125 Web projects from eight different companies part of the Tukutuku database were used to build prediction models. We benchmarked these models against baseline models (mean and median effort) and a WC base learner that does not benefit of the mapping. We also compared Dycom against a competitive CC approach from the literature (NN-filtering). We report a company-by- company analysis. [Results]: Dycom usually managed to achieve similar or better performance than a WC model while using only half of the WC training data. These results are also an improvement over previous studies that investigated the use of different strategies to adapt CC models to the WC data for Web effort estimation. [Conclusions]: We conclude that the use of Dycom for Web effort prediction is quite promising and in general supports previous results when applying Dycom to conventional software datasets. © 2015 IEEE.",Companies; Databases; Estimation; Optical wavelength conversion; Predictive models; Software; Training,"Minku L., Sarro F., Mendes E., Ferrucci F.",2015,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/ESEM.2015.7321199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961620838&doi=10.1109%2fESEM.2015.7321199&partnerID=40&md5=6434cdaab8d576281bb5a93e85844120,"School of Computer Science, University of Birmingham, United Kingdom; Department of Computer Science, University of Leicester, United Kingdom; CREST, Department of Computer Science, University College, London, United Kingdom; Blekinge Institute Technology, Sweden; University of Oulu, Finland; Department of Computer Science, University of Salerno, Italy",IEEE Computer Society,English,19493770,9781467378994
Scopus,On the Use of Requirements Measures to Predict Software Project and Product Measures in the Context of Android Mobile Apps: A Preliminary Study,"In this paper, we study the value of software project and product measures in the context of Android mobile apps. In particular, we focus on the effort to develop mobile apps and the number of graphical components in these apps. Estimation models are based on information from requirements specification documents (e.g., Number of actors, number of use cases, and number of classes). We have used a dataset containing information on 23 Android apps and employed a stepwise linear regression to build estimation models. The predictions have been compared with those obtained considering models built on software measures (e.g., Number of classes, number of files, and number of line of code). The results suggest that the measures from the artifacts produced in requirements engineering process are not worse predictors than those measures from source code. That is, requirements measures can effectively employed to estimate software project and product measures of a mobile app and estimations can be done early in the software development process. © 2015 IEEE.",Empirical study; mobile app development; requirements measures; software development effort estimation,"Francese R., Gravino C., Risi M., Scanniello G., Tortora G.",2015,Conference,"Proceedings - 41st Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2015",10.1109/SEAA.2015.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949535300&doi=10.1109%2fSEAA.2015.22&partnerID=40&md5=1f737dddadceb8735afff3828a33da9d,"DISTRA (MIT), University of Salerno, Italy; DiMIE, University of Basilicata, Italy",Institute of Electrical and Electronics Engineers Inc.,English,,9781467375856
Scopus,Software Project Estimation: The Fundamentals for Providing High Quality Information to Decision Makers,This book introduces theoretical concepts to explain the fundamentals of the design and evaluation of software estimation models. It provides software professionals with vital information on the best software management software out there. End-of-chapter exercises Over 100 figures illustrating the concepts presented throughout the book Examples incorporated with industry data. © 2015 IEEE Computer Society.,,Abran A.,2015,Book,Software Project Estimation: The Fundamentals for Providing High Quality Information to Decision Makers,10.1002/9781118959312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974612436&doi=10.1002%2f9781118959312&partnerID=40&md5=89ddd6aa931877177f3591ee9d673b1c,"Software Engineering Research Laboratory, Université du Québec, Canada",Wiley-IEEE Press,English,,9781118959312; 9781118954089
Scopus,A measurement method for sizing the structure of UML sequence diagrams,"Context The COSMIC functional size measurement method on UML diagrams has been investigated as a means to estimate the software effort early in the software development life cycle. Like other functional size measurement methods, the COSMIC method takes into account the data movements in the UML sequence diagrams for example, but does not consider the data manipulations in the control structure. This paper explores software sizing at a finer level of granularity by taking into account the structural aspect of a sequence diagram in order to quantify its structural size. These functional and structural sizes can then be used as distinct independent variables to improve effort estimation models. Objective The objective is to design an improved measurement of the size of the UML sequence diagrams by taking into account the data manipulations represented by the structure of the sequence diagram, which will be referred to as their structural size. Method While the design of COSMIC defines the functional size of a functional process at a high level of granularity (i.e. the data movements), the structural size of a sequence diagram is defined at a finer level of granularity: the size of the flow graph of their control structure described through the alt, opt and loop constructs. This new measurement method was designed by following the process recommended in Software Metrics and Software Metrology (Abran, 2010). Results The size of sequence diagrams can now be measured from two perspectives, both functional and structural, and at different levels of granularity with distinct measurement units. Conclusion It is now feasible to measure the size of functional requirements at two levels of granularity: at an abstract level, the software functional size can be measured in terms of COSMIC Function Point (CFP) units; and at a detailed level, the software structural size can be measured in terms of Control Structure Manipulation (CSM) units. These measures represent complementary aspects of software size and can be used as distinct independent variables to improve effort estimation models. © 2014 Elsevier B.V. All rights reserved.",Combined fragment; COSMIC ISO 19761; Different levels of granularity; Functional size measurement; Structural size measurement; UML sequence diagram,"Sellami A., Hakim H., Abran A., Ben-Abdallah H.",2015,Journal,Information and Software Technology,10.1016/j.infsof.2014.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921027199&doi=10.1016%2fj.infsof.2014.11.002&partnerID=40&md5=4c8397115872566cfe4f6ec70ce29dbe,"Computer Science Engineering Department, University of Sfax, Tunisia; Software Engineering and Information Technology Department, École de Technologie Supérieure, University of Quebec, Canada; Department of Information Systems, Software Engineering, King Abdulaziz University, Saudi Arabia",Elsevier,English,09505849,
Scopus,Cross- vs. Within-company cost estimation studies revisited: An extended systematic review,"[Objective] The objective of this paper is to extend a previously conducted systematic literature review (SLR) that investigated under what circumstances individual organizations would be able to rely on cross-company based estimation models. [Method] We applied the same methodology used in the SLR we are extending herein (covering the period 2006-2013) based on primary studies that compared predictions from cross-company models with predictions from within-company models constructed from analysis of project data. [Results] We identified 11 additional papers; however two of these did not present independent results and one had inconclusive findings. Two of the remaining eight papers presented both, trials where cross-company predictions were not significantly different from within-company predictions and others where they were significantly different. Four found that cross-company models gave prediction accuracy significantly different from within-company models (one of them in favor of cross-company models), while two found no significant difference. The main pattern when examining the study related factors was that studies where cross-company predictions were significantly different from within-company predictions employed larger within-company data sets. [Conclusions] Overall, half of the analyzed evidence indicated that cross-company estimation models are not significantly worse than within-company estimation models. Moreover, there is some evidence that sample size does not imply in higher estimation accuracy, and that samples for building estimation models should be carefully selected/filtered based on quality control and project similarity aspects. The results need to be combined with the findings from the SLR we are extending to allow further investigating this topic. Copyright 2014 ACM.",Cost estimation models; Cross-company data; Estimation accuracy; Systematic review; Within-company data,"Mendes E., Kalinowski M., Martins D., Ferrucci F., Sarro F.",2014,Conference,ACM International Conference Proceeding Series,10.1145/2601248.2601284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905466468&doi=10.1145%2f2601248.2601284&partnerID=40&md5=47f3d6b6abd4f28fdb6f23f05130798e,"Blekinge Institute of Technology, Karlskrona 37133, Sweden; University of Juiz de for A, Juiz de Fora 36036, Brazil; University of Salerno, Via Ponte don Melillo, 84084, Fisciano, Italy; University College London, London, WC1E 6BT, United Kingdom",Association for Computing Machinery,English,,9781450324762
Scopus,Approximate COSMIC size to early estimate web application development effort,"The aim of the paper is to investigate to what extend some COSMIC-based approximate countings can be useful for project managers to early estimate effort of Web applications. We considered the number of COSMIC Functional Processes and the Average Functional Process approach proposed by the COSMIC method documentation. We carried out an empirical analysis employing data from 25 Web applications to assess whether the two approximate sizes can be exploited to get accurate effort estimations. We compared the obtained estimations with those achieved with baseline benchmarks and employing the standard COSMIC method. The analysis highlights that the considered approximate countings provide good early estimations, significantly better than those obtained with baseline benchmarks. Moreover, the first counting provides results better than the Average Functional Process approach. The use of COSMIC-based approximate countings is a suitable approach for early effort estimations. © 2013 IEEE.",COSMIC; Early effort estimation; Empirical study; Linear regression,"De Marco L., Ferrucci F., Gravino C.",2013,Conference,"Proceedings - 39th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2013",10.1109/SEAA.2013.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889020868&doi=10.1109%2fSEAA.2013.41&partnerID=40&md5=d918932fc7fcd4c8cb3c7f42916a82eb,"Department of Management and Information Technology, DISTRA - MIT, University of Salerno, Italy",,English,,9780769550916
Scopus,A taxonomy of data quality challenges in empirical software engineering,"Reliable empirical models such as those used in software effort estimation or defect prediction are inherently dependent on the data from which they are built. As demands for process and product improvement continue to grow, the quality of the data used in measurement and prediction systems warrants increasingly close scrutiny. In this paper we propose a taxonomy of data quality challenges in empirical software engineering, based on an extensive review of prior research. We consider current assessment techniques for each quality issue and proposed mechanisms to address these issues, where available. Our taxonomy classifies data quality issues into three broad areas: first, characteristics of data that mean they are not fit for modeling, second, data set characteristics that lead to concerns about the suitability of applying a given model to another data set, and third, factors that prevent or limit data accessibility and trust. We identify this latter area as of particular need in terms of further research. © 2013 IEEE.",accessibility; commercial sensitivity; data quality; empirical software engineering; provenance; trustworthiness,"Bosu M.F., Macdonell S.G.",2013,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2013.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885221861&doi=10.1109%2fASWEC.2013.21&partnerID=40&md5=6f240faf6dbf3aa552383675fbe28dd9,"SERL, School of Computing and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand",,English,,9780769549958
Scopus,Data quality in empirical software engineering: A targeted review,"Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering. Copyright 2013 ACM.",Data quality; Data sets; Empirical software engineering; Literature review,"Bosu M.F., MacDonell S.G.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2460999.2461024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877292599&doi=10.1145%2f2460999.2461024&partnerID=40&md5=a486e54dc2af38c1387717c596b56f9a,"SERL, School of Computing and Mathematical Sciences, AUT University Auckland, 1142 New Zealand, New Zealand",,English,,9781450318488
Scopus,Optimum estimation of missing values in randomized complete block design by genetic algorithm,"Missing data are a part of almost all research, and we all have to decide how to deal with it from time to time. There are a number of alternative ways of dealing with missing data. The problem of handling missing data has been treated adequately in various real world data sets. Several statistical methods have been developed since the early 1970s, when the manipulation of complicated numerical calculations became feasible with the advancement of computers. The purpose of this research is to estimate missing values by using genetic algorithm (GA) approach in a randomized complete block design (RCBD) table and to compare the computational results with three other methods, namely, particle swarm optimization (PSO), Artificial Neural Network (ANN), approximate analysis and exact regression method. Furthermore, 30 independent experiments were conducted to estimate missing values in 30 RCBD tables by GA, PSO, ANN, exact regression and approximate analysis methods. Computational results indicated that the best answer (in the last 10-chromosome population) obtained by GA is frequently the same as the missing value, with the mean value being close to the missing observation. It is concluded that GA provides much better estimation than the other methods. The superiority of GA is shown through lower error estimations and also Pearson correlation experiment. Therefore, it is suggested to utilize GA approach of this study for estimating missing values for RCBD. © 2012 Elsevier B.V. All rights reserved.",Artificial Neural Network (ANN); Complete randomized block design; Genetic algorithm (GA); Missing values; Particle swarm optimization (PSO); Regression methods,"Azadeh A., Asadzadeh S.M., Jafari-Marandi R., Nazari-Shirkouhi S., Baharian Khoshkhou G., Talebi S., Naghavi A.",2013,Journal,Knowledge-Based Systems,10.1016/j.knosys.2012.06.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870053909&doi=10.1016%2fj.knosys.2012.06.014&partnerID=40&md5=cbc0a40a24a34bff98d19c6a08130f97,"Department of Industrial Engineering, Center of Excellence for Intelligent Experimental Mechanics, University of Tehran, P.O. Box 11365-4563, Iran; Department of Engineering Optimization Research, College of Engineering, University of Tehran, P.O. Box 11365-4563, Iran; Department of Mechanical and Industrial Engineering, University of Illinois, Urbana-Champaign, United States; Department of Industrial Engineering, North Carolina State University, Raleigh, United States",Elsevier B.V.,English,09507051,
Scopus,A principled evaluation of ensembles of learning machines for software effort estimation,"Background: Software effort estimation (SEE) is a task of strategic importance in software management. Recently, some studies have attempted to use ensembles of learning machines for this task. Aims: We aim at (1) evaluating whether readily available ensemble methods generally improve SEE given by single learning machines and which of them would be more useful; getting insight on (2) how to improve SEE; and (3) how to choose machine learning (ML) models for SEE. Method: A principled and comprehensive statistical com- parison of three ensemble methods and three single learn- ers was carried out using thirteen data sets. Feature selec- tion and ensemble diversity analyses were performed to gain insight on how to improve SEE based on the approaches singled out. In addition, a risk analysis was performed to investigate the robustness to outliers. Therefore, the bet- ter understanding/insight provided by the paper is based on principled experiments, not just an intuition or speculation. Results: None of the compared methods is consistently the best, even though regression trees and bagging using mul- tilayer perceptrons (MLPs) are more frequently among the best. These two approaches usually perform similarly. Re- gression trees place more important features in higher levels of the trees, suggesting that feature weights are important when using ML models for SEE. The analysis of bagging with MLPs suggests that a self-tuning ensemble diversity method may help improving SEE. Conclusions: Ideally, principled experiments should be done in an individual basis to choose a model. If an organisa- tion has no resources for that, regression trees seem to be a good choice for its simplicity. The analysis also suggests approaches to improve SEE. Copyright © 2011 ACM.",Ensembles of learning machines; Machine learning; Software cost/effort estimation,"Minku L.L., Yao X.",2011,Conference,ACM International Conference Proceeding Series,10.1145/2020390.2020399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054057515&doi=10.1145%2f2020390.2020399&partnerID=40&md5=dcfe881ffa391a286f91c46d27fcee85,"School of Computer Science, University of Birmingham, Edgbaston, Birmingham B15 2TT, United Kingdom",,English,,9781450307093
Scopus,Software cost estimation using soft computing approaches,"Software development has become an essential investment for many organizations. Software engineering practitioners have become more and more concerned about accurately predicting the cost of software products to be developed. Accurate estimates are desired but no model has proved to be successful at effectively and consistently predicting software development cost. This chapter investigates the use of the soft computing approaches in predicting the software development effort. Various statistical and intelligent techniques are employed to estimate software development effort. Further, based on the above-mentioned techniques, ensemble models are developed to forecast software development effort. Two types of ensemble models viz., linear (average) and nonlinear are designed and tested on COCOMO'81 dataset. Based on the experiments performed on the COCOMO'81 data, it was observed that the nonlinear ensemble using radial basis function network as arbitrator outperformed all the other ensembles and also the constituent statistical and intelligent techniques. The authors conclude that using soft computing models they can accurately estimate software development effort. © 2010, IGI Global.",,"Vinaykumar K., Ravi V., Carr M.",2009,Book Chapter,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",10.4018/978-1-60566-766-9.ch024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870563022&doi=10.4018%2f978-1-60566-766-9.ch024&partnerID=40&md5=29946888948d88926ba07ee9fd269dd9,"Institute for Development and Research in Banking Technology (IDRBT), Hyderabad, India",IGI Global,English,,9781605667669
Scopus,Using Tabu search to estimate software development effort,"The use of optimization techniques has been recently proposed to build models for software development effort estimation. In particular, some studies have been carried out using search-based techniques, such as genetic programming, and the results reported seem to be promising. At the best of our knowledge nobody has analyzed the effectiveness of Tabu search for development effort estimation. Tabu search is a meta-heuristic approach successful used to address several optimization problems. In this paper we report on an empirical analysis carried out exploiting Tabu Search on a publicity available dataset, i.e., Desharnais dataset. The achieved results show that Tabu Search provides estimates comparable with those achieved with some widely used estimation techniques. © Springer-Verlag Berlin Heidelberg 2009.",Effort estimation; Empirical analysis; Search-based approaches; Tabu search,"Ferrucci F., Gravino C., Oliveto R., Sarro F.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650643424&doi=10.1007%2f978-3-642-05415-0_22&partnerID=40&md5=b642197df7dd26fcdd4e7f6f0757a06c,"Dipartimento di Matematica e Informatica, University of Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy",,English,03029743,3642054145; 9783642054143
Scopus,The impact of social networking on software design quality and development effort in open source projects [L'impact des réseaux sociaux sur la qualité de la conception logicielle et l'effort de développement dans les projets de logiciels libres],"This paper focuses on Open Source (OS) social networks. The literature indicates that OS networks have a few nodes with a number of relationships significantly higher than the network's average, called hubs. It also provides numerous metrics that help verify whether a node is a hub, called centrality metrics. This paper posits that higher values of centrality metrics are positively correlated with project success. Second, it posits that higher values of centrality metrics are positively correlated with the ability of a project to attract new contributions. Third, it posits that projects with greater success have a lower software design quality. Hypotheses are tested on a sample of 56 applications written in Java from the SourceForge.net online OS repository. The corresponding social network is built by considering all the contributors, both developers and administrators, of our application sample and all contributors directly or indirectly connected with them within SourceForge.net, with a total of 57,142 nodes. Empirical results support our hypotheses, indicating that centrality metrics are significant drivers of project success that should be monitored from the perspective of a project administrator or team manager. However, they also prove that successful projects tend to have a significantly lower design quality of software. This has a number of consequences that could be visible to users and cause negative feedback effects over time.",Network centrality metrics; Open source; Social networks; Software design quality; Software development effort,"Barbagallo D., Francalanci C., Merlo F.",2008,Conference,ICIS 2008 Proceedings - Twenty Ninth International Conference on Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870970367&partnerID=40&md5=fdb5d6f3f43f4e6c3b123a545e84ba12,"Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy",,English; French,,
Scopus,Measurement in software engineering: From the roadmap to the crossroads,"Research on software measurement can be organized around five key conceptual and methodological issues: how to apply measurement theory to software, how to frame software metrics, how to develop metrics, how to collect core measures, and how to analyze measures. The subject is of special concern for the industry, which is interested in improving practices mainly in developing countries, where the software industry represents an opportunity for growth and usually receives institutional support for matching international quality standards. Academics are also in need of understanding and developing more effective methods for managing the software process and assessing the success of products and services, as a result of an enhanced awareness about the emergency of aligning business processes and information systems. This paper unveils the fundamentals of measurement in software engineering and discusses current issues and foreseeable trends for the subject. A literature review was performed within major academic publications in the last decade, and findings suggest a sensible shift of measurement interests towards managing the software process as a whole without losing from sight the customary focus on hard issues like algorithm efficiency and worker productivity. © 2008 World Scientific Publishing Company.",Complexity; Interpretive data; Measurement theory; Software engineering; Software management; Software measurement; Triangulation,"Bellini C.G.P., Pereira R.D.C.D.F., Becker J.L.",2008,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S021819400800357X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049095768&doi=10.1142%2fS021819400800357X&partnerID=40&md5=696b0b37a5ed78c6b73748726fa2e511,"Universidade Federal da Paraíba, João Pessoa, Brazil; Universidade Federal Do Rio Grande Do sul, Porto Alegre, Brazil",,English,02181940,
Scopus,A unified framework for defect data analysis using the MBR technique,"Failures of mission-critical software systems can have catastrophic consequences and, hence, there is strong need for scientifically rigorous methods for assuring high system reliability. To reduce the V&V cost for achieving high confidence levels, quantitatively based software defect prediction techniques can be used to effectively estimate defects from prior data. Better prediction models facilitate better project planning and risk/cost estimation. Memory Based Reasoning (MBR) is one such classifier that quantitatively solves new cases by reusing knowledge gained from past experiences. However, it can have different configurations by varying its input parameters, giving potentially different predictions. To overcome this problem, we develop a framework that derives the optimal configuration of an MBR classifier for software defect data, by logical variation of its configuration parameters. We observe that this adaptive MBR technique provides a flexible and effective environment for accurate prediction of mission-critical software defect data. © 2006 IEEE.",,"Challagulla V.U.B., Bastani F.B., Yen I.-L.",2006,Conference,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",10.1109/ICTAI.2006.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949121974&doi=10.1109%2fICTAI.2006.23&partnerID=40&md5=cf29b8d44060aad2d339017efac315aa,"Department of Computer Science, University of Texas, Dallas, United States",,English,10823409,0769527280; 9780769527284
Scopus,COCOMO-U: An extension of COCOMO II for cost estimation with uncertainty,"It is well documented that the software industry suffers from frequent cost overruns, and the software cost estimation remains a challenging issue. A contributing factor is, we believe, the inherent uncertainty of assessment of cost. Considering the uncertainty with cost drivers and representing the cost as*/ distribution of values can help us better understand the uncertainty of cost estimations and provide decision support for budge setting or cost control. In this paper, we use Bayesian belief networks to extend the COCOMOII for cost estimation with uncertainty, and construct the probabilistic cost model COCOMO-U. This model can be used to deal with the uncertainties of cost factors and estimate the cost probability distribution. We also demonstrate how the COCOMO-U is used to provide decision support for software development budget setting and cost control in a case study. © Springer-Verlag Berlin Heidelberg 2006.",,"Yang D., Wan Y., Tang Z., Wu S., He M., Li M.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11754305_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745891199&doi=10.1007%2f11754305_15&partnerID=40&md5=07d5299d36015cc1c73dd1430032c639,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China; Graduate University, Chinese Academy of Sciences, Beijing 100039, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China",Springer Verlag,English,03029743,3540341994; 9783540341994
Scopus,Using real options to select stable middlewareinduced software architectures,"The requirements that force decisions towards building distributed system architectures are usually of a non-functional nature. Scalability, openness, heterogeneity, and fault-tolerance are examples of such non-functional requirements. The current trend is to build distributed systems with middleware, which provide the application developer with primitives for managing the complexity of distribution, system resources, and for realising many of the non-functional requirements. As non-functional requirements evolve, the 'coupling' between the middleware and architecture becomes the focal point for understanding the stability of the distributed software system architecture in the face of change. It is hypothesised that the choice of a stable distributed software architecture depends on the choice of the underlying middleware and its flexibility in responding to future changes in non-functional requirements. Drawing on a case study that adequately represents a medium-size component-based distributed architecture, it is reported how a likely future change in scalability could impact the architectural structure of two versions, each induced with a distinct middleware: one with CORBA and the other with J2EE. An option-based model is derived to value the flexibility of the induced-architectures and to guide the selection. The hypothesis is verified to be true for the given change. The paper concludes with some observations that could stimulate future research in the area of relating requirements to software architectures. © IEE, 2005.",,"Bahsoon R., Emmerich W., Macke J.",2005,Journal,IEE Proceedings: Software,10.1049/ip-sen:20045059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344450333&doi=10.1049%2fip-sen%3a20045059&partnerID=40&md5=0ed282b83b3d4e4c280842067d1e0576,"London Software Systems, Department of Computer Science, University College London, Gower Street, London, WC1E 6BT, United Kingdom",,English,14625970,
Scopus,The challenge of introducing a new software cost estimation technology into a small software organisation,"Fostering innovation is the key to survival in today's IT business and is exemplified by introducing new technologies and methods to improve the development processes. This paper presents a follow-up case study of technology transfer in a small software organisation. A new software estimation technique, Web-CoBRA was introduced to a small software company to improve their software estimation process. Web-CoBRA was considerably more accurate than the company's current estimation process. However, despite management being aware of this improvement, the company has not fully adopted the new method. We used interviews and the Technology Acceptance Model (TAM) questionnaire to assess the extent to which Web-CoBRA was used by the company. We found take-up of part of the Web-CoBRA technology but the full technology and the support tools were not used. We identify the reasons for the failure to adopt the Web-CoBRA technology and identify several areas for improving technology transfer activities.",Software cost estimation; Software engineering; Technology transition; Web development,"Keung J., Jeffery R., Kitchenham B.",2004,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2004.1290457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942525336&doi=10.1109%2fASWEC.2004.1290457&partnerID=40&md5=309d52afc4edff25d33c03a13f4990a0,"National ICT Australia Ltd., Sch. of Comp. Sci. and Engineering, University of New South Wales, Sydney, NSW 2052, Australia",,English,,
Scopus,Assessing the maintenance processes of a software organization: An empirical analysis of a large industrial project,"The use of statistical process control methods can determine the process capability of sustaining stable levels of variability, so that processes will yield predictable results. This enables to prepare achievable plans, meet cost estimates and scheduling commitments, and deliver required product functionality and quality with acceptable and reasonable reliability. We present initial results of applying statistical analysis methods to the maintenance processes of a software organization rated at the CMM level 3 that is currently planning the assessment to move to the CMM level 4. In particular, we present results from an empirical study conducted on the massive adaptive maintenance process of the organization. We analyzed the correlation between the maintenance size and productivity metrics. The resulting models allow to estimate the costs of a project conducted according to the adopted maintenance processes. Model performances on future observations were assessed by means of a cross validation which guarantees a nearly unbiased estimate of the prediction error. Data about the single phases of the process were also available, thus allowing to analyze the distribution of the effort among the phases and the causes of variations. © 2002 Elsevier Science Inc. All rights reserved.",Cost estimation models; Massive software maintenance; Statistical process control,"De Lucia A., Pompella E., Stefanucci S.",2003,Journal,Journal of Systems and Software,10.1016/S0164-1212(02)00051-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038147033&doi=10.1016%2fS0164-1212%2802%2900051-1&partnerID=40&md5=95da22a2dd3b2afd9c7b9577b5e1ee6e,"Department of Engineering, Res. Centre on Software Technology, University of Sannio, Palazzo Bosco Lucarelli, Piazza Roma, Benevento 82100, Italy; EDS Italia Software S.p.A., Viale Edison, Loc. Lo Uttaro, Caserta 81100, Italy",,English,01641212,
Scopus,COCOMO Cost Model Using Fuzzy Logic,"When the COCOMO (Constructive Cost Model) was published at the beginning of the eighties, fuzzy logic was not grounded on solid theoretical foundations. This was not been achieved until Zadeh and others did so in the nineties. Thus, it is not surprising that some of the concepts defined or used in COCOMO are somewhat incompatible with the fuzzy logic. In our work, we investigate the issue of the compatibility of COCOMO with the fuzzy logic. In software metrics, specifically in software cost estimation, many factors (linguistic variables in fuzzy logic) such as the experience of programmers and the complexity of modules are measured on an ordinal scale composed of qualifications such as 'very low ' and 'low ' (linguistic values in fuzzy logic). In our work, we study the COCOMO'81 model, specifically its intermediate version. Our work is still applicable to the COCOMOII.",COCOMO; Fuzzy Logic; Software Cost estimation; Software Engineering,"Idri A., Abran A., Kjiri L.",2000,Conference,Proceedings of the Joint Conference on Information Sciences,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642407956&partnerID=40&md5=dfbd8ee2225e8062041edc8694874802,"Res. Lab. in Software Eng. Mgmt., Centre-ville Postal Station, UQAM, C.P. Box 8888, Montreal, Que. H3C 3P8, Canada; Lab. Génie Informatique, Equipe Génie Logiciel, ENSIAS, BP. 713, Agdal, Rabat, Morocco",,English,,0964345692
Scopus,Requirements in Engineering Projects,"This book focuses on various topics related to engineering and management of requirements, in particular elicitation, negotiation, prioritisation, and documentation (whether with natural languages or with graphical models). The book provides methods and techniques that help to characterise, in a systematic manner, the requirements of the intended engineering system. It was written with the goal of being adopted as the main text for courses on requirements engineering, or as a strong reference to the topics of requirements in courses with a broader scope. It can also be used in vocational courses, for professionals interested in the software and information systems domain. Readers who have finished this book will be able to: - establish and plan a requirements engineering process within the development of complex engineering systems; - define and identify the types of relevant requirements in engineering projects; - choose and apply the most appropriate techniques to elicit the requirements of a given system; - conduct and manage negotiation and prioritisation processes for the requirements of a given engineering system; - document the requirements of the system under development, either in natural language or with graphical and formal models. Each chapter includes a set of exercises. © Springer International Publishing Switzerland 2016. All rights reserved.",,"Fernandes J.M., Machado R.J.",2015,Book,Requirements in Engineering Projects,10.1007/978-3-319-18597-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955152511&doi=10.1007%2f978-3-319-18597-2&partnerID=40&md5=594536430731164daffc9b025657382f,"Departamento de Informática, Universidade do Minho, Braga, Portugal; Departamento de Sistemas de Informação, Universidade do Minho, Guimarães, Portugal; School of Engineering, Universidade do Minho, Portugal",Springer International Publishing,English,,9783319185972; 9783319185965
Scopus,"Towards improving statistical modeling of software engineering data: think locally, act globally!","Much research in software engineering (SE) is focused on modeling data collected from software repositories. Insights gained over the last decade suggests that such datasets contain a high amount of variability in the data. Such variability has a detrimental effect on model quality, as suggested by recent research. In this paper, we propose to split the data into smaller homogeneous subsets and learn sets of individual statistical models, one for each subset, as a way around the high variability in such data. Our case study on a variety of SE datasets demonstrates that such local models can significantly outperform traditional models with respect to model fit and predictive performance. However, we find that analysts need to be aware of potential pitfalls when building local models: firstly, the choice of clustering algorithm and its parameters can have a substantial impact on model quality. Secondly, the data being modeled needs to have enough variability to take full advantage of local modeling. For example, our case study on social data shows no advantage of local over global modeling, as clustering fails to derive appropriate subsets. Lastly, the interpretation of local models can become very complex when there is a large number of variables or data subsets. Overall, we find that a hybrid approach between local and traditional global modeling, such as Multivariate Adaptive Regression Splines (MARS) combines the best of both worlds. MARS models are non-parametric and thus do not require prior calibration of parameters, are easily interpretable by analysts and outperform local, as well as traditional models out of the box in four out of five datasets in our case study. © 2014, Springer Science+Business Media New York.",Clustering; Software metrics; Statistical modeling,"Bettenburg N., Nagappan M., Hassan A.E.",2015,Journal,Empirical Software Engineering,10.1007/s10664-013-9292-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928708037&doi=10.1007%2fs10664-013-9292-6&partnerID=40&md5=fd610b2cbba0324a900263aff5b00534,"Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen’s University, Ontario, Kingston  K1N 3L6, Canada",Kluwer Academic Publishers,English,13823256,
Scopus,A comparison of cross-versus single-company effort prediction models for web projects,"Background: In order to address the challenges in companies having no or limited effort datasets of their own, cross-company models have been a focus of interest for previous studies. Further, a particular domain of investigation has been Web projects. Aim: This study investigates to what extent effort predictions obtained using cross-company (CC) datasets are effective in relation to the predictions obtained using single-company (SC) datasets within the domain of web projects. Method: This study uses the Tukutuku database. We employed data on 125 projects from eight different companies and built cross and single-company models with stepwise linear regression (SWR) with and without relevancy filtering. We also benchmarked these models against mean and median based models. We report a case-by-case analysis per company as well as a meta-analysis of the findings. Results: Results showed that CC models provided poor predictions and performed significantly worse than SC models. However, relevancy filtered CC models yielded comparable results to that of SC models. These results corroborate with previous research. An interesting result was that the median-based models were consistently better than other models. Conclusions: We conclude that companies that carry out Web development may use a median-based CC model for prediction until it is possible for the company to build its own SC model, which can be used by itself or in combination with median-based estimations. © 2014 IEEE.",cross-company effort model; effort estimation; single-company effort model; web projects,"Turhan B., Mendes E.",2014,Conference,"Proceedings - 40th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2014",10.1109/SEAA.2014.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916624149&doi=10.1109%2fSEAA.2014.41&partnerID=40&md5=bdaabd3e4041caa159ef7a042223ba79,"Department of Information Processing Science, University of Oulu, Oulu, 90014, Finland; Software Engineering Research Laboratory, Blekinge Institute of Technology, Karlskrona, Sweden",Institute of Electrical and Electronics Engineers Inc.,English,,9781479957941
Scopus,Generic model of software cost estimation: A hybrid approach,"Software companies ensure to complete the project within time and cost, for which good planning and thinking is required. Software project estimation is a form of problem solving which cannot be solved in a single piece of data by using some formulae. Decomposition of the problem helps in concentrating on smaller parts so that they are not missed. It aids in controlling and approximating the software risks which are commendably fixed and accurate. This paper represents an innovative idea which is the working of Principal Component Analysis (PCA) with Artificial Neural Network (ANN) by keeping the base of Constructive Cost Model II (COCOMO II) model. Feed forward ANN uses delta rule learning method to train the network. Training of ANN is based on PCA and COCOMO II sample dataset repository. PCA is a type of classification method which can filter multiple input values into a few certain values. It also helps in reducing the gap between actual and estimated effort. The test results from this hybrid model are compared with COCOMO II and ANN. © 2014 IEEE.",Artificial Neural Network (ANN); COCOMO II; Hybrid; Principal Component Analysis (PCA),"Patil L.V., Waghmode R.M., Joshi S.D., Khanna V.",2014,Conference,"Souvenir of the 2014 IEEE International Advance Computing Conference, IACC 2014",10.1109/IAdCC.2014.6779528,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899118198&doi=10.1109%2fIAdCC.2014.6779528&partnerID=40&md5=b139af88883d2d026337b419628f0685,"Bharath University, Chennai, India; S.K.N.C.O.E., Pune, India; Dept of Comp., BVDUCOE, Pune, India; Dept. of I.T., Bharath University, Chennai, India",IEEE Computer Society,English,,
Scopus,Modeling optimal release policy under fuzzy paradigm in imperfect debugging environment,"Context In this study, a software optimal release time with cost-reliability criteria has been discussed in an imperfect debugging environment. Objective The motive of this study is to model uncertainty involved in estimated parameters of the software reliability growth model (SRGM). Method Initially the reliability parameters of SRGM are estimated using least square estimation (LSE). Considering the uncertainty involved in the estimated parameters due to human behavior being subjective in nature and the dynamism of the testing environment, the concept of fuzzy set theory is applicable in developing SRGM. Finally, using arithmetic operations on fuzzy numbers, the reliability and total software cost are calculated. Results Various reliability measures have been computed at different levels of uncertainties, and a comparison has been made with the existing results reported in the literature. Conclusion It is evident from the results that a better prediction of reliability measures, namely, software reliability and total software cost can be made under the fuzzy paradigm. © 2013 Elsevier B.V. All rights reserved.",Fuzzy set theory; Parameter estimation; Software reliability; SRGMs; Testing,"Pachauri B., Kumar A., Dhar J.",2013,Journal,Information and Software Technology,10.1016/j.infsof.2013.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884149678&doi=10.1016%2fj.infsof.2013.06.001&partnerID=40&md5=258a3280d5217d178d93915027224301,"Department of Applied Sciences, ABV-Indian Institute of Information Technology and Management Gwalior, Morena Link Road, Gwalior 474 015, India",,English,09505849,
Scopus,Software effort estimation with multiple linear regression: Review and practical application,"Software development effort estimation is the basis for the effective project planning and scheduling as well as for the project's budget definition. This article describes the most common methods used in the software effort estimation (SEE) and presents the study performed in a software development organization (SDO) that is implementing the software development process improvement framework Capability Maturity Model Integrated (CMMI). Currently SDO estimates the software effort based on the opinion of one area expert. The disadvantages of this method and the willingness to incorporate the best practices of CMMI encouraged the SDO to replace the existing effort estimation method by a formal one. The stepwise Multiple Linear Regression (MLR) technique was selected and used for the software development and software testing processes. The results achieved with MLR were compared with the estimates provided by the area expert. The model obtained for the testing team performed better results than the expert judgments, while for the development team no satisfactory model was found and a proposal for collecting data from new variables is presented.",CMMI; Effort estimation; Multiple linear regression; Practical case; Software development,"Fedotova O., Teixeira L., Alvelos A.H.",2013,Journal,Journal of Information Science and Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884223411&partnerID=40&md5=5d728d97f3232ce237d398ef89caf01b,"Department of Economics, Management and Industrial Engineering, Portugal; Governance, Competitiveness and Public Politics, University of Aveiro, 3810-193 Aveiro, Portugal; Institute of Electronics and Telematics, Engineering of Aveiro Campus, Universitário de Santiago, 3810-193 Aveiro, Portugal",,English,10162364,
Scopus,Studying software evolution using artefacts' shared information content,"In order to study software evolution, it is necessary to measure artefacts representative of project releases. If we consider the process of software evolution to be copying with subsequent modification, then, by analogy, placing emphasis on what remains the same between releases will lead to focusing on similarity between artefacts. At the same time, software artefactsstored digitally as binary stringsare all information. This paper introduces a new method for measuring software evolution in terms of artefacts' shared information content. A similarity value representing the quantity of information shared between artefact pairs is produced using a calculation based on Kolmogorov complexity. Similarity values for releases are then collated over the software's evolution to form a map quantifying change through lack of similarity. The method has general applicability: it can disregard otherwise salient software features such as programming paradigm, language or application domain because it considers software artefacts purely in terms of the mathematically justified concept of information content. Three open-source projects are analysed to show the method's utility. Preliminary experiments on udev and git verify the measurement of the projects' evolutions. An experiment on ArgoUML validates the measured evolution against experimental data from other studies. © 2010 Elsevier B.V. All rights reserved.",CompLearn; Information content; Information theory; Kolmogorov complexity; Similarity metric; Software evolution; Software measurement,Arbuckle T.,2011,Journal,Science of Computer Programming,10.1016/j.scico.2010.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958836039&doi=10.1016%2fj.scico.2010.11.005&partnerID=40&md5=ad79304ba9a57158633efa8ecd1f3bf1,"Computer Science and Information Systems, University of Limerick, Limerick, Ireland",,English,01676423,
Scopus,Probabilistic estimation of software size and effort,"We propose a probabilistic neural network (PNN) approach for simultaneously estimating values of software development parameter (either software size or software effort) and probability that the actual value of the parameter will be less than its estimated value. Using real-world software engineering datasets and V-fold sampling, we compare the PNN approach with the chi-squared automatic interaction detection (CHAID) approach and find that the PNN approach performs similar to the CHAID, but provides superior probability estimates. We also show how the method of odds likelihood ratios can be used to combine the PNN forecasted values with subjective managerial beliefs to improve probability estimates. © 2009 Elsevier Ltd. All rights reserved.",Probabilistic forecasting; Probabilistic neural networks; Software engineering,Pendharkar P.C.,2010,Journal,Expert Systems with Applications,10.1016/j.eswa.2009.11.085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249164468&doi=10.1016%2fj.eswa.2009.11.085&partnerID=40&md5=08e82f18df21537476b7473f1faa111b,"School of Business Administration, Pennsylvania State University at Harrisburg, 777 West Harrisburg Pike, Middletown, PA 17057, United States",,English,09574174,
Scopus,Architecture analysis of enterprise systems modifiability: A metamodel for software change cost estimation,"Enterprise architecture models can be used in order to increase the general understanding of enterprise systems and specifically to perform various kinds of analysis. The present paper proposes a metamodel for enterprise systems modifiability analysis, i. e. assessing the cost of making changes to enterprise-wide systems. The enterprise architecture metamodel is formalized using probabilistic relational models, which enables the combination of regular entity-relationship modeling aspects with means to perform enterprise architecture analysis. The content of the presented metamodel is validated based on survey and workshop data and its estimation capability is tested with data from 21 software change projects. To illustrate the applicability of the metamodel an instantiated architectural model based on a software change project conducted at a large Nordic transportation company is detailed. © 2010 Springer Science+Business Media, LLC.",Enterprise architecture; Metamodel; Probabilistic relational models; Software modifiability,"Lagerström R., Johnson P., Ekstedt M.",2010,Journal,Software Quality Journal,10.1007/s11219-010-9100-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956062351&doi=10.1007%2fs11219-010-9100-0&partnerID=40&md5=89f9b38d8dc390c93da364f88713104f,"Industrial Information and Control Systems, The Royal Institute of Technology, Osquldas väg 12, 100 44 Stockholm, Sweden",Kluwer Academic Publishers,English,09639314,
Scopus,Applying support vector regression for Web effort estimation using a cross-company dataset,"Support Vector Regression (SVR) is a new generation of Machine Learning algorithms, suitable for predictive data modeling problems. The objective of this paper is to investigate the effectiveness of SVR for Web effort estimation, in particular when dealing with a cross-company dataset. To gain a deeper insight on the method, we carried out an empirical study using four kernels for SVR, namely linear, polynomial, Gaussian, and sigmoid. Moreover, we used two variables' preprocessing strategies (normalization and logarithmic), and two different dependent variables (effort and inverse effort). As a result, SVR was applied using six different configurations for each kernel. As for the dataset, we employed the Tukutuku database, which is widely adopted in Web effort estimation studies. A hold-out approach was adopted to evaluate the prediction accuracy for all the configurations, using two training sets, each containing data on 130 projects randomly selected, and two test sets, each containing the remaining 65 projects. As benchmark, SVR-based predictions were also compared to predictions obtained using Manual StepWise Regression, Case-Based Reasoning, and Bayesian Networks. Our results suggest that SVR performed well, since on the first hold-out, the linear kernel with a logarithmic transformation of variables provided significantly superior prediction accuracy than all the other techniques, while for the second hold-out, the Gaussian kernel achieved significantly superior predictions than all other techniques, except for Manual StepWise Regression. © 2009 IEEE.",,"Corazza A., Di Martino S., Ferrucci F., Gravino C., Mendes E.",2009,Conference,"2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",10.1109/ESEM.2009.5315991,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449135256&doi=10.1109%2fESEM.2009.5315991&partnerID=40&md5=6ff9d731bc3ee9cb00c167bb29310ad7,"University of Napoli Federico II, Via Cinthia, I-80126, Napoli, Italy; University of Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy; University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,,9781424448418
Scopus,Farm price prediction using case-based reasoning approach-A case of broiler industry in Taiwan,"Since Taiwan joined the World Trade Organization (WTO) in 2002, pricing decision has become more essential to the development of the broiler industry. The effective prediction of broiler prices is essential from the viewpoint of the agriculture authority and the Poultry Association, thus a more realistic broiler price structure can assist the government to manage the national production resources more effectively. This research proposes a weighted case-based reasoning (CBR) approach to construct a price prediction model. The genetic algorithm model was adopted to find out the most suitable feature weights for CBR. Previous local production data and economic indices, along with information about imported chicken, were collected to build the prediction model. The experimental results indicated that the proposed CBR approach could exhibit a better prediction performance than the ones exhibited by linear regression, regression tree, and neural nets approaches. The findings also revealed that broiler prices were mostly influenced by the prices of colorful broilers and chicks. © 2009 Elsevier B.V. All rights reserved.",Broiler price; Case-based reasoning; Genetic algorithms; Price prediction,"Shih M.L., Huang B.W., Chiu N.-H., Chiu C., Hu W.Y.",2009,Journal,Computers and Electronics in Agriculture,10.1016/j.compag.2008.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349098783&doi=10.1016%2fj.compag.2008.12.005&partnerID=40&md5=782f79215d2b3709dee94a9f124a4fdb,"Department of Social Studies Education, National Tai-Tung University, Tai-Tung, Taiwan; Department of Applied Economics, National Chung-Hsing University, Taichung, Taiwan; Department of Information Management, Ching Yun University, Chungli, Taiwan; Department of Information Management, Yuan Ze University, Chungli, Taiwan",,English,01681699,
Scopus,Software effort estimation by analogy using attribute selection based on rough set analysis,"Estimation by analogy (EBA) predicts effort for a new project by learning from the performance of former projects. This is done by aggregating effort information of similar projects from a given historical data set that contains projects, or objects in general, and attributes describing the objects. While this has been successful in general, existing research results have shown that a carefully selected subset, as well as weighting, of the attributes may improve the performance of the estimation methods. In order to improve the estimation accuracy of our former proposed EBA method AQUA, which supports data sets that have non-quantitative and missing values, an attribute weighting method using rough set analysis is proposed in this paper. AQUA is thus extended to AQUA + by incorporating the proposed attribute weighting and selection method. Better prediction accuracy was obtained by AQUA+ compared to AQUA for five data sets. The proposed method for attribute weighting and selection is effective in that (1) it supports data sets that have non-quantitative and missing values; (2) it supports attribute selection as well as weighting, which are not supported simultaneously by other attribute selection methods; and (3) it helps AQUA+ to produce better performance. © 2008 World Scientific Publishing Company.",Attribute weighting; Effort estimation by analogy; Feature selection; Heuristics; Learning; Rough sets,"Li J., Ruhe G.",2008,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194008003532,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949108552&doi=10.1142%2fS0218194008003532&partnerID=40&md5=2b039e4a22c01b601f598a3a358b3e95,"Software Engineering Decision Support Laboratory, University of Calgary, Calgary, AB T2N1N4, Canada; Department of Computer Science, University of Calgary, 2500 University Dr., NW, Calgary, AB T2N1N4, Canada",,English,02181940,
Scopus,Evaluating ERP projects using DEA and regression analysis,"Enterprise Resource Planning (ERP) projects appear to be a dream come true. This study evaluates ten ERP projects based on their productivity using Data Envelopment Analysis (DEA). The results of the DEA for preeminent ERP projects are uploaded into a project database. Regression analysis is then applied to the data in the project database to predict the efforts required for new ERP projects to acquire high productivity. Extensive literature survey shows that the DEA is the preeminent slant to evaluate ERP projects. The upshots of the study are: (1) Function Points (FP) and project efforts are the performance indicators of the ERP projects from the viewpoint of software engineering; (2) Lines of Code (LOC) have considerable influence over the efficiency of ERP projects; and (3) DEA, in combination with regression analysis, produces fruitful results for the ERP projects. Future directions in the performance enhancement of ERP projects are also indicated. Copyright © 2008 Inderscience Enterprises Ltd.",Data Envelopment Analysis; DEA; Enterprise Resource Planning projects; FP; Function Points; Lines of Code; LOC; Regression analysis; Software engineering,"Parthasarathy S., Anbazhagan N.",2008,Journal,International Journal of Business Information Systems,10.1504/IJBIS.2008.016583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149096232&doi=10.1504%2fIJBIS.2008.016583&partnerID=40&md5=6bc8a79b1a86f9ef7ffc1c863bd6140d,"Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai 625 015, Tamil Nadu, India; Department of Mathematics, Alagappa University, Karaikudi 600 004, Tamil Nadu, India",Inderscience Publishers,English,17460972,
Scopus,Software cost estimation method and application,"Software cost estimation has played an important role in software development since its emergence in 1960's. Based on a classification of algorithmic model based methods, non- algorithmic model based methods and composite methods, the typical software cost estimation methods in history are overall reviewed. The issue of software sizing, which is closely related to software cost estimation, is also discussed in this paper. Then a three phases' evaluation criterion of software cost estimation methods is proposed and a case study on cost estimation of government sponsored projects in China is analyzed. At last, six possible trends from estimation models, estimation evolutions, estimation applications, estimation contents, supporting tools and human factors, are presented as a primary conclusion in the paper while viewing the future development for software cost estimation.",Algorithmic model; Application; Estimation; Evaluation; Software cost estimation; Software sizing,"Li M.-S., He M., Yang D., Shu F.-D., Wang Q.",2007,Journal,Ruan Jian Xue Bao/Journal of Software,10.1360/jos180775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249276084&doi=10.1360%2fjos180775&partnerID=40&md5=825017c2f0eb4ff58b9714c699c7fe57,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China; Graduate School, Chinese Academy of Sciences, Beijing 100049, China",,Chinese,10009825,
Scopus,A survey of software estimation techniques and project planning practices,"Paper provides in depth review of software and project estimation techniques existing in industry and literature, its strengths and weaknesses. Usage, popularity and applicability of such techniques are elaborated. In order to improve estimation accuracy, such knowledge is essential. Many estimation techniques, models, methodologies exists and applicable in different categories of projects. None of them gives 100% accuracy but proper use of them makes estimation process smoother and easier. Organizations should automate estimation procedures, customize available tools and calibrate estimation approaches as per their requirements. Proposed future work is to study factors involved in Software Engineering Approaches (Software Estimation in focus) for Offshore and Outsourced Software Development taking Pakistani IT Industry as a Case Study. © 2006 IEEE.",,Nasir M.,2006,Conference,"Proc. - Seventh ACIS Int. Conf. on Software Eng., Artific. Intelligence, Netw., and Parallel/Distributed Comput., SNPD 2006, including Second ACIS Int. Worshop on SAWN 2006",10.1109/SNPD-SAWN.2006.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845588912&doi=10.1109%2fSNPD-SAWN.2006.11&partnerID=40&md5=9a1aaa24ed6eef3a09ba33f3ca238898,"NUST Institute of Information Technology, National University of Sciences and Technology, Pakistan",,English,,076952611X; 9780769526119
Scopus,"Valuation of software initiatives under uncertainty: Concepts, issues, and techniques","State of the practice in software engineering economics often focuses exclusively on cost issues and technical considerations for decision making. Value-based software engineering (VBSE) expands the cost focus by also considering benefits, opportunities, and risks. Of central importance in this context is valuation, the process for determining the economic value of a product, service, or a process. Uncertainty is a major challenge in the valuation of software assets and projects. This chapter first introduces uncertainty along with other significant issues and concepts in valuation, and surveys the relevant literature. Then it discusses decision tree and options-based techniques to demonstrate how valuation can help with dynamic decision making under uncertainty in software development projects. © 2006 Springer-Verlag Berlin Heidelberg.",decision tree; discounted cash; flow; net present value; real options; Software economics; uncertainty; Valuation,"Erdogmus H., Favaro J., Halling M.",2006,Book Chapter,Value-Based Software Engineering,10.1007/3-540-29263-2_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349105125&doi=10.1007%2f3-540-29263-2_3&partnerID=40&md5=9c8723de4fac7979a37a38516080ad71,"Institute for Information Technology, National Research Council Canada, 1200 Montreal Rd., Ottawa, ON K1A 0R6, Canada; Consulenza Informatica, Via Gamerra 21, 56123 Pisa, Italy; Department of Finance, University of Vienna, Brünnerstr. 72, 1210 Vienna, Austria",Springer Berlin Heidelberg,English,,3540259937; 9783540259930
Scopus,Resource estimation for Web applications,"In the field of Software Engineering, several empirical methods have been developed to model software projects before they are undertaken to provide an estimate of required effort, development time and cost. In the case of web applications, this process is complicated by their complexity, multi-tiered nature, extensive use of non-code artifacts such as multimedia and often short time-scales. In this paper we describe a simple, highly adaptable model using COSMIC Full Function Points for application size measurement and design patterns as a measurement reference. Rather than a true derived model the aim is to provide a procedural framework for expert judgement which guides the practitioner through the estimation process, seeking to limit or mitigate variance in their judgement through algorithmic or statistical techniques. This hybrid has so far proven as accurate as expert judgement while remaining capable of application by a relatively inexperienced estimator. © 2004 IEEE.",,"Umbers P., Miles G.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844288358&doi=10.1109%2fMETRIC.2004.1357922&partnerID=40&md5=4169af0ffd203029d3973f25abd31151,"Internistic Ltd., Birmingham, United Kingdom; Lenoir-Rhyne College, NC, United States",,English,15301435,0769521290
Scopus,Metrics for managing customer view of software quality,"Traditional software metrics, such as code coverage, McCabe complexity, etc. address the needs of a software engineer. In contrast, managers of software development organizations face a broader set of issues. For example, an executive responsible for multiple products and releases has to understand the customer views of those products and put in place, appropriate actions across the products that will be of high business value. We present examples of data and metrics associated with service (i.e. product support) for field reported problems and customer critical situations, and customer satisfaction ratings across a comprehensive range of software product attributes. Issues arising in the data integration, analysis, and correlation of these metrics are highlighted. A systematic methodology for statistical analysis of the data that enables management to derive key, actionable drivers of change through the metrics is presented. We also present an outline of a decision support system developed at IBM for tracking and using software metrics to enable executives to make better informed decisions in supporting their products. © 2003 IEEE.",Customer satisfaction; Databases; Decision support systems; Knowledge management; Product development; Quality management; Software metrics; Software quality; Statistical analysis; Usability,"Chulani S., Ray B., Santhanam P., Leszkowicz R.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943192373&doi=10.1109%2fMETRIC.2003.1232467&partnerID=40&md5=c7cce75b535657c40ff920ebda2dc858,"IBM Almaden Research Center, Mailbox K55-B1, 650 Harry Road, San Jose, CA  95120, United States; IBM T. J. Watson Research Center, 19 Skyline Drive, Hawthorne, NY  10532, United States; IBM Software Group, Rte.100, Somers, NY  10589, United States",IEEE Computer Society,English,15301435,0769519873
Scopus,Evaluating the effect of inheritance on the modifiability of object-oriented business domain models,"This paper describes an experiment to assess the impact of inheritance on the modifiability of object-oriented business domain models. This experiment is part of a research project on the quality determinants of early systems development artefacts, with a special focus on the maintainability of business domain models. Currently there is little empirical information about the relationship between the size, structural and behavioural properties of business domain models and their maintainability. The situation is different in object-oriented software engineering where a number of experimental investigations into the maintainability of object-oriented software have been conducted. The results of our experiment indicate that extensive use of inheritance leads to models that are more difficult to modify. These findings are in line with the conclusions drawn from three similar controlled experiments on inheritance and modifiability of object-oriented software.",,"Poels G., Dedene G.",2001,Conference,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",10.1109/CSMR.2001.914964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035784471&doi=10.1109%2fCSMR.2001.914964&partnerID=40&md5=4bd0942ce9eb904f9b2e2b3e0b9ccd78,"Department of Business Admin., VLEKHO Business School, University of Sciences and Arts, Koningstraat 336, B-1030 Brussel, Belgium; Information Systems Group, Department of Applied Economics, Catholic University of Leuven, Naamsestraat 69, B-3000 Leuven, Belgium",,English,,
Scopus,Estimation of effort and complexity: An object-oriented case study,"The metrication of object-oriented software systems is still an underdeveloped part within the domain of the object paradigm. An empirical investigation aimed at finding appropriate measures and establishing simple, yet usable and cost-effective models for estimation and control of object-oriented system projects, was undertaken on a set of object-oriented projects implemented in a stable environment. First, the measures available were screened for possible correlations; then, the models suitable for estimation were derived and discussed. Effort was found to correlate well with the total number of classes and the total number of methods, both of which are known at the end of the design phase. A number of other models for estimation of the source code complexity were also defined. © 1998 Elsevier Science Inc. All rights reserved.",Object-oriented software; Software estimation; Software measurement,"Mišić V.B., Tešić D.N.",1998,Journal,Journal of Systems and Software,10.1016/S0164-1212(97)10014-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032072282&doi=10.1016%2fS0164-1212%2897%2910014-0&partnerID=40&md5=6fbc6a0864e7a763d3f82ae0bb777eb5,"Dept. of Info. and Syst. Management, Hong Kong Univ. of Sci. and Technol., Clear Water Bay, Kowloon, Hong Kong; Delta Banka AD, Belgrade",Elsevier Inc.,English,01641212,
Scopus,Metric framework for object-oriented real-time systems specification languages,"In this paper, a framework for maintaining control of and analyzing object-oriented system specifications of real-time systems by using a set of metrics covering technical, cognitive, and process-oriented views is presented. The indicators defined can be used for monitoring the evolution of system quality and for effort prediction. The use of metrics for the estimation of reusability, verifiability, and testability is analyzed. The metric framework is integrated in a CASE tool named TOOMS, which is based on TROL, a dual object-oriented language with both descriptive and operational capabilities. TOOMS allows one to describe the system at different levels of structural abstractions and at different levels of specification detail, such as many other languages and models for real-time systems (e.g., OSDL, ObjectTime, ObjectChart). According to this, the metrics proposed are capable of producing estimations at each level of system specification, thus allowing incremental specification/metrication. The metric framework must be regarded as a support for controlling the process of software development in order to guarantee the final quality.",,"Nesi P., Campanai M.",1996,Journal,Journal of Systems and Software,10.1016/0164-1212(95)00064-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030191037&doi=10.1016%2f0164-1212%2895%2900064-X&partnerID=40&md5=62172c9d2d1712f930a1001957d82c65,"Dept. of Systems and Informatics, Faculty of Engineering, University of Florence, Florence, Italy; CESVIT/CQ-ware, Center for Software Quality, Florence, Italy; Dept. of Systems and Informatics, University of Florence, Via S. Marta 3, 50139, Florence, Italy",Elsevier Inc.,English,01641212,
Scopus,Understanding software through numbers: A metric based approach to program comprehension,"There are many approaches to try to comprehend software. Code can be statically analysed and its structure and content displayed as lists, tables or graphs. Most of the literature on software comprehension deals with this approach. Code can also be dynamically executed and its reaction to impulses from outside registered and documented. This is a useful approach to understanding object‐orientated systems. A third approach is not to deal with the code at all, but instead to extract information from existing documents and to combine it into a whole. The problem here is that important documents are often missing and those that are available are out of date. A fourth approach is to interview the experts and to aggregate their views of the software, how it is composed and how it functions as opposed to how it should be composed and how it should function. All of these approaches have been examined and reported on by the participants of the ESPRIT DOCKET Project (1992). A fifth approach to software understanding, the one to be presented here, is that of comprehension through numbers by software managers, i.e. using metrics to support the decision making process. Copyright © 1995 John Wiley & Sons, Ltd",auditing; metrics; software comprehension; software cost estimation; static analysis,Sneed H.M.,1995,Journal,Journal of Software Maintenance: Research and Practice,10.1002/smr.4360070604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029405378&doi=10.1002%2fsmr.4360070604&partnerID=40&md5=01789c44b5b77ba386566307813504d4,"SES Software-engineering Service GmbH, Rosenheimer Landstr. 37, Ottobrunn, D-85521, Germany",,English,1040550X,
Scopus,Which models of the past are relevant to the present? A software effort estimation approach to exploiting useful past models,"Software Effort Estimation (SEE) models can be used for decision-support by software managers to determine the effort required to develop a software project. They are created based on data describing projects completed in the past. Such data could include past projects from within the company that we are interested in (WC projects) and/or from other companies (cross-company, i.e., CC projects). In particular, the use of CC data has been investigated in an attempt to overcome limitations caused by the typically small size of WC datasets. However, software companies operate in non-stationary environments, where changes may affect the typical effort required to develop software projects. Our previous work showed that both WC and CC models of the past can become more or less useful over time, i.e., they can sometimes be helpful and sometimes misleading. So, how can we know if and when a model created based on past data represents well the current projects being estimated? We propose an approach called Dynamic Cross-company Learning (DCL) to dynamically identify which WC or CC past models are most useful for making predictions to a given company at the present. DCL automatically emphasizes the predictions given by these models in order to improve predictive performance. Our experiments comparing DCL against existing WC and CC approaches show that DCL is successful in improving SEE by emphasizing the most useful past models. A thorough analysis of DCL’s behaviour is provided, strengthening its external validity. © 2016, The Author(s).",Cross-company learning; Machine learning; Model-based software effort estimation; Non-stationary environments; Online learning,"Minku L.L., Yao X.",2017,Journal,Automated Software Engineering,10.1007/s10515-016-0209-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007505021&doi=10.1007%2fs10515-016-0209-7&partnerID=40&md5=356f9b84a5fd55b6f6fd63b9a5fbc9bf,"Department of Informatics, University of Leicester, University Road, Leicester, LE1 7RH, United Kingdom; CERCIA, School of Computer Science, University of Birmingham, Edgbaston, Birmingham, B15 2TT, United Kingdom",Springer New York LLC,English,09288910,
Scopus,Improvements to the Function Point Analysis Method: A Systematic Literature Review,"Function point analysis (FPA) is a standardized method to systematically measure the functional size of software. This method is proposed by an international organization and it is currently recommended by governments and organizations as a standard method to be adopted for this type of measurement. This paper presents a compilation of improvements, focused on increasing the accuracy of the FPA method, which have been proposed over the past 13 years. The methodology used was a systematic literature review (SLR), which was conducted with four research questions aligned with the objectives of this study. As a result of the SLR, of the 1600 results returned by the search engines, 454 primary studies were preselected according to the criteria established for the SLR. Among these studies, only 18 specifically referred to accuracy improvements for FPA, which was the goal of this study. The low number of studies that propose FPA improvements might demonstrate the maturity of the method in the current scenario of software metrics. Specifically in terms of found issues, it was found that the step for calculating the functional size exhibited the highest number of problems, indicating the need to revise FPA in order to encompass the possible improvements suggested by the researchers. © 1988-2012 IEEE.",Accuracy improvement; function point analysis (FPA); systematic literature review (SLR),"De Freitas Junior M., Fantinato M., Sun V.",2015,Journal,IEEE Transactions on Engineering Management,10.1109/TEM.2015.2453354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960284185&doi=10.1109%2fTEM.2015.2453354&partnerID=40&md5=609cd48f9dfd8a67813b3a6fb6f77f3f,"School of Sciences, Arts and Humanities, University of Sao Paulo, Sao Paulo, 03828-000, Brazil",Institute of Electrical and Electronics Engineers Inc.,English,00189391,
Scopus,Early effort estimation in web application development,"Project planning in software industry represents one of the most complex tasks, especially when there is a need to estimate the time, cost and effort needed for development of software projects. In the field of development effort estimation for classical software projects a number of methods have been developed, tested and successfully implemented. Web projects are, by their nature, different than classical software projects, and there is a lack of methods and models that provides a high degree of confidence in development effort estimation. This paper analyzes the possibility of using a combination of functional size and conceptual models for the purpose of web application development effort estimation. Measurement of functional size can be effectively applied to the conceptual models of the data-driven web applications because of the existence of extensive count of data movements. For the purpose of this study 19 web applications with their conceptual models were employed. An effort model was built using simple linear regression analysis. Upon construction, evaluation and validation of the effort model prediction accuracy elements, R2, MMRE, and Pred(l), showed promising results for web projects used in the model construction and validation process. © 2015 Elsevier Inc. All rights reserved.",Effort estimation; Function points; Web Application,"Čeke D., Milašinović B.",2015,Conference,Journal of Systems and Software,10.1016/j.jss.2015.02.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924862124&doi=10.1016%2fj.jss.2015.02.006&partnerID=40&md5=b3b3bee5d74408c9e387ba3c7fe744fe,"Quality Assurance Office, University of Tuzla, Bosnia and Herzegovina; Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia",Elsevier Inc.,English,01641212,
Scopus,Improving fuzzy analogy based software development effort estimation,"Analogy-based estimation has recently emerged as a promising technique and a viable alternative to other conventional estimation methods. One of the most important research areas for analogy-based cost estimation is how to predict the effort of software projects when they are described by mixed numerical and categorical data. To address this issue, we have proposed, in an earlier work, a new approach called fuzzy analogy combining the key features of fuzzy logic and analogybased reasoning. However, fuzzy analogy may only be used when the possible values of the categorical attributes are derived from a numerical domain. The current study aims to extend our former approach to correctly handle categorical data. To this end, the fuzzy k-modes algorithm is used with two initialization techniques. The performance of the proposed approach was compared with that of classical analogy using the International Software Benchmarking Standards Group (ISBSG) dataset. The obtained results show significant improvement in estimation accuracy. © 2014 IEEE.",Case-based reasoning; Fuzzy clustering; Fuzzy logic; Mixed dataset; Software effort estimation,"Amazal F.A., Idri A., Abran A.",2014,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2014.46,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951282652&doi=10.1109%2fAPSEC.2014.46&partnerID=40&md5=864ca37db60e0c0338a8f4ae557a43a4,"Software Projects Management Research Team, ENSIAS, Mohamed v University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Superieure, Montreal, H3C IK3, Canada",IEEE Computer Society,English,15301362,9781479974252
Scopus,An analogy-based approach to estimation of software development effort using categorical data,"Analogy-based software development effort estimation methods have proved to be a viable alternative to other conventional estimation methods since they mimic the human problem solving approach. However, they are limited by their inability to correctly handle categorical data. Therefore, we have proposed, in an earlier work, a new approach called fuzzy analogy which extends classical analogy by incorporating the fuzzy logic concept in the estimation process. The proposed approach may be applied only when the categorical values are derived from numerical data. This paper extends fuzzy analogy to deal with categorical values that are not derived from numerical data. To this aim, we used the fuzzy k-modes algorithm, a well-known clustering technique for large datasets containing categorical values. Thereafter, we evaluate the accuracy of fuzzy analogy construction-based on fuzzy k-modes using the ISBSG R8 dataset. This evaluation shows that our proposed approach leads to significant improvement in estimation accuracy. © 2014 IEEE.",case-based reasoning; categorical data; fuzzy clustering; fuzzy logic; software effort estimation,"Amazal F.-A., Idri A., Abran A.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929643263&doi=10.1109%2fIWSM.Mensura.2014.31&partnerID=40&md5=84eb80725af9b61fcbeeb71e6ba04eb9,"Software Projects Management Research Team, ENSIAS, Mohamed v University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Superieure, Montreal, H3C IK3, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,A Particle Swarm Optimized Functional Link Artificial Neural Network (PSO-FLANN) in software cost estimation,We use particle swarm optimization (PSO) to train the functional link artificial neural network (FLANN) for software effort prediction. The combined framework is known as PSO-FLANN. This framework exploits the global classification capability of PSO and FLANN's complex nonlinear mapping between its input and output pattern space by using functional expansion. The Chebyshev polynomial has been used as choice of expansion in FLANN to exhaustively study the performance in three real time datasets. The simulation results show that it not only deals efficiently with noisy data but achieves improved accuracy in prediction. © 2013 Springer-Verlag.,Functional Link Artificial Neural Networks; Particle Swarm optimization; Software cost estimation,"Benala T.R., Chinnababu K., Mall R., Dehuri S.",2013,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-642-35314-7_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872222850&doi=10.1007%2f978-3-642-35314-7_8&partnerID=40&md5=0545315966cfecf32e4e93139a930ccd,"Anil Neerukonda Institute of Technology and Sciences, Sangivalasa-531162, Visakhapatnam, Andhra Pradesh, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of Systems Engineering, Ajou University, San 5, Woncheon-dong, Yeongtong-gu, Suwon 443-749, South Korea",Springer Verlag,English,21945357,9783642353130
Scopus,Probabilistic size proxy for software effort prediction: A framework,"Software effort prediction is an important and challenging activity that takes place during the early stages of software development, where costing is needed. Software size estimate is one of the most popular inputs for software effort prediction models. Accordingly, providing a size estimate with good accuracy early in the lifecycle is very important; it is equally challenging too. Estimates that are computed early in the development lifecycle, when it is needed the most, are typically associated with uncertainty. However, none of the prominent software effort prediction techniques or software size metrics addresses this issue satisfactorily. In this paper, we propose a framework for developing probabilistic size proxies for software effort prediction using information from conceptual UML models created early in the software development lifecycle. The framework accounts for uncertainty in software size and effort prediction by providing the estimate as a probability density function instead of a certain value. We conducted a case study using open source datasets and the results were encouraging. © 2012 Elsevier B.V. All rights reserved.",Effort prediction; Probabilistic error; Size proxy; Uncertainty,"Ahmed M.A., Ahmad I., Alghamdi J.S.",2013,Journal,Information and Software Technology,10.1016/j.infsof.2012.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869870359&doi=10.1016%2fj.infsof.2012.08.001&partnerID=40&md5=ce17ed33fdb0fe87a7e74fe7e2f0b7e9,"King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia",Elsevier B.V.,English,09505849,
Scopus,Preliminary results of a systematic review on requirements evolution,"Background: Software systems must evolve in order to adapt in a timely fashion to the rapid changes of stakeholder needs, technologies, business environment and society regulations. Numerous studies have shown that cost, schedule or defect density of a software project may escalate as the requirements evolve. Requirements evolution management has become one important topic in requirements engineering research. Aim: To depict a holistic state-of-the-art of requirement evolution management. Method: We undertook a systematic review on requirements evolution management. Results: 125 relevant studies were identified and reviewed. This paper reports the preliminary results from this review: (1) the terminology and definition of requirements evolution; (2) fourteen key activities in requirements evolution management; (3) twenty-eight metrics of requirements evolution for three measurement goals. Conclusions: Requirements evolution is a process of continuous change of requirements in a certain direction. Most existing studies focus on how to deal with evolution after it happens. In the future, more research attention on exploring the evolution laws and predicting evolution is encouraged.",management process; measurement; requirements change; requirements evolution; systematic literature review,"Li J., Zhang H., Zhu L., Jeffery R., Wang Q., Li M.",2012,Conference,IET Seminar Digest,10.1049/ic.2012.0002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865520818&doi=10.1049%2fic.2012.0002&partnerID=40&md5=ab41dde984b9db3f4ac420203c1775b4,"Institute of Software, Chinese Academy of Sciences, China; National ICT Australia, Australia; School of Computer Science and Engineering, University of New South Wales, Australia",,English,,9781849195416
Scopus,A permutation test based on regression error characteristic curves for software cost estimation models,"Background: Regression Error Characteristic (REC) curves provide a visualization tool, able to characterize graphically the prediction power of alternative predictive models. Due to the benefits of using such a visualization description of the whole distribution of error, REC analysis was recently introduced in software cost estimation to aid the decision of choosing the most appropriate cost estimation model during the management of a forthcoming project. Aims: Although significant information can be retrieved from a readable graph, REC curves are not able to assess whether the divergences between the alternative error functions can constitute evidence for a statistically significant difference. Method: In this paper, we propose a graphical procedure that utilizes (a) the process of repetitive permutations and (b) and the maximum vertical deviation between two comparative Regression Error Characteristic curves in order to conduct a hypothesis test for assessing the statistical significance of error functions. Results: In our case studies, the data used come from software projects and the models compared are cost prediction models. The results clearly showed that the proposed statistical test is necessary in order to assess the significance of the superiority of a prediction model, since it provides an objective criterion for the distances between the REC curves. Moreover, the procedure can be easily applied to any dataset where the objective is the prediction of a response variable of interest and the comparison of alternative prediction techniques in order to select the best strategy. Conclusions: The proposed hypothesis test, accompanying an informative graphical tool, is more easily interpretable than the conventional parametric and non-parametric statistical procedures. Moreover, it is free from normality assumptions of the error distributions when the samples are small-sized and highly skewed. Finally, the proposed graphical test can be applied to the comparisons of any alternative prediction methods and models and also to any other validation procedure. © 2011 Springer Science+Business Media, LLC.",Accuracy measures; Cost prediction models; Graphical comparison; Hypothesis test; Permutation test; REC curves,"Mittas N., Angelis L.",2012,Journal,Empirical Software Engineering,10.1007/s10664-011-9177-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857374517&doi=10.1007%2fs10664-011-9177-5&partnerID=40&md5=fc650a2d8c3a3cf646321f1fa5ab6c09,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,13823256,
Scopus,A comparative evaluation of effort estimation methods in the software life cycle,"Even though there are a number of software size and effort measurement methods proposed in literature, they are not widely adopted in the practice. According to literature, only 30% of software companies use measurement, mostly as a method for additional validation. In order to determine whether the objective metric approach can give results of the same quality or better than the estimates relying on work breakdown and expert judgment, we have validated several standard functional measurement and analysis methods (IFPUG, NESMA, Mark II, COSMIC, and use case points), on the selected set of small and medium size real-world web based projects at CMMI level 2. Evaluation performed in this paper provides objective justification and guidance for the use of a measurement-based estimation in these kinds of projects.",Comparative analysis; Effort estimation; Empirical evaluation; Software measurement,"Popović J., Bojić D.",2012,Journal,Computer Science and Information Systems,10.2298/CSIS110316068P,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858650272&doi=10.2298%2fCSIS110316068P&partnerID=40&md5=87a4d603d6c3f596fb71f89ec058f15b,"Faculty of Electrical Engineering, University of Belgrade, Bulevar Kralja Aleksandra 73, 11000 Belgrade, Serbia",,English,18200214,
Scopus,E-cosmic: A business process model based functional size estimation approach,"Software size is the key input for cost and effort estimation models. The effort and cost estimations as well as software size is needed at as early a phase of the project as possible. Conventional Early Functional Size Estimation methods enable predicting size at early phases but frequently variation of the results are high due to manual calculations and requirements of high degree of interpretation in situations where there is a lack of structured artifacts. On the other hand, automated Functional Size Measurement calculation approaches require constructs which are available in considerably late software development phases. In this study, an approach called e-Cosmic to overcome the reliability and subjectivity problems of early size estimation models is developed. As part of the approach a mapping between functional size measurement components of COSMIC FSM Method and common business process model constructs, was developed. The proposed method (e-Cosmic) is validated by three case studies. The result of the case studies has consistently showed less than 10% deviation with respect to the COSMIC FSM results. In addition, the automation of the estimation procedure eliminated measurer dependency and effort in conventional early functional size estimation methods. © 2011 IEEE.",Business Process Model; Early Size Estimation; Functional Size Measurement,"Kaya M., Demirörs O.",2011,Conference,"Proceedings - 37th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2011",10.1109/SEAA.2011.60,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955237724&doi=10.1109%2fSEAA.2011.60&partnerID=40&md5=8576a587e3c49f838c4f290d8e501633,"Middle East Technical University, Informatics Institute, Ankara, Turkey",,English,,9780769544885
Scopus,Estimating web application development effort using Web-COBRA and COSMIC: An empirical study,"Even if the adaptation of the COBRA method to the web has many appealing characteristics, its effectiveness in predicting Web application development effort has been previously assessed only in one empirical study, using the Web Objects measure. The study provided positive results, but it is widely recognized that several empirical investigations should be performed to validate/confirm the usefulness of effort estimation approaches. Moreover, the use of other size measures in combination with Web-COBRA has not yet been investigated in the literature. These two aspects encouraged us to further analyze the effectiveness of Web-COBRA. In particular, we used a dataset of 15 Web applications, whose functional size was measured using COSMIC. In this paper we describe the empirical analysis we conducted, whose results confirm that Web-COBRA can be profitably exploited for estimating Web application development effort, also when used in combination with COSMIC. © 2009 IEEE.",COSMIC; Effort estimation; Web applications,"Ferrucci F., Gravino C., Di Martino S.",2009,Conference,Conference Proceedings of the EUROMICRO,10.1109/SEAA.2009.47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549169919&doi=10.1109%2fSEAA.2009.47&partnerID=40&md5=d7109c26b566b4fb234c7d24dc0de5ca,"Dipartimento di Matematica e Informatica, University of Salerno, Via Ponte Don Melillo, Fisciano (SA), Italy; Dipartimento di Scienze Fisiche, Università degli Studi di Napoli Federico II, Via Cinthia, I-80126 Napoli, Italy",,English,10896503,9780769537849
Scopus,Investigating the use of chronological split for software effort estimation,"In previous studies, the authors investigated separately the use of two different types of chronological splits (project-by-project split and date-based split) for assigning projects to training sets and testing sets. The aim of this study is to compare the two types of chronological splits against each other, to see whether either leads to better prediction accuracy. Estimation models were built and evaluated using training and testing sets formed using project-by-project chronological splitting and date-based splitting using two different splitting dates. The authors used 906 projects from the ISBSG Release 10 repository. The authors found no significant differences between the accuracy of models built and evaluated with either of the different splitting methods. However, models built using later splitting dates were more accurate than models built using earlier splitting dates. Different accuracy with different splitting dates suggests that chronological splitting is useful. Therefore the authors recommend that training and testing sets should be formed with regard to chronology, and a date-based split appears sensible for researchers in this field. © 2009 The Institution of Engineering and Technology.",,"Lokan C., Mendes E.",2009,Journal,IET Software,10.1049/iet-sen.2008.0107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349728597&doi=10.1049%2fiet-sen.2008.0107&partnerID=40&md5=61c397b638520d8de02b976d99905967,"School of Engineering and Information Technology, UNSW at ADFA, Canberra, ACT 2600, Australia; Computer Science Department, University of Auckland, Private Bag, 92019 Auckland, New Zealand",,English,17518806,
Scopus,Investigating the use of chronological splitting to compare software cross-company and single-company effort predictions,"CONTEXT: Numerous studies have investigated the use of cross-company datasets to estimate effort for single-company projects; however to date only one has compared the effect of using a chronological split instead of a random split to assign projects to a training set and a validation set, finding no significant differences. OBJECTIVE: The aim of this study is to extend [15] using a project-by-project chronological split, and also to investigate how this type of split affects the results when comparing within- to cross-company effort estimation. METHOD: Chronological splitting was compared with two forms of cross-validation. Here a more realistic form of chronological splitting than the one used in [15] is investigated, in which a validation set contains a single project, and a regression model is built from scratch using as training set the set of projects completed before the validation project’s start date. We used 228 single-company projects and 678 cross-company projects from the ISBSG Release 10 repository. RESULTS: We obtained contradictory results when comparing cross- to single-company predictions for single-company projects. First, when results were compared using absolute residuals there were no differences between cross- and single-company predictions, or between techniques. However, when using z values, chronological splitting favoured cross-company models, and cross-validation (both types) favoured single-company models. CONCLUSIONS: Results were promising when using project-by-project splitting because: i) they favoured cross-company models; and ii) this type of splitting mimics an effort estimation scenario in a real environment. However, these results were obtained using z values only. Therefore we urge future studies comparing prediction models to document results obtained using both z values and absolute residuals, such that a full picture can be provided. © 2008 Evaluation and Assessment in Software Engineering. All rights reserved.",Chronological split; Cross-company estimation models; Effort estimation; Regression-based estimation models; Single-company estimation models; Software projects,"Lokan C., Mendes E.",2008,Conference,"12th International Conference on Evaluation and Assessment in Software Engineering, EASE 2008",10.14236/ewic/ease2008.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088204257&doi=10.14236%2fewic%2fease2008.15&partnerID=40&md5=a700f37cb8d90d855ee9e28c34b497eb,"School of IT and EE, UNSW@ADFA, Canberra, ACT  2600, Australia; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",BCS Learning and Development Ltd.,English,,
Scopus,Assessing the reproducibility and accuracy of functional size measurement methods through experimentation,"A number of Functional Size Measurement (FSM) methods have been proposed in the literature, but so far there has been little systematic evaluation of these methods. This paper describes a controlled experiment which compares Function Point Analysis, a standard FSM method supported by the International Function Point Users Group (IFPUG FPA) and OO-Method Function Points (OOmFP), a recently proposed FSM method for sizing object-oriented software systems that are developed using the OO-Method approach. The goal is to investigate whether OOmFP results in better size assessments within the context of an OO-Method development process. The methods are compared using two criteria defined in the ISO 14143-3: reproducibility and accuracy. The results show that, within its context, OOmFP is more consistent and accurate than IFPUG FPA.",Empirical Validation; FSM methods; IFPUG Function Point Analysis; OO-Method Function Points,"Abrahão S., Poels G., Pastor O.",2004,Conference,"Proceedings - 2004 International Symposium on Empirical Software Engineering, ISESE 2004",10.1109/isese.2004.1334906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244324051&doi=10.1109%2fisese.2004.1334906&partnerID=40&md5=4cf808893f063a48b5dfcd10ff3f0172,"Department of Information Systems, Valencia University of Technology, Camino de Vera, s/n, 46022, Valencia, Spain; Fac. of Econ. and Bus. Admin., Ghent University, Hoveniersberg 24, 9000 Ghent, Belgium",IEEE Computer Society,English,,0769521657; 9780769521657
Scopus,Towards a software change classification system: A rough set approach,"The basic contribution of this paper is the presentation of two methods that can be used to design a practical software change classification system based on data mining methods from rough set theory. These methods incorporate recent advances in rough set theory related to coping with the uncertainty in making change decisions either during software development or during post-deployment of a software system. Two well-known software engineering data sets have been used as means of benchmarking the proposed classification methods, and also to facilitate comparison with other published studies on the same data sets. Two technologies in computation intelligence (CI) are used in the design of the software change classification systems described in this paper, namely, rough sets (a granular computing technology) and genetic algorithms. Using 10-fold cross validated paired t-test, this paper also compares the rough set classification learning method with the Waikato Environment for Knowledge Analysis (WEKA) classification learning method. The contribution of this paper is the presentation of two models for software change classification based on two CI technologies.",Classification learning; Computational intelligence; Data mining; Genetic algorithm; Paired t-test; Rough sets; Software change classification; Software quality; Ten-fold cross validation,"Peters J.F., Ramanna S.",2003,Review,Software Quality Journal,10.1023/A:1023764510838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543075180&doi=10.1023%2fA%3a1023764510838&partnerID=40&md5=d10fcc789f40cdac4e6171b92060b75a,"Department of Electrical Engineering, University of Manitoba, Winnipeg, Man. R3T 5V6, Canada",Kluwer Academic Publishers,English,09639314,
Scopus,Optimal allocation of testing resources for modular software systems,"In this paper, based on software reliability growth models with generalized logistic testing-effort function, we study three optimal resource allocation problems in modular software systems during the testing phase: 1) minimization of the remaining faults when a fixed amount of testing-effort and a desired reliability objective are given; 2) minimization of the required amount of testing-effort when a specific number of remaining faults and a desired reliability objective are given; and 3) minimization of the cost when the number of remaining faults and a desired reliability objective are given. Several useful optimization algorithms based on the Lagrange multiplier method are proposed and numerical examples are illustrated. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. In addition, we also introduce the testing-resource control problem and compare different resource allocation methods. Finally, we demonstrate how these analytical approaches can be employed in the integration testing. Using the proposed algorithms, project managers can allocate limited testing-resource easily and efficiently and thus achieve the highest reliability objective during software module and integration testing. © 2002 IEEE.",Cost function; Lagrangian functions; Logistics; Optimization methods; Resource management; Software algorithms; Software reliability; Software systems; Software testing; System testing,"Huang C.-Y., Lo J.-H., Kuo S.-Y., Lyu M.R.",2002,Conference,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",10.1109/ISSRE.2002.1173228,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948767261&doi=10.1109%2fISSRE.2002.1173228&partnerID=40&md5=7be9ff004f2e2f07b104ddf48af777c2,"Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Computer Science and Engineering Department, Chinese University of Hong Kong, Shating, Hong Kong",IEEE Computer Society,English,10719458,0769517633
Scopus,The importance of human factors in planning the requirements capture stage of a project,"One of the reasons why existing software planning models fail to give accurate, consistent and reliable results is that they are based on inadequate factors which cannot be measured at the beginning of the development process. This paper presents the theoretical aspects of a new model that attempts to overcome the problems faced by existing models through considering human factors which are relevant to the process of requirements capture and analysis (RCA). Emphasis is given to those factors whose value can be measured at the beginning of the development process. The factors that are examined are classified into four main categories: team members' attitude, users' attitude, project management and technology, and finally, project characteristics. An explanation of all the factors is provided and a literature review is given. Finally, an analytical description of the research methodology followed and a brief presentation of the model developed are given. Copyright © 1996 Elsevier Science Ltd and IPMA.",Empirical research; Human factors; I.S. Project planning; Information requirements determination; Productivity; Rule-based representation,"Chatzoglou P.D., Macaulay L.A.",1997,Journal,International Journal of Project Management,10.1016/S0263-7863(96)00038-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031078244&doi=10.1016%2fS0263-7863%2896%2900038-5&partnerID=40&md5=fe61eba76e9eee520102322bd780c0af,"University of Cyprus, 75 Kallipoleous Street, CY 1678 Nicosia, Cyprus; Department of Computation, UMIST, PO Box 88, Manchester, M60 1QD, United Kingdom",Elsevier BV,English,02637863,
Scopus,Software-effort estimation with a case-based reasoner,"Software effort estimation is an important but difficult task. Existing algorithmic models often fail to predict effort accurately and consistently. To address this, we developed a computational approach to software effort estimation. cEstor is a case-based reasoning engine developed from an analysis of expert reasoning. cEstor's architecture explicitly separates case-independent productivity adaptation knowledge (rules) from case-specific representations of prior projects encountered (cases). Using new data from actual projects, uncalibrated cEstor generated estimates which compare favorably to those of the referent expert, calibrated Function Points and calibrated COCOMO. The estimates were better than those produced by uncalibrated Basic COCOMO and Intermediate COCOMO. The roles of specific knowledge components in cEstor (cases, adaptation rules, and retrieval heuristics) were also examined. The results indicate that case-independent productivity adaptation rules affect the consistency of estimates and appropriate case selection affects the accuracy of estimates, but the combination of an adaptation rule set and unrestricted case base can yield the best estimates. Retrieval heuristics based on source lines of code and a Function Count heuristic based on summing over differences in parameter values, were found to be equivalent in accuracy and consistency, and both performed better than a heuristic based on Function Count totals. © 1996 Taylor & Francis Group, LLC.",,"Prietula M.J., Vicinanza S.S., Mukhopadhyay T.",1996,Journal,Journal of Experimental and Theoretical Artificial Intelligence,10.1080/095281396147366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030503529&doi=10.1080%2f095281396147366&partnerID=40&md5=ad490cf15243d117d5564bda392f8ecb,"Centre for Accounting Research and Professional Education, Fisher School of Accounting, University of Florida, Gainesville, FL, 32611, United States; Energy Management Associates, Electronic Data Systems Inc., 100 Northcreek, Atlanta, GA, 30327, United States; Center for the Management of Technology, Graduate School of Industrial Administration, Carnegie Mellon University, Pittsburgh, PA, 15213, United States",,English,0952813X,
Scopus,An experiment in software sizing with structured analysis metrics,"This paper presents an experiment that was carried out to develop and statistically validate a model that estimates the number of lines of code in a system. The experiment, described using an experimentation frame-work, was conducted at three Canadian organizations. A linear model is developed for each of the two samples studied. These models are based on measurements taken on data flow diagrams. Statistical analysis reveals that the models produce sound estimates of the actual data (R2 = 0.724 and R2 = 0.563). Four evaluation criteria show that these models compare favorably to the other software-sizing models found in the literature. © 1991.",,"Bourque P., Côté V.",1991,Journal,The Journal of Systems and Software,10.1016/0164-1212(91)90053-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005424661&doi=10.1016%2f0164-1212%2891%2990053-9&partnerID=40&md5=12215d583200d8ffbb92e5c598999092,"Département de mathématiques et d'informatique, Université de Sherbrooke, Sherbrooke, Canada",,English,01641212,
Scopus,Evaluating Pred(p) and standardized accuracy criteria in software development effort estimation,"Software development effort estimation (SDEE) plays a primary role in software project management. But choosing the appropriate SDEE technique remains elusive for many project managers and researchers. Moreover, the choice of a reliable estimation accuracy measure is crucial because SDEE techniques behave differently given different accuracy measures. The most widely used accuracy measures in SDEE are those based on magnitude of relative error (MRE) such as mean/median MRE (MMRE/MedMRE) and prediction at level p (Pred(p)), which counts the number of observations where an SDEE technique gave MREs lower than p. However, MRE has proven to be an unreliable accuracy measure, favoring SDEE techniques that underestimate. Consequently, an unbiased measure called standardized accuracy (SA) has been proposed. This paper deals with the Pred(p) and SA measures. We investigate (1) the consistency of Pred(p) and SA as accuracy measures and SDEE technique selectors, and (2) the relationship between Pred(p) and SA. The results suggest that Pred(p) is less biased towards underestimates and generally selects the same best technique as SA. Moreover, SA and Pred(p) measure different aspects of technique performance, and SA may be used as a predictor of Pred(p) by means of the 3 association rules. Copyright © 2017 John Wiley & Sons, Ltd.",accuracy measure; MMRE; Pred(p); software development effort estimation; standardized accuracy,"Idri A., Abnane I., Abran A.",2018,Journal,Journal of Software: Evolution and Process,10.1002/smr.1925,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045686530&doi=10.1002%2fsmr.1925&partnerID=40&md5=3a3d72a1a9b32d7a365db2168b5e3a45,"Software Project Management Research Team, ENSIAS, University Mohammed V, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",John Wiley and Sons Ltd,English,20477481,
Scopus,Effort estimation in agile software development: Case study and improvement framework,"Effort estimation is more challenging in an agile context, as instead of exerting strict control over changes in requirements, dynamism is embraced. Current practice relies on expert judgment, where the accuracy of estimates is sensitive to the expertise of practitioners and prone to bias. To improve the effectiveness of the effort estimation process, the goal of this research is to investigate and understand the estimation process with respect to its accuracy in the context of agile software development from the perspective of agile development teams. Using case study research, 2 observations and eleven interviews were conducted with 3 agile development teams at SAP SE, a German multinational software corporation. The results reveal that factors such as the developer's knowledge, experience, and the complexity and impact of changes on the underlying system affect the magnitude as well as estimation accuracy. Furthermore, there is a need for a tool that incorporates expert knowledge, enables explicit consideration of cost drivers by experts and visualizes this information to improve the effectiveness of the effort estimation. On the basis of the findings of the case study, a framework, inspired by the quality improvement paradigm is proposed to improve effort estimation in agile development. Copyright © 2017 John Wiley & Sons, Ltd.","agile development, change impact analysis, effort estimation, framework, QIP","Tanveer B., Guzmán L., Engel U.M.",2017,Conference,Journal of Software: Evolution and Process,10.1002/smr.1862,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016564285&doi=10.1002%2fsmr.1862&partnerID=40&md5=8ee329461eb2f00e021e444a8188f4d7,"Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer Platz-1, Kaiserslautern  67663, Germany; SAP SE, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany and Institute for Theoretical Physics, University of Münster, Germany",John Wiley and Sons Ltd,English,20477481,
Scopus,Investigating heterogeneous ensembles with filter feature selection for software effort estimation,"Ensemble Effort Estimation (EEE) consists on predicting the software development effort by combining more than one single estimation technique. EEE has recently been investigated in software development effort estimation (SDEE) in order to improve the estimation accuracy. The overall results suggested that the EEE yield better prediction accuracy than single techniques. On the other hand, feature selection (FS) methods have been used in the area of SDEE for the purpose of reducing the dimensionality of a dataset size by eliminating the irrelevant and redundant features. Thus, the SDEE techniques are trained on a dataset with relevant features which can lead to improving the accuracy of their estimations. This paper aims at investigating the impact of two Filter feature selection methods: Correlation based Feature Selection (CFS) and RReliefF on the estimation accuracy of Heterogeneous (HT) ensembles. Four machine learning techniques (K-Nearest Neighbor, Support Vector Regression, Multilayer Perceptron and Decision Trees) were used as base techniques for the HT ensembles of this study. We evaluate the accuracy of these HT ensembles when their base techniques were trained on datasets preprocessed by the two feature selection methods. The HT ensembles use three combination rules: average, median, and inverse ranked weighted mean. The evaluation was carried out by means of eight unbiased accuracy measures through the leave-one-out-cross validation (LOOCV) technique over six datasets. The overall results suggest that all the attributes of most datasets used are relevant for building an accurate predictive technique since the ensembles constructed without features selection outperformed in general the ones using features selection. As for the combination rule, the median generally produces better results than the other two used in this empirical study. © 2017 Association for Computing Machinery.",Accuracy; Ensemble Effort Estimation; Features Selection; Filter; Machine Learning,"Hosni M., Idri A., Abran A.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3143434.3143456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038388727&doi=10.1145%2f3143434.3143456&partnerID=40&md5=65e1ed73d73e106bcf07adfa820d9557,"Software Project Management Research Team, Mohammed V University Rabat, Morocco; Software Project Management Research Team, Mohammed V University Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",Association for Computing Machinery,English,,9781450348539
Scopus,Multi-criteria decision making approach for the selection of software effort estimation model,"Software development with minimum effort has become a challenging task for the software developers. Software effort may be defined as the prediction process of the effort required to develop any software. Many software effort estimation models have been developed in the past, but it is observed that none of them can be applied successfully in all kinds of projects in different environments that raise the problem of the software effort estimation model selection. To select the suitable software effort estimation model, many conflicting selection criteria must be considered in the decision process. The present study emphasizes on the development of a fuzzy multi-criteria decision making approach by integrating Fuzzy Set Theory and Weighted Distance Based Approximation. To show the consistency of the proposed approach, methodology validation is also performed by making comparison with existing methodologies and to check the criticality of the selection criterion, sensitivity analysis is also performed. © 2017 Growing Science Ltd. All rights reserved and by the authors.",Effort estimation; MCDM; Selection criteria; WDBA,"Bansal A., Kumar B., Garg R.",2017,Journal,Management Science Letters,10.5267/j.msl.2017.3.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043583323&doi=10.5267%2fj.msl.2017.3.003&partnerID=40&md5=25ee77d71b54d9634e3eb4bd98f3d5f4,"Department of Computer Science & Engineering, K. R. Mangalam University, Gurgaon, Haryana, India; Department of Computer Science & Engineering, Hindu College of Engineering, Sonepat, Haryana, India",Growing Science,English,19239335,
Scopus,Adressing problems with external validity of repository mining studies through a smart data platform,"Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present Smart-SHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems. © 2016 ACM.",Smart data; Software analytics; Software mining,"Trautsch F., Herbold S., Makedonski P., Grabowski J.",2016,Conference,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",10.1145/2901739.2901753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974527552&doi=10.1145%2f2901739.2901753&partnerID=40&md5=ddc74744bb6b7e64a5929835d0a41236,"Institute for Computer Science, Georg-August-Universität, Göttingen, Germany","Association for Computing Machinery, Inc",English,,9781450341868
Scopus,Software evolution and maintenance: A practitioner's approach,"Software Evolution and Maintenance: A Practitioner's Approach introduces readers to a set of well-rounded educational materials, covering the fundamental developments in software evolution and common maintenance practices in the industry. Each chapter gives a clear understanding of a particular topic in software evolution, and discusses the main ideas with detailed examples. The authors first explain the basic concepts and then drill deeper into the important aspects of software evolution. While designed as a text in an undergraduate course in software evolution and maintenance, the book is also a great resource forsoftware engineers, information technology professionals, and graduate students in software engineering. Based on the IEEE SWEBOK (Software Engineering Body of Knowledge). Explains two maintenance standards: IEEE/EIA 1219 and ISO/IEC14764. Discusses several commercial reverse and domain engineering toolkits. Slides for instructors are available online. Software Evolution and Maintenance: A Practitioner's Approach equips readers with a solid understanding of the laws of software engineering, evolution and maintenance models, reengineering techniques, legacy information systems, impact analysis, refactoring, program comprehension, and reuse. © 2015 by John Wiley & Sons, Inc. All rights reserved.",,"Tripathy P., Naik K.",2014,Book,Software Evolution and Maintenance: A Practitioner's Approach,10.1002/9781118964637,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926076007&doi=10.1002%2f9781118964637&partnerID=40&md5=edf03b341aba20d83c6e0c57e4382451,"Knowledge Trust, Bhubaneswar, India; Department of Electrical and Computer Engineering, University of Waterloo, Canada",Wiley Blackwell,English,,9781118964637; 9780470603413
Scopus,On the use of machine learning and search-based software engineering for ill-defined fitness function: A case study on software refactoring,"The most challenging step when adapting a search-based technique for a software engineering problem is the definition of the fitness function. For several software engineering problems, a fitness function is ill-defined, subjective, or difficult to quantify. For example, the evaluation of a software design is subjective. This paper introduces the use of a neural network-based fitness function for the problem of software refactoring. The software engineers evaluate manually the suggested refactoring solutions by a Genetic Algorithm (GA) for few iterations then an Artificial Neural Network (ANN) uses these training examples to evaluate the refactoring solutions for the remaining iterations. We evaluate the efficiency of our approach using six different open-source systems through an empirical study and compare the performance of our technique with several existing refactoring studies. © 2014 Springer International Publishing Switzerland.",,"Amal B., Kessentini M., Bechikh S., Dea J., Said L.B.",2014,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-09940-8_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958543478&doi=10.1007%2f978-3-319-09940-8_3&partnerID=40&md5=c1149130a3b170bc01bb57adc55db78b,"University of Michigan, United States; University of Tunis, Tunisia",Springer Verlag,English,03029743,9783319099392
Scopus,How to build a good practice software project portfolio?,"What can we learn from historic data that is collected in three software companies that on a daily basis had to cope with highly complex project portfolios? In this paper we analyze a large dataset, containing 352 finalized software engineering projects, with the goal to discover what factors affect software project performance, and what actions can be taken to increase project performance when building a software project portfolio. The software projects were classified in four quadrants of a Cost/Duration matrix: analysis was performed on factors that were strongly related to two of those quadrants, Good Practices and Bad Practices. A ranking was performed on the factors based on statistical significance. The paper results in an inventory of 'what factors should be embraced when building a project portfolio?' (Success Factors), and 'what factors should be avoided when doing so?' (Failure Factors). The major contribution of this paper is that it analyzes characteristics of best performers and worst performers in the dataset of software projects, resulting in 7 Success Factors (a.o. steady heartbeat, a fixed, experienced team, agile (Scrum), and release-based), and 9 Failure Factors (a.o. once-only project, dependencies with other systems, technology driven, and rules-and regulations driven). Copyright © 2014 ACM.",Agile; Bad practice; Failure factor; Good practice; Learning cycle; Productivity; Quality; Success factor; Time-to-market,"Huijgens H., Van Solingen R., Van Deursen A.",2014,Conference,"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings",10.1145/2591062.2591187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903642802&doi=10.1145%2f2591062.2591187&partnerID=40&md5=7166217c588a020c89d3a0cdb3dcb81a,"Delft University of Technology and Goverdson, Delft, Netherlands; Delft University of Technology and Prowareness, Delft, Netherlands; Delft University of Technology, Delft, Netherlands",Association for Computing Machinery,English,,9781450327688
Scopus,LMES: A localized multi-estimator model to estimate software development effort,"Accurate estimation of software development effort is strongly associated with the success or failure of software projects. The clear lack of convincing accuracy and flexibility in this area has attracted the attention of researchers over the past few years. Despite improvements achieved in effort estimating, there is no strong agreement as to which individual model is the best. Recent studies have found that an accurate estimation of development effort in software projects is unreachable in global space, meaning that proposing a high performance estimation model for use in different types of software projects is likely impossible. In this paper, a localized multi-estimator model, called LMES, is proposed in which software projects are classified based on underlying attributes. Different clusters of projects are then locally investigated so that the most accurate estimators are selected for each cluster. Unlike prior models, LMES does not rely on only one individual estimator in a cluster of projects. Rather, an exhaustive investigation is conducted to find the best combination of estimators to assign to each cluster. The investigation domain includes 10 estimators combined using four combination methods, which results in 4017 different combinations. ISBSG, Maxwell and COCOMO datasets are utilized for evaluation purposes, which include a total of 573 real software projects. The promising results show that the estimate accuracy is improved through localization of estimation process and allocation of appropriate estimators. Besides increased accuracy, the significant contribution of LMES is its adaptability and flexibility to deal with the complexity and uncertainty that exist in the field of software development effort estimation. © 2013 Elsevier Ltd. All rights reserved.",Classification; Effort estimation; Estimator; Localization; Software project,"Bardsiri V.K., Jawawi D.N.A., Bardsiri A.K., Khatibi E.",2013,Journal,Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2013.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887023223&doi=10.1016%2fj.engappai.2013.08.005&partnerID=40&md5=fa898c0dcb1ffcfe681961d63369f81b,"Department of Computer Engineering, Bardsir Branch, Islamic Azad University, Kerman, Iran; Department of Software Engineering, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor Bahru, Malaysia",,English,09521976,
Scopus,Software cost estimation by classical and Fuzzy Analogy for Web Hypermedia Applications: A replicated study,"The aim of this paper is to evaluate and to compare the Classical Analogy and Fuzzy Analogy for software cost estimation on a Web software dataset. Hence, the paper aims to replicate the results of our precedent experiments on this dataset. Moreover, questions regarding the estimates accuracy, the tolerance of imprecision and uncertainty of cost drivers, and the favorable context to use estimation by analogy are discussed. This study approved the usefulness of Fuzzy Analogy for software cost estimation. © 2013 IEEE.",Estimation by Analogy; Fuzzy Logic; Genetic Algorithm,"Idri A., Zahi A.",2013,Conference,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",10.1109/CIDM.2013.6597238,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885622172&doi=10.1109%2fCIDM.2013.6597238&partnerID=40&md5=a7c6b5315989675548f9310b89df6f32,"Department of Software Engineering, ENSIAS, Mohamed v University, Morocco",,English,,9781467358958
Scopus,COTS integration and estimation for ERP,"This paper presents a comprehensive set of effort and schedule estimating models for predicting Enterprise Resource Planning (ERP) implementations, available in the open literature. The first set of models uses product size to predict ERP software engineering effort as well as total integration effort. Product size is measured in terms of the number of report, interface, conversion, and extension (RICE) objects configured and customized within the commercial ERP tool. Total integration effort captures software engineering plus systems engineering, program management, change management, development test & evaluation, and training development. The second set of models predicts the duration of ERP implementation stages in terms of RICE objects, staffing, and the number of test cases. The statistical models are based on data collected from 20 programs implemented within the federal government over the course of nine years beginning in 2000. The data was collected during the time period from 2006 to 2010. The models focus on the vendor's implementation team, and therefore should be applicable to commercial ERP implementations. Finally, ERP adopters/customers can use these models to validate Vendor's Implementation Team cost proposals or estimates. © 2012 Elsevier Inc.",Cost model; Effort estimation; Enterprise Resource Planning; Schedule estimation; Software engineering,"Rosa W., Packard T., Krupanand A., Bilbro J.W., Hodal M.M.",2013,Journal,Journal of Systems and Software,10.1016/j.jss.2012.09.030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924447&doi=10.1016%2fj.jss.2012.09.030&partnerID=40&md5=fcf260d7be2e1e566cf1bcba916bf3c8,"Air Force Cost Analysis Agency, Joint Base Andrews NAF, 1500 West Perimeter Rd, Washington, MD 20762-9998, United States; Cirrus Technology Inc., United States; Wyle, Inc., United States",,English,01641212,
Scopus,State of the practice in software effort estimation: A survey and literature review,"Effort estimation is a key factor for software project success, defined as delivering software of agreed quality and functionality within schedule and budget. Traditionally, effort estimation has been used for planning and tracking project resources. Effort estimation methods founded on those goals typically focus on providing exact estimates and usually do not support objectives that have recently become important within the software industry, such as systematic and reliable analysis of causal effort dependencies. This article presents the results of a study of software effort estimation from an industrial perspective. The study surveys industrial objectives, the abilities of software organizations to apply certain estimation methods, and actually applied practices of software effort estimation. Finally, requirements for effort estimation methods identified in the survey are compared against existing estimation methods. © 2011 IFIP International Federation for Information Processing.",effort estimation; project management; software; state of the art; state of the practice; survey,"Trendowicz A., Münch J., Jeffery R.",2011,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-22386-0_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053161618&doi=10.1007%2f978-3-642-22386-0_18&partnerID=40&md5=4b6c7519e418126cdad897c35039141c,"Fraunhofer IESE, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; University of New South Wales, School of Computer Science and Engineering, Sydney, NSW 2052, Australia; National ICT Australia, Australian Technology Park, Bay 15 Locomotive Workshop, Eveleigh, NSW 2015, Australia",,English,03029743,9783642223853
Scopus,A current assessment of software development effort estimation,"In most cases, software projects exceed their estimated effort. It is often assumed that inaccurate estimates are the main reason for these overruns. Contrarily, our results show that this assumption is not reasonable in many cases. We conducted a survey to gather data on the current situation of software development effort estimation. Apart from objective criteria, we also asked the participants for their subjective perceptions of the estimates. The participants' perceptions indicate that they do not agree with the objective assessments as 82% perceive their estimate as 'good' despite large overruns and provide reasonable arguments for their perceptions. Furthermore, many projects do not re-estimate the effort due to change requests leading to meaningless comparisons of estimated and actual effort. As a consequence, research needs to find new measures to assess the actual estimation accuracy. Besides the actual estimation process, professionals need to intensify their effort to manage the estimation processes' surroundings. © 2011 IEEE.",Effort estimation; Questionnaire survey; Software development,"Basten D., Mellis W.",2011,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/esem.2011.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858739008&doi=10.1109%2fesem.2011.32&partnerID=40&md5=ec5dba33672d9400e94fc6fa45219204,"Department of Information Systems and Systems Development, University of Cologne, Cologne, Germany",IEEE Computer Society,English,19493770,
Scopus,Evaluating the change of software fault behavior with dataset attributes based on categorical correlation,"Utilization of data mining in software engineering has been the subject of several research papers. Majority of subjects of those paper were in making use of historical data for decision making activities such as cost estimation and product or project attributes prediction and estimation. The ability to predict software fault modules and the ability to correlate relations between faulty modules and product attributes using statistics is the subject of this paper. Correlations and relations between the attributes and the categorical variable or the class are studied through generating a pool of records from each dataset and then select two samples every time from the dataset and compare them. The correlation between the two selected records is studied in terms of changing from faulty to non-faulty or the opposite for the module defect attribute and the value change between the two records in each evaluated attribute (e.g. equal, larger or smaller). The goal was to study if there are certain attributes that are consistently affecting changing the state of the module from faulty to none, or the opposite. Results indicated that such technique can be very useful in studying the correlations between each attribute and the defect status attribute. Another prediction algorithm is developed based on statistics of the module and the overall dataset. The algorithm gave each attribute true class and faulty class predictions. We found that dividing prediction capability for each attribute into those two (i.e. correct and faulty module prediction) facilitate understanding the impact of attribute values on the class and hence improve the overall prediction relative to previous studies and data mining algorithms. Results were evaluated and compared with other algorithms and previous studies. ROC metrics were used to evaluate the performance of the developed metrics. Results from those metrics showed that accuracy or prediction performance calculated traditionally using accurately predicted records divided by the total number of records in the dataset does not necessarily give the best indicator of a good metric or algorithm predictability. Those predictions may give wrong implication if other metrics are not considered with them. The ROC metrics were able to show some other important aspects of performance or accuracy. © 2011 Elsevier Ltd. All rights reserved.",Clustering; Correlation; Data mining; Fault prone modules; Prediction algorithms; Software mining; Software quality,"Alsmadi I., Najadat H.",2011,Journal,Advances in Engineering Software,10.1016/j.advengsoft.2011.03.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958251975&doi=10.1016%2fj.advengsoft.2011.03.010&partnerID=40&md5=46a7d3ce5547881a74fcec255722a8cc,"Yarmouk University, Faculty of CS and IT, Jordan; Jordan University of Science and Technology, Computer and IT Faculty, Jordan",Elsevier Ltd,English,09659978,
Scopus,Neural network model for software size estimation using use case point approach,"A realistic software size estimation is required in various subject areas of Software Engineering. Especially to predict performance of software system using Software Performance Engineering approach, software size is an important input parameter. Even though several estimation procedures are available the Neural Network model presents advantages over normal estimation procedure. In this paper we develop a Neural Network model to estimate the size of software using Use Case Point approach. The results are validated and a case study of Multi-Agent System is presented. ©2010 IEEE.",Multi-agent systems; Neural network; Soft ware size; Use case point,"Ajitha S., Suresh Kumar T.V., Evangelin Geetha D., Rajani Kanth K.",2010,Conference,"2010 5th International Conference on Industrial and Information Systems, ICIIS 2010",10.1109/ICIINFS.2010.5578675,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958537481&doi=10.1109%2fICIINFS.2010.5578675&partnerID=40&md5=15eccf1e316618b6dcf9ddfdbb6af246,"M.S Ramaiah Institute of Technology, Bangalore, India",,English,,9781424466535
Scopus,Support planning and controlling of early quality assurance by combining expert judgment and defect data- A case study,"Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or softwareintensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from historical projects. Due to the fact that many companies do not collect enough data for applying these approaches (especially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method combining commonly available measurement data and context-specific expert knowledge. To evaluate the method's applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model provides improved estimation accuracy when compared to applicable models based solely on data. The mean magnitude of relative error (MMRE) determined by crossvalidation is 29.6% compared to 76.5% obtained by the most accurate data-based model. © Springer Science + Business Media, LLC 2009.",Defect content estimation; Hybrid prediction model; Quality assurance effectiveness; Quality management; Software quality assurance,"Kläs M., Nakao H., Elberzhager F., Münch J.",2010,Conference,Empirical Software Engineering,10.1007/s10664-009-9112-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549160079&doi=10.1007%2fs10664-009-9112-1&partnerID=40&md5=7b7f68ccce2fa2843ba8806357f2b58b,"Fraunhofer Institute for Experimental Software Engineering, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Safety and Product Assurance Department, Japan Manned Space Systems Corporation, Urban Square Tsuchiura, 1-1-26, Kawaguchi, Tsuchiura, Ibaraki 300-0033, Japan; Department of Processes and Measurement, Fraunhofer Institute for Experimental Software Engineering (IESE), Kaiserslautern, Germany; Department of Testing and Inspections, Fraunhofer Institute for Experimental Software Engineering (IESE), Kaiserslautern, Germany",,English,13823256,
Scopus,Chapter 4 The Use of Simulation Techniques for Hybrid Software Cost Estimation and Risk Analysis,"Cost estimation is a crucial field for companies developing software or software-intensive systems. Besides point estimates, effective project management also requires information about cost-related project risks, for example, a probability distribution of project costs. One possibility to provide such information is the application of Monte Carlo simulation. However, it is not clear whether other simulation techniques exist that are more accurate or efficient when applied in this context. We investigate this question with CoBRA®,11CoBRA is a registered trademark of the Fraunhofer Institute for Experimental Software Engineering (IESE), Kaiserslautern, Germany. a cost estimation method that applies simulation, that is, random sampling, for cost estimation. This chapter presents an empirical study, which evaluates selected sampling techniques employed within the CoBRA® method. One result of this study is that the usage of Latin Hypercube sampling can improve average simulation accuracy by 60% and efficiency by 77%. Moreover, analytical solutions are compared with sampling methods, and related work, limitations of the study, and future research directions are described. In addition, the chapter presents a comprehensive overview and comparison of existing software effort estimation methods. © 2008 Elsevier Inc. All rights reserved.",,"Kläs M., Trendowicz A., Wickenkamp A., Münch J., Kikuchi N., Ishigai Y.",2008,Review,Advances in Computers,10.1016/S0065-2458(08)00604-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47849108735&doi=10.1016%2fS0065-2458%2808%2900604-9&partnerID=40&md5=4065496f57150d2dc4f9615a9d04c2b5,"Fraunhofer Institute for Experimental Software Engineering, 67663 Kaiserslautern, Germany; Oki Electric Industry Co., Ltd., Warabi-shi, Saitama, 335-8510, Japan; Information-Technology Promotion Agency, Software Engineering Center, Bunkyo-Ku, Tokyo, 113-6591, Japan",,English,00652458,9780123744265
Scopus,Software quality prediction using fuzzy integration: A case study,"Given the complexity of many contemporary software systems, it is often difficult to gauge the overall quality of their underlying software components. A potential technique to automatically evaluate such qualitative attributes is to use software metrics as quantitative predictors. In this case study, an aggregation technique based on fuzzy integration is presented that combines the predicted qualitative assessments from multiple classifiers. Multiple linear classifiers are presented with randomly selected subsets of automatically generated software metrics describing components from a sophisticated biomedical data analysis system. The external reference test is a software developer's thorough assessment of complexity, maintainability, and usability, which is used to assign corresponding quality class labels to each system component. The aggregated qualitative predictions using fuzzy integration are shown to be superior to the predictions from the respective best single classifiers. © Springer-Verlag 2007.",,Pizzi N.J.,2008,Journal,Soft Computing,10.1007/s00500-007-0217-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548036749&doi=10.1007%2fs00500-007-0217-4&partnerID=40&md5=ff034dff3f97b44c28f2baeedadf1533,"Institute for Biodiagnostics, National Research Council of Canada, 435 Ellice Avenue, Winnipeg, MB R3B 1Y6, Canada; Department of Computer Science, University of Manitoba, Winnipeg, MB R3T 2N2, Canada",,English,14327643,
Scopus,Building software cost estimation models using homogenous data,"Several studies have been conducted to determine if company-specific cost models deliver better prediction accuracy than cross-company cost models. However, mixed results have left the question still open for further investigation. We suspect this to be a consequence of heterogenous data used to build cross-company cost models. In this paper, we build cross-company cost models using homogenous data by grouping projects by their business sector. Our results suggest that it is worth to train models using only homogenous data rather than all projects available. © 2007 IEEE.",,"Premraj R., Zimmermann T.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949104980&doi=10.1109%2fESEM.2007.27&partnerID=40&md5=4991856b1fa6c7a248203c872f2ef5f2,"Saarland University, Saarbrücken, Germany",,English,,0769528864; 9780769528861
Scopus,Software project effort estimation based on multiple parametric models generated through data clustering,"Parametric software effort estimation models usually consists of only a single mathematical relationship. With the advent of software repositories containing data from heterogeneous projects, these types of models suffer from poor adjustment and predictive accuracy. One possible way to alleviate this problem is the use of a set of mathematical equations obtained through dividing of the historical project datasets according to different parameters into subdatasets called partitions. In turn, partitions are divided into clusters that serve as a tool for more accurate models. In this paper, we describe the process, tool and results of such approach through a case study using a publicly available repository, ISBSG. Results suggest the adequacy of the technique as an extension of existing single-expression models without making the estimation process much more complex that uses a single estimation model. A tool to support the process is also presented. © Science Press, Beijing, China and Springer Science + Business Media, LLC, USA 2007.",Clustering; Effort estimation; Software engineering; Software measurement,"Gallego J.J.C., Rodríguez D., Sicilia M.A., Rubio M.G., Crespo A.G.",2007,Journal,Journal of Computer Science and Technology,10.1007/s11390-007-9043-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249847882&doi=10.1007%2fs11390-007-9043-5&partnerID=40&md5=eb6a7d4c2081eb71665c38b68ad05ae4,"Department of Computer Science, University of Alcalá, Alcalá, Spain; Department of Computer Science, Carlos III University, Madrid, Spain",,English,10009000,
Scopus,An algorithm for the generation of segmented parametric software estimation models and its empirical evaluation,"Parametric software effort estimation techniques use mathematical cost-estimation relationships derived from historical project databases, usually obtained through standard curve regression techniques, Nonetheless, project databases especially in the case of consortium-created compilations like the ISBSG -, collect highly heterogeneous data, coming from projects that diverge in size, process and personnel skills, among other factors. This results in that a single parametric model is seldom able to capture the diversity of the sources, in turn resulting in poor overall quality, Segmented parametric estimation models use local regression to derive one model per each segment of data with similar characteristics, improving the overall predictive quality of parametrics. Further, the process of obtaining segmented models can be expressed in the form of a generic algorithm that can be used to produce candidate models in an automated process of calibration from the project database at hand, This paper describes the rationale for such algorithmic scheme along with the empirical evaluation of a concrete version that uses the EM clustering algorithm combined with the common parametric exponential model of size-effort, and standard quality-of-adjustment criteria. Results point out to the adequacy of the technique as an extension of existing single-relation models.",Clustering algorithms; EM algorithm; Parametric software estimation; Software project databases,"Cuadrado-Gallego J.J., Sicilia M.-A.",2007,Journal,Computing and Informatics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947143428&partnerID=40&md5=7ec14b7d5472614be5dc647d717e927b,"Computer Science Department, Polytechnic Building, University of Alcalá, Ctra. Barcelona km. 33.6, 28871 Alcala de Henares, Madrid, Spain",,English,13359150,
Scopus,Proportional intensity-based software reliability modeling with time-dependent metrics,"The black-box approach based on stochastic software reliability models is a simple methodology with only software fault data in order to describe the temporal behavior of fault-detection processes, but fails to incorporate some significant development metrics data observed in the development process. In this paper we develop proportional intensity-based software reliability models with time-dependent metrics, and propose a statistical framework to assess the software reliability with the time-dependent covariate as well as the software fault data. The resulting models are similar to the usual proportional hazard model, but possess somewhat different covariate structure from the existing one. We compare these metrics-based software reliability models with some typical non-homogeneous Poisson process models, which are the special cases of our models, and evaluate quantitatively the goodness-of-fit from the viewpoint of information criteria. As an important result, the accuracy on reliability assessment strongly depends on the kind of software metrics used for analysis and can be improved by incorporating the time-dependent metrics data in modeling. © 2006 IEEE.",,"Rinsaka K., Shibata K., Dohi T.",2006,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/COMPSAC.2006.68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247529978&doi=10.1109%2fCOMPSAC.2006.68&partnerID=40&md5=4dc29ac237c03876b5a5f657e942f269,"Department of Information Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi-Hiroshima 739-8527, Japan",,English,07303157,0769526551; 9780769526553
Scopus,Applying statistical methodology to optimize and simplify software metric models with missing data,"During the construction of a software metric model, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, a model constructed based on such an assumption may contain redundant predictor metric(s) and/or unnecessary predictor metric complexity. This is because the assumption made before the model construction is not verified after the model is constructed. To resolve the first problem (i.e., possible redundant predictor metric(s)), we propose a statistical hypothesis testing methodology to verify ""retrospectively"" the statistical significance of the impact of each predictor metric on the target metric. If the variation of a predictor metric does not correlate enough with the variation of the target metric, the predictor metric should be deleted from the model. For the second problem (i.e., unnecessary predictor metric complexity), we use ""goodness-of-fit"" to determine whether certain categories of a categorical predictor metric should be combined together. In addition, missing data often appear in the data sample used for constructing the model. We use a modified k-nearest neighbors (k-NN) imputation method to deal with this problem. A study using data from the ""Repository Data Disk - Release 6"" is reported. The results indicate that our methodology can be useful in trimming redundant predictor metrics and identifying unnecessary categories initially assumed for a categorical predictor metric in the model. Copyright 2006 ACM.",Imputation method; Missing data; Model optimization; Model simplification; Models; Software metrics,"Wong W.E., Zhao J., Chan V.K.Y.",2006,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/1141277.1141687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751054078&doi=10.1145%2f1141277.1141687&partnerID=40&md5=4c707d45aee9b320cab4d8b047acf5f8,"Department of Computer Science, University of Texas, Dallas, United States; Macao Polytechnic Institute, Macau",Association for Computing Machinery,English,,1595931082; 9781595931085
Scopus,Strategic software engineering: An interdisciplinary approach,"The pervasiveness of software in business makes it crucial that software engineers and developers understand how software development impacts an entire organization. Strategic Software Engineering: An Interdisciplinary Approach presents software engineering as a strategic, business-oriented, interdisciplinary endeavor, rather than simply a technical process, as it has been described in previous publications. The book addresses technical, scientific, and management aspects of software development in a way that is accessible to a wide audience. It provides a detailed, critical review of software development models and processes, followed with a strategic assessment of how process models evolved over time and how to improve them. The authors then focus on the relation between problem-solving techniques and strategies for effectively confronting real-world business problems. They also analyze the impact of interdisciplinary factors on software development, including the role of people and business economics. The book concludes with a brief look at specialized system development. The diverse backgrounds of the authors, encompassing computer science, information systems, technology, and business management, help create this book's integrated approach, which answers the demand for a comprehensive, interdisciplinary outlook encompassing all facets of how software relates to an organization. © 2005 by Taylor & Francis Group, LLC.",,"Deek F.P., McHugh J.A.M., Eljabiris O.M.",2005,Book,Strategic Software Engineering: An Interdisciplinary Approach,10.1201/9781420031119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055218726&doi=10.1201%2f9781420031119&partnerID=40&md5=34bdb5538da4fb1fd9622ad1c7d91b52,"New Jersey Institute of Technology, Newark, United States",CRC Press,English,,9781420031119; 0849339391; 9780849339394
Scopus,An exploratory study of object-oriented software component size determinants and the application of regression tree forecasting models,"Software component size estimation is an important task in software project management. For a component-based approach, two steps may be used to estimate the overall size of object-oriented (OO) software: a designer uses metrics to predict the size of the software components and then utilizes the sizes to estimate the overall project size. Using OO software metrics literature, we identified factors that may affect the size of an OO software component. Using real-life data from 152 software components, we then determined the effect of the identified factors on the prediction of OO software component size. The results indicated that certain factors and the type of OO software component play a significant role in the estimate. It is shown how a regression tree data mining approach can be used to learn decision rules to guide future estimates. © 2004 Elsevier B.V. All rights reserved.",Classification and regression tree; Data mining; Forecasting; Software engineering; Software size,Pendharkar P.C.,2004,Journal,Information and Management,10.1016/j.im.2003.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544223922&doi=10.1016%2fj.im.2003.12.004&partnerID=40&md5=935fce70ba11520e484200e2986ac374,"Information Systems, School of Business Administration, Pennsylvania Stt. Univ. Harrisburg, 777 West Harrisburg Pike, Middletown, PA 17057 4898, United States",Elsevier,English,03787206,
Scopus,"Metrology, measurement and metrics in software engineering","Up until recently software 'metrics' have been most often proposed as the quantitative tools of choice in software engineering, and the analysis of these had been most often discussed from the perspective referred to as 'measurement theory'. However, in other disciplines, it is the domain of knowledge referred to as 'metrology' that is the foundation for the development and use of measurement instruments and measurement processes. The IEEE-computer society, with the support of a consortium of industrial sponsors, has recently published a guide to the Software Engineering Body of Knowledge (SWEBOK) and, throughout this guide, measurement is pervasive as a fundamental engineering tool. We use our initial modelling of the sets of measurement concepts documented in the ISO international vocabulary of basic and general terms in metrology to investigate and position the measurement concepts within this body of knowledge, and to identify gaps where further research on software measurement is required. © 2003 IEEE.",Computer industry; Instruments; ISO; Knowledge engineering; Metrology; Position measurement; Software engineering; Software measurement; Software metrics; Vocabulary,"Abran A., Sellami A., Suryn W.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232451,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943143723&doi=10.1109%2fMETRIC.2003.1232451&partnerID=40&md5=5267c92fc693bd318267954c1ec8a6c7,"École de Technologie Supérieure, Canada",IEEE Computer Society,English,15301435,0769519873
Scopus,Object oriented design function points,"Estimating different characteristics viz., size, cost, etc. of software during different phases of software development is required to manage the resources effectively. Function points measure can be used as an input to estimate these characteristics of software. The Traditional Function Point Counting Procedure (TFPCP) can not be used to measure the functionality of an Object Oriented (OO) system. This paper suggests a counting procedure to measure the functionality of an OO system during the design phase from a designers' perspective. It is adapted from the TFPCP. The main aim of this paper is to use all the available information during the OO design phase to estimate Object Oriented Design Function Points (OODFP). The novel feature of this approach is that it considers all the basic concepts of OO systems such as inheritance, aggregation, association and polymorphism. © 2000 IEEE.",Function point analysis; Object Oriented design,"Janaki Ram D., Raju S.V.G.K.",2000,Conference,"Proceedings - 1st Asia-Pacific Conference on Quality Software, APAQS 2000",10.1109/APAQ.2000.883785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21244457047&doi=10.1109%2fAPAQ.2000.883785&partnerID=40&md5=9e3eb301e1e778afc4a7886b413c0fe6,"Distributed and Object Systems Lab., Indian Institute of Technology Madras, Chennai, 600 036, India",Institute of Electrical and Electronics Engineers Inc.,English,,0769508251; 9780769508252
Scopus,Cost estimation based on business models,"Software development requires early and accurate cost estimation in order to enhance likely success. System complexity needs to be measured and then correlated with development effort. One of the best known approaches to such measurement-based estimation in the area of Information Systems is Function Point Analysis (FPA). Although it is reasonably well used in practice, FPA has been shown to be formally ambiguous and to have some serious practical deficiencies as well, mainly in the context of newly emerged object-oriented modeling approaches. This paper reports results from an empirical study undertaken in Swiss industry covering 36 projects. We observed that a new formally sound approach, the System Meter (SM) method, which explicitly takes reuse into account, predicts effort substantially better than FPA.",,"Moser S., Henderson-Sellers B., Misic V.B.",1999,Journal,Journal of Systems and Software,10.1016/S0164-1212(99)00064-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033340464&doi=10.1016%2fS0164-1212%2899%2900064-3&partnerID=40&md5=72da235e95fc3e830f1b3ac90e7691f7,"Terrassenweg 18, Münsingen 3110, Switzerland; Department of Computer Science, Swinburne Univ. Technol., Hawthorn; University of Belgrade","Elsevier Science Inc, New York",English,01641212,
Scopus,Parametric estimation of programming effort: An object-oriented model,[No abstract available],,"Jenson R.L., Bartley J.W.",1991,Journal,The Journal of Systems and Software,10.1016/0164-1212(91)90048-B,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039650903&doi=10.1016%2f0164-1212%2891%2990048-B&partnerID=40&md5=14326a585bab065eb3ab4a63977dec7f,"Assistant Professor of Accounting, School of Accountancy, Utah State University, Logan, UtahUSA; Associate Professor of Accounting, Department of Accounting, North Carolina State University, Raleigh, North CarolinaUSA",,English,01641212,
Scopus,The continuity of continuous integration: Correlations and consequences,"The practice of continuous integration has firmly established itself in the mainstream of the software engineering industry, yet many questions surrounding it remain unanswered. Prominent among these is the issue of scalability: continuous integration has been reported to be possible to scale, but with difficulties. Understanding of the underlying mechanisms causing these difficulties is shallow, however: what is it about size that is problematic, which kind of size, and what aspect of continuous integration does it impede? Based on quantitative data from six industry cases encompassing close to 2000 engineers, complemented by interviews with engineers from five companies, this paper investigates the correlation between the continuity of continuous integration and size. It is found that not only is there indeed a correlation between the size and composition of a development organization and its tendency to integrate continuously; there is evidence that the size of the organization influences ways of working, which in turn correlate with the degree of continuity, raising the question of software manufacturability. It is further observed that developer behavior in ostensibly continuously integrating cases does not necessarily match expectations, and that frequent integration of the product itself does not automatically imply that each individual developer commits frequently. © 2017 Elsevier Inc.",Agile; Complexity; Continuity; Continuous integration; Developability; Manufacturability; Modularity; Scale; Size,"Ståhl D., Mårtensson T., Bosch J.",2017,Journal,Journal of Systems and Software,10.1016/j.jss.2017.02.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013224881&doi=10.1016%2fj.jss.2017.02.003&partnerID=40&md5=e1401f4ca64974936464862d493551a1,"Ericsson AB, Linköping, Sweden; Saab AB, Linköping, Sweden; Chalmers University of Technology, Gothenburg, Sweden",Elsevier Inc.,English,01641212,
Scopus,Investigating the use of moving windows to improve software effort prediction: a replicated study,"To date most research in software effort estimation has not taken chronology into account when selecting projects for training and validation sets. A chronological split represents the use of a project’s starting and completion dates, such that any model that estimates effort for a new project p only uses as its training set projects that have been completed prior to p’s starting date. A study in 2009 (“S3”) investigated the use of chronological split taking into account a project’s age. The research question investigated was whether the use of a training set containing only the most recent past projects (a “moving window” of recent projects) would lead to more accurate estimates when compared to using the entire history of past projects completed prior to the starting date of a new project. S3 found that moving windows could improve the accuracy of estimates. The study described herein replicates S3 using three different and independent data sets. Estimation models were built using regression, and accuracy was measured using absolute residuals. The results contradict S3, as they do not show any gain in estimation accuracy when using windows for effort estimation. This is a surprising result: the intuition that recent data should be more helpful than old data for effort estimation is not supported. Several factors, which are discussed in this paper, might have contributed to such contradicting results. Some of our future work entails replicating this work using other datasets, to understand better when using windows is a suitable choice for software companies. © 2016, Springer Science+Business Media New York.",Chronological splitting; Effort estimation; Moving window; Regression-based estimation models,"Lokan C., Mendes E.",2017,Journal,Empirical Software Engineering,10.1007/s10664-016-9446-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983399519&doi=10.1007%2fs10664-016-9446-4&partnerID=40&md5=e88a5c1827701593e5165f8cda1f8835,"School of Engineering & Information Technology, UNSW Canberra, Canberra, Australia; Faculty of Computing, Blekinge Institute of Technology, Karlskrona, SE-371 79, Sweden; Faculty of Information Technology and Electrical Engineering, University of Oulu, PO Box 3000, Oulu, 90014, Finland",Springer New York LLC,English,13823256,
Scopus,Data mining for software engineering and humans in the loop,"The field of data mining for software engineering has been growing over the last decade. This field is concerned with the use of data mining to provide useful insights into how to improve software engineering processes and software itself, supporting decision-making. For that, data produced by software engineering processes and products during and after software development are used. Despite promising results, there is frequently a lack of discussion on the role of software engineering practitioners amidst the data mining approaches. This makes adoption of data mining by software engineering practitioners difficult. Moreover, the fact that experts’ knowledge is frequently ignored by data mining approaches, together with the lack of transparency of such approaches, can hinder the acceptability of data mining by software engineering practitioners. To overcome these problems, this position paper provides a discussion of the role of software engineering experts when adopting data mining approaches. It also argues that this role can be extended to increase experts’ involvement in the process of building data mining models. We believe that such extended involvement is not only likely to increase software engineers’ acceptability of the resulting models, but also improve the models themselves. We also provide some recommendations aimed at increasing the success of experts involvement and model acceptability. © 2016, The Author(s).",Data mining; Machine learning; Software analytics; Software engineering,"Minku L.L., Mendes E., Turhan B.",2016,Journal,Progress in Artificial Intelligence,10.1007/s13748-016-0092-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020585073&doi=10.1007%2fs13748-016-0092-2&partnerID=40&md5=4b64c0c7d9673679df8b81fff3b55079,"Department of Computer Science, University of Leicester, Leicester, LE1 7RH, United Kingdom; Blekinge Institute Technology, Karlskrona, 371 79, Sweden; Department of Information Processing Science, University of Oulu, POB.3000, Oulu, 90014, Finland",Springer Verlag,English,21926352,
Scopus,Pareto efficient multi-objective optimization for local tuning of analogy-based estimation,"Analogy-based effort estimation (ABE) is one of the prominent methods for software effort estimation. The fundamental concept of ABE is closer to the mentality of expert estimation but with an automated procedure in which the final estimate is generated by reusing similar historical projects. The main key issue when using ABE is how to adapt the effort of the retrieved nearest neighbors. The adaptation process is an essential part of ABE to generate more successful accurate estimation based on tuning the selected raw solutions, using some adaptation strategy. In this study, we show that there are three interrelated decision variables that have great impact on the success of adaptation method: (1) number of nearest analogies (k), (2) optimum feature set needed for adaptation and (3) adaptation weights. To find the right decision regarding these variables, one need to study all possible combinations and evaluate them individually to select the one that can improve all prediction evaluation measures. The existing evaluation measures usually behave differently, presenting sometimes opposite trends in evaluating prediction methods. This means that changing one decision variable could improve one evaluation measure while it is decreasing the others. Therefore, the main theme of this research is how to come up with best decision variables that improve adaptation strategy and thus the overall evaluation measures without degrading the others. The impact of these decisions together has not been investigated before; therefore, we propose to view the building of adaptation procedure as a multi-objective optimization problem. The Particle swarm optimization algorithm (PSO) is utilized to find the optimum solutions for such decision variables based on optimizing multiple evaluation measures. We evaluated the proposed approaches over 15 datasets and using four evaluation measures. After extensive experimentation, we found that: (1) predictive performance of ABE has noticeably been improved, (2) optimizing all decision variables together is more efficient than ignoring any one of them, and (3) optimizing decision variables for each project individually yields better accuracy than optimizing them for the whole dataset. © 2015, The Natural Computing Applications Forum.",Adaptation strategy; Analogy-based effort estimation; Multi-objective optimization; Particle swarm optimization,"Azzeh M., Nassif A.B., Banitaan S., Almasalha F.",2016,Journal,Neural Computing and Applications,10.1007/s00521-015-2004-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940487623&doi=10.1007%2fs00521-015-2004-y&partnerID=40&md5=84e5ffe23e383b9c88baa4ec4ec921ca,"Department of Software Engineering, Applied Science University, POBOX 166, Amman, Jordan; Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Mathematics, Computer Science and Software Engineering, University of Detroit Mercy, Detroit, MI, United States; Department of Computer Science, Applied Science University, POBOX 166, Amman, Jordan",Springer-Verlag London Ltd,English,09410643,
Scopus,Function Point Analysis for Software Maintenance,"Context: Software maintenance is required to fix defects, adapt to changes in the environment, and meet new or changed user requirements. The effort of these tasks need to be estimated to track progress, manage resources, and make decisions. Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle. Function Points (FPs) represents software size by functions or modifications to functions, making them easier to calculate early in the lifecycle for new development projects or maintenance tasks. Several cost estimators use FPs to estimate the SLOC of a project to take advantage of existing cost models. Goal: Through empirical analysis, the authors want to determine whether FPs can effectively estimate maintenance tasks, as a better alternative to using SLOC as a software size metric. Additionally, the authors will demonstrate that FPs to SLOC ratios add uncertainty to effort estimates. Method: The empirical analysis will be run on Unified Code Count (UCC)'s dataset, a software tool maintained by University of Southern California (USC). Results: The analyses found that separating projects adding new functions from those modifying existing functions resulted in improved estimation models using FPs. The effort estimation model for projects adding functions to UCC had high prediction accuracy statistics, but less impressive results for projects modifying existing functions in UCC. The effort estimation accuracy became unsatisfactorily low when using a FPs to SLOC ratio. Conclusions: Cost estimators should not use FPs to SLOC ratios for effort estimation due to low prediction accuracy. FPs is only an effective size measure for a portion of UCC's maintenance tasks-specifically for the projects adding new functions to UCC. Another size measure may need to be considered that might be more effective independently or in conjunction with FPs for all of UCC's maintenance tasks. © 2016 ACM.",Backfiring; Cost Estimation; Effort Estimation; Function Point Analysis; Local Calibration; Project Management; Software Maintenance,"Hira A., Boehm B.",2016,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1145/2961111.2962613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991593721&doi=10.1145%2f2961111.2962613&partnerID=40&md5=9b3c53222364b8a2a7263ed8dac52f5f,"University of Southern California, Los Angeles, United States",IEEE Computer Society,English,19493770,9781450344272
Scopus,Using text clustering to predict defect resolution time: a conceptual replication and an evaluation of prediction accuracy,"Defect management is a central task in software maintenance. When a defect is reported, appropriate resources must be allocated to analyze and resolve the defect. An important issue in resource allocation is the estimation of Defect Resolution Time (DRT). Prior research has considered different approaches for DRT prediction exploiting information retrieval techniques and similarity in textual defect descriptions. In this article, we investigate the potential of text clustering for DRT prediction. We build on a study published by Raja (2013) which demonstrated that clusters of similar defect reports had statistically significant differences in DRT. Raja’s study also suggested that this difference between clusters could be used for DRT prediction. Our aims are twofold: First, to conceptually replicate Raja’s study and to assess the repeatability of its results in different settings; Second, to investigate the potential of textual clustering of issue reports for DRT prediction with focus on accuracy. Using different data sets and a different text mining tool and clustering technique, we first conduct an independent replication of the original study. Then we design a fully automated prediction method based on clustering with a simulated test scenario to check the accuracy of our method. The results of our independent replication are comparable to those of the original study and we confirm the initial findings regarding significant differences in DRT between clusters of defect reports. However, the simulated test scenario used to assess our prediction method yields poor results in terms of DRT prediction accuracy. Although our replication confirms the main finding from the original study, our attempt to use text clustering as the basis for DRT prediction did not achieve practically useful levels of accuracy. © 2015, Springer Science+Business Media New York.",Data clustering; Defect resolution time; Independent replication; Prediction; Simulation; Text mining,"Assar S., Borg M., Pfahl D.",2016,Journal,Empirical Software Engineering,10.1007/s10664-015-9391-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933556607&doi=10.1007%2fs10664-015-9391-7&partnerID=40&md5=6dc1fb96c4763d2f085822b925949370,"Ecole de Management, Institut Mines-Telecom, 9, rue C. Fourier, Evry, 91011, France; Department of Computer Science, Lund University, Box 118, Lund, SE-221 00, Sweden; Institute of Computer Science, University of Tartu, J. Liivi 2, Tartu, 50409, Estonia",Springer New York LLC,English,13823256,
Scopus,Can the outside-view approach improve planning decisions in software development projects?,"This study empirically tackles the question of whether taking an outside-view approach, recommended for reducing the irrational behaviours associated with the planning fallacy, can also reduce the time underestimation, scope overload and over-requirement problems plaguing planning decisions in software development. Drawing on descriptive behavioural decision theory, this study examines whether the planning fallacy, a cognitive bias referring to the tendency of people to underestimate costs and overestimate benefits in evaluating a task to be performed, can provide a theoretical platform for mitigating irrational behaviours in the planning of software development projects. In particular, we argue that taking an outside-view approach in planning decisions for software development may have the same mitigating effects on time underestimation, scope overload and over-requirement it has been shown to have on cost underestimation and benefit overestimation. In an experiment investigating this argument, participants were randomly assigned to four groups by manipulating two outside-view mechanisms: reference information about past completion times (present/absent) and role perspective (developer/consultant). After being presented with a to-be-developed software project, they were requested to estimate development times of various software features and to recommend which features to include within project scope given a fixed duration for the entire project. The results confirm that the three problems of time underestimation, scope overload and over-requirement are manifested in planning decisions for fixed-schedule software development projects. Moreover, the results show that these problems are mitigated, yet not eliminated, by presenting reference information about past completion times and by having a consultant role. © 2015 Wiley Publishing Ltd",experiment; inside view; outside view; over-requirement; planning decisions; planning fallacy; reference information; role perspective; scope overload; software development; time underestimation,"Shmueli O., Pliskin N., Fink L.",2016,Journal,Information Systems Journal,10.1111/isj.12091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945971918&doi=10.1111%2fisj.12091&partnerID=40&md5=c723d0dca50823f7cc8cb97384e74958,"Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, 84105, Israel",Blackwell Publishing Ltd,English,13501917,
Scopus,Investigating functional and code size measures for mobile applications: A replicated study,"In this paper we apply a measurement procedure proposed by van Heeringen and van Gorp to approximate the COSMIC size of mobile applications. We compare this procedure with the one introduced by D’Avanzo et al. We also replicate an empirical study recently carried out to assess whether the COSMIC functional size of mobile applications can be used to estimate the size of the final applications in terms of lines of code, number of bytes of the source code and bytecode. The results showed that the COSMIC functional size evaluated with van Heeringen and van Gorp’s method was well correlated to all the size measures taken into account. Nevertheless, the prediction accuracy did not satisfy the evaluation criteria and turned out ot be slightly worse than the one obtained in the original study and based on the approach proposed by D’Avanzo et al. © Springer International Publishing Switzerland 2015.",Android mobile applications; Code size measure; COSMIC; Empirical study; Functional size measurement; LOC,"Ferrucci F., Gravino C., Salza P., Sarro F.",2015,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-26844-6_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952331012&doi=10.1007%2f978-3-319-26844-6_20&partnerID=40&md5=8534a9aa4e5f3b550fe53e61ab54a674,"Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Computer Science, University College London, London, United Kingdom",Springer Verlag,English,03029743,9783319268439
Scopus,The use of fuzzy case-based reasoning in estimating costs in the early phase of the construction project,"This article presents the concept of a method for estimating costs in the early phase of the project using fuzzy case-based reasoning. The procedure of case-based reasoning is described briefly. The possibility of using fuzzy sets in enhancing the ability of the CBR method is discussed. Also, an outline of the modified procedure for case-based reasoning using fuzzy sets is described. © 2015 AIP Publishing LLC.",Case based reasoning; Cost estimating; Fuzzy set theory,Zima K.,2015,Conference,AIP Conference Proceedings,10.1063/1.4912842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939648193&doi=10.1063%2f1.4912842&partnerID=40&md5=e9de7582ba1a70844f8055e2831068d3,"Cracow University of Technology, Faculty of Civil Engineering, Institute of Building and Transport Management Section of Technology and Building Management, ul. Warszawska 24, Kraków;, 31-155, Poland",American Institute of Physics Inc.,English,0094243X,9780735412873
Scopus,Grey relational effort analysis technique using regression methods for software estimation,"Software project planning and estimation is the most important confront for software developers and researchers. It incorporates estimating the size of the software project to be produced, estimating the effort required, developing initial project schedules, and ultimately, estimating on the whole cost of the project. Numerous empirical explorations have been performed on the existing methods, but they lack convergence in choosing the best prediction methodology. Analogy based estimation is still one of the most extensively used method in industry which is based on finding effort from similar projects from the project repository. Two alternative approaches using analogy for estimation have been proposed in this study. Firstly, a precise and comprehensible predictive model based on the integration of Grey Relational Analysis (GRA) and regression has been discussed. Second approach deals with the uncertainty in the software projects, and how fuzzy set theory in fusion with grey relational analysis can minimize this uncertainty. Empirical results attained are remarkable indicating that the methodologies have a great potential and can be used as a candidate approaches for software effort estimation. The results obtained using both the methods are subjected to rigorous statistical testing using Wilcoxon signed rank test.",Estimation by analogy; Fuzzy clustering; GRA; Robust regression; Software Estimations,"Geeta N., Moin U., Arvinder K.",2014,Journal,International Arab Journal of Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903954053&partnerID=40&md5=05fbb052999db88122cd811e8d087ddb,"Department of Computer Science and Engineering, National Institute of Technology, India; Delhi Technological University, India; Department of Information Technology, University School of Information Technology, India",Zarka Private Univ,English,16833198,
Scopus,Effort estimation of FLOSS projects: A study of the Linux kernel,"Empirical research on Free/Libre/Open Source Software (FLOSS) has shown that developers tend to cluster around two main roles: ""core"" contributors differ from ""peripheral"" developers in terms of a larger number of responsibilities and a higher productivity pattern. A further, cross-cutting characterization of developers could be achieved by associating developers with ""time slots"", and different patterns of activity and effort could be associated to such slots. Such analysis, if replicated, could be used not only to compare different FLOSS communities, and to evaluate their stability and maturity, but also to determine within projects, how the effort is distributed in a given period, and to estimate future needs with respect to key points in the software life-cycle (e.g., major releases). This study analyses the activity patterns within the Linux kernel project, at first focusing on the overall distribution of effort and activity within weeks and days; then, dividing each day into three 8-hour time slots, and focusing on effort and activity around major releases. Such analyses have the objective of evaluating effort, productivity and types of activity globally and around major releases. They enable a comparison of these releases and patterns of effort and activities with traditional software products and processes, and in turn, the identification of company-driven projects (i.e., working mainly during office hours) among FLOSS endeavors. The results of this research show that, overall, the effort within the Linux kernel community is constant (albeit at different levels) throughout the week, signalling the need of updated estimation models, different from those used in traditional 9am-5pm, Monday to Friday commercial companies. It also becomes evident that the activity before a release is vastly different from after a release, and that the changes show an increase in code complexity in specific time slots (notably in the late night hours), which will later require additional maintenance efforts. © 2011 Springer Science+Business Media, LLC.",Complexity; Effort estimation; Effort models; Mining software repositories; Open source software,"Capiluppi A., Izquierdo-Cortázar D.",2013,Journal,Empirical Software Engineering,10.1007/s10664-011-9191-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872319608&doi=10.1007%2fs10664-011-9191-7&partnerID=40&md5=bb98ab7306bf8b7d1e58f8762682d4ed,"School of Architecture, Computing and Engineering (ACE), University of East London, Docklands Campus, 4-6 University Way, E16 2RD London, United Kingdom; Libre Software Engineering Lab (GSyC), Universidad Rey Juan Carlos, Edif. Biblioteca - Despacho B102, Camino del Molino s/n, 28943 Fuenlabrada, Madrid, Spain",,English,13823256,
Scopus,Predicting software maintenance effort through evolutionary-based decision trees,"Software effort prediction has been a challenge for researchers throughout the years. Several approaches for producing predictive models from collected data have been proposed, although none has become standard given the specificities of different software projects. The most commonly employed strategy for estimating software effort, the multivariate linear regression technique has numerous shortcomings though, which motivated the exploration of many machine learning techniques. Among the researched strategies, decision trees and evolutionary algorithms have been increasingly employed for software effort prediction, though independently. In this paper, we propose employing an evolutionary algorithm to generate a decision tree tailored to a software effort data set provided by a large worldwide IT company. Our findings show that evolutionarily-induced decision trees statistically outperform greedily-induced ones, as well as traditional logistic regression. Moreover, an evolutionary algorithm with a bias towards comprehensibility can generate trees which are easier to be interpreted by the project stakeholders, and that is crucial in order to improve the stakeholder's confidence in the final prediction. © 2012 ACM.",decision trees; evolutionary algorithms; LEGAL-Tree; software effort estimation,"Basgalupp M.P., Barros R.C., Ruiz D.D.",2012,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/2245276.2231966,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863601388&doi=10.1145%2f2245276.2231966&partnerID=40&md5=b40aad08ee481201d261d1c473163c4f,"ICT-UNIFESP, S. J. dos Campos, SP, Brazil; ICMC-USP, São Carlos, SP, Brazil; PUCRS, Porto Alegre, RS, Brazil",,English,,9781450308571
Scopus,Performance evaluation of windowing approach on effort estimation by analogy,"Background: In effort estimation model construction, it seems effective to window training project data so that only recently finished projects are used. This is because old projects might be less representative of an organization. The past study demonstrated windowing approach works with linear regression, which is one of global models. However, this approach has not been examined with local models. Local models use subset of historical data for model construction and thus windowing approach may influence on its performance more weakly. Aim: To investigate whether windowing approach works with local models. Method: We replicated the past study with EbA. Maxwell and CSC datasets were used for an experiment. Results: Windowing approach improved predictive performance. Although the difference was insignificant in any window size, the result indicated using windowing approach has positive effect on average. Conclusions: This result contributes to understand where windowing approach works well. © 2011 IEEE.",Effort estimation; Estimation by analogy; Performance evaluation; Windowing approach,"Amasaki S., Takahara Y., Yokogawa T.",2011,Conference,"Proceedings - Joint Conference of the 21st International Workshop on Software Measurement, IWSM 2011 and the 6th International Conference on Software Process and Product Measurement, MENSURA 2011",10.1109/IWSM-MENSURA.2011.29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856156612&doi=10.1109%2fIWSM-MENSURA.2011.29&partnerID=40&md5=ef22ce477a12a20ebc2c61e7d2b8c2a6,"Dept. of Systems Engineering, Okayama Prefectural University, Okayama, Japan",,English,,9780769544977
Scopus,Towards the derivation of an integrated process cost-modelling technique for complex manufacturing systems,"Cost modelling is used to support business decisions, especially, when the objective is to remain competitive on price and be able to realise outputs at low cost. Many researchers and industrialists have proposed and experimented with different cost-modelling techniques with a view to influencing design and production decisions at an early stage of the development process. This has led to cost-modelling methods which have been broadly classified in this paper as qualitative and quantitative. The paper identifies current best practice costmodelling techniques and their performance in complex and dynamic manufacturing environments. The review served as a platform to support the recommendation for an integrated cost-modelling methodology. The integrated methodology is based on the strengths of cost engineering, enterprise modelling, system dynamics and discrete event simulation modelling techniques. The method can help in the redesign and re-engineering of products and processes for better cost and value indications; support investment decision analysis; help determine appropriate business and manufacturing paradigms; influence 'make, buy or outsourcing' decisions and serve as a key process improvement tool. © 2011 Taylor & Francis.",Complexities; Cost modelling; Enterprise modelling (EM); Manufacturing enterprises (MEs); Simulation modelling (SM),"Agyapong-Kodua K., Wahid B.M., Weston R.H.",2011,Review,International Journal of Production Research,10.1080/00207543.2010.535861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855819834&doi=10.1080%2f00207543.2010.535861&partnerID=40&md5=e6f9a9321dc0c6c7ac824a2bdac1bbb5,"Precision Manufacturing Centre, University of Nottingham, Nottingham, NG7 2RD, United Kingdom; MSI Research Institute, Loughborough University, Loughborough, LE11 3TU, United Kingdom",,English,00207543,
Scopus,Software effort estimation based on optimized model tree,"Background: It is widely recognized that software effort estimation is a regression problem. Model Tree (MT) is one of the Machine Learning based regression techniques that is useful for software effort estimation, but as other machine learning algorithms, the MT has a large space of configurations and requires to carefully setting its parameters. The choice of such parameters is a dataset dependent so no general guideline can govern this process which forms the motivation of this work. Aims: This study investigates the effect of using the most recent optimization algorithm called Bees algorithm to specify the optimal choice of MT parameters that fit a specific dataset and therefore improve prediction accuracy. Method: We used MT with optimal parameters identified by the Bees algorithm to construct software effort estimation model. The model has been validated over eight datasets come from two main sources: PROMISE and ISBSG. Also we used 3-Fold cross validation to empirically assess the prediction accuracies of different estimation models. As benchmark, results are also compared to those obtained with Stepwise Regression, Case-Based Reasoning and Multi-Layer Perceptron. Results: The results obtained from combination of MT and Bees algorithm are encouraging and outperforms other well-known estimation methods applied on employed datasets. They are also interesting enough to suggest the effectiveness of MT among the techniques that are suitable for effort estimation. Conclusions: The use of the Bees algorithm enabled us to automatically find optimal MT parameters that are required to construct accurate effort estimation model for each individual dataset. Copyright © 2011 ACM.",Bees algorithm; Model tree; Software effort estimation,Azzeh M.,2011,Conference,ACM International Conference Proceeding Series,10.1145/2020390.2020396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054066579&doi=10.1145%2f2020390.2020396&partnerID=40&md5=821f2787e1d11dd39c9bb00bb3321fd8,"Faculty of Information Technology, Applied Science University, POBOX 166, Amman, Jordan",,English,,9781450307093
Scopus,A specific effort estimation method using function point,"Software estimation provides an im portant tool for project planning; whose quality and accuracy greatly affect the success of a project. Despite a plethora of estimation models, practitioners experience difficulties in applying them because most models attempt to include as many influential factors as possible in estimating software size and/or effort. This research suggests a different approach that simplifies and tailors a generic function point analysis model to increase ease of use. The proposed approach redefines the function type categories in the FPA model, on the basis of the target application's characteristics and system architecture. This method makes the function types more suitable for the particular application domain. It also enables function point counting by the programmers themselves instead of by an expert. An empirical study using historical data establishes the regression model and demonstrates that its prediction accuracy is comparable to that of a FPA model.",Effort estimation; Empirical model; Function point analysis; Project management; Size measurement,"Jeng B., Yeh D., Wang D., Lhu S.-L., Chen C.-M.",2011,Journal,Journal of Information Science and Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051512862&partnerID=40&md5=10057ec81a3758a340c9ff89b1aea95f,"Department of Information Management, National Sun Yat-sen University, Kaohsiung, 804, Taiwan; Department of Software Engineering, National Kaohsiung Normal University, Kaohsiung, 802, Taiwan; Department of Information System, China Steel Corporation, Kaohsiung, 812, Taiwan",,English,10162364,
Scopus,Effort estimation of component-based software development - A survey,"Effort estimation of software development is an important sub-discipline in software engineering. It has been the focus of much research mostly over the last couple of decades. In recent years, software development turned into engineering through the introduction of component-based software development (CBSD). The industry has reported significant advantages in using CBSD over traditional software development paradigms. However, the introduction of CBSD has also brought a host of unique challenges to software effort estimation which are quite different from those associated with traditional software development. Owing to the increasing tendency to use the CBSD approach in recent years, it is clear that effort estimation of CBSD is particularly an important area of research with a direct relevance to industry. In this study, the authors survey the most up-to-date research work published on predicting the effort of CBSD. The authors analyse the surveyed approaches in terms of modelling technique, the type of data required for their use, the type of estimation provided, lifecycle activities covered and their level of acceptability with regard to any validation. The aim of this survey is to provide a better understanding of the cost and schedule estimation approaches for CBSD. © 2011 The Institution of Engineering and Technology.",,"Wijayasiriwardhane T., Lai R., Kang K.C.",2011,Conference,IET Software,10.1049/iet-sen.2009.0051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959893059&doi=10.1049%2fiet-sen.2009.0051&partnerID=40&md5=27d84825406ca8347796171b55607c18,"Department of Computer Science and Computer Engineering, La Trobe University, VIC 3086, Australia; Department of Computer Science and Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea",,English,17518806,
Scopus,Applying a general regression neural network for predicting development effort of short-scale programs,"Software development effort prediction is considered in several international software processes as the Capability Maturity Model-Integrated (CMMi), by ISO-15504 as well as by ISO/IEC 12207. In this paper, data of two kinds of lines of code gathered from programs developed with practices based on the Personal Software Process (PSP) were used as independent variables in three models for estimating and predicting the development effort. Samples of 163 and 80 programs were used for verifying and validating, respectively, the models. The prediction accuracy comparison among a multiple linear regression, a general regression neural network, and a fuzzy logic model was made using as criteria the magnitude of error relative to the estimate (MER) and mean square error (MSE). Results accepted the following hypothesis: effort prediction accuracy of a general regression neural network is statistically equal than those obtained by a fuzzy logic model as well as by a multiple linear regression, when new and change code and reused code obtained from short-scale programs developed with personal practices are used as independent variables. © 2010 Springer-Verlag London Limited.",Fuzzy logic; General regression neural network; Multiple linear regression; Personal software process; Software development effort prediction; Software engineering education and training,Lopez-Martin C.,2011,Journal,Neural Computing and Applications,10.1007/s00521-010-0405-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952816396&doi=10.1007%2fs00521-010-0405-5&partnerID=40&md5=0d8d00f5cbdb8d6f6b29f304f309197b,"Information Systems Department, CUCEA, Guadalajara University, P.O. Box 45100, Guadalajara, Jalisco, Mexico",,English,09410643,
Scopus,Validating a size measure for effort estimation in model-driven Web development,"Web development is moving towards model-driven processes whose goal is the development of Web applications at a higher level of abstraction based on models and model transformations. This brings new opportunities to the Web project manager to make early estimates of the size and the effort required to produce Web applications based on their conceptual models. In the last few years, several studies for size and effort estimation have been performed. However, there are no studies regarding effort estimation in model-driven Web development. In this paper, we present the validation of a model-based size measure (OO-HFP) for Web effort estimation in the context of a model-driven Web development method. The validation is performed by comparing the prediction accuracy that OO-HFP provides with the accuracy provided by the standard function point analysis (FPA) method. The results of the study (using industrial data gathered from 31 Web projects) show that the effort estimates obtained for projects that are sized using OO-HFP are more accurate than the effort estimates obtained using the standard FPA method. This suggests that by following a model-driven development approach, the size measure obtained at the conceptual model of a Web application can be considered a suitable predictor of effort. © 2010 Elsevier Inc. All rights reserved.",Effort estimation; Functional size measurement; Measure validation; Model-driven development; Web engineering,"Abrahão S., Gómez J., Insfran E.",2010,Journal,Information Sciences,10.1016/j.ins.2010.05.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958152177&doi=10.1016%2fj.ins.2010.05.031&partnerID=40&md5=632efa1a98fbcdb6892796615271920c,"ISSI Research Group, Department of Information Systems and Computation, Universidad Politécnica de Valencia, Camino de Vera, s/n, 46022 Valencia, Spain; Department of Information Systems and Languages, University of Alicante, Campus de San Vicente Del Raspeig, Apartado 99, 03080 Alicante, Spain",,English,00200255,
Scopus,Impact of CMMI based software process maturity on COCOMO II's effort estimation,"The software capability maturity model has become a popular model for enhancing software development processes with the goal of developing high-quality software within budget and schedule. The software cost estimation model, constructive cost model, in its last update (constructive cost model II) has a set of seventeen cost drivers and a set of five scale factors. Process maturity is one of the five scale factors and its ratings are based on software capability maturity model. This paper examines the effect of process maturity on software development effort by deriving a new set of constructive cost model II's PMAT rating values based on the most recent version of CMM, i.e., capability maturity model integration. The precise data for the analysis was collected from the record of 40 historical projects which spanned the range of capability maturity model integration levels, from level 1 (lower half and upper half) to level 4, where eight data points were collected from each level. We followed the ideal scale factor method in order to withhold the effect of the constructive cost model II's PMAT scale factor. All prediction accuracies were measured using PRED. The study showed that the proposed model (with the new PMAT rating values) yielded better estimates as compared to the generic, constructive cost model II model's estimates.",CMMI; COCOMO; Cost driver; Effort estimation; Scale factor; SW-CMM,"Al Yahya M., Ahmad R., Lee S.",2010,Journal,International Arab Journal of Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751615590&partnerID=40&md5=f3864e097a58bc09182c7bcc20daf0fd,"Department of Software Engineering, University of Malaya, Malaysia",,English,16833198,
Scopus,Software effort estimation based on weighted fuzzy grey relational analysis,"Delivering accurate software effort estimation has been a research challenge for a long time, where none of the existing estimation methods has proven to consistently deliver an accurate estimate. Previous studies have demonstrated that estimation by analogy (EBA) is a viable alternative to other conventional estimation methods in terms of predictive accuracy. EBA offers a way to use a formal method with data from a past project to derive a new estimate. Two important research areas in EBA are addressed in this paper: software projects similarity measurement and attribute weighting. However, the inherent uncertainty of attribute measurement makes similarity measurement between two software projects subject to considerable uncertainty. To tolerate such inherent uncertainty we propose a new similarity measurement method by combining the advantages of Fuzzy Set Theory and Grey Relational Analysis. In addition, since each attribute has different influence on the project retrieval we propose a new approach to deal with this issue based upon the idea of Kendall's coefficient of concordance between the similarity matrix of project attributes and the similarity matrix of known effort values of the dataset. Our results show improved prediction accuracy when multiple project attributes are used with determined weights. © ACM 2009.",attribute weighting; fuzzy modeling; similarity measurement; software effort estimation,"Azzeh M., Neagu D., Cowling P.",2009,Conference,ACM International Conference Proceeding Series,10.1145/1540438.1540450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953754295&doi=10.1145%2f1540438.1540450&partnerID=40&md5=1db9e0757ad3c504a85df52a82f49b20,"Department of Computing, University of Bradford, Bradford BD7 1DP, United Kingdom",,English,,9781605586342
Scopus,Accurate estimates without local data?,"Models of software projects input project details and output predictions via their internal tunings. The output predictions, therefore, are affected by variance in the project details P and variance in the internal tunings T. Local data is often used to constrain the internal tunings (reducing T). While constraining internal tunings with local data is always the preferred option, there exist some models for which constraining tuning is optional. We show empirically that, for the USC COCOMO family of models, the effects of P dominate the effects of T i.e. the output variance of these models can be controlled without using local data to constrain the tuning variance (in ten case studies, we show that the estimates generated by only constraining P are very similar to those produced by constraining T with historical data). We conclude that, if possible, models should be designed such that the effects of the project options dominate the effects of the tuning options. Such models can be used for the purposes of decision making without elaborate, tedious, and time-consuming data collection from the local domain. Copyright © 2009 John Wiley & Sons, Ltd.",AI; Decision making; Model-based project management; Search; Software engineering,"Menzies T., Williams S., Elrawas O., Baker D., Boehm B., Hihn J., Lum K., Madachy R.",2009,Journal,Software Process Improvement and Practice,10.1002/spip.414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68349125859&doi=10.1002%2fspip.414&partnerID=40&md5=e0cdda817111123b90ff1c1c38fd71ac,"LCSEE, West Virginia University, Morgantown, WV, United States; CS, University of Southern California, Los Angeles, CA, United States; JPL, CA, United States; SE, Naval Postgraduate School, San Diego, CA, United States",,English,10774866,
Scopus,An approach to estimating work effort for enterprise systems software projects,"A major factor in the decision to develop enterprise software applications is the size of the project, which determines the amount of work necessary, which in turn will give decision makers some idea about the time required and the estimated cost. Two of the major approaches for such estimation—the Use Case Points (UCP) and COSMIC-FFP methods–are compared in this paper. The accuracy and reliability of each method are evaluated through the measurement of 50 use case studies. These were collected from the Customer Relationship Management systems development project of a major enterprise service institution. The results show that the UCP method is more accurate and reliable than the COSMIC-FFP in determining work effort. © 2007 Taylor & Francis Group, LLC.",COSMIC-FFP; Cost estimation; Enterprise applications; Industrial applications; Project management; Project schedule estimation; Requirements specifications; Software size estimation; Use case points,"Choi J., Ashokkumar S., Sircar S.",2007,Journal,Enterprise Information Systems,10.1080/17517570601088356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548031466&doi=10.1080%2f17517570601088356&partnerID=40&md5=45a99d73122acdeb09284ccf033b2ded,"Department of Information Systems and Operations Management, Raj Soin College of Business, Wright State University, Dayton, OH, 45435, United States; Raj Soin College of Business, Wright State University, Dayton, OH, 45435, United States; Richard T. Farmer School of Business, Miami University, Oxford, OH, 45056, United States",,English,17517575,
Scopus,Digging the development dust for refactoring,"Software repositories are rich sources of information about the software development process. Mining the information stored in them has been shown to provide interesting insights into the history of the software development and evolution. Several different types of information have been extracted and analyzed from different points of view. However, these types of information have not been sufficiently cross-examined to understand how they might complement each other. In this paper, we present a systematic analysis of four aspects of the software repository of an open source project -source-code metrics, identifiers, return-on-investment estimates, and design differencing - to collect evidence about refactorings that may have happened during the project development. In the context of this case study, we comparatively examine how informative each piece of information is towards understanding the refactoring history of the project and how costly it is to obtain. © 2006 IEEE.",,"Schofield C., Tansey B., Zhenchang X., Stroulia E.",2006,Conference,IEEE International Conference on Program Comprehension,10.1109/ICPC.2006.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845409754&doi=10.1109%2fICPC.2006.18&partnerID=40&md5=87decac5cf2f7e94393010d3367435e1,"Computing Science Department, University of Alberta, Edmonton, Alta. T6G 2H1, Canada",,English,,0769526012; 9780769526010
Scopus,Cost estimation in agile development projects,"One of the key measures of the resilience of a project is its ability to reach completion on time and on budget, regardless of the turbulent and uncertain environment it may operate within. Cost estimation and tracking are therefore paramount when developing a system. Cost estimation has long been a difficult task in systems development, and although much research has focused on traditional methods, little is known about estimation in the agile method arena. This is ironic given that the reduction of cost and development time is the driving force behind the emergence of the agile method paradigm. This study investigates the applicability of current estimation techniques to more agile development approaches by focusing on four case studies of agile method use across different organisations. The study revealed that estimation inaccuracy was a less frequent occurrence for these companies. The frequency with which estimates are required on agile projects, typically at the beginning of each iteration, meant that the companies found estimation easier than when traditional approaches were used. The main estimation techniques used were expert knowledge and analogy to past projects. A number of recommendations can be drawn from the research: estimation models are not a necessary component of the process; fixed price budgets can prove beneficial for both developers and customers; and experience and past project data should be documented and used to aid the estimation of subsequent projects.",Agile methods; Cost estimation; Project management; Software development; Systems development,"Keaveney S., Conboy K.",2006,Conference,"Proceedings of the 14th European Conference on Information Systems, ECIS 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870639499&partnerID=40&md5=416504817d69e7e81e0762eabeaa4aa6,"National University of Ireland, Galway, Ireland",,English,,
Scopus,Bayesian statistical effort prediction models for data-centred 4GL software development,"Constructing an accurate effort prediction model is a challenge in Software Engineering. This paper presents three Bayesian statistical software effort prediction models for database-oriented software systems, which are developed using a specific 4GL toolsuite. The models consist of specification-based software size metrics and development team's productivity metric. The models are constructed based on the subjective knowledge of human expert and calibrated using empirical data collected from 17 software systems developed in the target environment. The models' predictive accuracy is evaluated using subsets of the same data, which were not used for the models' calibration. The results show that the models have achieved very good predictive accuracy in terms of MMRE and pred measures. Hence, it is confirmed that the Bayesian statistical models can predict effort successfully in the target environment. In comparison with commonly used multiple linear regression models, the Bayesian statistical models'predictive accuracy is equivalent in general. However, when the number of software systems used for the models' calibration becomes smaller than five, the predictive accuracy of the best Bayesian statistical models are significantly better than the multiple linear regression model. This result suggests that the Bayesian statistical models would be a better choice when software organizations/practitioners do not posses sufficient empirical data for the models' calibration. The authors expect these findings to encourage more researchers to investigate the use of Bayesian statistical models for predicting software effort. © 2006 Elsevier B.V. All rights reserved.",4GL; Bayesian statistics; Effort prediction; Regression; Software metrics,"van Koten C., Gray A.R.",2006,Journal,Information and Software Technology,10.1016/j.infsof.2006.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749237671&doi=10.1016%2fj.infsof.2006.01.001&partnerID=40&md5=d63c1389d5c2dea975dec3b61d14f43d,"Department of Information Science, University of Otago, P.O. Box 56, Dunedin, New Zealand",,English,09505849,
Scopus,Assessing the maintainability benefits of design restructuring using dependency analysis,"Software developers and project managers often have to assess the quality of software design. A commonly adopted hypothesis is that a good design should cost less to maintain than a poor design. We propose a model for quantifying the quality of a design from a maintainability perspective. Based on this model, we propose a novel strategy for predicting the ""return on investment"" (ROI) for possible design restructurings using procedure level dependency analysis. We demonstrate this approach with two exploratory Java case studies. Our results show that common low level source code transformations change the system dependency structure in a beneficial way, allowing recovery of the initial refactoring investment over a number of maintenance activities. © 2003 IEEE.",Business; Costs; Investments; Predictive models; Project management; Quality management; Regression analysis; Software design; Software development management; Software quality,"Leitch R., Stroulia E.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943175348&doi=10.1109%2fMETRIC.2003.1232477&partnerID=40&md5=f1ca98e24bb0c6c73e6a10d649d02b66,"MacDonald, Dettwiler and Associates, Ltd., 13800 Commerce Parkway, Richmond, BC  V6V 2J3, Canada; Computing Science Department, University of Alberta, Edmonton, AB  T6G 2H1, Canada",IEEE Computer Society,English,15301435,0769519873
Scopus,Software Renewal Projects Estimation Using Dynamic Calibration,"Effort estimation is a long faced problem, but, in spite of the amount of research spent in this field, it still remains an open issue in the software engineering community. This is true especially in the case of renewal of legacy systems, where the current and well established approaches also fail. This paper presents an application of the method named Dynamic Calibration for effort estimation of renewal projects together with its experimental validation. The approach satisfies all the requirements of the estimation models. The results obtained applying dynamic calibration are compared with those obtained with a competitor method: estimation by analogy. The results are shown to be promising although further experimentation on field is needed.",,"Baldassarre M.T., Caivano D., Visaggio G.",2003,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2003.1235411,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956618884&doi=10.1109%2fICSM.2003.1235411&partnerID=40&md5=362270a7d6872f858bccb29ecdd6ee71,"Dipartimento di Informatica, Università di Bari, Via Orabona, 4, 70126 Bari, Italy",IEEE Computer Society,English,,769519059
Scopus,Measuring software functional size: Towards an effective measurement of complexity,"Data manipulation, or algorithmic complexity, is not taken into account adequately in any of the most popular functional size measurement methods. In this paper, we recall some well-known methods for measuring problem complexity in data manipulation and highlight the interest in arriving at a new definition of complexity. Up to now, the concept of effort has always been associated with complexity. This definition has the advantage of dissociating effort and complexity, referring instead to the characteristics and intrinsic properties of the software itself. Our objective is to propose some simple and practical approaches to the measurement of some of these characteristics which we consider particularly relevant and to incorporate them into a functional size measurement method.",,"Tran-Cao D., Levesque G., Abran A.",2002,Conference,Conference on Software Maintenance,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036437796&partnerID=40&md5=2f49dade8b2bbebd38a0316b632c0498,"Software Eng. Management Res. Lab., University of Quebec at Montreal, Montreal, Que., Canada; Cognitive and Computer Sciences, Universite du Quebec a Montreal, Montreal, Que., Canada; Universite du Quebec a Montreal, Montreal, Que., Canada; Ecole de Technologie Superieure, Montreal, Que., Canada",,English,,
Scopus,Maintenance and testing effort modeled by linear and nonlinear dynamic systems,"Maintenance and testing activities - conducted, respectively, on the release currently in use/to be delivered - absorb most of total lifetime cost of software development. Such economic relevance suggests investigating the maintenance and testing processes to find models allowing software engineers to better estimate, plan and manage costs and activities. Ecological systems in which predators and prey compete for surviving were investigated by applying suitable mathematical models. An analogy can be drawn between biological prey and software defects, and between predators and programmers. In fact, when programmers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find any new defect has an initial increase, followed by a decline, when almost all defects are removed, similar to prey and predator populations. This paper proposes to describe the evolution of the maintenance and testing effort by means of the predator-prey dynamic model. The applicability of the model is supported by the experimental data about two real world projects. The fit of the model when parameters are estimated on all available data is high, and accurate predictions can be obtained when an initial segment of the available data is used for parameter estimation. © 2001 Elsevier Science B.V.",Cost estimating; Maintenance and testing effort; Predator-prey dynamic model,"Calzolari F., Tonella P., Antoniol G.",2001,Journal,Information and Software Technology,10.1016/S0950-5849(01)00156-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035400435&doi=10.1016%2fS0950-5849%2801%2900156-2&partnerID=40&md5=4e0185edab0a0178535b754bba1356c9,"ITC-irst, Centro per la Ricerca Scientifica e Technologica, 1-38050, Pova, Trento, Italy",,English,09505849,
Scopus,Characterization of risky projects based on project managers' evaluation,"During the process of software development, senior managers often find indications that projects are risky and take appropriate actions to recover them from this dangerous status. If senior managers fail to detect such risks, it is possible that such projects may collapse completely. In this paper, we propose a new scheme for the characterization of risky projects based on an evaluation by the project manager. In order to acquire the relevant data to make such an assessment, we first designed a questionnaire from five viewpoints within the projects: requirements, estimations, team organization, planning capability and project management activities. Each of these viewpoints consisted of a number of concrete questions. We then analyzed the responses to the questionnaires as provided by project managers by applying a logistic regression analysis. That is, we determined the coefficients of the logistic model from a set of the questionnaire responses. The experimental results using actual project data in Company A showed that 27 projects out of 32 were predicted correctly. Thus we would expect that the proposed characterizing scheme is the first step toward predicting which projects are risky at an early phase of the development.",,"Mizuno Osamu, Kikuno Tohru, Takagi Yasunari, Sakamoto Keishi",2000,Conference,Proceedings - International Conference on Software Engineering,10.1145/337180.337226,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033711811&doi=10.1145%2f337180.337226&partnerID=40&md5=d579f06c86cb537fd3d5eb2167861f92,"Osaka Univ, Osaka, Japan","IEEE, Los Alamitos",English,02705257,
Scopus,Linear programming as a baseline for software effort estimation,"Software effort estimation studies still suffer from discordant empirical results (i.e., conclusion instability) mainly due to the lack of rigorous benchmarking methods. So far only one baseline model, namely, Automatically Transformed Linear Model (ATLM), has been proposed yet it has not been extensively assessed. In this article, we propose a novel method based on Linear Programming (dubbed as Linear Programming for Effort Estimation, LP4EE) and carry out a thorough empirical study to evaluate the effectiveness of both LP4EE and ATLM for benchmarking widely used effort estimation techniques. The results of our study confirm the need to benchmark every other proposal against accurate and robust baselines. They also reveal that LP4EE is more accurate than ATLM for 17% of the experiments and more robust than ATLM against different data splits and cross-validation methods for 44% of the cases. These results suggest that using LP4EE as a baseline can help reduce conclusion instability. We make publicly available an open-source implementation of LP4EE in order to facilitate its adoption in future studies. © 2018 Association for Computing Machinery.",Benchmarking; Linear programming; Software effort estimation,"Sarro F., Petrozziello A.",2018,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/3234940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053899656&doi=10.1145%2f3234940&partnerID=40&md5=f7d4c7beb63122a61d355a12ff133855,"Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom; School of Computing, University of Portsmouth, Portsmouth, PO1 3HE, United Kingdom",Association for Computing Machinery,English,1049331X,
Scopus,Integrating non-parametric models with linear components for producing software cost estimations,"A long-lasting endeavor in the area of software project management is minimizing the risks caused by under- or over-estimations of the overall effort required to build new software systems. Deciding which method to use for achieving accurate cost estimations among the many methods proposed in the relevant literature is a significant issue for project managers. This paper investigates whether it is possible to improve the accuracy of estimations produced by popular non-parametric techniques by coupling them with a linear component, thus producing a new set of techniques called semi-parametric models (SPMs). The non-parametric models examined in this work include estimation by analogy (EbA), artificial neural networks (ANN), support vector machines (SVM) and locally weighted regression (LOESS). Our experimentation shows that the estimation ability of SPMs is superior to their non-parametric counterparts, especially in cases where both a linear and non-linear relationship exists between software effort and the related cost drivers. The proposed approach is empirically validated through a statistical framework which uses multiple comparisons to rank and cluster the models examined in non-overlapping groups performing significantly different. © 2014 Elsevier Inc. All rights reserved.",Semi-parametric models; Software cost estimation,"Mittas N., Papatheocharous E., Angelis L., Andreou A.S.",2015,Journal,Journal of Systems and Software,10.1016/j.jss.2014.09.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912523849&doi=10.1016%2fj.jss.2014.09.025&partnerID=40&md5=78b7b74c570e9d34e8f24d1a14d8b050,"Department of Informatics, Aristotle University of Thessaloniki, Greece; Department of Computer Science, University of Cyprus, Cyprus; Swedish Institute of Computer Science (SICS), Kista, SE-16429, Sweden; Department of Electrical Engineering/Computer Engineering and Informatics, Cyprus University of Technology, Cyprus",Elsevier Inc.,English,01641212,
Scopus,MND-SCEMP: An empirical study of a software cost estimation modeling process in the defense domain,"The primary focus of weapon systems research and development has moved from a hardware base to a software base and the cost of software development is increasing gradually. An accurate estimation of the cost of software development is now a very important task in the defense domain. However, existing models and tools for software cost estimation are not suitable for the defense domain due to problems of accuracy. Thus, it is necessary to develop cost estimation models that are appropriate to specific domains. Furthermore, most studies of methodology development are aligned with generic methodologies that do not consider the pertinent factors to specific domains, whereas new methodologies should reflect specific domains. In this study, we apply two generic methodologies to the development of a software cost estimation model, before suggesting an integrated modeling process specifically for the national defense domain. To validate our proposed modeling process, we performed an empirical study of 113 software development projects on weapon systems in Korea. A software cost estimation model was developed by applying the proposed modeling process. The MMRE value of this model was 0.566 while the accuracy was appropriate for use. We conclude that the modeling process and software cost estimation model developed in this study is suitable for estimating resource requirements during weapon system development in South Korea's national defense domain. This modeling process and model may facilitate more accurate resource estimation by project planners, which will lead to more successful project execution. © 2012 Springer Science+Business Media, LLC.",Common cost factors; Defense domain; Domain specific cost factors; Software cost estimation modeling process; Software development process,"Lee T., Gu T., Baik J.",2014,Journal,Empirical Software Engineering,10.1007/s10664-012-9220-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893799537&doi=10.1007%2fs10664-012-9220-1&partnerID=40&md5=45ce1d9dd96e1b2bc1467ab45508b84c,"Joint Modeling and Simulation Center, Agency for Defense Development (ADD), DaeJeon, South Korea; Quality Control Team, Hyundai Automotive Electronics (AUTRON), Mtek IT Tower, 688-1, Sampyeong-dong, Bundang-gu, Seongnam-si, Gyeonggi-do 463-400, South Korea; Dept. of Computer Science, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro, Yuseong-gu, Daejeon 305-701, South Korea",,English,13823256,
Scopus,Evaluating software product quality: A systematic mapping study,"Evaluating software product quality (SPQ) is an important task to ensure the quality of software products. In this paper a systematic mapping study was performed to summarize the existing SPQ evaluation (SPQE) approaches in literature and to classify the selected studies according to seven classification criteria: SPQE approaches, research types, empirical types, data sets used in the empirical evaluation of these studies, artifacts, SQ models, and SQ characteristics. Publication channels and trends were also identified. 57 papers were selected. The results show that the main publication sources of the papers identified were journals. Data mining techniques are the most frequently approaches reported in literature. Solution proposals were the main research type identified. The majority of the selected papers were history-based evaluations using existing data, which were mainly obtained from open source software projects and domain specific projects. Source code was the main artifacts used by SPQE approaches. Well-known SQ models were mentioned by half of the selected papers and reliability is the SQ characteristic through which SPQE was mainly achieved. SPQE-related subjects seem to attract more interest from researchers since the past years. © 2014 IEEE.",,"Ouhbi S., Idri A., Aleman J.L.F., Toval A.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924054129&doi=10.1109%2fIWSM.Mensura.2014.30&partnerID=40&md5=dde0c90f7da7b7c14764aae521798235,"Software Project Management Research Team, ENSIAS, University Mohammed v Souissi, Rabat, Morocco; Dept. Informatica y Sistemas, University of Murcia, Murcia, Spain",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,A method to optimize the scope of a software product platform based on end-user features,"Context: Due to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.Aim: This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.Method: The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.Results In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. © 2014 Elsevier Inc. All rights reserved.",Product platform scope Software product line engineering Commonality decision,"Alsawalqah H.I., Kang S., Lee J.",2014,Journal,Journal of Systems and Software,10.1016/j.jss.2014.08.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908291258&doi=10.1016%2fj.jss.2014.08.034&partnerID=40&md5=b78a56f37674d45f046b08e09e784143,"Department of Information and Communications Engineering, Korea Advanced Institute of Science and Technology, 373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea",Elsevier Inc.,English,01641212,
Scopus,Assessment of voting ensemble for estimating software development effort,"This paper reports and discusses the results of an assessment study, which aimed to determine the extent to which the voting ensemble model offers reliable and improved estimation accuracy over five individual models (MLP, RBF, RT, KNN and SVR) in estimating software development effort. Five datasets were used for this purpose. The results confirm that individual models are not reliable as their performance is inconsistence and unstable across different datasets. However, the ensemble model provides more reliable performance than individual models. In three out of the five datasets that were used in this study, the ensemble model outperformed the individual models. In the other two datasets, the ensemble model achieved the second best performance, which was still very competitive as there was no statistically significant difference between it and the best models in these two datasets. © 2013 IEEE.",Computational intelligence; ensemble; software effort estimation,Elish M.O.,2013,Conference,"Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013",10.1109/CIDM.2013.6597253,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885628976&doi=10.1109%2fCIDM.2013.6597253&partnerID=40&md5=320e9a4af95a933d74456de738dec2ee,"Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia",,English,,9781467358958
Scopus,Applying soft computing approaches to predict defect density in software product releases: An empirical study,"There is non-linear relationship between software metrics and defects, which results to a complex mapping. Therefore, to focus on the defect density area, it is a critical business requirement of effective and practical approach, which can help find the defect density in software releases. Soft computing provides a better platform to solve the non-linear and complex mapping problem. The aim of this paper is to formulate, build, evaluate, validate and compare two main sections of soft computing, fuzzy logic and artificial neural network approaches in prediction of defect density of subsequent software product releases. In this research, these two approaches are formulated and applied to predict the existence of a defect in file of software release. Both approaches have also been validated against various releases of two commercial software product release data sets. The validation criteria include mean absolute error, root mean square error and graphical analysis. The analysis of the study shows that artificial neural network provides better results compared to Fuzzy Inference System; but applicability of best approach depends on the data availability and the quantum of data.",Artificial neural network; Defect; Defect density; Fuzzy inference system; Prediction; Release; Software metrics,"Kumar V., Sharma A., Kumar R.",2013,Journal,Computing and Informatics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876108869&partnerID=40&md5=66cbcd666675414b52a108cc6a35eb2d,"Data Communication and Product Engineering Services, Aricent Technologies, Gurgaon, India; Department of Computer Science and Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, India; School of Mathematics and Computer Applications, Thapar University, Patiala, India",,English,13359150,
Scopus,Choosing cost-effective configuration in cloud storage,Cloud storage provides a virtually unlimited storage spaces for customers. Customers can combine their data storages from different types of cloud storage following their own requirements. End customers often stuck in choosing desirable configuration from different types of cloud storage. How to spend the minimum costs on using the highly efficient cloud storage? How to balance the relationship between the expenditures and performance? This paper proposes a cost-effective optimal configuration model using the repeated game model that can provide optimal configuration solutions to customers. The data mining techniques are used in provisioning on cloud storage. Classification helps users to find the related data and trend analysis assists users to mine the future trend on data storage. A simulate experiment is discussed and verify the correctness of the proposed model.,,"Tsai W.-T., Qi G., Chen Y.",2013,Conference,"Proceedings - 2013 11th International Symposium on Autonomous Decentralized Systems, ISADS 2013",10.1109/ISADS.2013.6513413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990945242&doi=10.1109%2fISADS.2013.6513413&partnerID=40&md5=397074bbf31c1536cf7f6815a08a5c67,"School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, United States",Institute of Electrical and Electronics Engineers Inc.,English,,9781467350686
Scopus,Radial basis function network using intuitionistic fuzzy C means for software cost estimation,"Software development has become an important activity for many modern organisations. Software engineers have become more and more concerned about accurately predicting the cost and quality of software product under development. In the last few decades many software cost estimation models have been developed but no model has proved to be successful at effectively and consistently predicting software development cost. In this paper we propose the use of Radial Basis Function Network (RBFN) for software cost estimation using Intuitionistic Fuzzy C Means (IFCM) with Gaussian potential functions. This technique selects the most desirable cluster centres, thereby increasing the clustering accuracy which results in improved software cost estimations. A comparison of RBFN using IFCM, Fuzzy C Means (FCM) and conventional COCOMO model is presented. The datasets used in our study are the COCOMO81 dataset and NASA93 dataset. Experimental results are given to show the effectiveness of the proposed method. © 2013 Inderscience Enterprises Ltd.",Fuzzy clustering; IFCM; Intuitionistic fuzzy C means; Radial basis function neural network; Software cost estimation,"Kaushik A., Soni A.K., Soni R.",2013,Journal,International Journal of Computer Applications in Technology,10.1504/IJCAT.2013.054305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878690210&doi=10.1504%2fIJCAT.2013.054305&partnerID=40&md5=7f86a2030e0ece0b1d43f917702be1b9,"Department of Information and Technology, Maharaja Surajmal Institute of Technology, Janakpuri, New Delhi 110058, India",Inderscience Publishers,English,09528091,
Scopus,A metrics framework for evaluating SoA service granularity,"Service-Oriented Architecture (SOA) is intended to improve software interoperability by exposing dynamic applications as services. To evaluate the design of services in service-based systems, quality measurements are essential to decide tradeoffs between SOA quality attributes. Current SOA quality metrics pay little attention to service granularity as an important key design feature that impacts other internal SOA quality attributes. In this paper we introduce the structural attribute of service granularity for the analysis of other internal structural software attributes: complexity, cohesion and coupling. Consequently, metrics are proposed for measuring SOA internal attributes using syntax code. These metrics will assist in development of optimal service design by considering appropriate trade-offs. An example case study is included to demonstrate proposed metrics. © 2011 IEEE.",Product metrics; Service granularity; SOA; SOA quality; SOEA; Software metrics/measuremen,"Alahmari S., Zaluska E., De Roure D.C.",2011,Conference,"Proceedings - 2011 IEEE International Conference on Services Computing, SCC 2011",10.1109/SCC.2011.98,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053147171&doi=10.1109%2fSCC.2011.98&partnerID=40&md5=080a0208552bc2a6fa89da76c0e86c69,"School of Electronics and Computer Science, University Southampton, United Kingdom",,English,,9780769544625
Scopus,Contrasting ideal and realistic conditions as a means to improve judgment-based software development effort estimation,"Context: The effort estimates of software development work are on average too low. A possible reason for this tendency is that software developers, perhaps unconsciously, assume ideal conditions when they estimate the most likely use of effort. In this article, we propose and evaluate a two-step estimation process that may induce more awareness of the difference between idealistic and realistic conditions and as a consequence more realistic effort estimates. The proposed process differs from traditional judgmentbased estimation processes in that it starts with an effort estimation that assumes ideal conditions before the most likely use of effort is estimated. Objective: The objective of the paper is to examine the potential of the proposed method to induce more realism in the judgment-based estimates of work effort. Method: Three experiments with software professionals as participants were completed. In all three experiments there was one group of participants which followed the proposed and another group which followed the traditional estimation process. In one of the experiments there was an additional group which started with a probabilistically defined estimate of minimum effort before estimating the most likely effort. Results: We found, in all three experiments, that estimation of most likely effort seems to assume rather idealistic assumptions and that the use of the proposed process seems to yield more realistic effort estimates. In contrast, starting with an estimate of the minimum effort, rather than an estimate based on ideal conditions, did not have the same positive effect on the subsequent estimate of the most likely effort. Conclusion: The empirical results from our studies together with similar results from other domains suggest that the proposed estimation process is promising for the improvement of the realism of software development effort estimates. © 2011 Elsevier B.V. All rights reserved.",Effort estimation; Expert estimation; Human judgment,Jorgensen M.,2011,Journal,Information and Software Technology,10.1016/j.infsof.2011.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865790615&doi=10.1016%2fj.infsof.2011.07.001&partnerID=40&md5=a56fbf15c9580cc458acfc19cff172a6,"Simula Research Laboratory, Institute of Informatics, University of Oslo, Oslo, Norway",Elsevier B.V.,English,09505849,
Scopus,Component Point: A system-level size measure for Component-Based Software Systems,"System-level size measures are particularly important in software project management as tasks such as planning and estimating the cost and schedule of software development can be performed more accurately when a size estimate of the entire system is available. However, due to the black-box nature of components, the traditional software measures are not adequate for Component-Based Software Systems (CBSS). In this paper, we describe a Function Point-like measure, named Component Point (CP), for measuring the system-level size of a CBSS specified in the Unified Modelling Language. Our approach integrates three software measures and extends an existing size measure from the more matured Object-Oriented paradigm to the related and relatively young CBSS discipline. We then apply the proposed measure to a Global Positioning System and demonstrate its viability in sizing a CBSS. An empirical analysis is also provided in order to prove the validity and usefulness of the CP measure. © 2010 Elsevier Inc.",Component Point; Component-Based Software System; Function Point; Software measures; Software sizing,"Wijayasiriwardhane T., Lai R.",2010,Conference,Journal of Systems and Software,10.1016/j.jss.2010.07.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049334386&doi=10.1016%2fj.jss.2010.07.008&partnerID=40&md5=c04ee97cb24dd1e76801a609d68d3974,"Department of Computer Science and Computer Engineering, La Trobe University, Bundoora, Melbourne, VIC 3086, Australia",,English,01641212,
Scopus,Using Function Points to measure and estimate real-time and embedded software: Experiences and guidelines,"The developers of real-time and embedded software face-just like the developers of other types of software-the problem of estimating the cost of development. To this end, the most widely used methods and tools require that the functional size of the program to be developed is measured. However, the functional size measurement methods available-namely, Function Point Analysis and its evolutions-are traditionally considered not well suited for representing the functionality of real-time and embedded software. Actually, the problem is that the definition of Function Points and their counting rules make reference almost exclusively to traditional ""business"" software. In this paper, the problem of applying FPA to embedded and real-time software is tackled. A set of hints and examples-derived from industrial experience-are given, supporting the application of standard function point counting to real-time and embedded software. It is then shown that the obtained measures successfully supported the estimation of a set of programs in the avionics domain. © 2009 IEEE.",,"Lavazza L., Garavaglia C.",2009,Conference,"2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",10.1109/ESEM.2009.5316018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449179859&doi=10.1109%2fESEM.2009.5316018&partnerID=40&md5=721dc1215ab700b68202a44df6386d28,"University of Insubria, CEFRIEL, Italy; Intecs, Italy",,English,,9781424448418
Scopus,The software quality economics model for software project optimization,"There are many definitions of quality being given by experts that explains quality for manufacturing industry but still unable to define it with absolute clarity for software engineering. To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices is given. In this paper we propose a model which traces design decisions and the possible alternatives. With this model it is possible to minimize the cost of switching between design alternatives, when the current choice cannot fulfill the quality constraints. With this model we do not aim to automate the software design process or the identification of design alternatives. Much rather we aim to define a method with which it is possible to assist the software engineer in evaluating design alternatives and adjusting design decisions in a systematic manner. As of today there is very little knowledge is available about the economics of software quality. The costs incurred and benefits of implementing different quality practices over the software development life cycle are not well understood. There are some prepositions, which are not being tested comprehensively, but some useful Economic Model of Software Quality Costs (CoSQ) and data from industry are described in this article. Significant research is needed to understand the economics of implementing quality practices and its behaviour. Such research must evaluate the cost benefit trade-offs in investing in quality practices where the returns are maximized over the software development life cycle. From a developer's perspective, there are two types of benefits that can accrue from the implementation of good software quality practices and tools: money and time. A financial ROI calculation of cost savings and the schedule ROI calculation of schedule savings are given.",Cost optimization; Quality Cost Model; ROI calculation; Software Quality; TQM,"Lazić L., Kolašinac A., Avdić D.",2009,Journal,WSEAS Transactions on Computers,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59249106381&partnerID=40&md5=4ec00a43f13466bb37ea0e5b2b657ec7,"Technical Faculty, University of Novi Pazar, Vuka Kradžića bb, 36 300 Novi Pazar, Serbia",,English,11092750,
Scopus,Building theories from multiple evidence sources,"As emphasized in other chapters of this book, useful results in empirical software engineering require a variety of data to be collected through different studies - focusing on a single context or single metric rarely tells a useful story. But, in each study, the requirements of the local context are liable to impose different constraints on study design, the metrics to be collected, and other factors. Thus, even when all the studies focus on the same phenomenon (say, software quality), such studies can validly collect a number of different measures that are not at all compatible (say, number of defects required to be fixed during development, number of problem reports received from the customer, total amount of effort that needed to be spent on rework). Can anything be done to build a useful body of knowledge from these disparate pieces? This chapter addresses strategies that have been applied to date to draw conclusions from across such varied but valid data sets. Key approaches are compared and the data to which they are best suited are identified. Our analysis together with associated lessons learned provide decision support for readers interested in choosing and using such approaches to build up useful theories. © 2008 Springer-Verlag London.",,"Shull F., Feldmann R.L.",2008,Book Chapter,Guide to Advanced Empirical Software Engineering,10.1007/978-1-84800-044-5_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61849106593&doi=10.1007%2f978-1-84800-044-5_13&partnerID=40&md5=38896ed12353c3ac4488044a1e35f5a2,"Fraunhofer Center Maryland, College Park, MD 20742, United States",Springer London,English,,9781848000438
Scopus,The economics of open source software: An empirical analysis of maintenance costs,"A quality degradation effect ofpmprietary code has been observed as a consequence of maintenance. This quality degradation effect, called entropy, is a cause for higher maintenance costs. In the Open Source context, the quality of code is a fundamental tenet of open software developers. As a consequence, the quality degradation principle measured by entmpy cannot be assumed to be valid. The goal of the paper is to analyze the entropy of Open Source applications by measuring the evolution of maintenance costs over time. Analyses are based on cost data collected from a sample of 1251 Open Source application versions, compared with the costs estimated with a traditional model for proprietary software. Findings indicate that Open Source applications are less subject to entropy, have lower maintenance costs and also a lower need for maintenance interventions aimed at restoring quality. Finally, results show that a lower entropy is favored by greater functional simplicity. © 2007 IEEE.",,"Capra E., Francalanci C., Merlo F.",2007,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2007.4362652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349083517&doi=10.1109%2fICSM.2007.4362652&partnerID=40&md5=e74e68969abdbc55a2ba618f36e0087b,"Politécnico di Milano, Dipartimento di Elettronica ed Informazione",,English,,1424412560; 9781424412563
Scopus,"Increasing software effort estimation accuracy - Using experience data, estimation models and checklists","It is frequently suggested that using experience data, estimation models and checklists can increase software effort estimation accuracy. However, there has been limited empirical research on the subject. We conducted a study of eighteen of the latest projects of a software contractor. Quantitative and qualitative data was collected on several issues related to estimates, key project properties, and project outcome. It was found that in projects where experience data was utilized in the estimation process, they experienced a lesser magnitude of effort overruns. The use of a checklist also appeared to increase estimation accuracy. However, the utilization of an estimation model did not appear to have a substantial impact. © 2007 IEEE.",Checklists; Estimation models; Experience data; Software estimation,"Furulund K.M., Moløkken-Østvold K.",2007,Conference,Proceedings - International Conference on Quality Software,10.1109/QSIC.2007.4385518,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449139539&doi=10.1109%2fQSIC.2007.4385518&partnerID=40&md5=b2e4623e76e10447c2c83f1880bf8c1d,"Department of Informatics, P.O.Box 1080 Blindern, 0316 Oslo, Norway; Simula Research Laboratory, P.O.Box 134, 1325 Lysaker, Norway",,English,15506002,0769530354; 9780769530352
Scopus,Approaching the ERP project cost estimation problem: An experiment,This poster reports on a solution to ERP project cost estimation and on results from its first experimental application. © 2007 IEEE.,,Daneva M.,2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849098195&doi=10.1109%2fESEM.2007.22&partnerID=40&md5=44de11135ac0a5a7ac893288c6ce9498,University of Twente,,English,,0769528864; 9780769528861
Scopus,Exploring case-based reasoning for web hypermedia project cost estimation,"This paper compares several methods of analogy-based effort estimation, including the use of adaptation rules as a contributing factor to better estimation accuracy. Two data sets are used in the analysis. Results show that best predictions were obtained for the dataset that presented a continuous 'cost' function and was more 'unspoiled'. Copyright © 2005 Inderscience Enterprises Ltd.",Case-based reasoning; Prediction models; Web effort prediction; Web hypermedia; Web hypermedia metrics,"Mendes E., Mosley N., Counsell S.",2005,Journal,International Journal of Web Engineering and Technology,10.1504/IJWET.2005.007467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344466269&doi=10.1504%2fIJWET.2005.007467&partnerID=40&md5=6a4c84525e87e0717887e3836e5b2667,"Department of Computer Science, University of Auckland, 38 Princes Street, Auckland, New Zealand; MetriQ Limited, Wai-O-Taiki Bay, Auckland, New Zealand; Department of Computer Science and Information Systems, Brunel University, Uxbridge UB8 3PH, Middlesex, United Kingdom; WETA (Web Engineering, Technology and Applications) Research Group, New Zealand",Inderscience Publishers,English,14761289,
Scopus,Towards a functional size measure for object-oriented systems from requirements specifications,This paper describes a measurement protocol to map the concepts used in the OO-Method Requirements Model onto the concepts used by the COSMIC Full Function Points (COSMIC-FFP) functional size measurement method. This protocol describes a set of measurement operations for modeling and sizing object-oriented software systems from requirements specifications obtained in the context of the OO-Method. This development method starts from a requirements model that allows the specification of software functional requirements and generates a conceptual model through a requirements analysis process. The main contribution of this work is an extended set of rules that allows estimating the functional size of OO systems at an early stage of the development lifecycle. A case study is introduced to report the obtained results from a practical point of view.,COSMIC-FFP; Functional Size Measurement; Requirements Model,"Condori-Fernández N., Abrahão S., Pastor O.",2004,Conference,"Proceedings - Fourth International Conference on Quality Software, QSIC 2004",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14044273660&partnerID=40&md5=fc58425585db6559048235f492c23326,"Department of Information Systems, Valencia University of Technology, Camino de Vera s/n, 46071 Valencia, Spain",,English,,0769522076
Scopus,Effort estimation based on collaborative filtering,"Effort estimation methods are one of the important tools for project managers in controlling human resources of ongoing or future software projects. The estimations require historical project data including process and product metrics that characterize past projects. Practically, in using the estimation methods, it is a problem that the historical project data frequently contain substantial missing values. In this paper, we propose an effort estimation method based on Collaborative Filtering for solving the problem. Collaborative Filtering has been developed in information retrieval researchers, as one of the estimation techniques using defective data, i.e. data having substantial missing values. The proposed method first evaluates similarity between a target (ongoing) project and each past project, using vector based similarity computation equation. Then it predicts the effort of the target project with the weighted sum of the efforts of past similar projects. We conducted an experimental case study to evaluate the estimation performance of the proposed method. The proposed method showed better performance than the conventional regression method when the data had substantial missing values. © Springer-Verlag Berlin Heidelberg 2004.",,"Ohsugi N., Tsunoda M., Monden A., Matsumoto K.-I.",2004,Journal,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-24659-6_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048823368&doi=10.1007%2f978-3-540-24659-6_20&partnerID=40&md5=f55facb52888e3b4d886d2c2c1a5be3b,"Graduate School of Information Science, Nara Institute of Science and Technology, Kansai Science City, 630-0192, Japan",Springer Verlag,English,03029743,3540214216; 9783540214212
Scopus,Portfolio management method for deadline planning,"We introduce a portfolio management method that uses effort estimates to build sets of feasible deadlines for software projects at the bidding stage. Effort estimates can involve considerable error, and this must be taken into account when selecting deadlines. We show how a simple probability model can allow for possible errors. The model is built using a single effort estimate for each current project, together with historical data on estimated and actual effort for former projects. The probability model is used in two ways: firstly to find the probability of successfully meeting a set of proposed deadlines; and secondly to select deadlines that deliver a fixed probability of success. Rather than treating projects in isolation, we work with the full company portfolio, enabling setbacks in some projects to be balanced by gain in others. Our method is implemented with demonstrations in the tool PROJMAN. © 2003 IEEE.",Application software; Computer errors; Computer science; Contracts; Portfolios; Probability distribution; Project management; Risk analysis; Risk management; Statistics,"Fewster R.M., Mendes E.",2003,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2003.1232478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943138675&doi=10.1109%2fMETRIC.2003.1232478&partnerID=40&md5=00d61da522d0eb22cca009c265f59d82,"Department of Statistics, University of Auckland, Private Bag 92019, Auckland, New Zealand; Department of Computer Science, University of Auckland, Private Bag 92019, Auckland, New Zealand",IEEE Computer Society,English,15301435,0769519873
Scopus,The impact of design properties on development cost in object-oriented systems,"In the context of software cost estimation, system size is widely taken as a main driver of system development effort. But other structural design properties, such as coupling, cohesion, complexity have been suggested as additional cost factors. In this paper, using effort data from an object-oriented development project, we empirically investigate the relationship between class size and the development effort for a class, and what additional impact structural properties such as class coupling have on effort. We use Poisson regression and regression trees to build cost prediction models from size and design measures, and use these models to predict system development effort. We also investigate a recently suggested technique to combine regression trees with regression analysis, which aims at building more accurate models. Results indicate that fairly accurate predictions of class effort can be made based on simple measures of the class interface size alone (mean MREs below 30%). Effort predictions at the system level are even more accurate as, using Bootstrapping, the estimated 95% confidence interval for MREs is 3%-23%. But more sophisticated coupling and cohesion measures do not help to improve these predictions to a degree that would be practically significant. However, the use of hybrid models, combining Poisson regression and CART regression trees clearly improves the accuracy of the models, as compared to using Poisson regression alone.",Cost estimation; Empirical validation; Object-oriented measurement,"Briand L.C., Wüst J.",2001,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034998427&partnerID=40&md5=4bed249ddd0a6ea054c1c8e53a7d76d2,"Carleton Univiversity, Systems and Computer Engineering, 1125 Colonel By Drive, Ottawa, Ont. K1S 5B6, Canada",,English,,
Scopus,Software measurement methods: recipes for success?,"Measurement is a much advocated, yet infrequently applied technique of software engineering. A major contributory factor to this state of affairs is that the majority of software metrics are developed, collected and applied in a haphazard fashion. The result is metrics that frequently are poorly formulated, inappropriate to the specific needs and environment of the using organization and hard to analyse once collected. Over recent years a number of measurement methods-frameworks for developing and applying metrics-have been proposed and used to rectify this state of affairs. This paper describes and evaluates these various methods, identifies strengths and weaknesses leading to an agenda of further work. The requirements of a new measurement method are proposed and emphasizes increased application of measurement within context of software development processes. © 1994.",measurement methods; measurement process; software development processes; software metrics,"Roche J., Jackson M.",1994,Journal,Information and Software Technology,10.1016/0950-5849(94)90056-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149145289&doi=10.1016%2f0950-5849%2894%2990056-6&partnerID=40&md5=f20716133685bf0f88770ca13b601312,"School of Computing and Information Technology, University of Wolverhampton, Wolverhampton, WV1 1LY, United Kingdom",,English,09505849,
Scopus,An evaluation of software product metrics,"The study of quantitative aspects of software engineering can be divided into measurements that relate to the process of engineering software and those that relate to the software engineering product. This paper deals with the latter. Some of the most well known and some of the most promising metrics are reviewed. Well known metrics include the code metrics of Halstead and McCabe; however, code metrics suffer from a number of inherent limitations. The more promising but relatively unexplored fields of design and specification metrics are outlined. The paper concludes with some speculative remarks concerning the future of product metrics. © 1988.",product metrics; software engineering; software metrics,Shepperd M.,1988,Journal,Information and Software Technology,10.1016/0950-5849(88)90064-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023994262&doi=10.1016%2f0950-5849%2888%2990064-X&partnerID=40&md5=daea2ea4f8d9ab4bc98859bd36c2aa9e,"School of Computing and Information Technology, The Polytechnic: Wolverhampton, Wulfruna Street, Wolverhampton WV1 1LY, UK, United Kingdom",,English,09505849,
Scopus,Software cost optimization integrating fuzzy system and COA-Cuckoo optimization algorithm,"Processing the uncertainty in the software cost estimation is now highly needful considering the growing use of software cost optimization in modern organizations. In this paper, a new methodology for software cost optimization is introduced. The software cost estimation is an “approximate judgment” of the cost and effort incurred in a software project model. The proposed method (CUCKOO-FIS) integrates two optimization techniques-Cuckoo optimization, a meta-heuristic search algorithm and Fuzzy Inference System, a mathematical system based on fuzzy logic. The collaborated technique is applied to software cost estimation model for effort optimization and is successfully evaluated on the tera-PROMISE datasets. Many model based methods have been proposed earlier but this estimation using Cuckoo algorithm and Fuzzy sets which runs on non-algorithmic methods have showed results with improved accuracy in cost estimation. © 2017, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.",Cuckoo optimization algorithm (COA); CUCKOO-FIS; Fuzzy inference system (FIS); Fuzzy logic; Software cost optimization (SCO),"Kaushik A., Verma S., Singh H.J., Chhabra G.",2017,Journal,International Journal of Systems Assurance Engineering and Management,10.1007/s13198-017-0615-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035046905&doi=10.1007%2fs13198-017-0615-7&partnerID=40&md5=b14fc8be005e3354a0db4b881102550f,"Department of Information Technology, Maharaja Surajmal Institute of Technology, New Delhi, India",Springer India,English,09756809,
Scopus,Missing data imputation based on low-rank recovery and semi-supervised regression for software effort estimation,"Software effort estimation (SEE) is a crucial step in software development. Effort data missing usually occurs in real-world data collection. Focusing on the missing data problem, existing SEE methods employ the deletion, ignoring, or imputation strategy to address the problem, where the imputation strategy was found to be more helpful for improving the estimation performance. Current imputation methods in SEE use classical imputation techniques for missing data imputation, yet these imputation techniques have their respective disadvantages and might not be appropriate for effort data. In this paper, we aim to provide an effective solution for the effort data missing problem. Incompletion includes the drive factor missing case and effort label missing case. We introduce the low-rank recovery technique for addressing the drive factor missing case. And we employ the semi-supervised regression technique to perform imputation in the case of effort label missing. We then propose a novel effort data imputation approach, named low-rank recovery and semisupervised regression imputation (LRSRI). Experiments on 7 widely used software effort datasets indicate that: (1) the proposed approach can obtain better effort data imputation effects than other methods; (2) the imputed data using our approach can apply to multiple estimators well. © 2016 ACM.",Drive factor missing case; Effort label missing case; Low-rank recovery and semi-supervised regression imputation (LRSRI); Missing data problem; Software effort estimation,"Jing X.-Y., Qi F., Wu F., Xu B.",2016,Conference,Proceedings - International Conference on Software Engineering,10.1145/2884781.2884827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971422020&doi=10.1145%2f2884781.2884827&partnerID=40&md5=ccf322a53bbf1ec3af7eda7e5946f4f7,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; School of Automation, Nanjing University of Posts and Telecommunications, China; Department of Computer Science and Technology, Nanjing University, China",IEEE Computer Society,English,02705257,9781450339001; 9781450342056
Scopus,Application of fuzzy inference rules to early semi-automatic estimation of activity duration in software project management,"Expert judgment is widely used for activity duration estimation in software project management. While there are both advantages and disadvantages of expert judgment-based estimation, we propose the use of fuzzy inference rules for semi-automatic estimation to reduce the potential negative aspects of the expert judgment-based estimation. Fourteen fuzzy inference rules are introduced to elicit and adjust expert tacit knowledge, and expert judgment-based estimation results are complemented by fuzzy inference rules. The results from expert judgment and fuzzy inference rules are compared with the expert judgment-based approach using surveys and one-on-one interviews with project managers from different disciplines through analyses with data from past software projects. The use of fuzzy inference rules improves the estimation accuracy of the expert judgment-based approach by 39.35%. The proposed approach facilitates the experts to derive a more realistic and reliable activity duration estimation in software project management. © 2014 IEEE.",Activity duration estimation; expert judgment; fuzzy inference rules; fuzzy logic; software project management; software schedule estimation,"Tan C.H., Yap K.S., Ishibuchi H., Nojima Y., Yap H.J.",2014,Journal,IEEE Transactions on Human-Machine Systems,10.1109/THMS.2014.2320881,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907187870&doi=10.1109%2fTHMS.2014.2320881&partnerID=40&md5=9d338649961d02b4054419615d222148,"Universiti Tenaga Nasional, Selangor, 43009, Malaysia; Department of Computer Science and Intelligent Systems, Osaka Prefecture University, Sakai, 599-8531, Japan; Department of Mechanical Engineering, Faculty of Engineering Building, University of Malaya, Kuala Lumpur, 50603, Malaysia",Institute of Electrical and Electronics Engineers Inc.,English,21682291,
Scopus,Practitioner's knowledge representation: A pathway to improve software effort estimation,"The main goal of this book is to help organizations improve their effort estimates and effort estimation processes by providing a step-by-step methodology that takes them through the creation and validation of models that are based on their own knowledge and experience. Such models, once validated, can then be used to obtain predictions, carry out risk analyses, enhance their estimation processes for new projects and generally advance them as learning organizations. Emilia Mendes presents the Expert-Based Knowledge Engineering of Bayesian Networks (EKEBNs) methodology, which she has used and adapted during the course of several industry collaborations with different companies world-wide over more than 6 years. The book itself consists of two major parts: first, the methodology's foundations in knowledge management, effort estimation (with special emphasis on the intricacies of software and Web development) and Bayesian networks are detailed; then six industry case studies are presented which illustrate the practical use of EKEBNs. Domain experts from each company participated in the elicitation of the bespoke models for effort estimation and all models were built employing the widely-used Netica - tool. This part is rounded off with a chapter summarizing the experiences with the methodology and the derived models. Practitioners working on software project management, software process quality or effort estimation and risk analysis in general will find a thorough introduction into an industry-proven methodology as well as numerous experiences, tips and possible pitfalls invaluable for their daily work. © Springer-Verlag Berlin Heidelberg 2014. All rights are reserved.",,Mendes E.,2014,Book,Practitioner's Knowledge Representation: A Pathway to Improve Software Effort Estimation,10.1007/978-3-642-54157-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930591225&doi=10.1007%2f978-3-642-54157-5&partnerID=40&md5=90b5ce7e29d37b0df5a9942db2341a44,"Blekinge Institute of Technology, Karlskrona, Sweden",Springer-Verlag Berlin Heidelberg,English,,9783642541575; 3642541569; 9783642541568
Scopus,The effect of moving windows on software effort estimation: Comparative study with CART,"BACKGROUND: Several studies in software effort estimation have found that it can be effective to use a window of recent projects as training data for building an effort estimation model. The previous studies evaluated the use of a window with popular estimation models: linear regression (LR) and estimation by analogy (EbA). Many effort estimation models have been proposed, and the generality of windowing approach still remains uncertain for other effort estimation models, especially for those based on different theory. OBJECTIVE: This study investigates the effect of using a window on estimation accuracy with Classification and regression trees (CART). CART was recently found as a good performance method, and is based on a different theory from LR and EbA. METHOD: We compared the estimation accuracy of a windowing approach and growing approach with the same data set and procedure as the past studies. RESULTS: There is a difference in the estimation accuracy between using a window and not using a window. However, the effctive range of using windows on CART is narrower than that on LR. CONCLUSIONS: Windowing is also effective with CART. However, the range of effectiveness is narrower. The results contribute to the generality of the effectiveness of windowing approach. © 2014 IEEE.",CART; effort estimation; moving windows,"Amasaki S., Lokan C.",2014,Conference,"Proceedings - 2014 6th International Workshop on Empirical Software Engineering in Practice, IWESEP 2014",10.1109/IWESEP.2014.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920521550&doi=10.1109%2fIWESEP.2014.10&partnerID=40&md5=0f1c2b6ff8d29199414461ae34fd97f8,"Department of Systems Engineering, Okayama Prefectural University, Soja, Okayama, 719-1197, Japan; School of Engineering and Information Technology, UNSW Canberra, Canberra, ACT  2600, Australia",Institute of Electrical and Electronics Engineers Inc.,English,,9781479966660
Scopus,Relative estimation of software development effort: It matters with what and how you compare,"Estimating software development effort is frequently based on assessing the effort of one task relative to that of another. The author presents empirical results that show how relative estimation can result in biased assessments of similarity and overly optimistic effort estimates. Specifically, tasks tend to be assessed as more similar than they actually are. Furthermore, the similarity of two tasks depends on the direction of the comparison, and a comparison based on difference in work hours is distinct from one based on a ratio. From these results and other evidence, the author suggests ways to improve the accuracy of relative estimation. © 1984-2012 IEEE.",cost estimation; relative estimation; software psychology; story points,Jorgensen M.,2013,Journal,IEEE Software,10.1109/MS.2012.70,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879291507&doi=10.1109%2fMS.2012.70&partnerID=40&md5=6d889d6a5ad5f4c6ccc56d29cfe286b8,"Simula Research Laboratory, University of Oslo, Norway",,English,07407459,
Scopus,Estimating software testing complexity,"Context: Complexity measures provide us some information about software artifacts. A measure of the difficulty of testing a piece of code could be very useful to take control about the test phase. Objective: The aim in this paper is the definition of a new measure of the difficulty for a computer to generate test cases, we call it Branch Coverage Expectation (BCE). We also analyze the most common complexity measures and the most important features of a program. With this analysis we are trying to discover whether there exists a relationship between them and the code coverage of an automatically generated test suite. Method: The definition of this measure is based on a Markov model of the program. This model is used not only to compute the BCE, but also to provide an estimation of the number of test cases needed to reach a given coverage level in the program. In order to check our proposal, we perform a theoretical validation and we carry out an empirical validation study using 2600 test programs. Results: The results show that the previously existing measures are not so useful to estimate the difficulty of testing a program, because they are not highly correlated with the code coverage. Our proposed measure is much more correlated with the code coverage than the existing complexity measures. Conclusion: The high correlation of our measure with the code coverage suggests that the BCE measure is a very promising way of measuring the difficulty to automatically test a program. Our proposed measure is useful for predicting the behavior of an automatic test case generator. © 2013 Elsevier B.V. All rights reserved.",Branch coverage; Complexity; Evolutionary algorithms; Evolutionary testing; Search based software engineering; Testability,"Ferrer J., Chicano F., Alba E.",2013,Journal,Information and Software Technology,10.1016/j.infsof.2013.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885187023&doi=10.1016%2fj.infsof.2013.07.007&partnerID=40&md5=69082e60de330c9981ca076c343b7b18,"Departamento de Lenguajes y Ciencias de la Computación, Universidad de Málaga, E.T.S. Ingenieria Informatica, Campus de Teatinos, 29071 Málaga, Spain",Elsevier B.V.,English,09505849,
Scopus,Software effort estimation using a neural network ensemble,"Accurate software effort estimation is crucial for software consulting organizations to stay competitive in their software development costs and retain customers. Artificial Neural Network (ANN) is an effective tool to obtain accurate effort estimates. In this paper, software effort estimation models using Artificial Neural Network (ANN) ensembles and regression analysis are developed based on data collected from 163 software development projects. The main emphasis of the paper is in developing an effective experimental design to achieve superior effort estimation results. In addition, we compare the software effort estimation of ANNs and multiple regression analysis. We found two interesting results. First, variables other than size (function points) are not especially helpful in predicting software development effort. Second, a properly designed ANN ensemble significantly outperforms estimation using regression analysis and can achieve better effort estimate predictions.",Artificial neural network; Regression; Resampling technique; Software development effort estimation,"Pai D.R., McFall K.S., Subramanian G.H.",2013,Journal,Journal of Computer Information Systems,10.1080/08874417.2013.11645650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882691460&doi=10.1080%2f08874417.2013.11645650&partnerID=40&md5=789110c6ab48f41171a332036daa64a7,"Pennsylvania State University Middletown, PA 17057, United States; Southern Polytechnic State University Marietta, GA 30060, United States",International Association for Computer Information Systems,English,08874417,
Scopus,An activity-based costing model for long-term preservation and dissemination of digital research data: The case of DANS,"Financial sustainability is an important attribute of a trusted, reliable digital repository. The authors of this paper use the case study approach to develop an activity-based costing (ABC) model. This is used for estimating the costs of preserving digital research data and identifying options for improving and sustaining relevant activities. The model is designed in the environment of the Data Archiving and Networked Services (DANS) institute, a well-known trusted repository. The DANS-ABC model has been tested on empirical cost data from activities performed by 51 employees in frames of over 40 different national and international projects. Costs of resources are being assigned to cost objects through activities and cost drivers. The 'euros per dataset' unit of costs measurement is introduced to analyse the outputs of the model. Funders, managers and other decision-making stakeholders are being provided with understandable information connected to the strategic goals of the organisation. The latter is being achieved by linking the DANS-ABC model to another widely used managerial tool-the Balanced Scorecard (BSC). The DANS-ABC model supports costing of services provided by a data archive, while the combination of the DANS-ABC with a BSC identifies areas in the digital preservation process where efficiency improvements are possible. © 2012 The Author(s).",ABC; Activity-based costing; Balanced scorecard; BSC; Cost drivers; Cost estimation; Cost model; DANS; Data archiving; Digital preservation costs; Funding; Organisational structure; Performance measurement; Strategic planning; Total quality management,"Palaiologk A.S., Economides A.A., Tjalsma H.D., Sesink L.B.",2012,Journal,International Journal on Digital Libraries,10.1007/s00799-012-0092-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865777546&doi=10.1007%2fs00799-012-0092-1&partnerID=40&md5=b1040a799c5fdd52c2d07f883e3e6abb,"University of Macedonia, Amygdaleonas, Kavala 64012, Greece; Data Archiving and Networked Services (DANS), Postbus 93067, 2509 AB Den Haag, Netherlands; University of Macedonia, Egnatia 158, Thessaloniki 54006, Greece",,English,14325012,
Scopus,Software cost estimation by fuzzy analogy for ISBSG repository,"Software cost estimation is one of the most important and complex tasks in software project management. As a result, several techniques for estimating development effort have been suggested. Fuzzy Analogy is one of these techniques suggested to estimate project effort when it is described either by linguistic or numerical values. Based on reasoning by analogy and fuzzy logic, this technique uses fuzzy representation of software project attributes by using expert knowledge or clustering techniques. From this work, we evaluate the accuracy of this approach to estimate the software effort using the International Software Benchmarking Standards Group (ISBSG) repository.",,"Idri A., Amazal F.A.",2012,Conference,World Scientific Proc. Series on Computer Engineering and Information Science 7; Uncertainty Modeling in Knowledge Engineering and Decision Making - Proceedings of the 10th International FLINS Conf.,10.1142/9789814417747_0138,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892659716&doi=10.1142%2f9789814417747_0138&partnerID=40&md5=21ddfb0873281f771c8302cc23ae00e4,"Department of Software Engineering, ENSIAS, Mohamed V University, Rabat, Morocco",World Scientific Publishing Co. Pte Ltd,English,,9789814417730
Scopus,Software Development Cost and Time Forecasting Using a High Performance Artificial Neural Network Model,"Nowadays, mature software companies are more interested to have a precise estimation of software metrics such as project time, cost, quality, and risk at the early stages of software development process. The ability to precisely estimate project time and costs by project managers is one of the essential tasks in software development activities, and it named software effort estimation. The estimated effort at the early stage of project development process is uncertain, vague, and often the least accurate. It is because that very little information is available at the beginning stage of project. Therefore, a reliable and precise effort estimation model is an ongoing challenge for project managers and software engineers. This research work proposes a novel soft computing model incorporating Constructive Cost Model (COCOMO) to improve the precision of software time and cost estimation. The proposed artificial neural network model has good generalisation, adaption capability, and it can be interpreted and validated by software engineers. The experimental results show that applying the desirable features of artificial neural networks on the algorithmic estimation model improves the accuracy of time and cost estimation and estimated effort can be very close to the actual effort. © Springer-Verlag Berlin Heidelberg 2011.",Artificial Neural Networks; COCOMO Model; Soft Computing Techniques; Software Cost Estimation Models; Software Engineering; Software Project Management,"Attarzadeh I., Ow S.H.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-18129-0_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868662399&doi=10.1007%2f978-3-642-18129-0_4&partnerID=40&md5=d198ff9d622610c0a1f13f1f697bd96d,"Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, 50603 Kuala Lumpur, Malaysia",,English,18650929,9783642181283
Scopus,Quantifying the impact of different non-functional requirements and problem domains on software effort estimation,"The effort estimation techniques used in the software industry often tend to ignore the impact of Non-functional Requirements (NFR) on effort and reuse standard effort estimation models without local calibration. Moreover, the effort estimation models are calibrated using data of previous projects that may belong to problem domains different from the project which is being estimated. Our approach suggests a novel effort estimation methodology that can be used in the early stages of software development projects. Our proposed methodology initially clusters the historical data from the previous projects into different problem domains and generates domain specific effort estimation models, each incorporating the impact of NFR on effort by sets of objectively measured nominal features. We reduce the complexity of these models using a feature subset selection algorithm. In this paper, we discuss our approach in details, and we present the results of our experiments using different supervised machine learning algorithms. The results show that our approach performs well by increasing the correlation coefficient and decreasing the error rate of the generated effort estimation models and achieving more accurate effort estimates for the new projects. © 2011 IEEE.",Non-functional Requirements; Software Effort Estimation; Supervised Machine Learning,"Abdukalykov R., Hussain I., Kassab M., Ormandjieva O.",2011,Conference,"Proceedings - 2011 9th International Conference on Software Engineering Research, Management and Applications, SERA 2011",10.1109/SERA.2011.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155161853&doi=10.1109%2fSERA.2011.45&partnerID=40&md5=c583d593b4135c50e5a726309fb8fc49,"CSE Department, Concordia University, Montreal, QC, Canada",,English,,9780769544908
Scopus,A maturity model of software product quality,"The quality of a product can be assessed either directly by looking into the product itself, or indirectly through assessing the process used to develop that product. In the software engineering field, there are currently numerous capability and maturity models for assessing a set of specific software processes, but very few product maturity models for those interested in assessing the quality of software products. This paper presents a maturity model designed to directly assess the quality of a software product, i.e. the Software Product Quality Maturity Model (SPQMM). This model is based on the six-sigma view of product quality and handles - in submodels - the three views of quality specified in ISO 9126, that is, the Software Product Internal Quality Maturity Model (SPIQMM), the Software Product External Quality Maturity Model (SPEQMM), and the Software Product Quality-in-Use Maturity Model (SPQiUMM). Copyright © 2011, Australian Computer Society Inc.",Integrity levels; ISO 15026; ISO 9126; Maturity models; Quality assessment and evaluation; Six-sigma; Software product quality,"Al-Qutaish R.E., Abran A.",2011,Journal,Journal of Research and Practice in Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876729157&partnerID=40&md5=289036d883b3f3ada6e1532ca8fb233f,"Department of Management Information Systems, Al-Ain University of Science and Technology, Abu Dhabi Campus, P.O. Box 112612, Abu Dhabi, United Arab Emirates; Department of Software Engineering and IT, École de Technologie Supérieure (ÉTS), University of Québec, Montréal, QC H3C 1K3, Canada",,English,1443458X,
Scopus,"The longitudinal, chronological case study research strategy: A definition, and an example from IBM Hursley Park","Context: There is surprisingly little empirical software engineering research (ESER) that has analysed and reported the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. The ESER community also increasingly recognises the need to develop theories of software engineering phenomena e.g. theories of the actual behaviour of software projects at the level of the project and over time. Objective: To examine the use of the longitudinal, chronological case study (LCCS) as a research strategy for investigating the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. Method: Review the methodological literature on longitudinal case study. Define the LCCS and demonstrate the development and application of the LCCS research strategy to the investigation of Project C, a software development project at IBM Hursley Park. Use the study to consider prospects for LCCSs, and to make progress on a theory of software project behaviour. Results: LCCSs appear to provide insights that are hard to achieve using existing research strategies, such as the survey study. The LCCS strategy has basic requirements that data is time-indexed, relatively fine-grained and collected contemporaneous to the events to which the data refer. Preliminary progress is made on a theory of software project behaviour. Conclusion: LCCS appears well suited to analysing and reporting rich, fine-grained behaviour of phenomena over time. © 2011 Elsevier B.V. All rights reserved.",Chronology; Deadline effect; Longitudinal case study; Qualitative data; Software project; Theory development,Rainer A.,2011,Journal,Information and Software Technology,10.1016/j.infsof.2011.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955069467&doi=10.1016%2fj.infsof.2011.01.003&partnerID=40&md5=99ab5975989c4af833ad6f0193d973dc,"School of Computer Science, University of Hertfordshire, College Lane Campus, Hatfield, Hertfordshire AL10 9AB, United Kingdom",,English,09505849,
Scopus,AI-based models for software effort estimation,"Decision making under uncertainty is a critical problem in the field of software engineering. Predicting the software quality or the cost/ effort requires high level expertise. AI based predictor models, on the other hand, are useful decision making tools that learn from past projects' data. In this study, we have built an effort estimation model for a multinational bank to predict the effort prior to projects' development lifecycle. We have collected process, product and resource metrics from past projects together with the effort values distributed among software life cycle phases, i.e. analysis & test, design & development. We have used Clustering approach to form consistent project groups and Support Vector Regression (SVR) to predict the effort. Our results validate the benefits of using AI methods in real life problems. We attain Pred(25) values as high as 78% in predicting future projects. © 2010 IEEE.",,"Kocaguneli E., Tosun A., Bener A.",2010,Conference,"Proceedings - 36th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2010",10.1109/SEAA.2010.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449278878&doi=10.1109%2fSEAA.2010.19&partnerID=40&md5=2f0bfa944e4553cf7dd01e96547e506c,"Department of Computer Engineering, Bogazici University, Istanbul, Turkey",,English,,9780769541709
Scopus,An investigation of using neuro-fuzzy with software size estimation,"Neuro-Fuzzy refers to a hybrid intelligent system using both neural network and fuzzy logic. In this study, neuro-fuzzy is applied to a backfiring and categorical data size estimation model. Evaluation was conducted to determine whether a neuro-fuzzy approach improves software size estimations. It was found that a neuro-fuzzy approach provides minimal improvement over the traditional backfiring sizing technique. © 2009 IEEE.",,"Wong J., Ho D., Capretz L.F.",2009,Conference,Proceedings - International Conference on Software Engineering,10.1109/WOSQ.2009.5071557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049469326&doi=10.1109%2fWOSQ.2009.5071557&partnerID=40&md5=1f2aa4e449c0028f7b4f6ec6bc28ed24,"Quotemedia Ltd., Burnaby, BC V5C1B8, Canada; NFA Estimation Inc., Oshawa, ON L1G3S3, Canada; University of Western Ontario, Dept. Electrical and Comp. Eng., London, ON N6A5B9, Canada",,English,02705257,9781424437238
Scopus,Predicting maintainability of component-based systems by using fuzzy logic,"Software maintenance is a very broad activity in software development that includes error corrections, enhancement of capabilities, optimization, and deletion of obsolete capabilities and so on. Maintenance includes all changes to the product, once the client has agreed that it satisfied the specified document. Maintenance in case of component-based systems requires several different activities than in other legacy applications. Also, to measure maintainability for component-based systems as a single variable is still unexplored. Present paper discusses several maintainability related issues and proposes a fuzzy logic based approach to estimate the maintainability for component-based systems. It also validates the proposed approach by using Analytical Hierarchy Process by considering two class room based case studies. © 2009 Springer Berlin Heidelberg.",Component-based Systems; Fuzzy Logic; Interaction Complexity; Maintainability; Reusability,"Sharma A., Grover P.S., Kumar R.",2009,Conference,Communications in Computer and Information Science,10.1007/978-3-642-03547-0_55,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349928963&doi=10.1007%2f978-3-642-03547-0_55&partnerID=40&md5=fe05709fc16f1d0d734cc54a4aede9c7,"Amity Institute of Information Technology, Amity University, Noida, India; Guru Tegh Bahadur Institute of Technology, Guru Gobind Singh Indra Prastha University, Delhi, India; School of Mathematics and Computer Applications, Thapar University, Patiala, India",,English,18650929,9783642035463
Scopus,Improving the accuracy of effort estimation through fuzzy set representation of size,"Problem statement: The precision and reliability of the effort estimation is very important for the competitiveness of software companies. The uncertainty at the input level of the Constructive Cost Model (COCOMO) yields uncertainty at the output, which leads to gross estimation error in the effort estimation. Fuzzy logic-based cost estimation models are more appropriate when vague and imprecise information was to be accounted for and was used in this research to improve the effort estimation accuracy. This study proposed to extend the COCOMO by incorporating the concept of fuzziness into the measurements of size. The main objective of this research was to investigate the role of size in improving the effort estimation accuracy by characterizing the size of the project using trapezoidal function which gave superior transition from one interval to another. Approach: The methodology adopted in this study was use of fuzzy sets rather than classical intervals in the COCOMO. Using fuzzy sets, size of a software project can be specified by distribution of its possible values and these fuzzy sets were represented by membership functions. Though, Triangular membership functions (TAMF) was used in the literature to represent the size, but it was not appropriate to clear the vagueness in the project size. Therefore, to get a smoother transition in the membership function, the size of the project, its associated linguistic values were represented by trapezoidal shaped MF and rules. Results: After analyzing the results attained by means of applying COCOMO, triangular and trapezoidal MF models to the COCOMO dataset, it had been found that proposed model was performing better than ordinal COCOMO and trapezoidal function was performing better than triangular function, as it demonstrated a smoother transition in its intervals and the achieved results were closer to the actual effort. The relative error for COCOMO using trapezoidal function is lower than that of the error obtained using TAMF. Conclusion: From the experimental results, it was concluded that, by fuzzifying the project size using TPMF, the accuracy of effort estimation can be improved and the estimated effort can be very close to the actual effort. © 2009 Science Publications.",Constructive cost model; Fuzzy based effort estimation; Software cost estimation; Software effort estimation and project management; Trapezoidal membership function,"Reddy Ch.S., Raju K.V.S.V.N.",2009,Journal,Journal of Computer Science,10.3844/jcssp.2009.451.455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67749144184&doi=10.3844%2fjcssp.2009.451.455&partnerID=40&md5=29c452b1b8fae43ae11c012c95221de2,"Department of Computer Science and Systems Engineering, College of Engineering, Andhra University, Visakhapatnam, India",,English,15493636,
Scopus,Web cost estimation and productivity benchmarking,"Web cost estimation models and productivity analysis reports help project managers allocate resources more adequately, control costs, schedule and improve current practices, leading to projects that are finished on time and within budget. Therefore this chapter has two main objectives. The first is to introduce the concepts related to Web cost estimation & Web applications' sizing and present a case study where a real Web cost model is built; the second is to introduce the concepts of productivity measurement & benchmarking, and to also present a case study on productivity benchmarking. © Springer-Verlag Berlin Heidelberg 2009.",Bayesian networks; Case-based reasoning; Classification and regression trees; Effort accuracy; Multivariate Regression; Web cost estimation; Web productivity benchmarking; Web productivity measurement,Mendes E.,2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-95888-8_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249144449&doi=10.1007%2f978-3-540-95888-8_8&partnerID=40&md5=7c0d8958a3cf3f17d01a68ef171c7150,"University of Auckland, Private Bag 92019, Auckland, 0064 9, New Zealand, New Zealand",,English,03029743,
Scopus,Estimation of test code changes using historical release data,"In order to remain effective, test suites have to co-evolve alongside the production system. As such, quantifying the amount of changes in test code should be a part of effort estimation models for maintenance activities. In this paper, we verify to which extent (i) production code size, (ii) coverage measurements; and (iii) testability metrics predict the size of test code changes between two releases. For three Java and one C++ system, the size of production code changes appears to be the best predictor. We subsequently use this predictor to construct, calibrate and validate an estimation model using the historical release data. We demonstrate that is feasible to obtain a reliable prediction model, provided that at least 5 to 10 releases are available. © 2008 IEEE.",,"Van Rompaey B., Demeyer S.",2008,Conference,"Proceedings - Working Conference on Reverse Engineering, WCRE",10.1109/WCRE.2008.29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749196158&doi=10.1109%2fWCRE.2008.29&partnerID=40&md5=4450c32afd9f96e5fce35281cc9dbd18,"Lab. on Re-Engineering, University of Antwerp",,English,10951350,0769534295; 9780769534299
Scopus,A statistical framework for analyzing the duration of software projects,"The duration of a software project is a very important feature, closely related to its cost. Various methods and models have been proposed in order to predict not only the cost of a software project but also its duration. Since duration is essentially the random length of a time interval from a starting to a terminating event, in this paper we present a framework of statistical tools, appropriate for studying and modeling the distribution of the duration. The idea for our approach comes from the parallelism of duration to the life of an entity which is frequently studied in biostatistics by a certain statistical methodology known as survival analysis. This type of analysis offers great flexibility in modeling the duration and in computing various statistics useful for inference and estimation. As in any other statistical methodology, the approach is based on datasets of measurements on projects. However, one of the most important advantages is that we can use in our data information not only from completed projects, but also from ongoing projects. In this paper we present the general principles of the methodology for a comprehensive duration analysis and we also illustrate it with applications to known data sets. The analysis showed that duration is affected by various factors such as customer participation, use of tools, software logical complexity, user requirements volatility and staff tool skills. © 2007 Springer Science+Business Media, LLC.",Duration of a software project; Project cancellation; Software cost estimation; Survival analysis,"Sentas P., Angelis L., Stamelos I.",2008,Journal,Empirical Software Engineering,10.1007/s10664-007-9051-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43349102869&doi=10.1007%2fs10664-007-9051-7&partnerID=40&md5=32611d049cac33fad204f9b91368b19a,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,13823256,
Scopus,Software functional size: For cost estimation and more,"Determining software characteristics that will effectively support project planning, execution, monitoring and closure remains to be one of the prevalent challenges software project managers face. Functional size measures were introduced to quantify one of the primary characteristics of software. Although functional size measurement methods have not been without criticisms, they have significant promises for software project management. In this paper, we explore the contributions of functional size measurement to project management. We identified diverse uses of functional size by performing a literature survey and investigating how functional size measurement can be incorporated into project management practices by mapping the uses of functional size to the knowledge areas defined in project management body of knowledge (PMBOK). © Springer-Verlag Berlin Heidelberg 2008.",Functional size measurement; Software project management,"Ozkan B., Turetken O., Demirors O.",2008,Conference,Communications in Computer and Information Science,10.1007/978-3-540-85936-9_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898582696&doi=10.1007%2f978-3-540-85936-9_6&partnerID=40&md5=ec25bdf817c2355c0b63e75c3cef4a81,"Informatics Institute, Middle East Technical University, Ankara, 06531, Turkey",Springer Verlag,English,18650929,9783540859345
Scopus,Evaluation of preliminary data analysis framework in software cost estimation based on ISBSG R9 Data,"Previous research has argued that preliminary data analysis is necessary for software cost estimation. In this paper, a framework for such analysis is applied to a substantial corpus of historical project data (ISBSG R9 data), selected without explicit bias. The consequent analysis yields sets of dominant variables, which are then used to construct project effort estimation models. Performance of the predictors on the raw variables and the extracted sets of variables is then measured in terms of Mean Magnitude of Relative Error (MMRE), Median of Magnitude of Relative Error (MdMRE) and prediction at levels 0.05, 0.1, and 0.25. The results from the comparative evaluation suggest that more accurate prediction models can be constructed for the selected prediction techniques. The framework processed predictor variables are statistically significant, at the 95% confidence level for both parametric techniques and one non-parametric technique. The results are also compared with the latest published results obtained by other research based on the same data set. The comparison indicates that, the models constructed using framework processed data are generally more accurate. © 2008 Springer Science+Business Media, LLC.",ISBSG Data R9; Software cost estimation; Software engineering data analysis,"Liu Q., Qin W.Z., Mintram R., Ross M.",2008,Conference,Software Quality Journal,10.1007/s11219-007-9041-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48449083108&doi=10.1007%2fs11219-007-9041-4&partnerID=40&md5=9090bc550c13e3ba5ad503723375d9ca,"School of Software Engineering, Tongji University at Shanghai, Shanghai, China; School of Design, Engineering and Computing, Bournemouth University, Bournemouth, United Kingdom; Faculty of Technology, Southampton Solent University, Southampton, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Company-wide implementation of metrics for early software fault detection,"To shorten time-to-market and improve customer satisfaction, software development companies commonly want to use metrics for assessing and improving the performance of their development projects. This paper describes a measurement concept for assessing how good an organization is at finding faults when most cost-effective, i.e. in most cases early. The paper provides results and lessons learned from applying the measurement concept widely at a large software development company. A major finding was that on average, 64 percent of all faults found would have been more cost effective to find during unit tests. An in-depth study of a few projects at a development unit also demonstrated how to use the measurement concept for identifying which parts in the fault detection process that needs to be improved to become more efficient (e.g. reduce the amount of time spent on rework). © 2007 IEEE.",,"Damm L.-O., Lundberg L.",2007,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2007.25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548727377&doi=10.1109%2fICSE.2007.25&partnerID=40&md5=5170edc17711aea81b478732f1023ca8,"School of Engineering, Blekinge Institute of Technology, Box 520, SE-372 25 Ronneby, Sweden; Ericsson",,English,02705257,0769528287; 9780769528281
Scopus,Outlier elimination in construction of software metric models,"Software metric models are models relating various software metrics of software projects. Such models' purpose is to predict some of these metrics for certain future projects given the other metrics for those projects. The construction of software metric models derives such relationships and is usually based on data samples of concerned software metrics for past software projects. Often, in such a data sample, there are inevitably a few very extreme projects which have relationships among their metrics deviating substantially from those among the metrics for the remaining ""mainstream"" bulk of projects in the data sample. Such ""outlier"" projects exert considerable undue influence on the derivation of the said relationships during model construction in that the relationships so derived cannot candidly reflect the true ""mainstream"" relationships. The direct consequence is degraded prediction accuracy of the constructed models for future projects. To overcome this problem, we proposed a methodology to identify and thus eliminate such outliers prior to model construction. Our methodology makes use of the least of median squares (LMS) regression to uncover such outliers and is applicable irrespective of any subsequent model construction approaches. We also did a case study to apply our methodology, and the results prove our methodology being able to improve the prediction accuracy of most models experimented with in the study. Thus, our methodology is recommended for any further software metric model construction. This paper documents such a methodology and the successful case study. Copyright 2007 ACM.",Least of Median Squares (LMS); Models; Outliers; Software metrics,"Chan V.K.Y., Wong W.E.",2007,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/1244002.1244319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248862569&doi=10.1145%2f1244002.1244319&partnerID=40&md5=3fbe239268db72c6fc3ba125cf615dc2,"Macao Polytechnic Institute, School of Business, Rua de Luis Gonzaga Gomes, Macau; University of Texas at Dallas, Department of Computer Science, PO Box 830688, Richardson, TX 75083-0688, United States",Association for Computing Machinery,English,,1595934804; 9781595934802
Scopus,Assessing COTS integration risk using cost estimation inputs,"Most risk analysis tools and techniques require the user to enter a good deal of information before they can provide useful diagnoses. In this paper, we describe an approach to enable the user to obtain a COTS glue code integration risk analysis with no inputs other than the set of glue code cost drivers the user submits to get a glue code integration effort estimate with the Constructive COTS integration cost estimation (COCOTS) tool. The risk assessment approach is built on a knowledge base with 24 risk identification rules and a 3-level risk probability weighting scheme obtained from an expert Delphi analysis. Each risk rule is defined as one critical combination of two COCOTS cost drivers that may cause certain undesired outcome if they are both rated at their worst case ratings. The 3-level nonlinear risk weighting scheme represents the relative probability of risk occurring with respect to the individual cost driver ratings from the input. Further, to determine the relative risk impact, we use the productivity range of each cost driver in the risky combination to reflect the cost consequence of risk occurring. We also develop a prototype called COCOTS Risk Analyzer to automate our risk assessment method. The evaluation of our approach shows that it has done an effective job of estimating the relative risk levels of both small USC e-services and large industry COTS-based applications. Copyright 2006 ACM.",COCOTS; Cost driver; Risk assessment,"Yang Y., Boehm B., Clark B.",2006,Conference,Proceedings - International Conference on Software Engineering,10.1145/1134285.1134345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247148150&doi=10.1145%2f1134285.1134345&partnerID=40&md5=76cf2a167ab186360fd5e82a3441c025,"University of Southern California, 941 w. 37th Place, Los Angeles, CA 90089-0781, United States; Software Metrics, Inc., 4345 High Ridge Rd., Haymarket, VA 20169, United States",IEEE Computer Society,English,02705257,1595933751; 9781595933751
Scopus,Estimating LOC for information systems from their conceptual data models,"Effort and cost estimation is crucial in software management. Estimation of software size plays a key role in the estimation. Line of Code (LOG) is still a commonly used software size measure. Despite the fact that software sizing is well recognized as an important problem for more than two decades, there is still much problem in existing methods. Conceptual data model is widely used in the requirements analysis for information systems. It is also not difficult to construct conceptual data models in the early stage of developing information systems. Much characteristic of an information system is actually reflected from its conceptual data model. We explore into the use of conceptual data model for estimating LOC. This paper proposes a novel method for estimating LOG for an information system from its conceptual data model through the use of multiple linear regression model. We have validated the method through collecting samples from both the industry and open-source systems. Copyright 2006 ACM.",Conceptual data model; Line of code (LOC); Multiple linear regression model; Software sizing,"Tan H.B.K., Zhao Y., Zhang H.",2006,Conference,Proceedings - International Conference on Software Engineering,10.1145/1134285.1134331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247118850&doi=10.1145%2f1134285.1134331&partnerID=40&md5=5ed01b0b29538b9dd8bd99e38dd0a2aa,"School of Electrical and Electronic Engineering, Nanyang Technological University, Block S2, Nanyang Avenue, Singapore 639798, Singapore; School of Computer Science and Information Technology, RMIT University, Melbourne, Vic. 3001, Australia",IEEE Computer Society,English,02705257,1595933751; 9781595933751
Scopus,Enhancing input value selection in parametric software cost estimation models through second level cost drivers,"Parametric cost estimation models are widely used effort prediction tools for software development projects. These models are based on mathematical models that use as inputs specific values for relevant cost drivers. The selection of these inputs is, in many cases, driven by public prescriptive rules that determine the selection of the values. Nonetheless, such selection may in some cases be restrictive and somewhat contradictory with empirical evidence, in other cases the selection procedure is somewhat subject to ambiguity. This paper presents an approach to improve the quality of the selection of adequate cost driver values in parametric models through a process of adjustment to bodies of empirical evidence. The approach has two essential elements. Firstly, it proceeds by analyzing the diverse factors potentially affecting the values a cost driver input might adopt for a given project. And secondly, an aggregation mechanism device for the selection of input variables based on existing data is explicitly devised. This paper describes the rationale for the overall approach and provides evidence of its appropriateness through a concrete empirical study that analyses the COCOMO II DOCU cost driver. © Springer Science + Business Media, LLC 2006.",COCOMO II; Cost drivers; Empirical adjustment; Parametric estimation models; Software projects,"Cuadrado-Gallego J.J., Fernández-Sanz L., Sicilia M.-Á.",2006,Journal,Software Quality Journal,10.1007/s11219-006-0039-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749670154&doi=10.1007%2fs11219-006-0039-0&partnerID=40&md5=0e8bdd932c7eb4ada39ce412ecafa224,"E.U. Politécnica, Campus Universitario, Carretera Madrid-Barcelona, Km. 33, 600, Alcalá de Henares (Madrid), Spain; Dep. Sistemas Informaticos, Universidad Europea de Madrid, C/Tajo S/n, 28670 Villaviciosa de Odon, Madrid, Spain",Kluwer Academic Publishers,English,09639314,
Scopus,A model for detecting cost-prone classes based on mahalanobis-taguchi method,"In software development, comprehensive software reviews and testings are important activities to preserve high quality and to control maintenance cost. However it would be actually difficult to perform comprehensive software reviews and testings because of a lot of components, a lack of manpower and other realistic restrictions. To improve performances of reviews and testings in object-oriented software, this paper proposes a novel model for detecting cost-prone classes; the model is based on Mahalanobis-Taguchi method - an extended statistical discriminant method merging with a pattern recognition approach. Experimental results using a lot of Java software are provided to statistically demonstrate that the proposed model has a high ability for detecting cost-prone classes. Copyright © 2006 The Institute of Electronics, Information and Communication Engineers.",Cost-proneness; Discriminant analysis; Mahalanobis- Tagu chi method; Metrics; Prediction,"Aman H., Mochiduki N., Yamada H.",2006,Conference,IEICE Transactions on Information and Systems,10.1093/ietisy/e89-d.4.1347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646230542&doi=10.1093%2fietisy%2fe89-d.4.1347&partnerID=40&md5=6e2d0e3dd0bc4a301fb5b1205036f374,"Faculty of Engineering, Ehime University, Matsuyama-shi, 790-8577, Japan; FUJITSU Broad Solution and Consulting, Inc., Hong Kong; Ehime University, Japan; Faculty of Engineering, Ehime University, Japan; Information Processing Society of Japan (IPSJ), Japan; Japan Society for Software Science and Technology (JSSST), Japan; Institute of Electrical and Electronics Engineering (IEEE), Japan; Japan Society for Fuzzy Theory and Intelligent Informatics (SOFT), Japan; Association for Computing Machinery (ACM); Japanese Society for Artificial Intelligence (JSAI), Japan","Institute of Electronics, Information and Communication, Engineers, IEICE",English,09168532,
Scopus,The clients' impact on effort estimation accuracy in software development projects,"This paper focuses on the clients' impact on estimation accuracy in software development projects. Client related factors contributing to effort overruns as well as factors preventing overruns are investigated. Based on a literature review and a survey of 300 software professionals we find that: 1) Software professionals perceive that clients impact estimation accuracy. Changed and new requirements are perceived as the clients' most frequent contribution to overruns, while overruns are prevented by the availability of competent clients and capable decision makers. 2) Survey results should not be used in estimation accuracy improvement initiatives without further analysis. Surveys typically identify directly observable and project specific causes for overruns, while substantial improvement is only possible when the underlying causes are understood. © 2005 IEEE.",,"Grimstad S., Jørgensen M., Moløkken-Østvold K.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749051193&doi=10.1109%2fMETRICS.2005.47&partnerID=40&md5=fd38abe48570e13bf1f4f593664bf75d,"Simula Research Laboratory, Norway",,English,15301435,0769523714; 9780769523712
Scopus,Developing an Activity-based Costing Approach for System Development and Implementation,"This paper proposes the use of the Activity Based Costing (ABC) approach to software estimation. Like other more traditional approaches to software estimation, ABC provides man-day estimates. In addition, it also provides detailed costing information that is useful for management control and decision making. The paper shows how the ABC approach can be applied to software estimation by building an ABC model using data from twenty-two projects in a financial services firm. The model is then used for estimation, and comparisons between estimated and actual man-day are computed (variance analysis). The data generated by the model and from the variance analysis are useful for management control and decision making in areas such as resource allocation, outsourcing of specific development activities, and learning from adoption of new development tools and practices. © 2003, Authors. All rights reserved.",Activity-based Costing; Effort Estimation; Is Project Planning; Organizational Learning; Software Process Measurement; Time and Cost Estimation,"Ooi G., Soh C.",2003,Journal,Data Base for Advances in Information Systems,10.1145/937742.937748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751160369&doi=10.1145%2f937742.937748&partnerID=40&md5=60bf63042750dee3efd97359ba3e3ddc,"Nanyang Technological University, Singapore",,English,00950033,
Scopus,On the value of parameter tuning in heterogeneous ensembles effort estimation,"Accurate estimation of software development effort estimation (SDEE) is fundamental for efficient management of software development projects as it assists software managers to efficiently manage their human resources. Over the last four decades, while software engineering researchers have used several effort estimation techniques, including those based on statistical and machine learning methods, no consensus has been reached on the technique that can perform best in all circumstances. To tackle this challenge, Ensemble Effort Estimation, which predicts software development effort by combining more than one solo estimation technique, has recently been investigated. In this paper, heterogeneous ensembles based on four well-known machine learning techniques (K-nearest neighbor, support vector regression, multilayer perceptron and decision trees) were developed and evaluated by investigating the impact of parameter values of the ensemble members on estimation accuracy. In particular, this paper evaluates whether setting ensemble parameters using two optimization techniques (e.g., grid search optimization and particle swarm) permits more accurate estimates of SDEE. The heterogeneous ensembles of this study were built using three combination rules (mean, median and inverse ranked weighted mean) over seven datasets. The results obtained suggest that: (1) Optimized single techniques using grid search or particle swarm optimization provide more accurate estimation; (2) in general ensembles achieve higher accuracy than their single techniques whatever the optimization technique used, even though ensembles do not dominate over all single techniques; (3) heterogeneous ensembles based on optimized single techniques provide more accurate estimation; and (4) generally, particle swarm optimization and grid search techniques generate ensembles with the same predictive capability. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature.",Heterogeneous ensembles; Machine learning; Optimization techniques; Software effort estimation; Swarm optimization,"Hosni M., Idri A., Abran A., Nassif A.B.",2018,Journal,Soft Computing,10.1007/s00500-017-2945-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035807309&doi=10.1007%2fs00500-017-2945-4&partnerID=40&md5=83d094f56b80a21295cf4230621f445d,"Software Project Management Research Team ENSIAS, Mohammed V University, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada; Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates",Springer Verlag,English,14327643,
Scopus,Effort estimation in large-scale software development: An industrial case study,"Context: Software projects frequently incur schedule and budget overruns. Planning and estimation are particularly challenging in large and globally distributed agile projects. While software engineering researchers have been investigating effort estimation for many years to help practitioners to improve their estimation processes, there is little empirical research about effort estimation in large-scale distributed projects involving agile teams. Objective: The objective of this paper is three-fold: i) To identify how effort estimation is carried out in large-scale distributed agile projects; ii) to analyze the accuracy of the effort estimation processes in large-scale distributed agile projects; and iii) to identify and investigate the factors that impact the accuracy of effort estimates in large-scale distributed agile projects. Method: We performed an exploratory longitudinal case study. The data collection was operationalized through archival research and semi-structured interviews. Results: The main findings of the studied case are: 1) A two-stage estimation process, with re-estimation at the analysis stage, improves the accuracy of the effort estimates; 2) underestimation is the dominant trend; 3) less mature teams incur larger effort overruns; 4) requirements with larger size/scope incur larger effort overruns; 5) requirements developed in multi-site settings incur larger effort overruns as compared to requirements developed in a co-located setting; 6) requirements priorities impact the accuracy of the effort estimates. Conclusion: A two-stage effort estimation process can improve effort estimation accuracy and seems to address some of the challenges in large-scale agile software development. To improve effort estimates one needs to consider team maturity, distribution as well as requirements size and priorities. © 2018 Elsevier B.V.",Effort estimation; Global and agile software development; Large-scale software development,"Usman M., Britto R., Damm L.-O., Börstler J.",2018,Journal,Information and Software Technology,10.1016/j.infsof.2018.02.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044737087&doi=10.1016%2fj.infsof.2018.02.009&partnerID=40&md5=a98806624bdaeb9ecd42253fab393b96,"Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, 371 79, Sweden; Ericsson, Sweden",Elsevier B.V.,English,09505849,
Scopus,Combined classifier for cross-project defect prediction: an extended empirical study,"To facilitate developers in effective allocation of their testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on the past history of classes, methods, or certain other code elements. These techniques are effective provided that a sufficient amount of data is available to train a prediction model. However, sufficient training data are rarely available for new software projects. To resolve this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, was proposed and is regarded as a new challenge in the area of defect prediction. Thus far, only a few cross-project defect prediction techniques have been proposed. To advance the state of the art, in this study, we investigated seven composite algorithms that integrate multiple machine learning classifiers to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we performed experiments on 10 open-source software systems from the PROMISE repository, which contain a total of 5,305 instances labeled as defective or clean. We compared the composite algorithms with the combined defect predictor where logistic regression is used as the meta classification algorithm (CODEPLogistic), which is the most recent cross-project defect prediction algorithm in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experimental results show that several algorithms outperform CODEPLogistic: Maximum voting shows the best performance in terms of F-measure and its average F-measure is superior to that of CODEPLogistic by 36.88%. Bootstrap aggregation (BaggingJ48) shows the best performance in terms of cost effectiveness and its average cost effectiveness is superior to that of CODEPLogistic by 15.34%. © 2018, Higher Education Press and Springer-Verlag GmbH Germany, part of Springer Nature.",classifier combination; cross-project; defect prediction,"Zhang Y., Lo D., Xia X., Sun J.",2018,Journal,Frontiers of Computer Science,10.1007/s11704-017-6015-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042098320&doi=10.1007%2fs11704-017-6015-y&partnerID=40&md5=6db2e4894b65f5816d5b241a5d3ec8b0,"College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; School of Information Systems, Singapore Management University, Singapore, 641674, Singapore",Higher Education Press,English,20952228,
Scopus,Duplex output software effort estimation model with self-guided interpretation,"Context Software effort estimation (SEE) plays a key role in predicting the effort needed to complete software development task. However, the conclusion instability across learners has affected the implementation of SEE models. This instability can be attributed to the lack of an effort classification benchmark that software researchers and practitioners can use to facilitate and interpret prediction results. Objective To ameliorate the conclusion instability challenge by introducing a classification and self-guided interpretation scheme for SEE. Method We first used the density quantile function to discretise the effort recorded in 14 datasets into three classes (high, low and moderate) and built regression models for these datasets. The results of the regression models were an effort estimate, termed output 1, which was then classified into an effort class, termed output 2. We refer to the models generated in this study as duplex output models as they return two outputs. The introduced duplex output models trained with the leave-one-out cross validation and evaluated with MAE, BMMRE and adjusted R2, can be used to predict both the software effort and the class of software effort estimate. Robust statistical tests (Welch's t-test and Kruskal-Wallis H-test) were used to examine the statistical significant differences in the models’ prediction performances. Results We observed the following: (1) the duplex output models not only predicted the effort estimates, they also offered a guide to interpreting the effort expended; (2) incorporating the genetic search algorithm into the duplex output model allowed the sampling of relevant features for improved prediction accuracy; and (3) ElasticNet, a hybrid regression, provided superior prediction accuracy over the ATLM, the state-of-the-art baseline regression. Conclusion The results show that the duplex output model provides a self-guided benchmark for interpreting estimated software effort. ElasticNet can also serve as a baseline model for SEE. © 2017 Elsevier B.V.",Duplex output; Effort classification; Effort estimation; Multiple regression models,"Mensah S., Keung J., Bosu M.F., Bennin K.E.",2018,Journal,Information and Software Technology,10.1016/j.infsof.2017.09.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030449177&doi=10.1016%2fj.infsof.2017.09.010&partnerID=40&md5=7143271386c886fe1486c3472eac8b50,"Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Centre for Business, Information Technology and Enterprise, Wintec, Hamilton, New Zealand",Elsevier B.V.,English,09505849,
Scopus,Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study,"Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation. © 2017 Elsevier Inc.",Cross-validation; Empirical software engineering estimation; Imputation; KNN; Missing data,"Huang J., Keung J.W., Sarro F., Li Y.-F., Yu Y.T., Chan W.K., Sun H.",2017,Journal,Journal of Systems and Software,10.1016/j.jss.2017.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024833152&doi=10.1016%2fj.jss.2017.07.012&partnerID=40&md5=69d3f010b51fccd7e2b9e5719f182806,"Department of Computer Science, City University of Hong KongHong Kong, Hong Kong; Department of Computer Science, University College London, London, United Kingdom; Department of Industrial Engineering, Tsinghua UniversityBeijing, China; Department of Systems Engineering and Engineering Management, City University of Hong KongHong Kong, Hong Kong",Elsevier Inc.,English,01641212,
Scopus,The effects of perceived value and stakeholder satisfaction on software project impact,"Context In this paper we present a multiple case study on the insights of software organizations into stakeholder satisfaction and (perceived) value of their software projects. Our study is based on the notion that quantifying and qualifying project size, cost, duration, defects, and estimation accuracy needs to be done in relation with stakeholder satisfaction and perceived value. Objectives We contrast project metrics such as cost, duration, number of defects and estimation accuracy with stakeholder satisfaction and perceived value. Method In order to find out whether our approach is practically feasible in an industrial setting, we performed two case studies; one in a Belgian telecom company and the other in a Dutch software company. Results In this study we evaluate 22 software projects that were delivered during one release in the Belgian telecom company, and 4 additional large software releases (representing an extension of 174% in project size) that were delivered in a Dutch software company. Eighty-three (83) key stakeholders of two companies provide stakeholder satisfaction and perceived value measurements in 133 completed surveys. Conclusions We conclude that a focus on shortening overall project duration, and improving communication and team collaboration on intermediate progress is likely to have a positive impact on stakeholder satisfaction and perceived value. Our study does not provide any evidence that steering on costs helped to improve these. As an answer to our research question - how do stakeholder satisfaction and perceived value relate to cost, duration, defects, size and estimation accuracy of software projects? – we found five take-away-messages. © 2017 Elsevier B.V.",Cost duration index; Evidence-based software engineering; Perceived value; Software economics; stakeholder satisfaction,"Huijgens H., van Deursen A., van Solingen R.",2017,Journal,Information and Software Technology,10.1016/j.infsof.2017.04.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019035451&doi=10.1016%2fj.infsof.2017.04.008&partnerID=40&md5=31edfd2c2bef33d473be4035eb5f1e1e,"Delft University of Technology and Goverdson, Mekelweg 4, CD Delft, 2628, Netherlands; Delft University of Technology, Mekelweg 4, CD Delft, 2628, Netherlands; Delft University of Technology and Prowareness, Mekelweg 4, CD Delft, 2628, Netherlands",Elsevier B.V.,English,09505849,
Scopus,An approach to estimation of degree of customization for ERP projects using prioritized requirements,"Customization in ERP projects is a risky, but unavoidable undertaking that companies need to initiate in order to achieve alignment between their acquired ERP solution and their organizational goals and business processes. Conscious about the risks, many companies commit to leveraging the off-the-shelf built-in functionality in their chosen ERP package, keeping customization at a minimum level so that it does not jeopardize the project or the future projects that would build upon it. However, many organizations experience that once the project team enters the stage of implementing the solution, requests for customization increase in volume and diversity. Managing properly the process of customization gets increasingly harder. This paper addresses the problem of estimating the degree of customization at an early stage of ERP implementation. This will support customization decision makers in making value and cost trade-offs when approving requests for customization. We propose a solution approach in which customization requirements are reasoned in quantitative terms. Our approach uses client-prioritized requirements for the estimation of degree of customization during the ERP implementation. A case study is used to illustrate the application of the proposed approach. We also discuss the strengths and limitations of our approach as well as its implications for research and practice. © 2016 Elsevier Inc. All rights reserved.",Case study; Customization; Decision making; Enterprise resource planning (ERP) projects; Requirements prioritization,"Parthasarathy S., Daneva M.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2016.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964203444&doi=10.1016%2fj.jss.2016.04.006&partnerID=40&md5=415df9ce3efc266a7990ee664a1a595a,"Department of Computer Applications, Thiagarajar College of Engineering, Madurai, Tamil Nadu  625 015, India; University of Twente, Enschede, Netherlands",Elsevier Inc.,English,01641212,
Scopus,Software development efforts prediction using artificial neural network,"Software project managers need an accurate assessment of software development efforts to achieve reliable software within development budget and schedule. A single layer neural network (SLP) is reported to predict software development efforts from software quality metrics. Particle swarm optimisation for training, principal component analysis (PCA) for dimension reduction of input features and genetic algorithm for optimising artificial neural network architecture are used. Literature reported datasets are tested and the results are acceptable within the limits. However, SLP-NN without pre-processing with PCA is adequate and in some cases, reduction approach may be dropped. © The Institution of Engineering and Technology 2016.",,"Bisi M., Goyal N.K.",2016,Journal,IET Software,10.1049/iet-sen.2015.0061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973370327&doi=10.1049%2fiet-sen.2015.0061&partnerID=40&md5=c6f436eb89625d4bd62b257e54041960,"Reliability Engineering Centre, Indian Institute of Technology, Kharagpur, India",Institution of Engineering and Technology,English,17518806,
Scopus,Investigating Functional and Code Size Measures for Mobile Applications,"This paper investigates the use of the COSMIC functional size measurement method for mobile applications. Some proposals have been recently introduced to size mobile applications in terms of COSMIC. In this work we empirically analyse whether the COSMIC functional size of mobile applications can be exploited to estimate the size of the final applications in terms of lines of code and number of bytes of the source code and byte code. To this end, we take into account a total of 7 different code size measures collected from 13 Android applications. The results of the empirical study show that the COSMIC functional size is strongly correlated to all the size measures taken into account and that it can be also used to predict the mobile application size in terms of bytes with a high accuracy. © 2015 IEEE.",android mobile application; code size measure; COSMIC; empirical study; functional size measurement; LOC,"Ferrucci F., Gravino C., Salza P., Sarro F.",2015,Conference,"Proceedings - 41st Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2015",10.1109/SEAA.2015.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952347028&doi=10.1109%2fSEAA.2015.23&partnerID=40&md5=d9ce0889954ac65a82f0f81c7c7d61e9,"University of Salerno, Italy; University College London, United Kingdom",Institute of Electrical and Electronics Engineers Inc.,English,,9781467375856
Scopus,Analogy-based software development effort estimation in global software development,"Context: Software development has always been characterised by certain parameters. In the case of global software development, one of the important challenges for software developers is that of predicting the development effort of a software system on the basis of developer details, size, complexity, and other measures. Objective: The main research topics related to global software development effort estimation are the definition and empirical evaluation of a search-based approach with which to build new estimation models and the definition and empirical evaluation of all available early data. Datasets have been used as a basis to carry out an analogy-based estimation using similarity functions and measures. Method: Many of the problems concerning the existing effort estimation challenges can be solved by creating an analogy. This paper describes an enhanced analogy-based model for the estimation of software development effort and proposes a new approach using similarity functions and measures for software effort estimation. Result: A new approach for analogy-based reasoning with which to enhance the performance of cost estimation in distributed or combined software projects dealing with numerical and categorical data. The proposed method will be validated empirically using The International Software Benchmarking Standards Group dataset as a basis. Conclusion: The proposed estimation model could be a useful approach for early stage effort estimation on distributed projects. © 2015 IEEE.",Analogy-based estimation; Effort estimation; Global software development,El Bajta M.,2015,Conference,"Proceedings - 2015 IEEE 10th International Conference on Global Software Engineering Workshops, ICGSEW 2015",10.1109/ICGSEW.2015.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957927049&doi=10.1109%2fICGSEW.2015.19&partnerID=40&md5=c3b4560c70a9e611b029f6fcff6a71a3,"Software Project Management Research Team, ENSIAS, Mohammed v Souissi University, Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781479998746
Scopus,Mining the impact of object oriented metrics for change prediction using Machine Learning and Search-based techniques,"Change in a software is crucial to incorporate defect correction and continuous evolution of requirements and technology. Thus, development of quality models to predict the change proneness attribute of a software is important to effectively utilize and plan the finite resources during maintenance and testing phase of a software. In the current scenario, a variety of techniques like the statistical techniques, the Machine Learning (ML) techniques and the Search-based techniques (SBT) are available to develop models to predict software quality attributes. In this work, we assess the performance of ten machine learning and search-based techniques using data collected from three open source software. We first develop a change prediction model using one data set and then we perform inter-project validation using two other data sets in order to obtain unbiased and generalized results. The results of the study indicate comparable performance of SBT with other employed statistical and ML techniques. This study also supports inter project validation as we successfully applied the model created using the training data of one project on other similar projects and yield good results. © 2015 IEEE.",Change proneness; Empirical validation; Inter project validation; Machine learning techniques; Search-based techniques; Software Quality,"Malhotra R., Khanna M.",2015,Conference,"2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015",10.1109/ICACCI.2015.7275614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946238088&doi=10.1109%2fICACCI.2015.7275614&partnerID=40&md5=9dda927af0e130235915d1bb68b4a2c0,"Delhi Technological University, Delhi, India; Indiana University, Purdue University, Indianapolis, IN, United States; Acharya Narendra Dev College, University of Delhi, Delhi, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781479987917
Scopus,A robust optimization approach to the next release problem in the presence of uncertainties,"The next release problem is a significant task in the iterative and incremental software development model, involving the selection of a set of requirements to be included in the next software release. Given the dynamic environment in which modern software development occurs, the uncertainties related to the input variables of this problem should be taken into account. In this context, this paper presents a formulation to the next release problem considering the robust optimization framework, which enables the production of robust solutions. In order to measure the ""price of robustness"", which is the loss in solution quality due to robustness, a large empirical evaluation was executed over synthetical and real-world instances. Several next release planning situations were considered, including different number of requirements, estimating skills and interdependencies between requirements. All empirical results are consistent to show that the penalization with regard to solution quality is relatively small. In addition, the proposed model's behavior is statistically the same for all considered instances, which qualifies it to be applied even in large-scale real-world software projects. © 2014 Elsevier Inc. All rights reserved.",Next release problem; Robust optimization; Search based software engineering,"Paixao M., Souza J.",2015,Conference,Journal of Systems and Software,10.1016/j.jss.2014.09.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924984987&doi=10.1016%2fj.jss.2014.09.039&partnerID=40&md5=e5f2fe9ebbc1dead93fd9cd0d562e328,"State University of Ceará, 1700 Avenida Paranjana, Fortaleza, Brazil",Elsevier Inc.,English,01641212,
Scopus,Exploiting prior-phase effort data to estimate the effort for the subsequent phases: A further assessment,"Context. Development effort estimation is a managerial activity that takes place throughout the life-cycle of the software so that it may benefit from information that becomes available as the project progresses. Researchers investigated the use of prior-phase effort data to estimate the effort in subsequent phases, as well as early phase effort data to estimate the total development effort. Objective. We assessed the usefulness of the effort spent for each phase in order to predict the effort required during the subsequent phase and until the end of the development process. We compared the use of effort data against the use of Function Points (i.e., a functional size measure widely used for effort estimation) and verified whether it is useful to combine them. Method. We performed an empirical study employing 25 applications from a single software company. The company collected effort from 3 different phases (i.e., specification and analysis, system and object design, and implementation and testing). Linear regression was used to build the estimation models. Results. Our analysis revealed that we obtained more accurate estimations by using prior-phase effort data to estimate the effort of subsequent phases. The combination of the prior-phase efforts and Function Points allowed us to improve the estimations in some cases. Conclusion. The effort spent in the prior-phase of a project is a good predictor for the effort that will be required later. In a continuous estimation process project managers can benefit from this effort data to obtain more accurate estimations for the subsequent phase(s). Copyright 2014 ACM.",Effort estimation; Function points analysis; Functional size measures; Prior-phase effort,"Ferrucci F., Gravino C., Sarro F.",2014,Conference,ACM International Conference Proceeding Series,10.1145/2639490.2639509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905640622&doi=10.1145%2f2639490.2639509&partnerID=40&md5=fca4f67b7150f48c43e9b26dca581d27,"Department of Management and Information Technology, University of Salerno, Italy; CREST Department of Computer Science, University College London, United Kingdom",Association for Computing Machinery,English,,9781450328982
Scopus,Overestimation and underestimation of software cost models: Evaluation by visualization,"Software Cost Estimation (SCE) is a process related to the well-balanced management of a software project. Despite the evolving research activity, the task of estimating accurately the budget and the delivering time has been a research problem for many decades. Nowadays, the cost of a project is still estimated with error. The study of the error produced by estimation models or techniques has been focused on the sources producing it. Usually, the various studies consider underestimations and overestimations of the actual cost to have equal importance. However this is hardly true in practice and such a consideration could be extremely risky for an organization and of course for the customers. In this study, we consider the problem of weighing differently the overestimation and underestimation and we introduce in SCE the utilization of a recently presented graphical methodology, namely the analysis by Regression Receiver Operating Curves (RROC). Our purpose is to evaluate the predictive power of alternative estimation techniques when underestimation and overestimation are not of equal importance. The graphical representation through well-established notions from classification problems provides a straightforward tool for comparing prediction methodologies in different operating conditions. Such a consideration is realistic and desirable for project managers, since underestimation and overestimation of the actual cost have not the same impact on a company. The application of the proposed visualization analysis to real data illustrates the advantages for the critical issues of estimation process. © 2013 IEEE.",cost prediction models; overestimation; Software project management; underestimation; visualization tools,"Mittas N., Angelis L.",2013,Conference,"Proceedings - 39th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2013",10.1109/SEAA.2013.24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889067678&doi=10.1109%2fSEAA.2013.24&partnerID=40&md5=110937bdb14623d95f092ce05d69af3e,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki 54124, Greece",,English,,9780769550916
Scopus,The influence of selection bias on effort overruns in software development projects,"Context: A potentially important, but neglected, reason for effort overruns in software projects is related to selection bias. Selection bias-induced effort overruns occur when proposals are more likely to be accepted and lead to actual projects when based on effort estimates that are too low rather than on realistic estimates or estimates that are too high. The effect of this bias may be particularly important in bidding rounds, but is potentially relevant in all situations where there is effort or cost-based selection between alternatives. Objective: To better understand the relevance and management of selection bias effects in software development contexts. Method: First, we present a statistical model illustrating the relation between selection bias in bidding and other contexts and effort overruns. Then, we examine this relation in an experiment with software professionals who estimated and completed a set of development tasks and examine relevant field study evidence. Finally, we use a selection bias scenario to assess awareness of the effect of selection bias among software providers. Results: The results from the statistical model and the experiment demonstrated that selection bias is capable of explaining much of the effort overruns. The field evidence was also consistent with a substantial effect of selection bias on effort overruns, although there are alternative explanations for the findings. We found a low awareness of selection bias among the software providers. Conclusion: Selection bias is likely to be an important source of effort overruns and should be addressed to reduce problems related to over-optimistic effort estimates. © 2013 Elsevier B.V. All rights reserved.",Effort estimation; Effort overrun; Selection bias; Winner's curse,Jørgensen M.,2013,Journal,Information and Software Technology,10.1016/j.infsof.2013.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893683940&doi=10.1016%2fj.infsof.2013.03.001&partnerID=40&md5=6c07b54b80a8e0ffdff86715b1da0a00,"Simula Research Laboratory, Institute of Informatics, University of Oslo, Norway",Elsevier B.V.,English,09505849,
Scopus,Software effort prediction using fuzzy clustering and functional link artificial neural networks,We use the combined fuzzy C-Means (FCM) clustering algorithm and functional link artificial neural networks (FLANN) to achieve accurate software effort prediction. FLANN is a computationally efficient nonlinear network and is capable for complex nonlinear mapping between its input and output pattern space. The nonlinearity is introduced into the FLANN by passing the input pattern through a functional expansion unit. The proposed method uses three real time datasets. The Chebyshev polynomial has been used as choice of expansion to exhaustively study the performance. The simulation results show that it not only deals efficiently with noisy data but also proves to be a champion in producing promising results. © 2012 Springer-Verlag.,FLANN; Fuzzy C-Means; K-Means; Software cost estimation,"Benala T.R., Mall R., Dehuri S., Prasanthi V.L.",2012,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-35380-2_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871584359&doi=10.1007%2f978-3-642-35380-2_16&partnerID=40&md5=9276741096dff2ff16ea46abca7cd445,"Department of Computer Science and Engineering, Anil Neerukonda Institute of Technology and Sciences, Sangivalasa-531162, Visakhapatnam, Andhra Pradesh, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of System Engineering, Ajou University, San 5, Woncheon-dong, Yeongtong-gu, Suwon 443-749, South Korea",,English,03029743,9783642353796
Scopus,A comparative study of estimation by Analogy using data mining techniques,"Software Estimations provide an inclusive set of directives for software project developers, project managers, and the management in order to produce more realistic estimates based on deficient, uncertain, and noisy data. A range of estimation models are being explored in the industry, as well as in academia, for research purposes but choosing the best model is quite intricate. Estimation by Analogy (EbA) is a form of case based reasoning, which uses fuzzy logic, grey system theory or machine-learning techniques, etc. for optimization. This research compares the estimation accuracy of some conventional data mining models with a hybrid model. Different data mining models are under consideration, including linear regression models like the ordinary least square and ridge regression, and nonlinear models like neural networks, support vector machines, and multivariate adaptive regression splines, etc. A precise and comprehensible predictive model based on the integration of GRA and regression has been introduced and compared. Empirical results have shown that regression when used with GRA gives outstanding results; indicating that the methodology has great potential and can be used as a candidate approach for software effort estimation. © 2012 KIPS.",Data mining techniques; Estimation by analogy; Grey relational analysis; Robust regression; Software estimations,"Nagpal G., Uddin M., Kaur A.",2012,Journal,Journal of Information Processing Systems,10.3745/JIPS.2012.8.4.621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874181966&doi=10.3745%2fJIPS.2012.8.4.621&partnerID=40&md5=5a4d97fb90b7eb41e3a6387e6b3f5805,"Dept. of Computer Science and Engineering, National Institute of Technology, Jalandhar, India; Delhi Technological University, Delhi, India; University School of IT, Gurugobind Singh Indraprastha University, Delhi, India",,English,1976913X,
Scopus,Exploring software project effort versus duration trade-offs,"Estimates of effort and duration for a new software project often have to be adjusted to deal with an imposed target delivery date or a constraint on staffing. Estimating methods assume an effort/duration trade-off relationship based mostly on theory or expert judgment. This paper describes a process for analyzing actual project effort and duration data which is designed to explore the trade-off relationship. I assume a reference relationship of a simple power-curve with variable power ""N"" and use this (a) as a means of comparing the trade-off relationships assumed by four well-known estimating methods, and (b) as the basis for a process to analyze actual project data. Results are presented of applying the process to 16 sub-sets of project data. These suggest, for example, that the value of ""N"" differs between new development projects and enhancement projects. The Web Extra presents more results for each step in the effort-duration trade-off process described in the main article. © 2012 IEEE.",cost estimation; performance measures; project control and modeling; schedule and organizational issues; software metrics; software project management; time estimation,Symons C.,2012,Journal,IEEE Software,10.1109/MS.2011.126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865352209&doi=10.1109%2fMS.2011.126&partnerID=40&md5=578610cfd3c35fe32be5eab6d1056055,"Common Software Measurement International Consortium, United Kingdom",,English,07407459,
Scopus,Efficient effort estimation system viz. function points and quality assurance coverage,"Software development effort estimation is important for quality management in the software development industry, yet its automation still remains a challenging issue. Accurate estimation of software effort is critical in software engineering. Existing methods for software cost estimation will use very few quality factors for the estimation. So, in order to overcome this drawback, the authors proposed an efficient effort estimation system based on quality assurance coverage. This study is a basis for the improvement of software effort estimation research through a series of quality attributes along with constructive cost model (COCOMO). The classification of software system for which the effort estimation is to be calculated based on COCOMO classes. For this quality assurance ISO 9126 quality factors are used and for the weighing factors the function point metric is used as an estimation approach. Effort is estimated for MS word 2007 using the following models: Albrecht and Gaffney model, Kemerer model, SMPEEM model (Software Maintenance Project Effort Estimation Model and FP Matson, Barnett and Mellichamp model. © 2012 The Institution of Engineering and Technology.",,"Azath H., Wahidabanu R.S.D.",2012,Journal,IET Software,10.1049/iet-sen.2011.0146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866843543&doi=10.1049%2fiet-sen.2011.0146&partnerID=40&md5=7b2a3ea9795223acf061c804e8a0c9a9,"King College of Technology, Department of Information Technology, Namakkal, Tamil Nadu, India; Government College of Engineering, Department of Electronics and Communication Engineering, Salem, Tamil Nadu, India",,English,17518806,
Scopus,A framework for user-centric model for evaluating the performance of distributed software system architecture,"In this work we carried out a review of models for evaluating the performance of distributed software system architecture (DSSA) and Information System (IS) success evaluation models with a view to establishing the utilization of organizational variables in the evaluation of DSSA performance. The findings from the review show that the existing DSSA performance evaluation models are machine-centric and existing IS success measurement models do not map organizational variables with DSSA components. In view of these, we developed a user-centric model for DSSA performance evaluation using organizational variables. Our model utilizes neuro-fuzzy logic in matching organizational/user variables with DSSA evaluation factors. © 2012 Elsevier Ltd. All rights reserved.",Distributed software system architecture; End users; Evaluation; Organizational variables; Performance; Software developers; Software systems,"Akinnuwesi B.A., Uzoka F.-M.E., Olabiyisi S.O., Omidiora E.O.",2012,Journal,Expert Systems with Applications,10.1016/j.eswa.2012.02.067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859217650&doi=10.1016%2fj.eswa.2012.02.067&partnerID=40&md5=b2c4663b001d56d4cb218c40712ee53c,"Department of Information Technology, Bells University of Technology, Ota, Ogun State, Nigeria; Department of Computer Science and Information Systems, Mount Royal University Calgary, Canada; Department of Computer Science and Engineering, Ladoke Akintola University of Technology, Ogbomoso, Oyo State, Nigeria",,English,09574174,
Scopus,Software productivity: Harmonization in ISO/IEEE software engineering standards,"The software productivity is an important key of software quality factors. The productivity measure has become a tool for managers since it is used to compare the performance between different companies (benchmarking) and to compare the efficiency of different developers in the same company. Therefore, it allows doing strategic planning and decision making based on such measurement. A variety of international standardization bodies such as IEEE and ISO as well as software engineering researchers have proposed a set of factors which influence the software productivity attribute, and also a set of measures to evaluate it. However, there is no unique model that integrates all the software productivity best practices. The aim of this paper is to survey the available international standards and research work on software productivity and figure out the key differences in order to propose a standards-based model. Such model will include the set of quality attributes that could be used to reflect the software productivity, and a set of measures that allows evaluating the software developer's productivity. © 2012 ACADEMY PUBLISHER.",IEEE Std. 1045; ISO 9126; Measurements; Productivity drivers; Quality attributes; Quality models; Software developers productivity,"Cheikhi L., Al-Qutaish R.E., Idri A.",2012,Journal,Journal of Software,10.4304/jsw.7.2.462-470,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857929017&doi=10.4304%2fjsw.7.2.462-470&partnerID=40&md5=55088a24a8af596776fe0e2ddf74892c,"ÉNSIAS, Université Mohammed V- Souissi, Rabat, Morocco; Al Ain University of Science and Technology, Abu Dhabi Campus, Abu Dhabi, United Arab Emirates",,English,1796217X,
Scopus,Rank-based refactoring decision support: Two studies,"Refactoring can result in code with improved maintainability and is considered a preventive maintenance activity. Managers of large projects need ways to decide where to apply scarce resources when performing refactoring. There is a lack of tools for supporting such decisions. We introduce a rank-based software measure-driven refactoring decision support approach to assist managers. The approach uses various static measures to develop a weighted rank, ranking classes or packages that need refactoring. We undertook two case studies to examine the effectiveness of the approach. Specifically, we wanted to see if the decision support tool yielded results similar to those of human analysts/managers and in less time so that it can be used to augment human decision making. In the first study, we found that our approach identified classes as needing refactoring that were also identified by humans. In the second study, a hierarchical approach was used to identify packages that had actually been refactored in 15 releases of the open source project Tomcat. We examined the overlap between the tool's findings and the actual refactoring activities. The tool reached 100/86.7% recall on the package/class level. Though these studies were limited in size and scope, it appears that this approach is worthy of further examination. © 2011 Springer-Verlag London Limited.",Decision support; Maintainability; Refactoring; Software engineering,"Zhao L., Hayes J.H.",2011,Journal,Innovations in Systems and Software Engineering,10.1007/s11334-011-0154-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255135548&doi=10.1007%2fs11334-011-0154-3&partnerID=40&md5=72e6479010c134fb3d7c8c89df269a36,"Department of Computer Science, University of Kentucky, Lexington, KY 40506, United States",,English,16145046,
Scopus,Using web objects for development effort estimation of web applications: A replicated study,"The spreading of Web applications has motivated the definition of size measures suitable for such kind of software systems. Among the proposals existing in the literature, Web Objects were conceived by Reifer specifically for Web applications as an extension of Function Points. In this paper we report on an empirical analysis we performed exploiting 25 Web applications developed by an Italian software company. The results confirm the ones obtained in a previous study and extend them in several aspects, showing the robustness of the measure with respect to the size and technologies of the applications, and to the employed estimation techniques. © 2011 Springer-Verlag.",Effort estimation technique; Empirical studies; Size measure; Web applications,"Di Martino S., Ferrucci F., Gravino C., Sarro F.",2011,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-21843-9_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960238138&doi=10.1007%2f978-3-642-21843-9_16&partnerID=40&md5=96564ddfba7a4b007f739051b7cd495a,"University of Napoli Federico II, 80126, Napoli, Italy; University of Salerno, Via Ponte Don Melillo, 84084, Fisciano (SA), Italy",,English,03029743,9783642218422
Scopus,A COSMIC measurement procedure for sizing Web applications developed using the OO-H method,"In the last years some model-driven development approaches have been proposed to automatically generate Web applications starting from conceptual models. Among them, the Object-Oriented Hypermedia method (OO-H) has been successfully used in the development of industrial Web applications. This method is based on two conceptual models, the UML class diagram to represent content and behavioral requirements, and the Navigation Access diagram to represent navigational requirements. The need of functional sizing this kind of Web applications motivated us to define a COSMIC measurement procedure to be applied on the OO-H conceptual models. In this paper, we present the mapping rules that have been conceived together with an example of their application. © 2010 ACM.",COSMIC; Functional size measurement; Model-driven development; OO-H; Web applications,"Abrahão S., De Marco L., Ferrucci F., Gravino C., Sarro F.",2010,Conference,"24th European Conference on Object-Oriented Programming, ECOOP 2010 Workshop Proceedings - Workshop 1: Workshop on Advances in Functional Size Measurement and Effort Estimation, FSM'10",10.1145/1921705.1921707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952415871&doi=10.1145%2f1921705.1921707&partnerID=40&md5=4ad534a8bf3ff923bdb7ad6eb040284b,"Department of Computer Science and Computation, Camino de Vera, s/n, 46022 Valencia, Spain; Dipartimento di Matematica e Informatica, University of Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy",,English,,9781450305396
Scopus,Software effort prediction using regression rule extraction from neural networks,"Neural networks are often selected as tool for software effort prediction because of their capability to approximate any continuous function with arbitrary accuracy. A major drawback of neural networks is the complex mapping between inputs and output, which is not easily understood by a user. This paper describes a rule extraction technique that derives a set of comprehensible IF-THEN rules from a trained neural network applied to the domain of software effort prediction. The suitability of this technique is tested on the ISBSG R11 data set by a comparison with linear regression, radial basis function networks, and CART. It is found that the most accurate results are obtained by CART, though the large number of rules limits comprehensibility. Considering comprehensible models only, the concise set of extracted rules outperform the pruned CART tree, making neural network rule extraction the most suitable technique for software effort prediction when comprehensibility is important. © 2010 IEEE.",Data mining; Rule extraction; Software effort prediction,"Setiono R., Dejaeger K., Verbeke W., Martens D., Baesens B.",2010,Conference,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",10.1109/ICTAI.2010.82,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751553710&doi=10.1109%2fICTAI.2010.82&partnerID=40&md5=652d898951ff974ed963970ea39d066e,"National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore; Catholic University of Leuven, Department of Decision Sciences and Information Management, Belgium; University College Ghent, University Ghent, Department of Business Administration and Public Management, Belgium",,English,10823409,9780769542638
Scopus,Software stage-effort estimation based on association rule mining and Fuzzy set theory,"Relaying on early effort estimation to predict the required number of resources is not often sufficient, and could lead to under or over estimation. It is widely acknowledge that that software development process should be refined regularly and that software prediction made at early stage of software development is yet kind of guesses. Even good predictions are not sufficient with inherent uncertainty and risks. The stage-effort estimation allows project manager to re-allocate correct number of resources, re-schedule project and control project progress to finish on time and within budget. In this paper we propose an approach to utilize prior effort records to predict stage effort. The proposed model combines concepts of Fuzzy set theory and association rule mining. The results were good in terms of prediction accuracy and have potential to deliver good stage-effort estimation. © 2010 IEEE.",Association rule mining; Fuzzy set theory; Stage effort estimation,"Azzeh M., Cowling P.I., Neagu D.",2010,Conference,"Proceedings - 10th IEEE International Conference on Computer and Information Technology, CIT-2010, 7th IEEE International Conference on Embedded Software and Systems, ICESS-2010, ScalCom-2010",10.1109/CIT.2010.76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249254911&doi=10.1109%2fCIT.2010.76&partnerID=40&md5=28c7c02ca4cc06ae177dd0c46402da76,"Department of Computing, University of Bradford, Bradford, BD7 1DP, United Kingdom",,English,,9780769541082
Scopus,Monetary pricing of software development risks: A method and empirical illustration,"The ability to price (monetize) software development risks can benefit various aspects of software development decision-making. This paper presents a risk pricing method that estimates two parameters for every individual risk factor: extra cost incurred per unit exposure, and project sensitivity, to that factor. Since variability is a widely used measure of risk in finance and decision sciences, the method derives risk pricing parameters by relating variability in risk factors to variability in project cost. This approach rests on the fact that a parametric cost estimator predicts project cost by adjusting the ""nominal"" cost of a project based on the expected values of risk factors (cost drivers), but the actual project cost often deviates from prediction because the actual values of risk factors normally deviate from expectations. In addition, to illustrate the viability of the method, the paper applies the method empirically with COCOMO data, to approximate risk pricing parameters for four risk factors (Personnel Capability, Process Maturity, Technology Platform, and Application Task). Importantly, though, the method could work equally well with data recorded based on other parametric cost estimators. The paper also discusses several areas that can benefit from benchmark risk pricing parameters of the kind we obtain. © 2010 Elsevier Inc. All rights reserved.",Economics; Risk management; Software development,"Appari A., Benaroch M.",2010,Journal,Journal of Systems and Software,10.1016/j.jss.2010.06.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957370427&doi=10.1016%2fj.jss.2010.06.012&partnerID=40&md5=b34dbda5f4f956cb02957c2e279c1d65,"Dartmouth College, Tuck School of Business, 100 Tuck Hall, Hanover, NH 03755, United States; Syracuse University, Martin J. Whitman School of Management, 721 University Avenue, Syracuse, NY 13210, United States",,English,01641212,
Scopus,BBN based approach for improving the software development process of an SME - A case study,"This article proposes an approach for improving the software process of a small/medium company. The methodology is presented through a case study during which estimation models have been applied, evaluated and introduced in a telecommunication software development process. The proposed methodology uses Bayesian Belief Networks to represent the relationships among implementation, product and process metrics and their impact on the development effort. The estimation models that were derived were applied and evaluated on the on-going projects of the company. Finally, by performing the same analysis on data from the International Software Benchmarking Standards Group (ISBSG) repository, it is demonstrated how one company can utilize data from other companies when it lacks sufficient data of its own. Copyright © 2009 John Wiley & Sons, Ltd.",Quality metrics; Software cost estimation; Software process improvement; Telecommunications software,"Bibi S., Stamelos I., Gerolimos G., Kollias V.",2010,Journal,Journal of Software Maintenance and Evolution,10.1002/spip.418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749318510&doi=10.1002%2fspip.418&partnerID=40&md5=19b448f44917969d5a737fc41afeae98,"Department of Informatics, Aristotle University of Thessaloniki, 54124 Thessaloniki, Greece; Teletel S.A, 124 Kifissias Avenue, 11526 Athens, Greece",,English,1532060X,
Scopus,Agile technologies in open source development,"The analysis of commonalities and differences between agile technology and open source software development is needed to understand how advancement approaches have evolved and whether they produce concrete benefits in terms of software quality and customer satisfaction. Agile Technologies in Open Source Development explores the overlap between open source and agile technologies, providing valuable strategies for advancement in software. This innovative publication provides a valuable resource to assist project managers, engineers, and developers interested in experimenting with new approaches in software expansion. © 2010 by IGI Global. All rights reserved.",,"Russo B., Scotto M., Sillitti A., Succi G.",2009,Book,Agile Technologies in Open Source Development,10.4018/978-1-59904-681-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898167616&doi=10.4018%2f978-1-59904-681-5&partnerID=40&md5=a38d5c184c0480a0194c0a044e7ec261,"Free University of Bozen-Bolzano, Italy; Center for Applied Software Engineering, Free University of Bozen-Bolzano, Italy",IGI Global,English,,9781599046815
Scopus,Improving the accuracy of software effort estimation based on multiple least square regression models by estimation error-based data partitioning,"Accurate software effort estimation is one of the key factors to a successful project by making a better software project plan. To improve the estimation accuracy of software effort, many studies usually aimed at proposing novel effort estimation methods or combining several approaches of the existing effort estimation methods. However, those researches did not consider the distribution of historical software project data which is an important part impacting to the effort estimation accuracy. In this paper, to improve effort estimation accuracy by least squares regression, we propose a data partitioning method by the accuracy measures, MRE and MER which are usually used to measure the effort estimation accuracy. Furthermore, the empirical experimentations are performed by using two industry data sets (the ISBSG Release 9 and the Bank data set which consists of the project data performed in a bank in Korea). © 2009 IEEE.",Data partitioning; Effort estimation; Least squares regression,"Seo Y.-S., Yoon K.-A., Bae D.-H.",2009,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2009.57,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76349098427&doi=10.1109%2fAPSEC.2009.57&partnerID=40&md5=ce3aa6f15ed9d40a6fb02bbb7b691b8c,"Division of Computer Science, College of Information Science and Technology, KAIST, 373-1 Guseong-dong, Yuseong-gu, Daejeon 305-701, South Korea",,English,15301362,9780769539096
Scopus,Temporal software change prediction using neural networks,"Predicting changes in software entities (e.g. source files) that are more likely to change can help in the efficient allocation of the project resources. A powerful change prediction tool can improve maintenance and evolution tasks in software projects in terms of cost and time factors. The vast majority of research works have focused on determining ""where"" the most change-prone entities are, and ""how"" the change will be propagated through a system. This article suggests that knowing ""when"" changes are likely to happen can also provide another consideration for managers and developers to plan their maintenance activities more efficiently. To address this issue, a Neural Network-based Temporal Change Prediction (NNTCP) framework is proposed. This novel framework indicates ""where"" the changes are likely to happen (i.e. hot spots), and then adds the time dimension to predict ""when"" it may occur. In proving this concept, the NNTCP framework is applied in two large-scale open source software projects, Mozilla and Eclipse. The results obtained indicate NNTCP can predict the occurrence of several future revisions with reasonable performance. © 2009 World Scientific Publishing Company.",Neural network; Software change prediction; Temporal prediction,"Amoui M., Salehie M., Tahvildari L.",2009,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194009004489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249113141&doi=10.1142%2fS0218194009004489&partnerID=40&md5=a543fd443a22f1ebb029054e96024370,"Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada",,English,02181940,
Scopus,Reducing biases in individual software effort estimations: A combining approach,"Software effort estimation techniques abound, each with its own set of advantages and disadvantages, and no one proves to be the single best answer. Combining estimating is an appealing approach. Avoiding the difficult problem of choosing the single ""best"" technique, it solves the problem by asking which techniques would help to improve accuracy, assuming that each has something to contribute. In this paper, we firstly introduce the systematic ""external"" combining idea into the field of software effort estimation, and estimate software effort using Optimal Linear Combining (OLC) method with an experimental study based on a real-life data set. The result indicates that combining different techniques can significantly improve the accuracy and consistency of software effort estimation by making full use of information provided by all components, even the much ""worse"" one. Copyright 2008 ACM.",Combining; OLE; Software effort estimation,"Li Q., Wang Q., Yang Y., Li M.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949152841&doi=10.1145%2f1414004.1414041&partnerID=40&md5=b9dab519df252527cc2e354577f915d3,"Institute of Software, Chinese Academy of Sciences, China; Graduate University, Chinese Academy of Sciences, China",,English,,9781595939715
Scopus,Maximising data retention from the ISBSG repository,"BACKGROUND: In 1997 the International Software Benchmarking Standards Group (ISBSG) began to collect data on software projects. Since then they have provided copies of their repository to researchers and practitioners, through a sequence of releases of increasing size. PROBLEM: Questions over the quality and completeness of the data in the repository have led some researchers to discard substantial proportions of the data in terms of observations, and to discount the use of some variables in the modelling of, among other things, software development effort. In some cases the details of the discarding of data has received little mention and minimal justification. METHOD: We describe the process we used in attempting to maximise the amount of data retained for modelling software development effort at the project level, based on previously completed projects that had been sized using IFPUG/NESMA function point analysis (FPA) and recorded in the repository. RESULTS: Through justified formalisation of the data set and domain-informed refinement we arrive at a final usable data set comprising 2862 (of 3024) observations across thirteen variables. CONCLUSION: a methodical approach to the pre-processing of data can help to ensure that as much data is retained for modelling as possible. Assuming that the data does reflect one or more underlying models, such retention should increase the likelihood of robust models being developed. © 2008 Evaluation and Assessment in Software Engineering. All rights reserved.",Data formalisation; Effort prediction; Empirical software engineering; FPA; ISBSG repository; Regression,"Deng K., MacDonell S.G.",2008,Conference,"12th International Conference on Evaluation and Assessment in Software Engineering, EASE 2008",10.14236/ewic/ease2008.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085852406&doi=10.14236%2fewic%2fease2008.3&partnerID=40&md5=6f38768807f8a52ba2406e2ad9c9b36f,"School of Computing and Mathematical Sciences, Auckland University of Technology, Private Bag 92006, Auckland, 1142, New Zealand",BCS Learning and Development Ltd.,English,,
Scopus,Evaluation of a black-box estimation tool: A case study,"For the past 30 years, various estimation models and tools have been developed to help managers perform estimation tasks. Some of these estimation tools date from the late 1970s, and have been progressively modernized by their vendors in terms of tools' user interfaces and new functions to facilitate not only project estimation but also detailed project planning. For organizations interested in using such estimation tools, it is crucial to know about their predictive performance. However, it is not an industry practice for the vendors to document the performance of these commercial estimation tools; estimation tool builders have not provided information on the performance of their models with respect to their own initial data repositories, nor on subsequent versions. Basically, such estimation tools are often black boxes with undocumented performance properties. Various researchers have attempted to analyze the performance of such black-box estimation tools within the constraints of research data sets that were fairly small compared to the larger ones as claimed by tool vendors. The research presented here revisits this issue, this time with a much larger data set from the International Software Benchmarking Standards Group (ISBSG). This new study is presented in three steps. First, the data set is analyzed by the programming language and corresponding subsamples are identified, including identification of obvious outliers with respect to effort and size. Second, estimation models are built directly from such samples, in a white-box fashion, with and without outliers. Third, a commercial software estimation tool widely distributed throughout the world is tested against the same set of samples. In summary, for the majority of samples available, the black-box tool fares fairly poorly. Lessons learned are of two types: prospective tool users should demand that tool vendors benchmark their black-box tool against publicly available repositories; and the interval of confidence of the output provided by their tool, as well as the basis for such output (e.g. in terms of both the number of observations and currentness of such data), must be documented. Copyright © 2007 John Wiley & Sons, Ltd.",Software engineering effort estimation; Software engineering planning; Software engineering productivity analysis,"Abran A., Ndiaye I., Bourque P.",2007,Review,Software Process Improvement and Practice,10.1002/spip.310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247523848&doi=10.1002%2fspip.310&partnerID=40&md5=6e3373763ad2f52df63e85fb1a88ef5d,"École de Technologie Supérieure, Université du Québec, Montréal, Canada; CGI Group, Montréal, Canada; École de Technologie Supérieure, Software Engineering Dept., Université du Québec, 1100 Notre-Dame St. West, Montréal, H3C IK3, Canada",,English,10774866,
Scopus,Web effort estimation,"Software effort models and effort estimates help project managers allocate resources, control costs, and schedule and improve current practices, leading to projects that are finished on time and within budget. In the context of Web development and maintenance, these issues are also crucial, and very challenging, given that Web projects have short schedules and a highly fluidic scope. Therefore this chapter has two main objectives. The first is to introduce the concepts related to effort estimation and in particular Web effort estimation. The second is to present a case study where a real effort prediction model based on data from completed industrial Web projects is constructed step by step. © 2006 Springer-Verlag Berlin Heidelberg.",Data analysis; Effort models; Manual stepwise regression; Prediction accuracy; Web effort estimation; Web size measures,"Mendes E., Mosley N., Counsell S.",2006,Book Chapter,Web Engineering,10.1007/3-540-28218-1_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892332645&doi=10.1007%2f3-540-28218-1_2&partnerID=40&md5=3989adb2f0a3807ddbea38756c32f5ba,"Department of Computer Science, University of Auckland Science Centre, 38 Princes Street, Auckland, New Zealand; MetriQ (NZ) Limited, 19 Clairville Crescent, Glendowie, Auckland, New Zealand; School of Information Systems and Computing, Brunel University, St John's Building, Uxbridge UB8 3PH, United Kingdom",Springer Berlin Heidelberg,English,,3540281967; 9783540281962
Scopus,An approach of a technique for effort estimation of iterations in software projects,"The estimation of effort and cost is still one of the hardest tasks in software project management. At the moment, there are many techniques to accomplish this task, such as Function Points, Use Case Points and COCOMO, but there is not much information available about how to use those techniques in non-waterfall software lifecycles such as iterative or spiral lifecycles projects. This paper shows the results obtained when applying a technique to estimate the effort of each construction iteration in software development projects that use iterative-incremental lifecycles. The results were obtained from software projects of a fourth-year course in Informatics. The technique proposes the use of Function Points and COCOMO II. © 2006 IEEE.",,"Pow-Sang J.A., Jolay-Vasquez E.",2006,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2006.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949245508&doi=10.1109%2fAPSEC.2006.20&partnerID=40&md5=74390e103ea4a2f6d0fc1c56cad64f13,"Pontificia Universidad Católica del Perú, Peru; Carnegie Mellon University, United States",,English,15301362,0769526853; 9780769526850
Scopus,Software functionality: A game theoretic analysis,"Digital products are now widely traded over the Internet. Many researchers have started to investigate the optimal competitive strategies and market environments for such products. This paper studies the competitive decisions made about software, a major class of digital products that can be easily sold through computer networks. Instead of focusing on traditional competitive dimensions, such as price or quantity, we study the number of functions that should be incorporated into the software. Using game theoretic analysis, we show that there is no fixed strategy that is optimal for software developers in a duopoly market with one-stage simultaneous moves. This happens because, given one developer's decision, there is always an incentive for the other developer to deviate and achieve higher payoffs. Nevertheless, a unique reactive equilibrium does emerge if we consider the two-stage variation of the model, where the two developers both enjoy substantial profits by serving different segments of the market. Essentially, the first mover commits himself to a certain functionality level that induces a rational follower to target his software to the (previously) unattended segment. We discuss our results in light of scale economies in the software development process and market segmentation.",Development cost; Digital products; Functionality; Game theory; Software,"Hui K.L., Tam K.Y.",2002,Journal,Journal of Management Information Systems,10.1080/07421222.2002.11045710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036607260&doi=10.1080%2f07421222.2002.11045710&partnerID=40&md5=e98d69edc02781b8dfe80e529da58f4e,"University of Singapore, Hong Kong University of Science and Technology, Singapore, Singapore; Hong Kong University of Science and Technology, Information and Systems Management, Hong Kong",,English,07421222,
Scopus,Software engineering productivity measurement using function points: A case study,"This paper reports on the findings of an empirically based case study of the use of function points analysis (FPA) by the information systems division of a large financial services company. The software engineering productivity figures measured by FPA in this company varied widely across the departments of the division and projects. Investigation of the reasons for the variations showed that, in addition to factors such as the technology platform and application characteristics, organizational and human factors affect the accuracy and reliability of productivity figures. Elucidating the lessons from this case, this paper suggests that three factors - knowledge of the FPA, calibration of the function point productivity indicator and rigour of the measurement process - are critical to the successful implementation of an FPA programme. These findings and the issues identified in the paper will be of interest to academics in the area of software productivity measurement and companies considering FPA as a productivity metric. © 2000, Association for Information Technology Trust. All rights reserved.",,"Bok H.S., Raman K.S.",2000,Journal,Journal of Information Technology,10.1080/026839600344429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034394259&doi=10.1080%2f026839600344429&partnerID=40&md5=ec77ce972383148298c12eaa3dbb99ca,"5 Tanah Merah Kechil Road, 17-06, 466665, Singapore; School of Computing, National University of Singapore, 10 Kent Ridge Crescent, 117543, Singapore",,English,02683962,
Scopus,Software quality measurement: Concepts and fuzzy neural relational model,"A fuzzy neural relational model of software quality derived from the McCall hierarchical software quality measurement framework (HSQF), is introduced. The HSQF has three fundamental levels (factorsrarr/criteriararr/metrics) which has a rather natural generalization in the context of fuzzy sets. Vectors of factors, criteria, and metrics are treated as fuzzy sets. On each level, fuzzy objects (fuzzy set and fuzzy relation) are introduced. A learning algorithm is proposed to calibrate the relations at the topmost levels of the software quality model. A learning scenario and detailed learning formulas are given. A brief illustration of the model is also given. © 1998 IEEE.",fuzzy sets; learning; neural networks; software metrics; software quality,"Pedrycz W., Peters J.F., Ramanna S.",1998,Conference,1998 IEEE International Conference on Fuzzy Systems Proceedings - IEEE World Congress on Computational Intelligence,10.1109/FUZZY.1998.686259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031629537&doi=10.1109%2fFUZZY.1998.686259&partnerID=40&md5=4e490c132226610526bc1c4d887ac887,"Computational Intelligence Laboratory, Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB  R3T 2N2, Canada; Department of Business Computing, University of Winnipeg, Winnipeg, MB  R3B 2E9, Canada; Department of Electrical and Computer Engineering, University of Manitoba, WP, MB  R3T 2N2, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,078034863X; 9780780348639
Scopus,Estimating CASE development size from outline specifications,Size is a significant variable in cost/estimation modelling and is widely regarded as one of the most important determinants of effort. The research documented here was carried out in response to problems associated with estimation in project management. This paper discusses some empirical evidence arising from a study of design size estimation. The approach used here is to count the number of lines in the automatically generated CASE report. These are then used in the estimation equations as a measurement unit. Results from this study are shown to support earlier conjectures that design size can be estimated from early system specifications in CASE development environments. The study concludes that accurate size estimations can be made using data from early system specifications.,CASE; CASE application products; Computer-aided software engineering; Size estimation; Size measurement; Software process modelling,Cockcroft S.,1996,Journal,Information and Software Technology,10.1016/0950-5849(95)01065-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030172376&doi=10.1016%2f0950-5849%2895%2901065-3&partnerID=40&md5=86b85b1e7a38248c0bbc900ee80ff1e5,"Sch. of Comp. and Info. Science, University of Otago, PO Box 56, Dunedin, New Zealand",Elsevier,English,09505849,
Scopus,Neural networks in specification level software size estimation,"This paper presents a neural network approach to software size estimation. A multilayer feedforward network is trained using backpropagation algorithm. The training and testing data consist of randomly generated Structured Analysis (SA) descriptions as input data and corresponding algorithm based size metric values as output data. The size metrics used in the experiments are Albrecht's Function Points, Symons's Mark II Function Points, and DeMarco's Function Bang metric. The experiments indicate that neural networks can learn to calculate software size estimates. In each of our experiments we found that the results depend on the features of the input data, the metric, and the convergence criteria used. The results also encourage to develop a general input set to represent size related features of graph based system descriptions. © 1993 IEEE.",,"Hakkarainen J., Laamanen P., Rask R.",1993,Conference,Proceedings of the Annual Hawaii International Conference on System Sciences,10.1109/HICSS.1993.284242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989241820&doi=10.1109%2fHICSS.1993.284242&partnerID=40&md5=885fc5b2780fdb57f8a6e758c7d1d551,"University of Joensuu, Department of Computer Science, Finland",IEEE Computer Society,English,15301605,0818632305
Scopus,Application of cost-estimation techniques: industrial perspective,"The paper describes the approach adopted for software project estimation within the Telecommunications Systems Group, TSG, of GPT, UK. The material associated with this estimation strategy forms part of a general software metrics initiative known as the PRISM programme. PRISM stands for 'PRocess Improvement Support via Measurement' and is part of TSG's continual process improvement strategy. GPT is an international organization of some 24 000 employees world wide, manufacturing and marketing major telecommunications products that include the System X and DCO exchanges. This involves a significant investment in the production of real-time software. TSG accounts for approximately 50% of the engineering effort within GPT. © 1992.",cost estimation; software metrics; telecommunications,Goodman P.,1992,Journal,Information and Software Technology,10.1016/0950-5849(92)90012-E,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249012381&doi=10.1016%2f0950-5849%2892%2990012-E&partnerID=40&md5=68d42ed642842387e2275c606700cd62,"GPT (Telecommunications Systems Group), Sopers Lane, Poole, Dorset BH17 7EQ, United Kingdom",,English,09505849,
Scopus,Software productivity measurements,"In the arena of software metrics, measuring productivity associated with software products and their development has historically been a difficult process. A reliable set of operational productivity measurement procedures is needed to analyze a software product and its development process. Software productivity measurements are the roots of the tree of productivity improvement. Without good measurements, progress is unlikely. To improve the quality of a software product, and the productivity of the development process, accurate measurements of input to and output from the development process must be made and appropriate productivity factors must be identified and understood. Standardizing the measurement procedures within a development organization is a critical step. To achieve accurate software productivity measurements, the software development process should be well understood and the software productivity measurements should be closely linked to the development process and the development environment. This paper describes the software productivity measurement metrics, the important productivity factors, and some applications of the software productivity data for the 5ESS® switch developed for the United States market. The measurement of production rate and the study of productivity factors have established a solid foundation for the pursuit of productivity and quality improvement within the 5ESS switch development community. © 1991 IEEE.",,"Yu W.D., Smith D.P., Huang S.T.",1991,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/CMPSAC.1991.170239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869033633&doi=10.1109%2fCMPSAC.1991.170239&partnerID=40&md5=f48fa500b526e4dd2b97b3736cbe223a,"ATandT Bell Laboratories, 1200 East Warrenville Road, Naperville, IL  60566, United States",IEEE Computer Society,English,07303157,0818621524
Scopus,Bellwethers: A Baseline Method for Transfer Learning,"Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of 'bellwethers': given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared. © 1976-2012 IEEE.",bad smells; defect prediction; effort estimation; issue close time; prediction; Transfer learning,"Krishna R., Menzies T.",2019,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2018.2821670,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044791951&doi=10.1109%2fTSE.2018.2821670&partnerID=40&md5=b3d6059205c6928084f5b65b295a4c3a,"Department of Computer Science, North Carolina State University, Raleigh, NC  27695, United States",Institute of Electrical and Electronics Engineers Inc.,English,00985589,
Scopus,Software development effort estimation using regression fuzzy models,"Software effort estimation plays a critical role in project management. Erroneous results may lead to overestimating or underestimating effort, which can have catastrophic consequences on project resources. Machine-learning techniques are increasingly popular in the field. Fuzzy logic models, in particular, are widely used to deal with imprecise and inaccurate data. The main goal of this research was to design and compare three different fuzzy logic models for predicting software estimation effort: Mamdani, Sugeno with constant output, and Sugeno with linear output. To assist in the design of the fuzzy logic models, we conducted regression analysis, an approach we call ""regression fuzzy logic."" State-of-the-art and unbiased performance evaluation criteria such as standardized accuracy, effect size, and mean balanced relative error were used to evaluate the models, as well as statistical tests. Models were trained and tested using industrial projects from the International Software Benchmarking Standards Group (ISBSG) dataset. Results showed that data heteroscedasticity affected model performance. Fuzzy logic models were found to be very sensitive to outliers. We concluded that when regression analysis was used to design the model, the Sugeno fuzzy inference system with linear output outperformed the other models. © 2019 Ali Bou Nassif et al.",,"Nassif A.B., Azzeh M., Idri A., Abran A.",2019,Journal,Computational Intelligence and Neuroscience,10.1155/2019/8367214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062599935&doi=10.1155%2f2019%2f8367214&partnerID=40&md5=efd8130615f3d9857ff368174c7a2db1,"Department of Electrical and Computer Engineering, University of Sharjah, P.O. Box 27272, Sharjah, United Arab Emirates; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Department of Software Engineering, Applied Science Private University, P.O. Box 166, Amman, Jordan; Software Project Management Research Team, ENSIAS, Mohammed v University, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, QC, Canada",Hindawi Limited,English,16875265,
Scopus,Uncertainty management in software effort estimation using a consistent fuzzy analogy-based method,"Software effort estimation is a critical task in software project development management. Unfortunately, the uncertainty and inaccuracy are inherent properties of the software effort estimation environment. These are caused by the limited capabilities of the managers, to foresee, measure and describe factors influencing the software effort. The promising Fuzzy Analogy-based Software Effort Estimation model (FASEE) employs successfully fuzzy logic with approximate reasoning theory to handle imprecision and reasoning under uncertainty. Also, FASEE use possibility distribution to quantify the uncertainty in the estimate that aid the software managers to assess risks. Yet, the FASEE suffer from the low data quality and the uncertainty induced in the reasoning process. In this paper, we propose an enhancement of the FASEE, by imposing consistency criteria to deal with the aforementioned drawbacks. So, the underlying model, called Consistent Fuzzy Analogy-based Software Effort Estimation (C-FASEE) is endowed with two capabilities. The first one introduces consistency criteria in attribute representation by fuzzy sets to enable fitting each attribute to the software effort. The second one introduces a new relation of confidence to measure the extent that the resulted most similar projects respect the assumption “similar projects have similar efforts”. Moreover, the C-FASEE method provide a fuzzy estimate of the most possible fuzzy set will the true effort of the new software project falls in. This allow to the software manager to assess risks more optimally. The proposed C-FASEE is validated over thirteen software project datasets that represent different complexities. The obtained results are compared to variant methods of the analogy-based software effort estimation approach. The experimental results show that our proposal provides a good estimation accuracy of and has significantly best performance against the comparison methods. © 2018 Elsevier B.V.",Fuzzy analogy; Fuzzy logic; Possibility distribution; Software effort estimation; Uncertainty management,"Ezghari S., Zahi A.",2018,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2018.03.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056403091&doi=10.1016%2fj.asoc.2018.03.022&partnerID=40&md5=8bcc6efc056a30e7e9de76cc78aac72a,"Intelligent System and Application Laboratory (SIA), Faculty of Science and Technology, Fez, 2202, Morocco",Elsevier Ltd,English,15684946,
Scopus,A light-weight incremental effort estimation model for use case driven projects,"Use case analysis has been widely adopted in modern software engineering due to its strength in capturing the functional requirements of a system. It is often done with a UML use case model that formalizes the interactions between actors and a system in the requirements elicitation iteration, and with architectural alternatives explored and user interface details specified in the following analysis and design iteration. On the other hand, to better support decision making in software management, effort estimation models are required to provide estimates about the required project effort at the very early stage of a project, which, however, provides little information for accurately evaluating system complexity. To solve this dilemma, an incremental approach of integrating information available throughout the early iterations to provide multiple effort estimations is preferred in keeping the balance between utility and accuracy. In this paper, we proposed an effort estimation model that incorporates two sub-models to provide two points of effort estimation during the early iterations of a use case driven project. Our proposed model is lightweight due to the fact that its size metrics are defined to be countable directly from the artifacts of the early iterations. To better calibrate the model, especially in considering the situation of having limited data points available, we also introduced a normalization framework in our model calibration process to reduce noise from the effort data. By calibrating the proposed sub-models with the data points collected from 4 historical projects, we demonstrated that the sub-models fit the data set well, and the later-phase model is superior to the early-phase model for it fits the data set better and shows less uncertainty in the calibrated parameters. © 2017 IEEE.",automated analysis; data normalization; effort estimation; function points; incremental estimation model; model calibration; model-based analysis; object-oriented modeling; project management; software size metrics; unified modeling language; use case analysis; use case driven process; use case points,"Qi K., Boehm B.W.",2017,Conference,"2017 IEEE 28th Annual Software Technology Conference, STC 2017",10.1109/STC.2017.8234456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049358075&doi=10.1109%2fSTC.2017.8234456&partnerID=40&md5=cfe92b86cf166335db12157d99462f51,"University of Southern California, United States",Institute of Electrical and Electronics Engineers Inc.,English,,9781538610886
Scopus,Fuzzy Analogy Based Effort Estimation: An Empirical Comparative Study,"Software Development Effort Estimation (SDEE) plays a primary role in software project management. Among several techniques suggested for estimating software development effort, analogy-based software effort estimation approaches stand out as promising techniques.In this paper, the performance of Fuzzy Analogy is compared with that of six other SDEE techniques (Linear Regression, Support Vector Regression, Multi-Layer Perceptron, M5P and Classical Analogy). The evaluation of the SDEE techniques was performed over seven datasets with two evaluation techniques (All-in and Jackknife). The first step of the evaluation aimed to ensure that the SDEE techniques outperformed random guessing by using the Standardized Accuracy (SA). Then, we used a set of reliable performance measures (Pred(0.25), MAE, MBRE, MIBRE and LSD) and Borda count to rank them and identify which techniques are the most accurate.The results suggest that when using All-in evaluation, Fuzzy Analogy statistically outperformed the other SDEE techniques regardless of the dataset used. However, when using Jackknife evaluation, the results obtained depended on the dataset and the SDEE technique used. The results suggest that Fuzzy Analogy is a promising technique for software development effort estimation. © 2017 IEEE.",Accuracy Evaluation; Analogy Based Software Effort Estimation; Fuzzy Analogy; Software Development Effort Estimation,"Idri A., Abnane I.",2017,Conference,IEEE CIT 2017 - 17th IEEE International Conference on Computer and Information Technology,10.1109/CIT.2017.29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032383384&doi=10.1109%2fCIT.2017.29&partnerID=40&md5=2d4f527b6750abd19f4a3787348a493a,"Software Project Management Research Team, ENSIAS, Mohamed v University of Rabat, Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781538609583
Scopus,Dynamics of task allocation in global software development,"Context: Global software development (GSD) promises high-quality software at low cost. GSD enables around-the-clock development to achieve maximum production in a short period of time by using expertise around the globe. This development is only possible if tasks are effectively distributed among sites to ensure smooth development. Therefore, one of the key challenges of GSD is to design a task allocation strategy. Objective: The objective of this study is to identify various factors that influence task allocation decisions in GSD and to assess their relative importance. We also aim to determine the interrelationship between the factors along with role played by product architecture and communication and coordination needs during task allocation. Methods: We used multiple methods to collect data about the task allocation factors and process. A web-based survey of 54 GSD practitioners from around the globe was conducted to identify the factors and their relative importance for task allocation decision. The selection of the sample was performed via the snowball sampling technique. To increase the sample size, the survey was also posted on social media, that is, Facebook, LinkedIn, and Twitter. Nonparametric statistical tests were applied on the response data to identify correlations and significance. Interviews were conducted from 11 project managers having 10 to 30 years GSD experience to gain insight into the dynamics of task allocation process. Results: The survey results highlight “expertise,” “site characteristics,” and “task site dependency” as the most important factors for a task allocation decision. The interview study has highlighted the importance of situation-specific decision making during task allocation. The significance of factors varies with the characteristics of task, characteristics of organization, type of GSD, and objective of doing GSD. The culture and time differences between distributed sites have been assigned a low priority by the majority of the practitioners. The most common way of distributing task is functional area of expertise and phase-based division, where detailed architecture is not considered. Interdependent modules are not allocated to distributed sites because of communication and coordination overhead. Our results also demonstrate a correlation between various factors and support Conway's law. Conclusions: We have interesting results in which certain factors are ranked differently from the prevalent views in the GSD literature. The survey results have also confirmed the application of Conway's law in practice for task allocation, where interdependent modules are not allocated to distributed sites. The significance of factors varies with characteristics of task, characteristics of organization, type of GSD, and objective of GSD, which require trade-off between factors. The need of a well-defined situation-specific task allocation framework is evident from the results of survey and interview study. The outline of a task allocation framework for GSD is presented. Copyright © 2016 John Wiley & Sons, Ltd.",framework; global software development; interview study; practitioners view; survey; task allocation,"Imtiaz S., Ikram N.",2017,Journal,Journal of Software: Evolution and Process,10.1002/smr.1832,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991736250&doi=10.1002%2fsmr.1832&partnerID=40&md5=244da0b4902ff051776a58b64f1ce6a7,"Faculty of Computing, Riphah International University, Islamabad, Pakistan",John Wiley and Sons Ltd,English,20477481,
Scopus,Towards benchmarking feature subset selection methods for software fault prediction,"Despite the general acceptance that software engineering datasets often contain noisy, irrelevant or redundant variables, very few benchmark studies of feature subset selection (FSS) methods on real-life data from software projects have been conducted. This paper provides an empirical comparison of state-of-the-art FSS methods: information gain attribute ranking (IG); Relief (RLF); principal component analysis (PCA); correlation-based feature selection (CFS); consistencybased subset evaluation (CNS); wrapper subset evaluation (WRP); and an evolutionary computation method, genetic programming (GP), on five fault prediction datasets from the PROMISE data repository. For all the datasets, the area under the receiver operating characteristic curve—the AUC value averaged over 10-fold cross-validation runs—was calculated for each FSS method-dataset combination before and after FSS. Two diverse learning algorithms, C4.5 and naïve Bayes (NB) are used to test the attribute sets given by each FSS method. The results show that although there are no statistically significant differences between the AUC values for the different FSS methods for both C4.5 and NB, a smaller set of FSS methods (IG, RLF, GP) consistently select fewer attributes without degrading classification accuracy. We conclude that in general, FSS is beneficial as it helps improve classification accuracy of NB and C4.5. There is no single best FSS method for all datasets but IG, RLF and GP consistently select fewer attributes without degrading classification accuracy within statistically significant boundaries. © Springer International Publishing Switzerland 2016.",Empirical; Fault prediction; Feature subset selection,"Afzal W., Torkar R.",2016,Book Chapter,Studies in Computational Intelligence,10.1007/978-3-319-25964-2_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955278082&doi=10.1007%2f978-3-319-25964-2_3&partnerID=40&md5=58ee5dc941037f48b98caeb148b6e8ed,"School of Innovation, Design and Engineering, Mälardalen University, Västerås, Sweden; Blekinge Institute of Technology, Karlskrona, Sweden; Chalmers University of Technology, Gothenburg, Sweden; University of Gothenburg, Gothenburg, Sweden; Department of Computer Science, Bahria University, Islamabad, Pakistan",Springer Verlag,English,1860949X,
Scopus,Benchmarking software development productivity of CMMI level 5 projects,"In this paper, data envelopment analysis variable returns to scale (DEA VRS) model is applied to data collected on 79 software development projects from a leading CMMI level 5 organization. We divide overall software effort into software development effort, software quality conformance effort (EoC), and software maintenance non-conformance (EoNC) effort due to poor software quality at delivery time. Partitioning effort into software development and software quality metrics provides us a comprehensive model to measure productivity of software projects and to identify best practice projects. Some of positive productivity drivers from the DEA best practice efficient projects point to good customer rapport and application familiarity. Inefficient projects had problems such as customer requirements volatility, and the use of unfamiliar technology. The DEA results identify 12 “best practice” projects that can be emulated for software process improvement. Additionally, our results point to approximately 50 % potential for productivity improvement in software projects to get to the level of “best practice” projects. This study shows that including EoC and EoNC as inputs has a positive impact on the best practice frontier. © 2015, Springer Science+Business Media New York.",CMMI; Data envelopment analysis; Fortune 500; Productivity; Software quality,"Pai D.R., Subramanian G.H., Pendharkar P.C.",2015,Journal,Information Technology and Management,10.1007/s10799-015-0234-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939654627&doi=10.1007%2fs10799-015-0234-4&partnerID=40&md5=00110bf5f603d0c972566d27ba4b8f82,"School of Business Administration, Pennsylvania State University at Harrisburg, 777 West Harrisburg Pike, Middletown, PA  17057, United States",Springer New York LLC,English,1385951X,
Scopus,Systematic mapping study of missing values techniques in software engineering data,"Missing Values (MV) present a serious problem facing research in software engineering (SE) which is mainly based on statistical and/or data mining analysis of SE data. The simple method of dealing with MV is to ignore data with missing observations. This leads to losing valuable information and then obtaining biased results. Therefore, various techniques have been developed to deal adequately with MV, especially those based on imputation methods. In this paper, a systematic mapping study was carried out to summarize the existing techniques dealing with MV in SE datasets and to classify the selected studies according to six classification criteria: research type, research approach, MV technique, MV type, data types and MV objective. Publication channels and trends were also identified. As results, 35 papers concerning MV treatments of SE data were selected. This study shows an increasing interest in machine learning (ML) techniques especially the K-nearest neighbor algorithm (KNN) to deal with MV in SE datasets and found that most of the MV techniques are used to serve software development effort estimation techniques. © 2015 IEEE.",Machine learning; Missing values; Software engineering data; Systematic mapping study,"Idri A., Abnane I., Abran A.",2015,Conference,"2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2015 - Proceedings",10.1109/SNPD.2015.7176280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947080832&doi=10.1109%2fSNPD.2015.7176280&partnerID=40&md5=d82d376ab83550a21470787ee8a42f45,"Software Project Management Research Team, ENSIAS, Mohamed v University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, Montréal, H3C IK3, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781479986767
Scopus,Mining correlations of ATL model transformation and metamodel metrics,"Model transformations are considered to be the ""heart"" and ""soul"" of Model Driven Engineering, and as a such, advanced techniques and tools are needed for supporting the development, quality assurance, maintenance, and evolution of model transformations. Even though model transformation developers are gaining the availability of powerful languages and tools for developing, and testing model transformations, very few techniques are available to support the understanding of transformation characteristics. In this paper, we propose a process to analyze model transformations with the aim of identifying to what extent their characteristics depend on the corresponding input and target metamodels. The process relies on a number of transformation and metamodel metrics that are calculated and properly correlated. The paper discusses the application of the approach on a corpus consisting of more than 90 ATL transformations and 70 corresponding metamodels. © 2015 IEEE.",,"Di Rocco J., Di Ruscio D., Iovino L., Pierantonio A.",2015,Conference,"Proceedings - 7th International Workshop on Modeling in Software Engineering, MiSE 2015",10.1109/MiSE.2015.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964298534&doi=10.1109%2fMiSE.2015.17&partnerID=40&md5=5a5fc092939a51d9c90a4157fc1a8e1c,"DISIM, Unversità degli Studi dell'Aquila, Italy",Institute of Electrical and Electronics Engineers Inc.,English,,9781479919345
Scopus,Predicting unit testing effort levels of classes: An exploratory study based on Multinomial Logistic Regression modeling,"The study aims at investigating empirically the ability of a Quality Assurance Indicator (Qi), a metric that we proposed in a previous work, to predict different levels of unit testing effort of classes in object-oriented systems. To capture the unit testing effort of classes, we used four metrics to quantify various perspectives related to the code of corresponding unit test cases. Classes were classified, according to the involved unit testing effort, in five categories (levels). We collected data from two open source Java software systems (ANT and JFREECHART) for which JUnit test cases exist. In order to explore the ability of the Qi metric to predict different levels of the unit testing effort of classes, we decided to explore the possibility of using the Multinomial Logistic Regression (MLR) method. The performance of the Qi metric has been compared to the performance of three well-known source code metrics related respectively to size, complexity and coupling. Results suggest that the MLR model based on the Qi metric is able to accurately predict different levels of the unit testing effort of classes. © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.",Empirical analysis; Junit code; Metrics; Multinomial Logistic Regression; Object-oriented systems; Prediction; Software testability; Unit testing effort,"Badri M., Toure F., Lamontagne L.",2015,Conference,Procedia Computer Science,10.1016/j.procs.2015.08.528,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962599384&doi=10.1016%2fj.procs.2015.08.528&partnerID=40&md5=d6ec341da5e452b2c35aafa7cc5b52cc,"Department of Mathematics and Computer Science, University of Quebec, Trois-Rivières, QC, Canada; Department of Computer Science and Software Engineering, Laval University, Laval, QC, Canada",Elsevier B.V.,English,18770509,
Scopus,A multivariate statistical framework for the analysis of software effort phase distribution,"Context In software project management, the distribution of resources to various project activities is one of the most challenging problems since it affects team productivity, product quality and project constraints related to budget and scheduling. Objective The study aims to (a) reveal the high complexity of modelling the effort usage proportion in different phases as well as the divergence from various rules-of-thumb in related literature, and (b) present a systematic data analysis framework, able to offer better interpretations and visualisation of the effort distributed in specific phases. Method The basis for the proposed multivariate statistical framework is Compositional Data Analysis, a methodology appropriate for proportions, along with other methods like the deviation from rules-of-thumb, the cluster analysis and the analysis of variance. The effort allocations to phases, as reported in around 1500 software projects of the ISBSG R11 repository, were transformed to vectors of proportions of the total effort and were analysed with respect to prime project attributes. Results The proposed statistical framework was able to detect high dispersion among data, distribution inequality and various interesting correlations and trends, groupings and outliers, especially with respect to other categorical and continuous project attributes. Only a very small number of projects were found close to the rules-of-thumb from the related literature. Significant differences in the proportion of effort spent in different phrases for different types of projects were found. Conclusion There is no simple model for the effort allocated to phases of software projects. The data from previous projects can provide valuable information regarding the distribution of the effort for various types of projects, through analysis with multivariate statistical methodologies. The proposed statistical framework is generic and can be easily applied in a similar sense to any dataset containing effort allocation to phases. © 2014 Elsevier B.V. All rights reserved.",Biplot; Cluster analysis; Compositional data analysis; Phased effort analysis; Software effort distribution,"Chatzipetrou P., Papatheocharous E., Angelis L., Andreou A.S.",2015,Journal,Information and Software Technology,10.1016/j.infsof.2014.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921022835&doi=10.1016%2fj.infsof.2014.11.004&partnerID=40&md5=f2f43ee9dca532ea3824ab7dd307d4cb,"Department of Informatics, Aristotle University of Thessaloniki, Greece; Department of Computer Science, University of Cyprus, Nicosia, Cyprus; Swedish Institute of Computer Science (SICS), Kista, Stockholm, SE-16429, Sweden; Department of Computer Engineering and Informatics, Cyprus University of Technology, Limassol, Cyprus",Elsevier,English,09505849,
Scopus,RELREA - An analytical approach for evaluating release readiness,"As part of incremental and iterative software development, decisions about ""Is the software product ready to be released at some given release date?"" have to be made at the end of each release, sprint or iteration. While this decision is critically important, so far it is largely done either informally or in a simplistic manner, relying on a small set of isolated metrics. In this paper, we present an analytical approach combining the goal-oriented definition of the most relevant readiness metrics with their individual evaluation and their subsequent analytical integration into an aggregated evaluation measure. The applicability of the proposed approach called RELREA is demonstrated for an ongoing public project hosted on GitHub, a web-based hosting service for software development projects. Initial evidence shows that the method is supportive in evaluating release readiness at any point of the development cycle, making projections on the final release readiness and allows determination of bottleneck factors to achieve readiness. Copyright © 2014 by Knowledge Systems Institute Graduate School.",Aggregation; Case study; Fuzzy set; Release criteria; Release date; Release readiness,"Shahnewaz S.M., Ruhe G.",2014,Conference,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938385665&partnerID=40&md5=dfb4bb8ee66274d45e40467caabac6d0,"Department of Computer Science, Department of ECE, University of Calgary, Calgary, AB, Canada",Knowledge Systems Institute Graduate School,English,23259000,
Scopus,"Object-Oriented Analysis and Design for Information Systems: Modeling with UML, OCL, and IFML","Object-Oriented Analysis and Design for Information Systems clearly explains real object-oriented programming in practice. Expert author Raul Sidnei Wazlawick explains concepts such as object responsibility, visibility and the real need for delegation in detail. The object-oriented code generated by using these concepts in a systematic way is concise, organized and reusable. The patterns and solutions presented in this book are based in research and industrial applications. You will come away with clarity regarding processes and use cases and a clear understand of how to expand a use case. Wazlawick clearly explains clearly how to build meaningful sequence diagrams. Object-Oriented Analysis and Design for Information Systems illustrates how and why building a class model is not just placing classes into a diagram. You will learn the necessary organizational patterns so that your software architecture will be maintainable. Learn how to build better class models, which are more maintainable and understandable. Write use cases in a more efficient and standardized way, using more effective and less complex diagrams. Build true object-oriented code with division of responsibility and delegation. © 2014 Elsevier Inc. All rights reserved.",,Wazlawick R.S.,2014,Book,"Object-Oriented Analysis and Design for Information Systems: Modeling with UML, OCL, and IFML",10.1016/C2012-0-06942-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903265070&doi=10.1016%2fC2012-0-06942-6&partnerID=40&md5=0c497dcb021402c8259b61467d02885c,"UFSC, Florianópolis, Brazil",Elsevier Inc.,English,,9780124186736
Scopus,A personal perspective on the evolution of empirical software engineering,"This paper offers a four-decade overview of the evolution of empirical software engineering from a personal perspective. It represents what I saw as major milestones in terms of the kind of thinking that affected the nature of the work. I use examples from my own work as I feel that work followed the evolution of the field and is representative of the thinking at various points in time. I try to say where we fell short and where we need to go, in the end discussing the barriers we still need to address. © 2013 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,Basili V.R.,2013,Book Chapter,Perspectives on the Future of Software Engineering: Essays in Honor of Dieter Rombach,10.1007/978-3-642-37395-4_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905451776&doi=10.1007%2f978-3-642-37395-4_17&partnerID=40&md5=3301942c64185d1902cba9eb81929976,"Fraunhofer Center for Empirical Software Engineering, University of Maryland, College Park, United States",Springer-Verlag Berlin Heidelberg,English,,9783642373954; 3642373941; 9783642373947
Scopus,An investigation of software effort phase distribution using compositional data analysis,"One of the most significant problems faced by project managers is to effectively distribute the project resources and effort among the various project activities. Most importantly, project success depends on how well, or how balanced, the work effort is distributed among the project phases. This paper aims to obtain useful information regarding the correlation of the composition of effort attributed in phases for around 1,500 software projects of the ISBSG R11 database based on a promising statistical method called Compositional Data Analysis (CoDA). The motivation for applying this analysis is the observation that certain types of project data (effort distributions and attributes) do not relate in a direct way but present a spurious correlation. Effort distribution is compared to the project life-cycle activities, organization type, language type, function points and other prime project attributes. The findings are beneficial for building a basis for software cost estimation and improving future empirical software studies. © 2012 IEEE.",compositional data analysis; phased effort analysis; software effort distribution,"Chatzipetrou P., Papatheocharous E., Angelis L., Andreou A.S.",2012,Conference,"Proceedings - 38th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2012",10.1109/SEAA.2012.50,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869860214&doi=10.1109%2fSEAA.2012.50&partnerID=40&md5=ace6438a20b1e93c5e56bc41437cb437,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Computer Science, University of Cyprus, Nicosia, Cyprus; Department of Computer Engineering and Informatics, Cyprus University of Technology, Limassol, Cyprus",,English,,9780769547909
Scopus,Cost estimation for model-driven engineering,"Cost estimation studies in model-driven engineering (MDE) are scarce; first, due to difficulty in quantifying qualitative characteristics of MDE that supposedly influence software development effort and second, due to the complexity of measuring varied artifacts that are generated and used in an end-to-end MDE toolset. A cost estimation approach is therefore needed that can incorporate characteristics of MDE that affect economies of scale and effort in application development with the size computation of various artifacts in MDE. We plan to use the constructive cost model (COCOMO) II to obtain baseline cost estimation of MDE applications. Our main contributions are a method to capture the qualitative characteristics of MDE in terms of cost drivers in COCOMO II and a method for computation of various artifacts generated by an MDE toolset. Our initial exploration of these ideas suggests that it is possible to automate cost estimation for MDE. © 2012 Springer-Verlag.",COCOMO II; Cost Estimation; Model-driven Engineering,"Sunkle S., Kulkarni V.",2012,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-33666-9_42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867647963&doi=10.1007%2f978-3-642-33666-9_42&partnerID=40&md5=479eddad93cca10ffd008ed512d1d964,"Tata Research Development and Design Center, Tata Consultancy Services, 54B, Industrial Estate, Hadapsar, Pune 411013, India",,English,03029743,9783642336652
Scopus,Resampling methods in software quality classification,"In the presence of a number of algorithms for classification and prediction in software engineering, there is a need to have a systematic way of assessing their performances. The performance assessment is typically done by some form of partitioning or resampling of the original data to alleviate biased estimation. For predictive and classification studies in software engineering, there is a lack of a definitive advice on the most appropriate resampling method to use. This is seen as one of the contributing factors for not being able to draw general conclusions on what modeling technique or set of predictor variables are the most appropriate. Furthermore, the use of a variety of resampling methods make it impossible to perform any formal meta-analysis of the primary study results. Therefore, it is desirable to examine the influence of various resampling methods and to quantify possible differences. Objective and method: This study empirically compares five common resampling methods (hold-out validation, repeated random sub-sampling, 10-fold cross-validation, leave-one-out cross-validation and non-parametric bootstrapping) using 8 publicly available data sets with genetic programming (GP) and multiple linear regression (MLR) as software quality classification approaches. Location of (PF, PD) pairs in the ROC (receiver operating characteristics) space and area under an ROC curve (AUC) are used as accuracy indicators. Results: The results show that in terms of the location of (PF, PD) pairs in the ROC space, bootstrapping results are in the preferred region for 3 of the 8 data sets for GP and for 4 of the 8 data sets for MLR. Based on the AUC measure, there are no significant differences between the different resampling methods using GP and MLR. Conclusion: There can be certain data set properties responsible for insignificant differences between the resampling methods based on AUC. These include imbalanced data sets, insignificant predictor variables and high-dimensional data sets. With the current selection of data sets and classification techniques, bootstrapping is a preferred method based on the location of (PF, PD) pair data in the ROC space. Hold-out validation is not a good choice for comparatively smaller data sets, where leave-one-out cross-validation (LOOCV) performs better. For comparatively larger data sets, 10-fold cross-validation performs better than LOOCV. © 2012 World Scientific Publishing Company.",classification; genetic programming; multiple regression; prediction; Resampling methods,"Afzal W., Torkar R., Feldt R.",2012,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194012400037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861864123&doi=10.1142%2fS0218194012400037&partnerID=40&md5=73a60c1bab6f0afb410aed8ca012f8d4,"Department of Graduate Studies and Applied Sciences, Bahria University, Islamabad, Pakistan; Blekinge Institute of Technology, 371 79, Karlskrona, Sweden",,English,02181940,
Scopus,"Computer, Network, Software, and Hardware Engineering with Applications","There are many books on computers, networks, and software engineering but none that integrate the three with applications. Integration is important because, increasingly, software dominates the performance, reliability, maintainability, and availability of complex computer and systems. Books on software engineering typically portray software as if it exists in a vacuum with no relationship to the wider system. This is wrong because a system is more than software. It is comprised of people, organizations, processes, hardware, and software. All of these components must be considered in an integrative fashion when designing systems. On the other hand, books on computers and networks do not demonstrate a deep understanding of the intricacies of developing software. In this book you will learn, for example, how to quantitatively analyze the performance, reliability, maintainability, and availability of computers, networks, and software in relation to the total system. Furthermore, you will learn how to evaluate and mitigate the risk of deploying integrated systems. You will learn how to apply many models dealing with the optimization of systems. Numerous quantitative examples are provided to help you understand and interpret model results. This book can be used as a first year graduate course in computer, network, and software engineering; as an on-the-job reference for computer, network, and software engineers; and as a reference for these disciplines. © 2012 Institute of Electrical and Electronics Engineers, Inc.",,Schneidewind N.F.,2012,Book,"Computer, Network, Software, and Hardware Engineering with Applications",10.1002/9781118181287,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891585243&doi=10.1002%2f9781118181287&partnerID=40&md5=1db62d91b60aec98937e63ac1ab5a867,"Department of Information Sciences, Software Engineering Group, Naval Postgraduate School, United States",John Wiley and Sons,English,,9781118037454
Scopus,Optimization of fuzzy analogy in software cost estimation using linguistic variables,"One of the most important objectives of software engineering community has been the increase of useful models that beneficially explain the development of life cycle and precisely calculate the effort of software cost estimation. In analogy concept, there is deficiency in handling the datasets containing categorical variables though there are innumerable methods to estimate the cost. Due to the nature of software engineering domain, generally project attributes are often measured in terms of linguistic values such as very low, low, high and very high. The imprecise nature of such value represents the uncertainty and vagueness in their elucidation. However, there is no efficient method that can directly deal with the categorical variables and tolerate such imprecision and uncertainty without taking the classical intervals and numeric value approaches. In this paper, a new approach for optimization based on fuzzy logic, linguistic quantifiers and analogy based reasoning is proposed to improve the performance of the effort in software project when they arc described in either numerical or categorical data. The performance of this proposed method exemplifies a pragmatic validation based on the historical NASA dataset. The results were analyzed using the prediction criterion and indicates that the proposed method can produce more explainable results than other machine learning methods. © 2012 Published by Elsevier Ltd.",Analogy; Categorical variables; Cost estimation; Dataset; Fuzzy logic; Linguistic values; Project attributes,"Malathi S., Sridhar S.",2012,Conference,Procedia Engineering,10.1016/j.proeng.2012.06.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901047290&doi=10.1016%2fj.proeng.2012.06.025&partnerID=40&md5=c144b62d6ceea63d15f64d82184d3f5d,"Dept of CSE, Sathyabama University, Chennai, India; Dept of CSL and IT, Sathyabama University, Chennai, India",Elsevier Ltd,English,18777058,
Scopus,Effort estimation of web applications through web CMF objects,"Several Content Management Frameworks (CMF), either open source or with a commercial license, are available to help the publication of huge amounts of information and to develop Web applications. Unfortunately, developing Web applications through a CMF is not exempt from cost and time overruns, as in traditional software projects, and currently there is no estimation model able to adequately measure the effort of Web application development. This work presents a new methodology for estimating the effort of Web applications developed with a Content Management Framework (CMF). We present the new key elements for analysis and planning, needed to define every important step in developing a Web application through a CMF. Using those elements, it is possible to estimate the effort needed to build such an application in man-days. We also present the experimental validation of the proposed methodology performed on a 9-project dataset, provided by three different Italian software companies. © 2012 IEEE.",Content Management Framework; Experimental validation; Web application effort estimation; Web Objects,"Corona E., Concas G., Marchesi M., Barabino G., Grechi D.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900873794&doi=10.1109%2fIWSM-MENSURA.2012.12&partnerID=40&md5=6ef57d3e952b73bd9435806d635054d7,"Dept. of Electrical and Electronic Eng., University of Cagliari, Cagliari, Italy; Dept. of Biophysical and Electronic Eng., University of Genova, Genova, Italy",IEEE Computer Society,English,,
Scopus,Case study: COSMIC approximate sizing approach without using historical data,"In mature engineering disciplines, international consensus can be reached on measurement, as evidenced through established measurement standards. In software engineering, there are 5 functional size measurement standards. These standards work best when the functionality to be measured is fully known, although this usually doesn't happen in the early phases of software development. The techniques most often used to approximate the sizing of the software to be developed in the early phases involve historical data. However, gathering historical data is a challenge in itself. This paper proposes the use of a fuzzy logic model to approximate the functional size of a piece of software. © 2012 IEEE.",Approximate Sizing; COSMIC; EPCU; FSM; Functional Size; Fuzzy Logic,"Souto F.V., Abran A.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900853823&doi=10.1109%2fIWSM-MENSURA.2012.34&partnerID=40&md5=f1dda2e84f8599421454aa4fd8ad3b28,"École de Technologie Supérieure, University of Québec, Dept. of Software Engineering, Montréal, Canada",IEEE Computer Society,English,,
Scopus,Predicting software defects: A cost-sensitive approach,"Find software defects is a complex and slow task which consumes most of the development budgets. In order to try reducing the cost of test activities, many researches have used machine learning to predict whether a module is defect-prone or not. Defect detection is a cost-sensitive task whereby a misclassification is more costly than a correct classification. Yet, most of the researches do not consider classification costs in the prediction models. This paper introduces an empirical method based in a COCOMO (COnstructive COst MOdel) that aims to assess the cost of each classifier decision. This method creates a cost matrix that is used in conjunction with a threshold-moving approach in a ROC (Receiver Operating Characteristic) curve to select the best operating point regarding cost. Public data sets from NASA (National Aeronautics and Space Administration) IV&V (Independent Verification & Validation) Facility Metrics Data Program (MDP) are used to train the classifiers and to provide some development effort information. The experiments are carried out through a methodology that complies with validation and reproducibility requirements. The experimental results have shown that the proposed method is efficient and allows the interpretation of the classifier performance in terms of tangible cost values. © 2011 IEEE.",COCOMO; Defect prediction; machine learning; MDP; NASA; pattern recognition; ROC curve; software metrics; testing costs,"Bezerra M.E.R., Oliveira A.L.I., Adeodatoz P.J.L.",2011,Conference,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",10.1109/ICSMC.2011.6084055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755163956&doi=10.1109%2fICSMC.2011.6084055&partnerID=40&md5=b44189a13dd4ed735402c0118c9f678d,"Center of Informatics, Federal University of Pernambuco, UFPE, Recife PE, 50.732-970, Brazil",,English,1062922X,9781457706523
Scopus,An analysis of trends in productivity and cost drivers over years,"Background: Software engineering practices have evolved considerably over the last four decades, changing the way software systems are developed and delivered. Such evolvement may result in improvements in software productivity and changes in factors that affect productivity. Aims: This paper reports our empirical analysis on how changes in software engineering practices are reflected in COCOMO cost drivers and how software productivity has evolved over the years. Method: The analysis is based on the COCOMO data set of 341 software projects developed between 1970 and 2009. We analyze the productivity trends over the years, comparing productivity of different types and countries. To explain the overall impact of cost drivers on productivity and explain its trends, we propose a measure named Difficulty which is based on the COCOMO model and its cost drivers. Results: The results of our analysis indicate that the overall productivity of the projects in the data set has increased noticeably over the last 40 years. Our analysis also shows that the productivity trends and productivity variability can be explained by using the proposed Difficulty measure. Conclusions: Our analysis provides empirical evidence that the productivity trends can be characterized by the improvements in software tools, processes, and platforms among other factors. The Difficulty measure can be used to justify and compare productivity among projects of different characteristics, e.g., different domains, platforms, complexity, and personnel experience. Although we define the measure using the COCOMO cost drivers, it may not fully represent the most important factors influencing productivity. One direction for our future work is to analyze the effectiveness of the measure using more cost drivers on more data points. Copyright © 2011 ACM.",COCOMO; Cost drivers; Product factors; Productivity trends; Software productivity,"Nguyen V., Huang L.G., Boehm B.",2011,Conference,ACM International Conference Proceeding Series,10.1145/2020390.2020393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054053209&doi=10.1145%2f2020390.2020393&partnerID=40&md5=9c9185a678071605d39a05e60225ec11,"Faculty of Information Technology University of Science, Vietnam National University - Ho Chi Minh City, 227 Nguyen Van Cu, Ho Chi Minh city, Viet Nam; Computer Science and Engineering Dept., Southern Methodist University, PO Box 750122, Dallas, TX 75275-0122, United States; Computer Science Department, University of Southern California, 941 W. 37th Pl, SAL 326, Los Angeles, CA 90089, United States",,English,,9781450307093
Scopus,Data quality: Cinderella at the software metrics ball?,"In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since - whether we like it or not - our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers. © 2011 ACM.",data quality; empirical research; software metrics,Shepperd M.,2011,Conference,Proceedings - International Conference on Software Engineering,10.1145/1985374.1985376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959818838&doi=10.1145%2f1985374.1985376&partnerID=40&md5=834ab31572e7638d734a4a2b47d48039,"Dept. of Information Systems and Computing, Brunel University, Uxbridge, UB8 3PH, United Kingdom",,English,02705257,9781450305938
Scopus,Function point measurement from Web application source code based on screen transitions and database accesses,"A function point (FP) is a unit of measurement that expresses the degree of functionality that an information system provides to a user. Many software organizations use FPs to estimate the effort required for software development. However, it is essential that the definition of 1 FP be based on the software development experience of the organization. In the present study, we propose a method by which to automatically extract data and transaction functions from Web applications under several conditions using static analysis. The proposed method is based on the International Function Point Users Group (IFPUG) method and has been developed as an FP measurement tool. We applied the proposed method to several Web applications and examined the difference between FP counts obtained by the tool and those obtained by a certified FP specialist (CFPS). The results reveal that the numbers of data and transaction functions extracted by the tool is approximately the same as the numbers of data and transaction functions extracted by the specialist. © 2011 Elsevier Inc. All rights reserved.",Empirical Software Engineering; Estimation; Function point; IFPUG; Web application,"Edagawa T., Akaike T., Higo Y., Kusumoto S., Hanabusa S., Shibamoto T.",2011,Journal,Journal of Systems and Software,10.1016/j.jss.2011.01.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953709101&doi=10.1016%2fj.jss.2011.01.029&partnerID=40&md5=6641bcfea5a0ea2f40eef454c39960cf,"Department of Computer Science, Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, Osaka 565-0871, Japan; Hitachi Systems and Services, Ltd., Otaku, Tokyo, Japan",,English,01641212,
Scopus,Regularities in learning defect predictors,"Collecting large consistent data sets of real world software projects from a single source is problematic. In this study, we show that bug reports need not necessarily come from the local projects in order to learn defect prediction models. We demonstrate that using imported data from different sites can make it suitable for predicting defects at the local site. In addition to our previous work in commercial software, we now explore open source domain with two versions of an open source anti-virus software (Clam AV) and a subset of bugs in two versions of GNU gcc compiler, to mark the regularities in learning predictors for a different domain. Our conclusion is that there are surprisingly uniform assets of software that can be discovered with simple and repeated patterns in local or imported data using just a handful of examples. © 2010 Springer-Verlag.",Code metrics; Cross-company; Defect prediction; Software quality,"Turhan B., Bener A., Menzies T.",2010,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-13792-1_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955461370&doi=10.1007%2f978-3-642-13792-1_11&partnerID=40&md5=ad18b95583f91734620a63c970029204,"Department of Information Processing Science, University of Oulu, Oulu 90014, Finland; Department of Computer Engineering, Boǧaziçi University, Istanbul 34342, Turkey; Lane Dept. of CS and EE, West Virginia University, Morgantown, WV, United States",,English,03029743,3642137911; 9783642137914
Scopus,EsrcTool: A tool to estimate the software risk and cost,"Function Point is a well known established method to estimate the size of software projects. There are several areas of the software engineering in which we can use the function point analysis (FPA) like project planning, project construction, software implementation etc. In this paper we have used the function point approach in order to develop the architecture of the esrcTool. This tool is used for two different purposes, firstly, to estimate the risk in the software and secondly to estimate the cost of the software. In the literature of software engineering there are so many models to estimate the risk in the software like Soft Risk Model, SRAM, SRAEM and so on. But in the esrc Tool we have used SRAEM i.e. Software Risk Assessment and Estimation Model, because in this model FP is used as an input variable, and on the other hand side, in order to determine the cost of the software we have used the International Software Benchmarking Standards Group Release Report (ISBSG). © 2010 IEEE.",Cost; FPA; Software risk; SRAEM,"Sadiq M., Rahman A., Ahmad S., Asim M., Ahmad J.",2010,Conference,"2nd International Conference on Computer Research and Development, ICCRD 2010",10.1109/ICCRD.2010.29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955224923&doi=10.1109%2fICCRD.2010.29&partnerID=40&md5=fa4e6a2f1bb48c84397322640797d039,"University Polytechnic, Faculty of Engineering and Technology, Jamia Millia Islamia (A Central University ), New Delhi-25, India; M.Tech. Scholar, Department of Computer Science and Engineering, AL-Falah School of Engineering and Technology, Dhauj,Faridabad, Haryana., India",,English,,9780769540436
Scopus,Comparing the estimation performance of the EPCU model with the expert judgment estimation approach using data from industry,"Software project estimates are more useful when made early in the project life cycle: this implies that these estimates are to be made in a highly uncertain environment with information that is vague and incomplete. To tackle these challenges in practice, the estimation method most used at this early stage is the Expert Judgment Estimation approach. However, there are a number of problems with it, such as the fact that the expertise is specific to the people and not to the organization, and the fact that this intuitive estimation expertise is neither well described nor well understood; in addition, the expertise is difficult to assess and cannot be replicated systematically. Estimation of Projects in Contexts of Uncertainty (EPCU) is an estimation method based on fuzzy logic that mimics the way experts make estimates. This paper describes the experiment designed and carried out to compare the performance of the EPCU model against the Expert Judgment Estimation approach using data from industry projects. © 2010 Springer-Verlag Berlin Heidelberg.",EPCU; Estimation projects; Expert judgment estimation; Fuzzy sets; Uncertainty contexts,"Valdés F., Abran A.",2010,Journal,Studies in Computational Intelligence,10.1007/978-3-642-13273-5_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952708554&doi=10.1007%2f978-3-642-13273-5_15&partnerID=40&md5=3985c347cfa95efa74f20bc6d8039ed8,"Dept. of Software Engineering, École de Technologie Supérieure, Montréal, Canada",,English,1860949X,9783642132728
Scopus,Improving a web usability inspection technique using qualitative and quantitative data from an observational study,"We have proposed a checklist-based usability inspection technique (WDP - Web Design Perspectives-Based Usability Evaluation) specific for Web applications' usability evaluation and we are following an experimentation-based methodology to support its development and improvement. This paper describes an observational study conducted, aimed at eliciting how inspectors apply the WDP technique. We discuss the quantitative and qualitative results of the result and their impact on improving the WDP technique. We analyzed the qualitative data using the procedures from the Grounded Theory (GT) method. © 2009 IEEE.",Inspection technique; Observational study; Qualitative analysis; Web usability evaluation,"Conte T., Vaz V., Massolar J., Mendes E., Travassos G.H.",2009,Conference,SBES 2009 - 23rd Brazilian Symposium on Software Engineering,10.1109/SBES.2009.25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049091762&doi=10.1109%2fSBES.2009.25&partnerID=40&md5=68d1336d8a592210282a35c2024b4bf6,"Computer Science Department (DCC), Federal University of Amazonas (UFAM), Manaus, Brazil; Systems Engineering and Computer Science Program, COPPE, UFRJ, Rio de Janeiro, Brazil; Computer Science Department, University of Auckland, Auckland, New Zealand",,English,,9780769538440
Scopus,The impact of complexity on software design quality and costs: An exploratory empirical analysis of open source applications,"It is well known that complexity affects software development and maintenance costs. In the Open Source context, the sharing of development and maintenance effort among developers is a fundamental tenet, which can be thought as a driver to reduce the impact of complexity on maintenance costs. However, complexity is a structural property of code, which is not quantitatively accounted for in traditional cost models. This paper introduces the concept of functional complexity, which weights the well-established McCabe's cyclomatic complexity metric to the number of interactive functional elements that an application provides to users. Such metric is used to analyze how Open Source development costs are affected by complexity. Traditional cost models, like CoCoMo, do not take into account the impact of complexity in estimating costs by means of accurate indicators. In contrast, results show how a higher complexity is associated with a lower design quality of code, and, hence, higher maintenance costs. Consequently, results suggest that a reliable effort estimation should be based on a precise evaluation of software complexity. Analyses are based on quality, complexity, and maintenance effort data collected for 59 Open Source applications (corresponding to 906 versions) selected from the SourceForge.net repository.",Costs and quality; Open source software complexity,"Francalanci C., Merlo F.",2008,Conference,"16th European Conference on Information Systems, ECIS 2008",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870648497&partnerID=40&md5=5b278a02bb82b94941aaee0747aac771,"Politecnico di Milano, Dipartimento di Elettronica e Informazione, via Ponzio 34/5, I-20133 Milano, Italy",,English,,
Scopus,Any other cost estimation inhibitors?,"The purpose of this qualitative multiple case study is to research the current causes to cost estimation errors and revisit the research question of a well published quantitative study to explore whether responses would be consistent using qualitative methodology. Both overlaps and deviations are to be expected as a result of this comparison. The focus of this paper is on differences where new issues are likely to be found. The 8 resulting cross case cost estimation inhibitors validate 7 out of 16 issues found in the quantitative study. They are also related to the theory within and outside the area of software engineering resulting in five underlying software cost estimation inhibitors and providing a deeper understanding for the snapshot of the current causes of cost estimation errors reported by the practitioners. Lastly, the question of the practitioners' ability to report upon software cost estimation inhibitors is discussed. We argue that there might be causes of cost estimation errors that practitioners might be unwilling to reveal or might even be completely unaware of. Thus, they might unconsciously be providing researchers with invalid data. Copyright 2008 ACM.",Automotive; Case study; Cost estimation; Empirical software engineering,"Magazinović A., Pernstål J.",2008,Conference,ESEM'08: Proceedings of the 2008 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,10.1145/1414004.1414042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949228225&doi=10.1145%2f1414004.1414042&partnerID=40&md5=dcffe0bd7328a1aa0f95c439a98bcfd3,"Chalmers Computer Science and Engineering, SE - 421 96 Gothenburg, Sweden",,English,,9781595939715
Scopus,Value estimation for software product management,"Value-based approach of software engineering proved to be one of the most important branches of software engineering because it elicits and reconciles stakeholder's value propositions with respect to the system into a mutually satisfactory set of objectives for the system [2]. Thus most of software organizations in market-driven environment nowadays adopt value-based approach with the focus on maximizing the value gained from their products against consumed resources. This leads to a need for a value estimation methodology to incorporate all the software product value aspects altogether while measuring the product value. Most of the existing methodologies focus on measuring product financial value and neglect the non-financial value [6]. Value point measurement will facilitate quantifying the total value obtained from the product and compare it against planned product budget at early phases of the product life cycle. Such comparison will be used as a sort of justification for product feasibility. This paper illustrates a new estimation methodology for the software product value called ""Value Point"". VP measures value gained from the software product through quantifying value obtained from each product requirement [5]. The process for value point counting will be illustrated through a designed product management framework. A case study is performed to demonstrate the added value from the proposed methodology. © 2008 IEEE.",Product management; Release planning; Value point; Value-based estimation,"Mohamed S.I., Wahba A.M.",2008,Conference,"2008 IEEE International Conference on Industrial Engineering and Engineering Management, IEEM 2008",10.1109/IEEM.2008.4738261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749192624&doi=10.1109%2fIEEM.2008.4738261&partnerID=40&md5=0de6efa8ec74b87cc8e0f10105967602,"Department of computer and system Engineering, Ain Shams University, Cairo, Egypt",,English,,9781424426300
Scopus,Incomplete-case nearest neighbor imputation in software measurement data,"Missing values are commonly encountered in software measurement data, and nearest neighbor imputation (kNNI) is one of the most popular imputation procedures used by researchers and practitioners in empirical software enginee ring. Imputation techniques are used to replace missing values with one or more alternatives. Traditionally, kNNI uses only complete cases as possible donors for imputation (called compl ete case kNNI or CCkNNI), however a variant of CCkNNI called incomplete casek nearest neighbor imputation (ICkNNI) is an attractive alternative which has received very little at tention. We present a detailed comparative study of CCkNNI and ICkNNI with missing software measurement data, and demonstrate that using incomplete cases often increases the effe ctiveness of nearest neighbor imputation (especially at higher m issingness levels), regardless of the type of missingness. © 2007 IEEE.",,"Van Hulse J., Khoshgoftaar T.M.",2007,Conference,"2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007",10.1109/IRI.2007.4296691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949104761&doi=10.1109%2fIRI.2007.4296691&partnerID=40&md5=5d6aa9220907cf09cbeab489963a2104,"Department of Computer Science and Engineering, Florida Atlantic University, Boca Raton, FL, United States",,English,,1424414997; 9781424414994
Scopus,Software development as a design or a production project: An empirical study of project monitoring and control,"Purpose - The paper seeks to investigate whether project managers regard software development projects as design problems or production problems. Design/methodology/approach - Project management literature was examined to determine what evidence there should be to indicate whether a software development project was regarded as a problem to be solved or a product to be produced. Data were then collected through structured interview of project managers currently engaged in managing software development projects. The data were analysed to determine how project managers regarded their projects and whether this matched a theoretical expectation. Findings - The empirical data indicated that most project managers regard their projects as production problems, where it is assumed that the underlying problem is largely understood, the project encapsulated in a planned schedule of activities and there will be an emphasis on monitoring the project against the planned progress. Research limitations/implications - Owing to the small sample size of fewer than 30 project managers, external validity is weak. More research is needed to confirm these results over a larger sample and to probe more subtle orientation to production or design projects. Practical implications - The research developed a simple test to indicate the degree of novelty of the application to be developed. The test indicates whether the application is novel and should be treated as a design problem, or well known and therefore should be treated as a production problem. Originality/value - The paper draws attention to the need for project managers to evaluate the type of application to be developed and to adopt an appropriate project management approach. The paper also provides a simple test to achieve that objective. © Emerald Group Publishing Limited.",Design management; Outsourcing; Production management; Project management; Software engineering,"McBride T., Henderson-Sellers B., Zowghi D.",2007,Journal,Journal of Enterprise Information Management,10.1108/17410390710717147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846053288&doi=10.1108%2f17410390710717147&partnerID=40&md5=c83486d741b4c6eb662e5763a1458a1a,"Faculty of Information Technology, University of Technology, Sydney, Australia",,English,17410398,
Scopus,"A project management approach to using simulation for cost estimation on large, complex software development projects","An engineering manager can use this article to develop an approach to estimating cost and scheduling for large, complex software development projects that considers the inherent diffi culties of developing accurate estimates for such projects. None of the approaches or tools available today can estimate the true cost of software with any high degree of accuracy early in a project. This article provides an approach that uses a software development process simulation model that considers and conveys the level of uncertainty that exists when developing an initial estimate. A NASA project will be analyzed using simulation and data from the Software Engineering Laboratory to show the benefi ts of such an approach. © 2007 by the American Society for Engineering Management.",Cost estimation; Discrete event simulation; Project management; Software development,"Mizell C., Malone L.",2007,Journal,EMJ - Engineering Management Journal,10.1080/10429247.2007.11431746,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949192824&doi=10.1080%2f10429247.2007.11431746&partnerID=40&md5=2bb0e936fa0d87b4f3fa559324db8589,"NASA, Kennedy Space Center, United States; University of Central Florida, United States",,English,10429247,
Scopus,A study on the distribution and cost prediction of requirements changes in the software life-cycle,"Software development is a dynamic process. Requirements change (RC) is inevitable and brings great challenges to the software development. How to precisely predict requirements change is especially important in the field of requirements engineering. In this paper, an assessment framework for the factors of RCs' distribution is constructed firstly. Apart from the rough prediction method based on the statistic process control of RCs, an artificial neural network method for predicting RCs' distribution is presented. In this case, the weight of each factor is calculated by a fuzzy logic method, called experts ranking. Furthermore, we propose a model to pre-evaluate the cost caused by RCs. With some practical projects data, a validation experiment has been drawn, whose result shows that our method and model are practical and efficient to predict the distribution and cost of RCs. © Springer-Verlag Berlin Heidelberg 2005.",,"Mao C., Lu Y., Wang X.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11608035_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745166925&doi=10.1007%2f11608035_14&partnerID=40&md5=0063c17acb16917d5afa14bb82d22bcb,"College of Computer Science and Technology, Huazhong University of Science and Technology, 430074 Wuhan, China",,English,03029743,3540311122; 9783540311126
Scopus,Improving dynamic calibration through statistical process control,"Dynamic Calibration (DC), presented by the authors in previous works has proved to be a flexible approach for massive maintenance software project estimation, able to recalibrate an estimation model in use according to relevant process performance changes pointed out by the Project Manager. Nevertheless, it results quite subjective in its application and tightly based on manager experience. In this work the authors present an improvement of the approach based on the use of Statistical Process Control (SPC) technique. SPC is a statistically based method able to quickly highlight shift in process performances. It is well known in manufacturing contexts and it has recently emerged in the software engineering community. In this work, authors have integrated SPC in DC as decision support tool for identifying when recalibration of the estimation model must be carried out. This extension makes DC less ""person-based"", more deterministic and transferable in its use than the previous version. The extended approach has been experimented on industrial data related to a renewal project and the results compared with both, a concurrent approach such as analogy based estimation and its previous version. The results are encouraging and stimulate further investigation. © 2005 IEEE.",,"Baldassarre M.T., Boffoli N., Caivano D., Visaggio G.",2005,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2005.53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646915753&doi=10.1109%2fICSM.2005.53&partnerID=40&md5=8edc9cfbb29155466b9a231c29331e63,"Department of Informatics, University of Bari, RCOST, Bari, Italy",,English,,
Scopus,An integrated framework for simulation-based software process improvement,"In this article, we present an integrated framework for software process improvement according to the Capability Maturity Model (CMM). The framework is double-integrated. First, it is based on the systematic integration of dynamic modules to build dynamic models that model and simulate each maturity level proposed in the reference model. As a consequence, a hierarchical set of dynamic models is developed following the same hierarchy of levels suggested in the CMM. Second, the dynamic models of the framework are integrated with different static techniques commonly used in planning, control, and process evaluation. The paper describes the reasons found to follow this approach, the integration process of models and techniques, the implementation of the framework, and shows an example of how it can be used in a software process improvement concerning the cost of software quality. Copyright © 2004 John Wiley & Sons, Ltd.",Model reusability; Modeling; Simulation; Software process improvement,"Ruiz M., Ramos I., Toro M.",2004,Conference,Software Process Improvement and Practice,10.1002/spip.198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12844260151&doi=10.1002%2fspip.198&partnerID=40&md5=9187a2dc47930577f411df7609af4f3d,"Department of Computer Languages, University of Cádiz, Spain; Department of Computer Languages, University of Seville, Spain; Department of Computer Languages, Escugla Superior de Ingenieria, C/Chile 1, 1003 Cádiz, Spain",,English,10774866,
Scopus,Assessing COTS assessment: how much is enough?,"COTS products are now ubiquitous and clearly have become a key factor in modern software systems development. If COTS are chosen poorly, a project will likely fail. As a result, the careful assessment of COTS products has become an essential element of the development process. There are numerous approaches to COTS assessment; however none of them address the crucial question of how much assessment effort to perform. If too little assessment is done, inappropriate COTS may be used; if too much assessment is done, the effort expended may place the project at risk. It is important to achieve a satisfactory balance between COTS uncertainty risks and risks resulting from project delay. To address this, we develop a method for the strategic planning of COTS assessment by determining ""how much is enough"" effort (in time, cost, or quality) with respect to critical project risk factors such as project schedule, market window, and a multitude of COTS assessment attributes such as availability, ease of use, maturity, and vendor support. The method is practical, and provides valuable aid in the planning of COTS based system development.© Springer-Verlag 2004.",COTS; COTS assessment; COTS evaluation; COTS integration; Software risk,"Port D., Chen S.",2004,Journal,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-24645-9_29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048814693&doi=10.1007%2f978-3-540-24645-9_29&partnerID=40&md5=e9729d14df56e750cf93b71dd3707cc8,"Department of Information Technology Management, University of Hawaii at Manoa, Honolulu, HI, United States; Center for Software Engineering, University of Southern California, Los Angeles, CA, United States",Springer Verlag,English,03029743,354021903X; 9783540219033
Scopus,Supporting decision-making in software engineering with process simulation and empirical studies,"Decision-making is a complex and important task in software engineering. The current state-of-the-practice is rather non-systematic as it typically relies upon personal judgment and experience without using explicit models. Empirical studies can help but they are costly to conduct and, to some extent, context dependent. Typically it is not efficient or even possible to conduct empirical studies for a large number of context parameter variations. Process simulation offers decision support as well, but currently suffers from a lack of empirical knowledge on the determinants of underlying system dynamics. In this paper we present an assessment of empirical knowledge and simulation techniques for the area of quality assurance planning. There is a strong interdependency between process simulation and empirical models for decision-making in this area: (a) profound empirical knowledge enables process simulation to support decision-making, and (b) the analysis of simulation results can point out situations and factors for which conducting empirical studies would be most worthwhile. This paper discusses critically some of the most important challenges for decision-making in the area of quality assurance planning.",Decision-making support; Empirical models; Process simulation; Software quality planning,"Rus I., Halling M., Biffl S.",2003,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194003001391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348146383&doi=10.1142%2fS0218194003001391&partnerID=40&md5=58b45d89afb0e5b34691e5e883461784,"Fraunhofer Center Maryland, 4321 Hartwick Rd, Suite 500, College Park, MD 20740, United States; Johannes Kepler University of Linz, Linz, Austria; Vienna University of Technology, Vienna, Austria",,English,02181940,
Scopus,A software process improvement support system: spis,"The Capability Maturity Model (CMM) proposed by the Software Engineering Institute (SEI) of Carnegie Mellon University (CMU) is an effective framework for software process improvement (SPI) because it can increase development productivity and improve software quality for many software development companies. However, the CMM only indicates 'what' needs to be improved, it does not indicate 'how to' perform software process improvement activities (SPIA). In this paper we describe a Software Process Improvement support System (SPIS) that does indicate 'how to' perform SPIA. It is designed to be used mainly for internal SPIA by software development companies and their project teams. The SPIS is composed of seven support tools and promotes SPIA. The tools include a specific capability maturity model (SCMM), rules for assessing a company's maturity level, and an assessor qualification system for organizing the assessment team. These tools were extracted deductively from our experience and have proved to be effective when used for the SPIA at our company. ®: CMM is registered trademarks in the U.S. Patent and Trademark Office. SM: CMM is a service mark of Carnegie Mellon University.",CMM; Metrics; QC; Software process,"Fukuyama S.-I., Miyamura S.-I., Takagi H., Tanaka R.",2000,Journal,IEICE Transactions on Information and Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033706035&partnerID=40&md5=ee356960e311bff9765588e07b1cfa8a,"NTT Software Corporation, Yokohama-shi, 231-8551, Japan","Institute of Electronics, Information and Communication, Engineers, IEICE",English,09168532,
Scopus,A Rule-Based Approach to Developing Software Development Prediction Models,"Managers of software development projects increasingly recognize the importance of planning and estimation and now have many sophisticated tools at their disposal. Despite this many systems are still delivered way behind schedule, cost far more to produce than original budget estimates and fail to meet user requirements. It is the contention of the authors that many existing tools are inadequate because they fail to embrace the significant body of knowledge accumulated by past and present project managers. This paper presents a new approach to planning which enables project managers to learn from the experience of others. The authors have adopted a bottom-up approach to planning which goes from the specific (planning the requirements capture and analysis process - RCA) to the general (planning the whole development process). A model, called MARCS, was constructed to give predictions of the resources (time, effort, cost, people) needed for the completion of and outcomes of the RCA process. Based on the predictions about the RCA process, the model then attempts to predict the resources and outcomes of the whole development process. MARCS is a combination of rule-based models and its main advantage is that it incorporates both qualitative and quantitative factors that can be easily identified and measured in the beginning of the development process. Empirical data concerning 107 projects developed by more than 70 organizations within UK, gathered through a two-stage mail survey was used for the construction and validation of the MARCS planning model.",Decision rules; IS project planning; Planning models; Requirements,"Chatzoglou P.D., Macaulay L.A.",1998,Journal,Automated Software Engineering,10.1023/A:1008621131645,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032047894&doi=10.1023%2fA%3a1008621131645&partnerID=40&md5=58289252f56fb55e3bd0fec8ce6dcdbb,"Dept. of Public and Business Admin., Univ. of Cyprus, P.O. Box 537, CY 1678 Nicosia, Cyprus; Department of Computation, UMIST, P.O. Box 88, Manchester, M60 1QD, United Kingdom",Springer Netherlands,English,09288910,
Scopus,A software metric for cost estimation and efficiency measurement in data processing system development,"The three basic structural elements of a data processing system are shown to be files, flows, and processes. A metric for the size of a data processing system is introduced which is a function of the number of files, flows, and processes of the system. The validity and reliability of this metric are demonstrated. It is shown how the metric may be used to estimate the cost of developing a data processing system at an early stage in the development process. Furthermore, it is demonstrated how the metric may be used to determine the efficiency of data processing system development. © 1983.",,"van der Poel K.G., Schach S.R.",1983,Journal,The Journal of Systems and Software,10.1016/0164-1212(83)90033-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020816236&doi=10.1016%2f0164-1212%2883%2990033-X&partnerID=40&md5=b691a3df860bc9d7080c634818e8c2e7,"Groote Schuur Hospital, South Africa; University of Cape Town, South Africa",,English,01641212,
Scopus,The end user requirement for project management software accuracy,"This research explains the relationship between the end user requirement and accuracy of PMS (Project Management Software). The research aims are to analyze the PMS accuracy and measuring the probability of PMS accuracy in achieving ±1% of the end user requirement. The bias statistical method will be used to prove the PMS accuracy that based on the hypothesis testing. The result indicates the PMS is still accurate to be implemented in Aceh-Indonesia area projects that using the SNI (National Indonesia Standard as current method) with the accuracy index of ±7.5%. The achievement probability of reaching the end user requirement is still low of ±21.77%. In case of the PMS, the low achievement of the end user requirement is not only caused by the low accuracy of the PMS but also caused by the amount of variability error, which is influenced by the amount of variation of the project activity. In this study, we confirm that it is necessary to reconcile both conditions between the PMS accuracy and the end user requirements. © Copyright 2018 Institute of Advanced Engineering and Science. All rights reserved.",Accuracy index; Achievement probability; Bias%; End user requirement; Hypothesis testing; National Indonesia standard; Project management software,Fachrurrazi,2018,Journal,International Journal of Electrical and Computer Engineering,10.11591/ijece.v8i2.pp1112-1121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042861191&doi=10.11591%2fijece.v8i2.pp1112-1121&partnerID=40&md5=e200f5a79dc7587aa1f9febc7acca8f8,"Department of Civil Engineering, Syiah Kuala University, Aceh, Indonesia",Institute of Advanced Engineering and Science,English,20888708,
Scopus,Characterizing Software Developers by Perceptions of Productivity,"Understanding developer productivity is important to deliver software on time and at reasonable cost. Yet, there are numerous definitions of productivity and, as previous research found, productivity means different things to different developers. In this paper, we analyze the variation in productivity perceptions based on an online survey with 413 professional software devel-opers at Microsoft. Through a cluster analysis, we identify and describe six groups of developers with similar perceptions of productivity: Social, lone, focused, balanced, leading, and goal-oriented developers. We argue why personalized recommendations for improving software developers' work is important and discuss design implications of these clusters for tools to support developers' productivity. © 2017 IEEE.",perceptions; Productivity; software developers,"Meyer A.N., Zimmermann T., Fritz T.",2017,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/ESEM.2017.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042370191&doi=10.1109%2fESEM.2017.17&partnerID=40&md5=47945710ea4850317c9ce329dd21d1ac,"University of Zurich, Zurich, Switzerland; Microsoft Research, Redmond, WA, United States; University of British Columbia, Vancouver, BC, Canada",IEEE Computer Society,English,19493770,9781509040391
Scopus,Development Effort Estimation in HPC,"In order to cover the ever increasing demands for computational power, while meeting electrical power and budget constraints, HPC systems are continuing to increase in hardware and software complexity. As a direct consequence, this also leads to increased development efforts to parallelize, tune or port applications. For an informed decision on how to spend available budgets, we therefore need quantitative metrics to estimate the development effort in HPC. While development effort estimation is widely used in software engineering, applying it to HPC, with its strong focus on performance, is not straightforward. In this paper, we first review existing approaches of effort estimation for general computing and then derive a novel methodology to estimate development effort specifically targeted at HPC. Further, we propose a concept to identify factors impacting development effort and encapsulate it in an effort log tool to collect data on development time. © 2016 IEEE.",,"Wienke S., Miller J., Schulz M., Muller M.S.",2016,Conference,"International Conference for High Performance Computing, Networking, Storage and Analysis, SC",10.1109/SC.2016.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992617059&doi=10.1109%2fSC.2016.9&partnerID=40&md5=7897133fdaff81e0747cbe3fa89a619d,"IT Center, RWTH Aachen University, Aachen, Germany; JARA.High-Performance Computing, Aachen, Germany; Lawrence Livermore National Laboratory, Livermore, CA, United States",IEEE Computer Society,English,21674329,9781467388153
Scopus,Comparison of back propagation training algorithms for software defect prediction,"The cost of deleting a software bug increases ten times as it is floated onto the next phase of software development lifecycle (SDLC). This makes the task of the project managers difficult and also degrades the quality of the output software product. Software defect prediction (SDP) was proposed as a solution to the problem which could anticipate the defective modules and hence, deal with them in an efficient and effective manner in advance. The adequacy of artificial neural networks (ANNs) to handle the complex nonlinear relationships between the software metrics and the defect data demonstrates their suitability to build the defect prediction models. In this paper, multilayer feed forward back propagation based neural networks were constructed using seven defect datasets from the PROMISE repository. An empirical comparison of Levenberg-Marquardt (LM), Resilient back propagation (RP) and Bayesian Regularization (BR) back propagation training algorithms was performed using statistical measures such as MSE and R2 values and the parameters computed from the confusion matrix. Bayesian based back propagation training method performed better than the LM and RP techniques in terms of minimizing mean square error and type II error and maximizing accuracy, sensitivity and R2 value. An accuracy of more than 90 percent was achieved by BR on all the seven datasets and the best data fit during the regression analysis was shown with a R2 value of 0.96. Overall, it is the context and the criticality of the software project which will aid the project managers to prioritize the performance measures and hence, decide upon the training algorithm to be applied, according to the goals and resources available. © 2016 IEEE.",artificial neural network; back propagation; Bayesian regularization; Levenberg-Marquardt; resilient back propagation; software defect; software metrics,"Arora I., Saha A.",2016,Conference,"Proceedings of the 2016 2nd International Conference on Contemporary Computing and Informatics, IC3I 2016",10.1109/IC3I.2016.7917934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020024693&doi=10.1109%2fIC3I.2016.7917934&partnerID=40&md5=02ea5b9f32aa3c85dd1fdb6a80b56536,"University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Dwarka, Delhi, 110078, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509052554
Scopus,Software fault prediction using Mamdani type fuzzy inference system,"High quality software requires the occurrence of minimum number of failures while software runs. Software fault prediction is the determining whether software modules are prone to fault or not. Identification of the modules or code segments which need detailed testing, editing or, reorganising can be possible with the help of software fault prediction systems. In literature, many studies present models for software fault prediction using some soft computing methods which use training/testing phases. As a result, they require historical data to build models. In this study, to eliminate this drawback, Mamdani type fuzzy inference system (FIS) is applied for the software fault prediction problem. Several FIS models are produced and assessed with ROC-AUC as performance measure. The results achieved are ranging between 0.7138 and 0.7304; they are encouraging us to try FIS with the different software metrics and data to demonstrate general FIS performance on this problem. © 2016 Inderscience Enterprises Ltd.",FIS; Fuzzy inference system; Method-level metrics; Software fault prediction,"Erturk E., Sezer E.A.",2016,Conference,International Journal of Data Analysis Techniques and Strategies,10.1504/IJDATS.2016.075971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978414683&doi=10.1504%2fIJDATS.2016.075971&partnerID=40&md5=b60806eb1baf9abb196b3030295f390a,"Scientific and Technological Research Council of Turkey (TUBITAK), Software Technologies Research Institute, Ankara, 06100, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, 06800, Turkey",Inderscience Publishers,English,17558050,
Scopus,Software cost estimating for CMMI Level 5 developers,"This article provides analysis results of Capability Maturity Model Integrated Level 5 projects for developers earning the highest level possible, using actual software data from their initial project estimates. Since there were no measures to verify software performance, this level was used a proxy for high quality software. Ordinary least squares regression was used to predict final effort hours with initially estimated variables obviates the need to estimate growth or shrinkage for typical changes occurring in software projects, regardless of software developer (contracted or in-house). The OLS equations, or cost estimating relationship equations, were evaluated by a series of standards: statistical significance, visual inspection, goodness of fit measures, and academically set thresholds for accuracy measures used in software cost estimating: mean magnitude of relative error and prediction (for determining the percentage of records with 25%, or less, based on their magnitude of relative error score). As several initial estimated variables were strongly correlated to the reported final effort hours and each other, each variable was examined separately. Thirty records from software projects completed in 2003-2008 for the highest process maturity level were used to compute statistically significant equations with implicit growth or shrinkage in their make-up.",CMMI Level 5; Ordinary least squares regression; Software effort estimation; US DOD,"Wallshein C.C., Loerch A.G.",2015,Journal,Journal of Systems and Software,10.1016/j.jss.2015.03.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929176084&doi=10.1016%2fj.jss.2015.03.069&partnerID=40&md5=5009ba67e4d20fb92ad2e01355bbbb28,"Naval Center for Cost Analysis, Washington, DC  20350-1000, United States; George Mason University, Fairfax, VA  22030, United States",Elsevier Inc.,English,01641212,
Scopus,Tools & methods for software effort estimation using use case points model - A review,"Software effort estimation is primary requisite in software development life cycle. Most of the software projects failed due to inaccurate effort estimation. So, to overcome this shortcoming many techniques were introduced in past by various researchers. There are many techniques exists for estimating the software project effort such as learning oriented, model based and expert based techniques. This paper represents review on various techniques used for effort estimation but main focus is on the tools and frameworks developed for efforts estimation which based on Use Case Point (UCP) model. These tools come up with extra features such as consider more factors that may affect project delivery and ability to give better estimate than existing one. © 2015 IEEE.",Software effort estimation; Tools; use case point method,"Saroha M., Sahu S.",2015,Conference,"International Conference on Computing, Communication and Automation, ICCCA 2015",10.1109/CCAA.2015.7148498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939550085&doi=10.1109%2fCCAA.2015.7148498&partnerID=40&md5=28b6dbfe22d7b8140f26b3f696617643,"Computer Science and Engineering, Ajay Kumar Garg Engineering College, Ghaziabad, India; Department of Computer Science and Engg., Ajay Kumar Garg Engineering College, Ghaziabad, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781479988907
Scopus,RISDM: A requirements inspection systems design methodology: Perspective-based design of the pragmatic quality model and question set to SRS,"The quality of the SRS (Software Requirements Specification) is the key to the success of software development. The inspection for the verification and validation of SRS are widely practiced, however, the techniques of inspection are rather ad hoc, and largely depend on the knowledge and skill of the people. This article proposes RISDM (Requirements Inspection Systems Design Methodology) to design the RIS (Requirements Inspection System) to be conducted by a thirdparty inspection team. The RISDM includes a meta-model and design process of RIS, PQM (Pragmatic Quality Model) of SRS, and a technique to generate inspection question set based on the PQM and PBR (Perspective-Based Reading). We have been applying the RIS designed by the proposed RISDM to more than 140 projects of a wide variety of software systems in NTT DATA for five years. By analyzing the statistics from the experience, we discovered some key quality characteristics of SRS reveal strong correlation to the project cost and level of quality to be used for evaluating the maturity of the SRS and predicting the risk. © 2014 IEEE.",Pragmatic Quality Model; Question Set; Requirements Inspection; Requirements Verification and Validation; Risk Prediction; SRS,"Saito S., Takeuchi M., Yamada S., Aoyama M.",2014,Conference,"2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings",10.1109/RE.2014.6912264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909974156&doi=10.1109%2fRE.2014.6912264&partnerID=40&md5=611b0363b641ff5cb004bd4602403d24,"RandD Headquarters, NTT DATA Corporation, Tokyo, Japan; Software Innovation Center, NTT Corporation, Tokyo, Japan; Dep. of Software Engineering, Nanzan University, Seto3, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,9781479930333
Scopus,A versatile approach for the estimation of software development effort based on SRS document,"Requirement engineering is a disciplined application of proven principles, methods, tools and notations to describe the behavior of a system. Generally a project suffers because of improper determination and documentation of software requirements. The intricacies in software development processes are often not understood by the client. Hence, requirements submitted by client need a careful examination. In order to produce proper artifact, it is necessary that the software requirements should be documented into the software requirement specification (SRS) on the basis of the recommendations of IEEE 830:1998 document. To address these issues, a measure for early estimation of software complexity based on SRS of the yet to be developed software is proposed. Further, this complexity measure serves as a basis for computing and deriving early estimation of software development effort too. The results obtained from the proposed measure are able to establish that the measure is accurate and precise as compared to various other existing measures that are based on the algorithm/parameters, software code, function point analysis and use case points. All this is accomplished using SRS document at an early phase of software development process. The proposed measure is robust, comprehensive, early alarming and compares well with other development effort estimation measures proposed in the past. © 2014 World Scientific Publishing Company.",Functional requirements; non-functional requirements; requirement based complexity; requirement based function point; requirement based software development effort; SRS,"Sharma A., Vardhan M., Kushwaha D.S.",2014,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194014500016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902344411&doi=10.1142%2fS0218194014500016&partnerID=40&md5=b74c4808c3bf6d927dbb9b13bb5207c8,"GLA University, Mathura, India; National Institute of Technology, Raipur, India; MNNIT, Allahabad, India",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Adapting multi-criteria decision analysis for assessing the quality of software products. Current approaches and future perspectives,"Our great reliance on software-based systems and services nowadays requires software products of the highest quality. An essential prerequisite for developing software of guaranteed quality in a predictable way is the ability to model and objectively assess its quality throughout the project lifecycle. A potential approach must handle the abstract and multi-dimensional character of quality.Analogies between software quality assessment (SQA) and Multi-Criteria Decision Analysis (MCDA) motivated us to investigate how MCDA methods can support SQA. Thus we (1) identified the most relevant requirements for an SQA method, (2) reviewed existing SQA methods regarding potential benefits from using MCDA methods, and (3) assessed some popular MCDA methods regarding their applicability for SQA.We found that although a number of SQA methods proposed in recent years already adapt MCDA methods, the exact rationales for selecting a particular method are usually unclear or arbitrary. Usually, neither the goals nor the constraints of SQA are explicitly considered. Existing SQA methods do not meet the relevant requirements and mostly share the same weaknesses independent of whether they employ MCDA or not. In many cases, popular MCDA techniques are unsuitable for SQA because they do not meet its basic constraints, such as handling situations where data are scarce. We discuss the gaps identified in MCDA.The results led us to the conclusion that future research should focus on resolving the deficits of the existing SQA methods or of the most promising MCDA techniques rather than on inventing new methods from scratch. © 2014 Elsevier Inc.",Multi-criteria decision analysis; Multi-criteria decision support; Software; Software quality assessment,"Trendowicz A., Kopczyńska S.",2014,Book Chapter,Advances in Computers,10.1016/B978-0-12-800162-2.00004-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894648210&doi=10.1016%2fB978-0-12-800162-2.00004-X&partnerID=40&md5=fb3b99a912bcf53bcf0f85cbbe66dc11,"Fraunhofer Institute for Experimental Software Engineering IESE, Fraunhofer-Platz 1, Kaiserslautern 67663, Germany; Institute of Computing Science Faculty of Computing Poznan, University of Technology, Piotrowo 2, Poznań 60-965, Poland",Academic Press Inc.,English,00652458,
Scopus,A systematic mapping of factors affecting accuracy of software development effort estimation,"Software projects often do not meet their scheduling and budgeting targets. Inaccurate estimates are often responsible for this mismatch. This study investigates extant research on factors that affect accuracy of software development effort estimation. The purpose is to synthesize existing knowledge, propose directions for future research, and improve estimation accuracy in practice. A systematic mapping study (a comprehensive review of existing research) is conducted to identify such factors and their impact on estimation accuracy. Thirty-two factors assigned to four categories (estimation process, estimator's characteristics, project to be estimated, and external context) are identified in a variety of research studies. Although the significant impact of several factors has been shown, results are limited by the lack of insight into the extent of these impacts. Our results imply a shift in research focus and design to gather more in-depth insights. Moreover, our results emphasize the need to argue for specific design decisions to enable a better understanding of possible influences of the study design on the credibility of the results. For software developers, our results provide a useful map to check the assumptions that undergird their estimates, to build comprehensive experience databases, and to adequately staff design projects. © 2014 by the Association for Information Systems.",Effort estimation; Information system projects; Software development; Systematic mapping,"Basten D., Sunyaev A.",2014,Journal,Communications of the Association for Information Systems,10.17705/1cais.03404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892561550&doi=10.17705%2f1cais.03404&partnerID=40&md5=085f2b671f9b8026027b405d22f0967d,"Department of Information Systems and Systems Development, University of Cologne, Germany; Department of Information Systems, University of Cologne, Germany",Association for Information Systems,English,15293181,
Scopus,Web application functional size estimation based on COSMIC method and UWE approach,"In the last years measuring functional size of software applications became one of the most popular methods when there is need for creating models for effort, costs and budget estimation in software development. Functional Size Measurement (FSM) methods quantify the software from its user's perspective, disregarding quality and technical criteria. In context of conceptual models where models represent application at higher level of abstraction, functional size of developed software application can be measured on the basis of conceptual model and requirements analysis. Requirements analysis is usually conducted thru usage of use case, sequence, activity and other UML diagrams. In this work, we have investigated possibility of using COSMIC FSM method for functional size estimation of web applications whose basic model is created using UML-based Web Engineering (UWE) approach. An example of developed web application with UWE requirements model used for COSMIC measurement is presented and discussed. © 2013 MIPRO.",,"Ceke D., Durek M., Kasapovic S.",2013,Conference,"2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2013 - Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886925336&partnerID=40&md5=2580c078bf0754fc1dfb292ef48bf8b7,"University of Tuzla, Technical Department, Tuzla, Bosnia and Herzegovina; University of Zagreb, Faculty of Electricl Engineering and Computing, Department of Applied Computing, Croatia; University of Tuzla, Faculty of Electrical Engineering, Department of Communications, Tuzla, Bosnia and Herzegovina",,English,,9789532330762
Scopus,Using object oriented software metrics for mobile application development,"Developing and maintaining software for multiple platforms can be challenging. So, scheduling and budget planning, cost estimation, software debugging, software performance optimization etc. is required. In traditional software, this can be achieved using software metrics. The objective of our article was to examine whether the traditional software metrics are appropriate for measuring the mobile applications' source code. To achieve this goal, a small-scale application was developed across three different platforms (Android, iOS and Windows Phone). The code was then evaluated, using the traditional software metrics. After applying the metrics and analysing the code, we obtained comparable results, regardless of the platform. If we aggregate the results, we can argue that traditional software metrics can be used for mobile applications' source code as well. However, further analysis is required, in light of a more complex mobile application. Copyright © by the paper's authors. Copying permitted only for private and academic purposes.",Mobile development; Multi-platform mobile development; Object-oriented metrics; Software product metrics,"Jošt G., Huber J., Heričko M.",2013,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924368656&partnerID=40&md5=3d70bb6292ceffc4582eab1f394446fc,"University of Maribor, UM-FERI, Smetanova ulica 17, Maribor, 2000, Slovenia",CEUR-WS,English,16130073,
Scopus,Genetic Algorithm for optimizing functional link artificial neural network based software cost estimation,"As Software becomes more complex and its scope dynamically increases, the importance of research on developing methods for estimating software development efforts has perpetually increased. Such accurate estimation has a prominent impact on the success of projects.The proposed work uses Functional Link neural network (FLANN) based estimation, which is essentially a machine learning approach, is one of the most popular techniques. In this paper the author has proposed a 2 step process for software effort prediction. In first phase known as training phase the FLANN selects the matching class (datasets) for the given input, which is improved by optimizing the parameters of each individual dataset by Genetic algorithm. In second step known as testing phase, the prediction process is done by Functional Link Artificial Neural Networks. The proposed method uses COCOMO-II as base model. The experimental results show that our method could significantly improve prediction accuracy of conventional Functional Link Artificial Neural Networks (FLANN) and has potential to become an effective method for software cost estimation. © 2012 Springer-Verlag GmbH Berlin Heidelberg.",COCOMO-II; FLANN; Genetic algorithm; Software cost estimation,"Benala T.R., Dehuri S., Satapathy S.C., Madhurakshara S.",2012,Conference,Advances in Intelligent and Soft Computing,10.1007/978-3-642-27443-5-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880341667&doi=10.1007%2f978-3-642-27443-5-9&partnerID=40&md5=088b8689e166a4c41ab162d6254bba0d,"Anil Neerukonda Institute of Technology and Sciences Sangivalasa, Visakhapatnam, Andhra Pradesh, India; Department of Information and Communication Technology, Fakir Mohan University Vyasa Vihar, Balasore 756019, India",,English,18675662,9783642274428
Scopus,StatREC: A graphical user interface tool for visual hypothesis testing of cost prediction models,"Background: During the previous decades there has been noted a significantly increased research interest on the construction of prediction models for accurate estimation of software cost. Despite the development of sophisticated methodologies, there is a continuous debate concerning the divergent and controversial conclusions of the related literature. Nowadays, due to this fact, the research community attempts to systematically base the whole comparison and evaluation process on formal frameworks and structured guidelines in concordance with modern statistical practices and methodologies, so as to resolve the problem of inconsistent findings. Aims: Towards this direction, we present StatREC, a Graphical User Interface, which facilitates the visualization and hypothesis testing of error distributions through their graphical representation as REC curves. Conclusions: The advantage of StatREC is that it provides to the non-expert user a robust, highly interactive, rich in graphics and easily interpretable way to perform comparisons among alternative models. The goal of StatREC is to support project managers during the decision-making process on the cost of software development. Copyright © 2012 ACM.",Graphical comparison; Graphical user interface; Permutation test; Prediction models; REC curves; Software cost estimation,"Mittas N., Mamalikidis I., Angelis L.",2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867711649&doi=10.1145%2f2365324.2365331&partnerID=40&md5=fdd14e04eff963e12b9f8dac0cf39d6b,"Department of Informatics, Aristotle University, Thessaloniki, 54124, Greece; Department of Electrical Engineering, Technological Educational Institute, Kavala, Greece",,English,,9781450312417
Scopus,"On some recent advances in complex software networks: Modeling, analysis, evolution and applications","Complex software networks, as a typical kind of man-made complex networks, have attracted more and more attention from various fields of science and engineering over the past ten years. With the dramatic increase of scale and complexity of software systems, it is essential to develop a systematic approach to further investigate the complex software systems by using the theories and methods of complex networks and complex adaptive systems. This paper attempts to briefly review some recent advances in complex software networks and also develop some novel tools to further analyze complex software networks, including modeling, analysis, evolution, measurement, and some potential real-world applications. More precisely, this paper first describes some effective modeling approaches for characterizing various complex software systems. Based on the above theoretical and practical models, this paper introduces some recent advances in analyzing the static and dynamical behaviors of complex software networks. It is then followed by some further discussions on potential real-world applications of complex software networks. Finally, this paper outlooks some future research topics from an engineering point of view. © 2012 World Scientific Publishing Company.",complex adaptive systems; Complex networks; software adaptivity; software evolution; software measurement,"Wang H., He K., Li B., Lü J.",2012,Journal,International Journal of Bifurcation and Chaos,10.1142/S0218127412500241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862780921&doi=10.1142%2fS0218127412500241&partnerID=40&md5=6639f008567afcce3d03d0b178b96afa,"State Key Laboratory of Software Engineering, Wuhan University, Wuhan 430072, China; School of Economics and Management Engineering, Beijing University of Civil Engineering and Architecture, Beijing 100044, China; LSC, Institute of Systems Science, Academy of Mathematics and Systems Science, Beijing 100190, China; School of Electrical and Computer Engineering, RMIT University, Melbourne VIC 3000, Australia",World Scientific Publishing Co. Pte Ltd,English,02181274,
Scopus,A systematic review on software measurement programs,"Most of the measurement programs fail to achieve targeted goals. This paper presents outcomes of systematic review on software measurement programs. The aim of the study was to analyse Applications, success factors, existing measurement models/frameworks and tools. 1579 research studies were reviewed in the beginning and on basis of predefined criteria 28 studies were chosen for analysis. The selection of research studies was done on the basis of structured procedure of systematic review. Outcome of this study consists of observations and suggestions on the basis of analysis of selected studies. © 2011 IEEE.",Measurement framework; Measurement models; Measurement program; Measurement tools measurement success factors; Software,"Tahir T., Jafar A.",2011,Conference,"Proceedings - 2011 9th International Conference on Frontiers of Information Technology, FIT 2011",10.1109/FIT.2011.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857188380&doi=10.1109%2fFIT.2011.15&partnerID=40&md5=4966906f9596cb2611370f750f6a7ccc,"Department of Computer Science, Comsats Institute of Information Technology, Lahore, Pakistan; Blekinge Institute of Technology, SE 371 79, Karlskrona, Sweden",,English,,9780769546254
Scopus,Neural networks for accurate estimation of software metrics,"Now a days, software requirements are widely increasing and modern software technologies are rapidly growing. Therefore, planning and managing the software projects are more important as compared to the past. Unsuitable project planning has been the main reason of software project fails in recent years. Estimating is performed at the early stages of project and it is one of the most important activities in software project planning. During the recent years many different methods have been proposed to estimate the software metrics. Software development effort and software size are most important metrics in this field. Since uncertain nature of software projects makes it difficult to estimate the metrics, soft computing techniques have been widely used in software metrics estimation. These techniques can improve the accuracy of estimations in software projects by means of neural networks, genetic algorithm, fuzzy logic and so on. Among all mentioned methods, due to high flexibility and adaptability, neural networks have been used more than the other methods. A comparative study can be useful to discriminate the performance of neural networks regarding the software metrics estimation. In this paper 15 previous research works in term of using neural networks for software estimation were investigated basically. Analyzing the selected research works showed that neural networks can estimate the software metrics more accurate than most common algorithmic methods.",Neural network; Performance evaluation; Soft computing; Software metrics,"Vahid Khatibi B., Jawawi D.N.A., Mohd Hashim S.Z., Khatibi E.",2011,Journal,International Journal of Advancements in Computing Technology,10.4156/ijact.vol3.issue10.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82755170401&doi=10.4156%2fijact.vol3.issue10.8&partnerID=40&md5=2e60d380044ea0e9a04e64ef9008fbd7,"Department of Software Engineering, Universiti Teknologi Malaysia (UTM) Skudai 81310, Johor Bahru, Malaysia; Department of Computer Engineering, Bardsir Branch, Islamic Azad University Kerman, Iran",,English,20058039,
Scopus,New metric for measuring programmer productivity,Measuring programmer productivity is a challenging exercise because it is not well understood since it is without bounded rules and ill-defined meanings of complicated set of parameters. This is made worse by new software development paradigms such as agile and mashup methods and the ensuing advances in software engineering practices and ways of designing and developing software. The existing metrics for measuring performance of solo and pairs of programmers does not appear to be suitable to meet these new forms of software development. This paper presents a new metrics which can be used to measure performance productivity of programming effort by solo and pair programmers in a much more comprehensive manner. All of the key parameters are looked at objectively and defined for and from software engineering lifecycle points of view to make productivity performance calculations for solo and pair programmers. The methodology used for formulating and composing the formulae and defining the parameters as coefficients is explained. These parameters are based on a weighting system to derive the effective performance results. It is a more objective and quick way for the performance analysts to define a table of weights for the parameters based on several criteria within the different aspects of the software development lifecycle. The composition of the formulae and the metric for measuring programmer productivity performance is presented and examples are used to illustrate the method and the outcomes. © 2011 IEEE.,,"Solla M., Patel A., Wills C.",2011,Conference,ISCI 2011 - 2011 IEEE Symposium on Computers and Informatics,10.1109/ISCI.2011.5958906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052125869&doi=10.1109%2fISCI.2011.5958906&partnerID=40&md5=6b889013e71a47592e68918f0d9d8b31,"Department of Computer Science, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, 43600 Bangi, Selangor Darul Ehsan, Malaysia; Department of Computer Science, Faculty of Sciences, AlFateh University, Tripoli, Libyan Arab Jamahiriya; Faculty of Computing Information Systems and Mathematics, Kingston University, Penrhyn Road, Kingston upon Thames KT1 2EE, United Kingdom",,English,,9781612846903
Scopus,Back-propagation artificial neural network for ERP adoption cost estimation,"Small and medium size enterprises (SMEs) are greatly affected by cost escalations and overruns Reliable cost factors estimation and management is a key for the success of Enterprise Resource Planning (ERP) systems adoptions in enterprises generally and SMEs specifically. This research area is still immature and needs a considerable amount of research to seek solid and realistic cost factors estimation. Majority of research in this area targets the enhancement of estimates calculated by COCOMO family models. This research is the beginning of a series of models that would try to replace COCOMO with other models that could be more adequate and focused on ERP adoptions. This paper introduces a feed-forward back propagation artificial neural network model for cost factors estimation. We comment on results, merits and limitations of the model proposed. Although the model addresses SMEs, however, it could be extended and applied in various environments and contexts. © 2011 Springer-Verlag.",cost estimation; ERP; neural networks; SMEs,"Kotb M.T., Haddara M., Kotb Y.T.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-24355-4_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054062656&doi=10.1007%2f978-3-642-24355-4_19&partnerID=40&md5=dd22ee5fb7d0df636ce0e5824319437a,"University of Agder, Canada; University of Western Ontario, Canada",Springer Verlag,English,18650929,9783642243547
Scopus,Sensitivity of results to different data quality meta-data criteria in the sample selection of projects from the ISBSG dataset,"Background: Most prediction models, e.g. effort estimation, require preprocessing of data. Some datasets, such as ISBSG, contain data quality meta-data which can be used to filter out low quality cases from the analysis. However, an agreement has not been reached yet between researchers about these data quality selection criteria. Aims: This paper aims to analyze the influence of data quality meta-data criteria in the number of selected projects, which can have influence in the models obtained. For this, a case study has been selected to gain a more complete understanding of what might be important to focus in future research. Method: Data quality meta-data selection criteria of some works based on ISBSG dataset which propose prediction models were reviewed first. Considerable attention has been paid to two data quality meta-data variables in ISBSG dataset Release 11 which are Data Quality Rating and Unadjusted Function Point Rating. Secondly, this paper considers data from 830 projects which have been collected from the ISBSG dataset after a preliminary screening. This first screening leads mainly to a subset of projects with comparable definitions in size and effort. Then data quality meta-data criteria are applied in order to infer their influence. Results: Overall, it seems that data selection criteria, regardless data quality meta-data concerns, involve an important reduction in sample size. From 5052 projects, only 830 are really considered. Then 262 projects remain for analysis if the maximum quality rate is applied for both data quality meta-data variables. But, since the initial data preparation focuses the problem of missingness for a certain purpose, data quality criteria seem not to be the clue for the analysis results. However, some variability has been observed. Conclusions: Whilst this analysis is supported by a case study, it is hoped that it contributes to a better understanding of the subject. In fact, results found suggest that in those studies where the selection criteria of projects are not very strictly applied, these data quality criteria must be carefully taken into account.",Data quality meta-data; Datasets; Effort; Empirical research; Functional size; Prediction models; Software projects,"Fernández-Diego M., Martínez-Gómez M., Torralba-Martínez J.-M.",2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649775587&doi=10.1145%2f1868328.1868348&partnerID=40&md5=bd5031d4e62629165af81a8b74414aca,"Universidad Politécnica de Valencia, Camino de Vera, s/n, 46022 Valencia, Spain",,English,,9781450304047
Scopus,Knowledge extraction of the behaviour of software developers by the analysis of time recording logs,"Software development project management has a poor reputation in terms of avoiding cost and schedule overruns. The cause of this situation is based on the feature of the software development process that is characterized by quickly growing complexity and change. Therefore, there are many uncertainties to define exactly the necessary time to complete a tasks according to the person's performance. In this scenario Soft-Computing techniques may offer new approaches with the aim of helping the participants of the project to manage their time, give priority to their activities and readjust the work to complete satisfactorily the project tasks. This work presents an automatic features extraction process with the aim of defining the elements involved in a software project. This knowledge is represented by means fuzzy sets and fuzzy prototypes. The source of data is the Personal Software Project time recording logs. A preliminary experiment illustrates the feasibility of this approach. © 2010 IEEE.",,"Peralta A., Romero F.P., Olivas J.A., Polo M.",2010,Conference,"2010 IEEE World Congress on Computational Intelligence, WCCI 2010",10.1109/FUZZY.2010.5584364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549255531&doi=10.1109%2fFUZZY.2010.5584364&partnerID=40&md5=e674b7546333aa8770016d07b3dd5892,"Department of Information Systems and Technologies, University of Castilla la Mancha, Paseo de la Universidad s/n, 13071 Ciudad Real, Spain",,English,,9781424469208
Scopus,Analogies and differences between Machine Learning and expert based software project effort estimation,"This paper presents a review and comparison of the software project cost estimation methods that have emerged with more impact in recent years; Expertise and Machine Learning methods. These methods and models have been selected according to an own criteria focusing onto Analogy estimation models and Case Based Reasoning approaches, assuming that they are widely utilized by researchers and with good accurate results. Finally we show a comparative analysis of the seven models proposed inside the Machine Learning methods with advantages and disadvantages between them. © 2010 IEEE.",Analogy; CBR; Effort estimation; Expertise judgment; Machine learning; Software engineering,"Cuadrado-Gallego J.J., Rodríguez-Soria P., Martín-Herrera B.",2010,Conference,"Proceedings - 11th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD2010",10.1109/SNPD.2010.47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956042369&doi=10.1109%2fSNPD.2010.47&partnerID=40&md5=848e986007908e11464d92c09b2e67bd,"Universidad de Alcalá, Departamento de Ciencias de la Computación, 28805 Alcalá de Henares, Madrid, Spain; Ecole de Téchnologie Superieure (ETS), Université du Québec À Montreal, Canada",,English,,9780769540887
Scopus,Towards approximating COSMIC functional size from user requirements in agile development processes using text mining,"Measurement of software size from user requirements is crucial for the estimation of the developmental time and effort. COSMIC, an ISO/IEC international standard for functional size measurement, provides an objective method of measuring the functional size of the software from user requirements. COSMIC requires the user requirements to be written at a level of granularity, where interactions between the internal and the external environments to the system are visible to the human measurer, in a form similar to use case descriptions. On the other hand, requirements during an agile software development iteration are written in a less formal way than use case descriptions - often in the form of user stories, for example, keeping with the goal of delivering a planned release as quickly as possible. Therefore, size measurement in agile processes uses methods (e.g. story-points, smart estimation) that strictly depend on the subjective judgment of the experts, and avoid using objective measurement methods like COSMIC. In this paper, we presented an innovative concept showing that using a supervised text mining approach, COSMIC functional size can be automatically approximated from informally written textual requirements, demonstrating its applicability in popular agile software development processes, such as Scrum. © 2010 Springer-Verlag.",,"Hussain I., Kosseim L., Ormandjieva O.",2010,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-13881-2_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955459519&doi=10.1007%2f978-3-642-13881-2_8&partnerID=40&md5=26e36d61bc38de3f7d668fe2bafe78de,"Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada",,English,03029743,3642138802; 9783642138805
Scopus,Selection of strategies in judgment-based effort estimation,"We currently know little about the factors that motivate the selection and change of strategy in judgment-based effort estimation. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments' two main results are the identification of the importance of ""estimation surprises"" (large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental ""accessibility"" of the strategies. We found, for example, that the use of a type of strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before. © 2010 Elsevier Inc. All rights reserved.",Cognitive strategies; Software cost estimation; Software psychology,Jørgensen M.,2010,Journal,Journal of Systems and Software,10.1016/j.jss.2009.12.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953133006&doi=10.1016%2fj.jss.2009.12.028&partnerID=40&md5=a79bc2bfdfa814fd4162b438cbc262a9,"Simula Research Laboratory, P.O. Box 134, NO-1325 Lysaker, Norway; University of Oslo, P.O. Box 1072, Blindern, NO-0316 Oslo, Norway",,English,01641212,
Scopus,Using chronological splitting to compare cross- and single-company effort models: Further investigation,"Numerous studies have used historical datasets to build and validate models for estimating software development effort. Very few used a chronological split (where projects' end dates are used so that training sets only contain projects that were completed before the start date of each project in the validation set), and only one compared chronological split to random split. Therefore the aim of this study is to investigate further and compare the use of chronological and random splitting. We do so in the context of comparing cross-company and single-company models for effort estimation. We used 450 single-company projects and 741 cross-company projects from the ISBSG Release 10 repository, and estimates were obtained using manual stepwise regression. We found that with these data the use of chronological splitting, and different splitting dates, did not affect prediction accuracy. We were not able to obtain a converging set of findings when comparing cross- to single-company predictions given that different accuracy measures presented contradictory results. Copyright © 2009, Australian Computer Society, Inc.",Chronological Splitting; Cost estimation; Cross-company data; Effort estimation; Estimation accuracy; Single-company data,"Lokan C., Mendes E.",2009,Conference,Conferences in Research and Practice in Information Technology Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868711954&partnerID=40&md5=fca2f87e35d6e18c921653c9a8875f74,"School of IT and EE, UNSW, ADFA, Canberra ACT 2600, Australia; Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,14451336,9781920682729
Scopus,Improvement opportunities and suggestions for benchmarking,"During the past 10 years, the amount of effort put on setting up benchmarking repositories has considerably increased at the organizational, national and even at international levels to help software managers to determine the performance of software activities and to make better software estimates. This has enabled a number of studies with an emphasis on the relationship between software product size, effort and cost drivers in order to either measure the average performance for similar software projects or to develop estimation models and then refine them using the collected data. However, despite these efforts, none of those methods are yet deemed to be universally applicable and there is still no agreement on which cost drivers are significant in the estimation process. This study discusses some of the possible reasons why in software engineering, practitioners and researchers have not yet been able to come up with reasonable and well quantified relationships between effort and cost drivers although considerable amounts of data on software projects have been collected. An improved classification of application types in benchmarking repositories is also proposed. © Springer-Verlag Berlin Heidelberg 2009.",Benchmarking repositories; Cost drivers; Effort estimation; Performance measurement,"Gencel C., Buglione L., Abran A.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650662575&doi=10.1007%2f978-3-642-05415-0_11&partnerID=40&md5=2d0787b1319ec25ad6fc340b802add68,"Blekinge Institute of Technology, Sweden; Ecole de Téchnologie Superieure (ETS), Université du Québec à Montreal (UQAM), Canada; Nexen (Engineering Group), Italy",,English,03029743,3642054145; 9783642054143
Scopus,Estimating manual test execution effort and capacity based on execution points,"In some domains, such as the mobile application domain, functional tests may not be feasible to automate due to limited test automation technology (multimedia support, etc.) or high cost to port automated tests for different phone models and operating systems. In this context, test teams should be able to estimate the effort required to manually execute tests on schedule. Although several effort estimation approaches have been proposed over the years, none of them are appropriate for estimating manual test execution effort. The main reason for that is the lack of size measures for test projects. In this paper, we present execution points, a measure for the size and execution complexity of tests that is calculated based on test specifications. We also show how to estimate manual test execution effort and capacity based on this measure. Finally, results from an empirical study suggested the viability of our proposed measure.",Capacity estimation; Effort estimation; Manual test execution; Size measure,"Aranha E., Borba P.",2009,Journal,International Journal of Computers and Applications,10.2316/Journal.202.2009.3.202-2964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950791818&doi=10.2316%2fJournal.202.2009.3.202-2964&partnerID=40&md5=1773e8b2ac0514e2687fc728030c0b49,"School of Sciences and Technology, Federal University of Rio Grande do Norte, Campus Universitário, 59078970, Natal, Brazil; Informatics Center, Federal University of Pernambuco, Cidade Universitaria, Av. Professor Luis Freire s/n, 50740-540, Recife, Brazil",,English,1206212X,
Scopus,An information systems design product theory for software project estimation and measurement systems,"There is relatively little research on software Project Estimation and Measurement Systems (PEMS). Commercial PEMS vary in functionality and effectiveness. Their intended users thus do not know what to expect from PEMS and how to evaluate them. This paper creates an information system design product theory for the class of PEMS that prescribes the meta-requirements, the meta-design, and applicable theories for all products within the class. Meta-requirements and the meta-design are derived from the project estimation and measurement literature, experiences obtained during more than ten years of empirical work in Finnish Software Measurement Association, and a commercially available PEMS. © 2009 IEEE.",Functional size measurement; Knowledge management; Organizational learning; Outsourcing; Software process improvement; Software project estimation and benchmarking,"Forselius P., Käkölä T.",2009,Conference,"Proceedings of the 42nd Annual Hawaii International Conference on System Sciences, HICSS",10.1109/HICSS.2009.65,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63349105045&doi=10.1109%2fHICSS.2009.65&partnerID=40&md5=9267d3ae248205020606c7dc4a0453e7,"4SUM Partners, Tekniikantie 14, Espoo 02150, Finland; Claremont Graduate University, United States; University of Jyväskylä, Jyväskylä 40014, Finland",,English,,9780769534503
Scopus,Effect of cmmi-based software process maturity on software schedule estimation,"The Software Capability Maturity Model (SW-CMM) has become a popular model for enhancing software development processes with the goal of developing high-quality software within budget and schedule. The software cost estimation model, COnstructive COst MOdel (COCOMO), in its last update (COCOMO II) has a set of seventeen cost drivers and a set of five scale factors. Process Maturity (PMAT) is one of the five scale factors and its ratings are based on SW-CMM. This paper investigates the impact of process maturity on software development Schedule (cycle time) by deriving a new set of COCOMO II's PMAT rating values based on the most recent version of CMM, i.e. Capability Maturity Model Integration (CMMI). The precise data for the analysis were collected from the record of 40 historical projects which spanned the range of CMMI Levels, from Level 1 (Lower half and Upper half) to Level 4, where eight data points were collected from each level. The Ideal Scale Factor (ISF) method is applied in order to withhold the effect of the COCOMO II's PMAT scale factor. All prediction accuracies evaluations were measured using PRED (.20). The study shows that the proposed model (with the new PMAT rating values) produced better schedule estimates as compared to the generic COCOMO II model's schedule estimates.",CMMI; COCOMO; Cost driver; Scale factor; Schedule estimation; SW-CMM,"Alyahya M.A., Ahmad R., Lee S.P.",2009,Journal,Malaysian Journal of Computer Science,10.22452/mjcs.vol22no2.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955165443&doi=10.22452%2fmjcs.vol22no2.3&partnerID=40&md5=4ecfa115a0266d02d540e0f6087c3c41,"Department of Software Engineering, FCSIT, University of Malaya, Kuala Lumpur, Malaysia",Faculty of Computer Science and Information Technology,English,01279084,
Scopus,Do base functional component types affect the relationship between software functional size and effort?,"One of the most debated issues in Software Engineering is effort estimation and one of the main points is about which could be (and how many) the right data from an historical database to use in order to obtain reliable estimates. In many of these studies, software size (measured in either lines of code or functional size units) is the primary input. However, the relationship between effort and the components of functional size (BFC - Base Functional Components) has not yet been fully analyzed. This study explores whether effort estimation models based on BFCs types, rather than those based on a single total value, would improve estimation models. For this empirical study, the project data in the International Software Benchmarking Standards Group (ISBSG) Release 10 dataset, which were functionally sized by the COSMIC FFP method, are used. © 2008 Springer-Verlag Berlin Heidelberg.",Base Functional Component; COSMIC-FFP; Effort Estimation; Functional Size Measurement; International Software Benchmarking Standards Group (ISBSG),"Gencel C., Buglione L.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85553-8_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249132694&doi=10.1007%2f978-3-540-85553-8_6&partnerID=40&md5=5139de739c4b325ab079e6e88952b44a,"Bilgi Group Software Research, Training, Consultancy Ltd., Middle East Technical University Teknokent, Ankara, Turkey; École de Technologie Supérieure (ETS), Engineering.it S.p.A.",,English,03029743,3540855521; 9783540855521
Scopus,Refactoring effect estimation based on complexity metrics,"Refactoring is a set of operations to improve maintainability or understandability or other attributes of a software system without changing the external behavior of it, and it is getting much attention recently. However it is difficult to perform appropriate refactorings since the impact of refactoring should justify the cost. Therefore, before a refactoring is really performed, the effect and the cost of it should be estimated. The estimation makes it possible for us to adequately assess whether each refactoring should be performed or not. This paper shows that it is difficult for developers to perform appropriate refactorings, and proposes a method estimating refactoring effect. The method has been implemented as a software tool, and a case study was conducted with it. The result of the case study showed that the estimation of the tool helped a developer of the target software system to perform an appropriate refactoring. © 2008 IEEE.",,"Higo Y., Matsumoto Y., Kusumoto S., Inoue K.",2008,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2008.4483210,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249163700&doi=10.1109%2fASWEC.2008.4483210&partnerID=40&md5=09c68840c10e24cb9111e975c4120f29,"Graduate School of Information Science and Technology, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka 560-8531, Japan",,English,,0769531008; 9780769531007
Scopus,Predicting software fault proneness model using neural network,"Importance of construction of models for predicting software quality attributes is increasing leading to usage of artificial intelligence techniques such as Artificial Neural Network (ANN). The goal of this paper is to empirically compare traditional strategies such as Logistic Regression (LR) and ANN to assess software quality. The study used data collected from public domain NASA data set. We find the effect of software metrics on fault proneness. The fault proneness models were predicted using LR regression and ANN methods. The performance of the two methods was compared by Receiver Operating Characteristic (ROC) analysis. The areas under the ROC curves are 0.78 and 0.745 for the LR and ANN model, respectively. The predicted model shows that software metrics are related to fault proneness. The models predict faulty classes with more than 70 percent accuracy. The study showed that ANN method can also be used in constructing software quality models and more similar studies should further investigate the issue. Based on these results, it is reasonable to claim that such a model could help for planning and executing testing by focusing resources on fault-prone parts of the design and code. © 2008 Springer-Verlag Berlin Heidelberg.",Artificial neural network; Empirical validation; Metrics; Software quality,"Singh Y., Kaur A., Malhotra R.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249127895&doi=10.1007%2f978-3-540-69566-0_18&partnerID=40&md5=c53a3e3c157bd2341adde096fea07eca,"University School of Information Technology, Guru Gobind Singh Indraprastha University, Kashmere Gate, Delhi 110006, India",,English,03029743,3540695648; 9783540695646
Scopus,Criteria for estimating effort for requirements changes,"IT practitioners realize that poor scheduling can cause project failure. This is because schedule overruns may be caused by the effort involved in making requirement changes. A software process improvement challenge is to better estimate the cost and effort of requirements changes. Difficulties with such effort estimation is partially caused by lack of data for analysis supported with little information about the data types involved in the requirements changes. This research is an exploratory study, based on change request forms, in requirements change categorization. This categorization can be used to develop an empirical model for requirements change effort as input into a cost estimation model. An empirically based estimation model will provide IT practitioners with a basis for better estimation of effort needed for requirements changes. © Springer-Verlag Berlin Heidelberg 2008.",Change request; Requirements changes; Rework effort,"Chua B.B., Bernardo D.V., Verner J.",2008,Conference,Communications in Computer and Information Science,10.1007/978-3-540-85936-9_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862284102&doi=10.1007%2f978-3-540-85936-9_4&partnerID=40&md5=4fc31dc1cb24fe0a99879ff0487107fc,"Faculty of Information Technology, University Of Technology, Sydney, Australia; National ICT, Australia",Springer Verlag,English,18650929,9783540859345
Scopus,An evaluation of function point counting based on measurement-oriented models,"OBJECTIVE: It is well known that Function Point Analysis suffers from several problems. In particular, the measurement criteria and procedure are not defined precisely. Even the object of the measurement is not defined precisely: it is given by whatever set of documents and information representing the user requirements. As a consequence, measurement needs to be performed by an “expert”, who can compensate the lack of precision of the method with the knowledge of common practices and interpretations. The paper aims at evaluating a methodology for function point measurement based on the representation of the system through UML models: this methodology aims at providing a precise definition of the object of the measurement, as well as the measurement procedure and rules. METHODS: An experimental application of the methodology is presented. A set of analysts (having different degrees of experience) were trained in the methodology and were then given the same requirements to model. The resulting models were measured by a few measurers, also trained in UML model-based counting. RESULTS: The results show that the variability of the FP measure is small compared to the one obtained after applying “plain” FPA, as described in the literature. More precisely, whereas the influence of the modeller on the result appears to be negligible (i.e., a counter gets the same results from different models of the same application), the variability due to the measurer is more significant (i.e., different counters get different results from the same model), but still small when compared to the results reported in the literature on FPA. CONCLUSIONS: The number of data points that we were able to collect was not big enough to allow reliable conclusions from a rigorous statistical viewpoint. Nevertheless, the results of the experiment tend to confirm that the considered technique decreases noticeably the variability of FP measures. © 2008 Evaluation and Assessment in Software Engineering. All rights reserved.",Experimental assessment; Function point analysis; Functional size measurement; Measurement techniques; Measurement-oriented modelling,"Del Bianco V., Gentile C., Lavazza L.",2008,Conference,"12th International Conference on Evaluation and Assessment in Software Engineering, EASE 2008",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862188264&partnerID=40&md5=0c3305f0c4da70fc5f184a3aa3f7376a,"University of Insubria, Italy; CEFRIEL, University of Insubria, Italy",BCS Learning and Development Ltd.,English,,
Scopus,Incremental effort prediction models in agile development using radial basis functions,"Despite significant investment in research, the lightweight estimation of development effort is still an unsolved problem in software engineering. This study proposes a new, lightweight effort estimation model aimed at iterative development environments, as Agile Processes. The model is based on Radial Basis Functions. It is experimented in two semi-industrial projects conducted using a customized version of Extreme Programming (XP). The results are promising and evidence that the proposed model can be developed incrementally and from scratch for new projects without resorting to historical data. Copyright © (2007) by Knowledge Systems Institute (KSI).",,"Moser R., Pedrycz W., Succi G.",2007,Conference,"19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650730783&partnerID=40&md5=f55d60f20402bcb9c2b90cfc96fc6381,"Free University of Bolzano, Italy; University of Alberta, Canada",,English,,9781627486613
Scopus,Design of a fuzzy decision-making model and its application to software functional size measurement,"COSMIC-FFP as a Functional Size Measurement method has greatly improved the estimation of realtime software system. Based on math expectation and variance, this paper puts forward the concepts of fuzzy expectation and fuzzy variance. And then the fuzzy regression analysis is formed on the basis of these concepts. The effort estimation decision-making model, which can determine regression frequency according to actual needs, is established based on this fuzzy regression analysis. This effort estimation decisionmaking model can also make the disturbance of morbid data and bad information in regression analysis to the decision result decrease bit by bit until we are satisfied. When applying this estimation model to software metrics of COSMIC-FFP, we can estimate the effort accurately, which leads to a correct decision. © 2006 IEEE.",,"Xunmei G., Guoxin S., Lizhong X.",2007,Conference,"CIMCA 2006: International Conference on Computational Intelligence for Modelling, Control and Automation, Jointly with IAWTIC 2006: International Conference on Intelligent Agents Web Technologies ...",10.1109/CIMCA.2006.82,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849113991&doi=10.1109%2fCIMCA.2006.82&partnerID=40&md5=8493608c45d836937a6728f490c5e611,"College of Information Science and Engineering, East China University of Science and Technology, Shanghai, 200237, China",,English,,0769527310; 9780769527314
Scopus,Model size matters,"Size is an important attribute of software artefacts; for most artefact types exists a body of measurement knowledge. As software engineering is becoming more and more model-centric, it is surprising that there exists only little work on model size metrics (MoSMe). In this position paper we identify the goals justifying the need for MoSMe, such as prediction, description and progress measurement. Additionally, we identify challenges that make it difficult to measure the size of UML models and that MoSMe have to deal with. Finally, we propose a classification of MoSMe and concrete examples of metrics for the size of UML models. © Springer-Verlag Berlin Heidelberg 2007.",GQM; Measurement; Metrics; Models; Prediction; Size; UML,Lange C.F.J.,2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69489-2_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149127023&doi=10.1007%2f978-3-540-69489-2_26&partnerID=40&md5=72fac9b2c4ea5bf3d907333d6db83fcb,"Department of Mathematics and Computer Science, Technische Universiteit, Eindhoven, Netherlands",Springer Verlag,English,03029743,9783540694885
Scopus,Generating a test strategy with Bayesian Networks and common sense,"Testing still represents an important share of the overall development effort and, coming late in the software life cycle, it is on the critical path both from a schedule and quality perspective. In an effort to conduct smarter software testing, Motorola Labs have developed the Bayesian Test Assistant (BTA), an advanced decision support tool to optimize all Verification and Validation activities, in Development and System Testing. With Bayesian Networks, the theory underlying BTA, Motorola Labs built a library of causal models to predict, from key process, people and product factors, the quality of artefacts at each step of the software development. In this paper we present how BTA links the predictions from development models by mapping dependencies between components or subsystems to predict the level of risk in each system feature. As a result, and well before System Testing starts, BTA generates a test strategy that optimizes the writing of test cases. During System Test, BTA scores test cases to select an optimum set for each test step, leading to a faster discovery of defects. We also describe how BTA was deployed on large Telecomm system releases in several Motorola organizations and the improvement driven so far in System testing. Keywords: software testing, test strategy, Bayesian networks, defect prediction. © 2006 IEEE.",,"Gras J.-J., Gupta R., Pérez-Miñana E.",2006,Conference,"Proceedings - Testing: Academic and Industrial Conference - Practice and Research Techniques, TAIC PART 2006",10.1109/TAIC-PART.2006.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-45449093217&doi=10.1109%2fTAIC-PART.2006.10&partnerID=40&md5=b8eabec1c3a075eb3c3855e4454d3452,"European Software and System Engineering Research Lab, Paris, France; Motorola Labs, Basingstoke, United Kingdom",,English,,0769526721; 9780769526720
Scopus,Comparison and assessment of improved grey relation analysis for software development effort estimation,"The goal of software project planning is to provide a framework that allows project manager to make reasonable estimates of the resources. In fact, software development is highly unpredictable - only 10% of projects on time and budget. Thus, it is very important for software project managers to accurately and precisely estimate software development effort since the resources are limited. One of the most widely used approaches of software effort estimation is the Analogy method. Since the method of Analogy is constructed on the foundation of distance-based similarity, there are still some drawbacks and restrictions for application. For example, the anomalistic and outlying values will influence the function to determine similarity. Contrarily, Grey Relational Analysis (GRA) is a distinct measurement from the traditional distance scale and can dig out the realistic law from small-sample data. In this paper, we will show how to apply GRA to evaluate the effort estimation results for different data sequences and to compare its accuracy with that of Analogy method. Experimental result shows that the GRA provides a better predictive performance than other methods. We can see that the GRA is more suitable for predicting software development effort with unbalanced dataset. ©2006 IEEE.",,"Hsu C.-J., Huang C.-Y.",2006,Conference,ICMIT 2006 Proceedings - 2006 IEEE International Conference on Management of Innovation and Technology,10.1109/ICMIT.2006.262302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249649645&doi=10.1109%2fICMIT.2006.262302&partnerID=40&md5=7bf5c935ac564e9430145cb9d127cc87,"Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",,English,,1424401488; 9781424401482
Scopus,A case study on the success of introducing general non-construction activities for project management and planning improvement,"The creation of a proper work breakdown structure (WBS) is essential in performing successful project effort estimation and project management. The use of WBS is required on the level 1 of CMMI. There is, however, no standard WBS available. In this paper, the results of a pilot project in which new activities were introduced into the TietoEnator's WBS are reported. The activities were non-construction activities which are necessary but not directly related to the actual software construction. The study shows that the success of the introduction of such activities very much depends on the naming of the activities and how they are introduced to the employees. Additionally, it turned out that the pre-thought set of non-construction activities included activities that should not have been in the set at all as individual activities. © Springer-Verlag Berlin Heidelberg 2006.",,"Haapio T., Ahonen J.J.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11767718_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746238130&doi=10.1007%2f11767718_15&partnerID=40&md5=0cd196e9348b72bf040177743058f7b9,"TietoEnator Telecom and Media, P.O. Box 1779, FI-70601 Kuopio, Finland; Department of Computer Science, University of Kuopio, P.O. Box 1627, FI-70211 Kuopio, Finland",Springer Verlag,English,03029743,3540346821; 9783540346821
Scopus,Evaluating software project prediction systems,[No abstract available],,Shepperd M.,2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749073594&doi=10.1109%2fMETRICS.2005.22&partnerID=40&md5=29ffc250d396dd24c82aba583ec73c7a,"School of IS, Computing and Maths, Brunel University, United Kingdom",,English,15301435,0769523714; 9780769523712
Scopus,Ensemble imputation methods for missing software engineering data,"One primary concern of software engineering is prediction accuracy. We use datasets to build and validate prediction systems of software development effort, for example. However it is not uncommon for datasets to contain missing values. When using machine learning techniques to build such prediction systems, handling of incomplete data is an important issue for classifier learning since missing values in either training or test set or in both sets can affect prediction accuracy. Many works in machine learning and statistics have shown that combining (ensemble) individual classifiers is an effective technique for improving accuracy of classification. The ensemble strategy is investigated in the context of incomplete data and software prediction. An ensemble Bayesian multiple imputation and nearest neighbour single imputation method, BAMINNSI, is proposed that constructs ensembles based on two imputation methods. Strong results on two benchmark industrial datasets using decision trees support the method. © 2005 IEEE.",Decision trees; Ensemble; Imputation; Incomplete data; Machine learning; Software prediction,"Twala B., Cartwright M.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749051840&doi=10.1109%2fMETRICS.2005.21&partnerID=40&md5=b829bb0786d4bad4e6724383492b6df2,"Brunel University, UB8 3PH, United Kingdom",,English,15301435,0769523714; 9780769523712
Scopus,Function Points,"Functional size measurement-measuring the functionality delivered by a software application to its users-is vital to software project managers. Its uses include estimation, managing scope in project planning and tracking, and measuring productivity. There is no direct scale for measuring software functionality. One must instead decide which aspects of software to measure, that seem to capture functionality; how to measure those aspects; and how to combine the measurements into an overall measure of application size. Many methods for how to do this have been proposed, beginning with Allan Albrecht's invention of Function Point Analysis in 1979. The methods vary in what things are measured and how the measuring is done; and also in usefulness, applicability, and industry acceptance. Two methods have achieved significant industry acceptance, and a third is on its way. In this chapter we trace the evolution of functional size measurement, from Albrecht's original ideas through to the recent development of ISO standards. We describe Albrecht's method, and what has been learned about it through extensive experience and empirical research. We present some interesting variants on Albrecht's method that did not achieve widespread success, as well as the two other methods that can now be described as mainstream. We describe research concerning function points, and conclude with thoughts on the current status of functional size measurement and some expectations for future work. © 2005 Elsevier Inc. All rights reserved.",,Lokan C.J.,2005,Review,Advances in Computers,10.1016/S0065-2458(05)65007-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645930983&doi=10.1016%2fS0065-2458%2805%2965007-3&partnerID=40&md5=8f33e0baec9f13c602d294ab927170fd,"Northcott Dr. Canberra, ACT 2600, Australia",,English,00652458,0120121654; 9780120121656
Scopus,Assessing variation in development effort consistency using a data source with missing data,"In this study the authors analyse the International Software Benchmarking Standards Group data repository, Release 8.0. The data repository comprises project data from several different companies. However, the repository exhibits missing data, which must be handled in an appropriate manner, otherwise inferences may be made that are biased and misleading. The authors re-examine a statistical model that explained about 62% of the variability in actual software development effort (Summary Work Effort) which was conditioned on a sample from the repository of 339 observations. This model exhibited covariates Adjusted Function Points and Maximum Team Size and dependence on Language Type (which includes categories 2nd, 3rd, 4th Generation Languages and Application Program Generators) and Development Type (enhancement, new development and re-development). The authors now use Bayesian inference and the Bayesian statistical simulation program, BUGS, to impute missing data avoiding deletion of observations with missing Maximum Team size and increasing sample size to 616. Providing that by imputing data distributional biases are not introduced, the accuracy of inferences made from models that fit the data will increase. As a consequence of imputation, models that fit the data and explain about 59% of the variability in actual effort are identified. These models enable new inferences to be made about Language Type and Development Type. The sensitivity of the inferences to alternative distributions for imputing missing data is also considered. Furthermore, the authors contemplate the impact of these distributions on the explained variability of actual effort and show how valid effort estimates can be derived to improve estimate consistency. © 2005 Springer Science + Business Media, Inc.",Bayesian inference; Development Type; Deviance statistic; Function Points; Language Type; Linear regression models; MAR; MCAR; Negative log likelihood statistic; RSQ Adjusted,"Moses J., Farrow M.",2005,Conference,Software Quality Journal,10.1007/s11219-004-5261-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444405971&doi=10.1007%2fs11219-004-5261-z&partnerID=40&md5=b8f6891b324e7f775afd1b2677e800c1,"School of Computing and Technology, University of Sunderland, SR6 0DD, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Empirical studies on requirement management measures,The goal of this research is to demonstrate that a subset of a set of 38 requirements management measures are good predictors of stability and volatility of requirements and change requests. At the time of writing we have theoretically validated ten of these 38 measures. We are currently planning and performing an industrial case study where we want to reach the goal described above.,,Loconsole A.,2004,Conference,Proceedings - International Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544330467&partnerID=40&md5=f684d6ba5c879632d3cbc5df294eb8ef,"Department of Computing Science, Umeå University, SE-90187 Umeå, Sweden",,English,02705257,
Scopus,A survey of the life cycle costs of IT application systems [Untersuchung der lebenszykluskosten von IT-anwendungen],"IT management focuses on planning and developing new IT solutions. The importance of production (operation, support, maintenance) and further development of existing solutions is often neglected, although these tasks are responsible for the majority of today's IT costs. The paper introduces a life cycle model for IT application systems and presents the results of a survey of the life cycle costs of 30 IT application systems. Within the survey, the distribution of costs over the application life cycle was recorded and evaluated. The results show the central importance of recurring costs for production and further development. For a production time of 5 years these costs amounted to 79% of all life cycle costs, whereas only 21 % of the costs were incurred during the planning and initial development stages.",IT Application Systems; IT Management; IT Production; Life Cycle Costs; Software Development; Total Cost of Ownership,"Zarnekow R., Scheeg J., Brenner W.",2004,Review,Wirtschaftsinformatik,10.1007/bf03250935,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042522450&doi=10.1007%2fbf03250935&partnerID=40&md5=f65252185ba8dd5f99966023c5f4a382,"Universität St. Gallen, Inst. für Wirtschaftsinformatik, Müller-Friedberg-Str. 8, CH-9000 St. Gallen, Switzerland; Deutsche Telekom AG, Billing Services, Dolivostr. 17, 64293 Darmstadt, Germany",Friedr. Vieweg und Sohn Verlags GmbH,German,09376429,
Scopus,Software engineering handbook,"Unfortunately, much of what has been written about software engineering comes from an academic perspective which does not always address the everyday concerns that software developers and managers face. With decreasing software budgets and increasing demands from users and senior management, technology directors need a complete guide to the subject of software engineering. The successor to the bestselling Software Engineering Productivity Handbook, this book fulfills that need. Written by an expert with over 25 years of practical experience in building systems, The Software Engineering Handbook covers the full spectrum of software engineering methodologies, techniques, and tools and provides details on how to reach the goals of quality management in a software-engineering environment. It includes a wide variety of information, from the guidelines for the Malcom Baldridge Quality Award to the IEEE measures for reliable software. 65 field-tested how-to chapters provide techniques, guidelines, and philosophies that will assist developers in implementing quality and productivity programs. The author provides readers with a wealth of information and advice in a multitude of areas including management of resources, methods, quality, and metrics. The book concludes with 19 appendices filled with guides, templates, forms, and examples that illustrate important software engineering techniques such as: software requirement specification, software design specification, and a complete test plan including use of automated estimation tools. © 2003 by CRC Press LLC.",,Keyes J.,2002,Book,Software Engineering Handbook,10.1201/9781420031416,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055369487&doi=10.1201%2f9781420031416&partnerID=40&md5=f0792b81d961b8f4e396c2d938b69cea,"New Art Technologies, Inc., United States",CRC Press,English,,9781420031416; 0849314798; 9780849314797
Scopus,An initial model of product line economics,"In this paper we describe an initial model of product line economics, which aims at filling exactly this gap. The model integrates characteristics of the software process with aspects of the market, where the later are used for valuation. We present the model in three layers, each adding a layer of issues that are taken into account to the previous one. This layering mirrors the levels of complexity in existing models of reuse economics. © Springer-Verlag Berlin Heidelberg 2002.",,Schmid K.,2002,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-47833-7_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944060466&doi=10.1007%2f3-540-47833-7_5&partnerID=40&md5=7ef0a07a04a2eeb55a0c511c65c223e3,"Fraunhofer Institute for Experimental Software Engineering (IESE), Sauerwiesen 6, Kaiserslautern, D-67661, Germany",Springer Verlag,English,03029743,3540436596
Scopus,Cost estimating rationale capture,"The use of expert judgement (EJ) within the cost estimating process is studied. The development of a cost estimating rationale capture (CERC) framework is also discussed. This framework can then be used to capture cost estimating assumption, judgment and rationale to inform noncost estimators.",,"Roy R., Rush C.S., Tuer G.",2002,Conference,AACE International. Transactions of the Annual Meeting,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036352271&partnerID=40&md5=e0d62077c224c510a85405a319bdca69,"School of Indust. and Mfg. Science, Cranfield University, Cranfield, Bedford MK43 0AL, United Kingdom",Association for the Advancement of Cost Engineering,English,00657158,
Scopus,An empirical study of certain object-oriented software metrics,"This research focuses on analyzing certain software metrics in an object-oriented (OO) environment. The metrics collected and analyzed includes size, number of message (NOM) sends, reuse, inherited methods, and hierarchical nesting level. The site used is the factory systems department of a large manufacturing company. This department uses SmallTalk as the OO programming language to implement the OO design paradigm. Using automated tools developed in SmallTalk, these metrics were collected from three domain applications comprising 600 classes. Four propositions are empirically tested and the results provided in this study. © 2001 Elsevier Science Inc.",Inheritance; Object-oriented development; Software metrics; Software reuse; Software size,"Subramanian G., Corbin W.",2001,Journal,Journal of Systems and Software,10.1016/S0164-1212(01)00048-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035886071&doi=10.1016%2fS0164-1212%2801%2900048-6&partnerID=40&md5=0fa512bc6a83932b18b8e60caf3403cb,"School of Business, Information Systems, Penn State Harrisburg, E355 Olmsted Building, 777 West Harrisburg Pike, Middletown, PA 17057, United States; Harrisburg, PA, United States",,English,01641212,
Scopus,Adapting function points to object oriented information systems,"The object oriented paradigm has become widely used to develop large information systems. This paper presents a method for estimating the size and effort of developing object oriented software. The approach is analogous to function points, and it is based on counting rules that pick up the elements in a static object model and combine them in order to produce a composite measure. Rules are proposed for counting “Object Oriented Function Points” from an object model, and several questions are identified for empirical research.A key aspect of this method is its flexibility. An organization can experiment with different counting policies, to find the most accurate predictors of size, effort, etc. in its environment.“Object Oriented Function Points” counting has been implemented in a Java tool, and results on size estimation obtained from a pilot project with an industrial partner are encouraging. © Springer-Verlag Berlin Heidelberg 1998.",Function points; Object oriented design metrics; Size estimation,"Antoniol G., Calzolari F., Cristoforetti L., Fiutem R., Caldiera G.",1998,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961328113&partnerID=40&md5=7cc8b774339d2ded42b4a3b61047b0a9,"I.T.C.-I.R.S.T, Via alla Cascata, Povo, Trento  I-38050, Italy; University of Maryland, Dept. of Computer Science, College Park, MD  20742, United States",Springer Verlag,English,03029743,354064556X; 9783540645566
Scopus,A model for estimating efforts required for developing small-scale business applications,"Estimating the amount of effort required for developing an information system is an important project management concern. The author has developed a model that estimates small-scale software development effort in 4GL and end-user computing environments. In addition to presenting and evaluating the proposed model, this paper evaluates two of the most popular models currently used to estimate software development effort, i.e., lines of code (LOC), and function points (FP). Results of the study show a significant correlation between the software development effort and all three models. Compared to LOC and FP models, the models developed in this research are less costly and easier to use in a small-scale software development environment. © 1997 Elsevier Science Inc.",,"Heiat A., Heiat N.",1997,Journal,Journal of Systems and Software,10.1016/S0164-1212(96)00159-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031258192&doi=10.1016%2fS0164-1212%2896%2900159-8&partnerID=40&md5=5f7feee96bb95407e6aede5b996ed80c,"Montana State University-Billings, 1500 N. 30th Street, Billings, MT, United States; Montana State University-Billings, 1500 N. 30th Street, Billings, MT 59101, United States",Elsevier Inc.,English,01641212,
Scopus,Project estimation using Screenflow Engineering,"Software project estimation is a topic that has been widely researched, yielding a multitude of different estimation models, tools and techniques aimed at increasing the accuracy of cost, effort and time estimates of proposed software projects. However, surveys in the United Kingdom, The Netherlands and New Zealand have identified a very low use of such models and tools, despite recognition of their importance. This paper introduces a method of software project estimation used in a New Zealand case. The method used is part of the Screenflow Engineering process. This is based on the premise that computer system applications should share a common pool of data, which is updated on-line and made available simultaneously to any user of any application. © 1996 IEEE.",,Paynter J.,1996,Conference,"Proceedings - 1996 International Conference Software Engineering: Education and Practice, SEEP 1996",10.1109/SEEP.1996.533994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347079396&doi=10.1109%2fSEEP.1996.533994&partnerID=40&md5=f7300f83dbe74f4aed343771bcb0cecb,"University of Auckland, New Zealand",Institute of Electrical and Electronics Engineers Inc.,English,,0818673796; 9780818673795
Scopus,"Products, processes and metrics","The paper reviews developments in the arena of software engineering product metrics, with special reference to system architecture metrics. Some of the weaknesses of current approaches are examined, in particular the very weak notion of process embodied by a product metric. It is argued that the consequence of this oversight is uncertainty in the application and interpretation of metrics. This in turn has led to a slow uptake of product metrics by the software industry. The paper then demonstrates, by means of a simple design metric example, that the application of concepts from the area of software process modelling to product metrics can help overcome many of these deficiencies; it also results in quantitative process models that have potential for the design and construction of software tools and environments. © 1992.",metrics; software design; software process models,Shepperd M.,1992,Journal,Information and Software Technology,10.1016/0950-5849(92)90072-W,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344907699&doi=10.1016%2f0950-5849%2892%2990072-W&partnerID=40&md5=032440295038d49b95a87ec5fc203699,"Department of Computing and Cognition, Bournemouth Polytechnic, Talbot Campus, Fern Barrow, Poole BH12 5BB, UK, United Kingdom",,English,09505849,
Scopus,Determining relevant training data for effort estimation using Window-based COCOMO calibration,"Context: A software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project. Objectives: We investigate the use of moving windows to determine relevant training data for COCOMO calibration. Method: We present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data. Results: Our results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates. Conclusions: This study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available. Additionally, when the change in software development over time is rapid, the use of windows is more justifiable for improving estimation accuracy. © 2018",COCOMO; Model calibration; Moving windows; Project management; Software estimation; Window-based calibration,"Nguyen V., Boehm B., Huang L.",2019,Journal,Journal of Systems and Software,10.1016/j.jss.2018.10.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055346555&doi=10.1016%2fj.jss.2018.10.019&partnerID=40&md5=e87e294090b213d620a750df36780a78,"Faculty of Information Technology, University of Science, Vietnam National University - Ho Chi Minh city, 227 Nguyen Van Cu, Dist. 5, Ho Chi Minh city, Viet Nam; Computer Science Department, University of Southern California, 941W. 37th Pl, SAL 326, Los Angeles, CA  90089, United States; Computer Science & Engineering Dept., Southern Methodist University, Dallas, TX  75275-0122, United States",Elsevier Inc.,English,01641212,
Scopus,Definition and evaluation of a COSMIC measurement procedure for sizing Web applications in a model-driven development environment,"Context. Model-driven development approaches facilitate the production of Web applications. Among them, the Object-Oriented Hypermedia method (OO-H) has been successfully used for the development of industrial Web applications. Similarly to other development approaches, it is important also in this context to put measures in place to support project managers in resource allocation, cost and schedule control, and productivity monitoring. Objective. This motivated us to define a measurement procedure, named OO-HCFP, specifically conceived for OO-H Web applications based on COSMIC, a second-generation functional size measurement method. Method. We present mapping and measurement rules devised to automatically derive size measures from OO-H models. We also carry out an empirical study to evaluate whether our proposed measurement procedure, OO-HCFP, is useful for estimating the effort needed to realise industrial Web applications developed with OO-H. Results. The estimates obtained by using OO-HCFP are more accurate than those obtained by using other measurement approaches based on Function Points and design measures. Conclusions. The proposed approach can be profitably exploited to size Web applications developed with OO-H. Based on our experience, we also provide some guidelines to support the formulation of COSMIC measurement procedures for other model-driven approaches. © 2018 Elsevier B.V.",COSMIC; Functional size measurement; Model-driven development; OO-H method; Web applications,"Abrahão S., De Marco L., Ferrucci F., Gomez J., Gravino C., Sarro F.",2018,Journal,Information and Software Technology,10.1016/j.infsof.2018.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050865497&doi=10.1016%2fj.infsof.2018.07.012&partnerID=40&md5=56d586e34b30c8c155fb2fe47a4f215c,"Department of Computer Science, Universitat Politècnica de València, Spain; Department of Computer Science, University of Salerno, Italy; Department of Languages and Information Systems, Universidad de Alicante, Spain; CREST, Department of Computer Science, University College London, London, United Kingdom",Elsevier B.V.,English,09505849,
Scopus,Heuristic prediction of rainfall using machine learning techniques,"This paper is carried on the heuristic prediction of rainfall using machine learning techniques. As we know agriculture was the predominant of our country and economy. While a regular rain pattern is usually played vital for healthy agriculture but too much rainfall or too little rainfall can be harmful, even it led to devastating of crops. This paper discusses the rate of rainfall in previous years according to various crops seasons like rabi, Kharif, zaid and predicts the rainfall in future seasons. The paper also measures the different categories of data by linear regression method in metrics for effective understanding of agriculture in India. We have selected a real dataset which consists of past year's rainfall rate according to various seasons. Results of this application help farmers to make a correct decision to harvest a particular crop accordingly to crops seasons. Linear regression helps to find. © 2017 IEEE.",Data Mining; Machine Learning; Rainfall prediction; Regression,"Thirumalai C., Harsha K.S., Deepak M.L., Krishna K.C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046647371&doi=10.1109%2fICOEI.2017.8300884&partnerID=40&md5=978c9384770e3a576afd432222036bc2,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Software complexity analysis using halstead metrics,"Software Complexity influences inward connections. Higher the multifaceted nature, bigger the deformities. Programming complexity for any product or a program is hard to discover without utilizing any measurements. The unpredictability, time and exertion fluctuate starting with one program then onto the next. For this reason, Halstead measurements are presented which recognizes the product complexity of a program by utilizing source line with the assistance of operands and operators. This metric was produced by Maurice Halstead to decide a quantitative measure of complexity specifically from the operands and operators in the module. This article gives a correlation between a program that was composed in two unique dialects to distinguish unpredictability, time and exertion of both and furthermore to gauge which programming dialect was better in wording less time to execute, least exertion. © 2017 IEEE.",Halstead Metrics; Software Complexity,"Hariprasad T., Vidhyagaran G., Seenu K., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300883,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046622880&doi=10.1109%2fICOEI.2017.8300883&partnerID=40&md5=314861e1e4b5c32149ef7bd5134469db,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Software effort estimation based on open source projects: Case study of Github,"Context Managers usually want to pre-estimate the effort of a new project for reasonably dividing their limited resources. In reality, it is common practice to train a prediction model based on effort datasets to predict the effort required by a project. Sufficient data is the basis for training a good estimator, yet most of the data owners are unwilling to share their closed source project (CSP) effort data due to the privacy concerns, which means that we can only obtain a small number of effort data. Effort estimator built on the limited data usually cannot satisfy the practical requirement. Objective We aim to provide a method which can be used to collect sufficient data for solving the problem of lack of training data when building an effort estimation model. Method We propose to mine GitHub to collect sufficient and diverse real-life effort data for effort estimation. Specifically, we first demonstrate the feasibility of our cost metrics (including functional point analysis and personnel factors). In particular, we design a quantitative method for evaluating the personnel metrics based on GitHub data. Then we design a samples incremental approach based on AdaBoost and Classification And Regression Tree (ABCART) to make the collected dataset owns dynamic expansion capability. Results Experimental results on the collected dataset show that: (1) the personnel factor is helpful for improving the performance of the effort estimation. (2) the proposed ABCART algorithm can increase the samples of the collected dataset online. (3) the estimators built on the collected data can achieve comparable performance with those of the estimators which built on existing effort datasets. Conclusions Effort estimation based on Open Source Project (OSP) is an effective way for getting the effort required by a new project, especially for the case of lacking training data. © 2017",AdaBoost; Automated function point; Effort data collection; Open source project; Software effort estimation,"Qi F., Jing X.-Y., Zhu X., Xie X., Xu B., Ying S.",2017,Journal,Information and Software Technology,10.1016/j.infsof.2017.07.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026813040&doi=10.1016%2fj.infsof.2017.07.015&partnerID=40&md5=8d2a2034b6d8b719a82098cbfd726d44,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; School of Automation, Nanjing University of Posts and Telecommunications, China; School of Computer and Information Engineering, Henan University, China",Elsevier B.V.,English,09505849,
Scopus,Automated change-prone class prediction on unlabeled dataset using unsupervised method,"Context Software change-prone class prediction can enhance software decision making activities during software maintenance (e.g., resource allocating). Researchers have proposed many change-prone class prediction approaches and most are effective on labeled datasets (projects with historical labeled data). These approaches usually build a supervised model by learning from historical labeled data. However, a major challenge is that this typical change-prone prediction setting cannot be used for unlabeled datasets (e.g., new projects or projects with limited historical data). Although the cross-project prediction is a solution on unlabeled dataset, it needs the prior labeled data from other projects and how to select the appropriate training project is a difficult task. Objective We aim to build a change-prone class prediction model on unlabeled datasets without the need of prior labeled data. Method We propose to tackle this task by adopting a state-of-art unsupervised method, namely CLAMI. In addition, we propose a novel unsupervised approach CLAMI+ by extending CLAMI. The key idea is to enable change-prone class prediction on unlabeled dataset by learning from itself. Results The experiments among 14 open source projects show that the unsupervised methods achieve comparable results to the typical supervised within-project and cross-project prediction baselines in average and the proposed CLAMI+ slightly improves the CLAMI method in average. Conclusion Our method discovers that it is effective for building change-prone class prediction model by using unsupervised method. It is convenient for practical usage in industry, since it does not need prior labeled data. © 2017 Elsevier B.V.",Change-prone prediction; Software maintenance; Unlabeled dataset; Unsupervised prediction,"Yan M., Zhang X., Liu C., Xu L., Yang M., Yang D.",2017,Journal,Information and Software Technology,10.1016/j.infsof.2017.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021967431&doi=10.1016%2fj.infsof.2017.07.003&partnerID=40&md5=e706a1f99a571869a18c49328a99a6cc,"Key Laboratory of Dependable Service Computing in Cyber Physical Society Ministry of Education, Chongqing, 400044, China; School of Software Engineering, Chongqing University, Chongqing, 401331, China",Elsevier B.V.,English,09505849,
Scopus,Less is more: Minimizing code reorganization using XTREE,"Context: Developers use bad code smells to guide code reorganization. Yet developers, textbooks, tools, and researchers disagree on which bad smells are important. How can we offer reliable advice to developers about which bad smells to fix? Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a framework that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, so XTREE requires programmers to do less work. Further, XTREE's recommendations are more responsive to the particulars of different data sets. Finally XTREE's recommendations may be generalized to identify the most crucial factors affecting multiple datasets (see the last figure in paper). Conclusion: Before undertaking a code reorganization based on a bad smell report, use a framework like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods. © 2017 Elsevier B.V.",Bad smells; Decision trees; Performance prediction,"Krishna R., Menzies T., Layman L.",2017,Journal,Information and Software Technology,10.1016/j.infsof.2017.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016444613&doi=10.1016%2fj.infsof.2017.03.012&partnerID=40&md5=8b759acaa01c5136355d900e55157249,"Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Fraunhofer CESE, College Park, United States",Elsevier B.V.,English,09505849,
Scopus,Analyzing and predicting effort associated with finding and fixing software faults,"Context: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults. Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests. Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem. Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures and safety-critical failures required more effort to fix than pre-release and non-critical counterparts, respectively; median values were two or more times higher. (3) Failures with fixes spread across multiple components or across multiple types of software artifacts required more effort. The spread across artifacts was more costly than spread across components. (4) Surprisingly, some types of faults associated with later life-cycle activities did not require significant effort. (5) The level of fix implementation effort was predicted with 73% overall accuracy using the original, imbalanced data. Oversampling techniques improved the overall accuracy up to 77% and, more importantly, significantly improved the prediction of the high level effort, from 31% to 85%. Conclusions: This paper shows the importance of tying software failures to changes made to fix all associated faults, in one or more software components and/or in one or more software artifacts, and the benefit of studying how the spread of faults and other factors affect the fix implementation effort. © 2017 Elsevier B.V.",Analysis; Case study; Prediction; Software faults and failures; Software fix implementation effort,"Hamill M., Goseva-Popstojanova K.",2017,Journal,Information and Software Technology,10.1016/j.infsof.2017.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011277924&doi=10.1016%2fj.infsof.2017.01.002&partnerID=40&md5=af43d777d88a4abc4a2234c26b3f2cdc,"School of Informatics, Computing, Cyber Systems, Northern Arizona University, Flagstaff, AZ, United States; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV  26506, United States",Elsevier B.V.,English,09505849,
Scopus,Does cloned code increase maintenance effort?,"In-spite of a number of in-depth investigations regarding the impact of clones in the maintenance phase there is no concrete answer to the long lived research question, Does the presence of code clones increase maintenance effort?. Existing studies have measured different change related metrics for cloned and non-cloned regions, however, no study calculates the maintenance effort spent for these code regions. In this paper, we perform an in-depth empirical study in order to compare the maintenance efforts required for cloned and non-cloned code. For the purpose of our study we implement a prototype tool which is capable of estimating the effort spent by a developer for changing a particular method. It can also predict effort that might need to be spent for making some changes to a particular method. Our estimation and prediction involve automatic extraction and analysis of the entire evolution history of a candidate software system. We applied our tool on hundreds of revisions of six open source subject systems written in three different programming languages for calculating the efforts spent for cloned and non-cloned code. According to our experimental results: (i) cloned code requires more effort in the maintenance phase than non-cloned code, and (ii) Type 2 and Type 3 clones require more effort compared to the efforts required by Type 1 clones. According to our findings, we should prioritize Type 2 and Type 3 clones when making clone management decisions. © 2017 IEEE.",,"Mondal M., Roy C.K., Schneider K.A.",2017,Conference,"IWSC 2017 - 11th IEEE International Workshop on Software Clones, co-located with SANER 2017",10.1109/IWSC.2017.7880507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017278259&doi=10.1109%2fIWSC.2017.7880507&partnerID=40&md5=e1128dca1b8371f1fc11b441ce930743,"Department of Computer Science, University of Saskatchewan, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781509065950
Scopus,A systematic review on software cost estimation in Agile Software Development,"In the last few years, the size and functionality of software have experienced a massive growth. Along with this, cost estimation plays a major role in the whole cycle of software development, and hence, it is a necessary task that should be done before the development cycle begins and may run throughout the software life cycle. It helps in making accurate estimation for any project so that appropriate charges and delivery date can be obtained. It also helps in identifying the effort required for developing the application, which assures the project acceptance or denial. Since late 90's, Agile Software Development (ASD) methodologies have shown high success rates for projects due to their capability of coping with changing requirements of the customers. Commencing product development using agile methods is a challenging task due to the live and dynamic nature of ASD. So, accurate cost estimation is a must for such development models in order to fine-tune the delivery date and estimation, while keeping the quality of software as the most important priority. This paper presents a systematic survey of cost estimation in ASD, which will be useful for the agile users to understand current trends in cost estimation in ASD. © 2017 Eastern Macedonia and Thrace Institute of Technology.",Agile software development (ASD); Software cost estimation; Software effort estimation; Software measurement and metrics; Software project management (SPM),"Bilgaiyan S., Sagnika S., Mishra S., Das M.",2017,Review,Journal of Engineering Science and Technology Review,10.25103/jestr.104.08,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030027539&doi=10.25103%2fjestr.104.08&partnerID=40&md5=72d060295e60360f0d309360b141bb4d,"School of Computer Engineering, KIIT University, Bhubaneswar, Odisha, 751024, India",Eastern Macedonia and Thrace Institute of Technology,English,17919320,
Scopus,Systematic Mapping Study of Dealing with Error in Software Development Effort Estimation,"Over the last decades, the software engineeringcommunity has investigated new techniques for softwaredevelopment effort estimation. Unfortunately, the estimateswere not always accurate. Error approaches are then, aninteresting track for improving the projects runningperformances and their financial profitability. The aim of thissystematic mapping study is to summarize and synthesize theexisting studies dealing with effort estimation error anduncertainty and to classify them based on research approaches, contribution types, accuracy criteria, datasets, errorapproaches and effort estimation techniques used. In total 19papers published between 1990 and 2015 were selected. Weobserved a balance between the managerial approaches andthe technical ones. Furthermore, the proposed errortechniques and frameworks improve in general the accuracy ofeffort estimation techniques. Fuzzy logic, bootstrapping andrisk analysis are promising avenues that could be combinedwith various estimation techniques. © 2016 IEEE.",Error; Software development effort estimation; Systematic mapping study; Uncertainty,"Koutbi S.E., Idri A., Abran A.",2016,Conference,"Proceedings - 42nd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2016",10.1109/SEAA.2016.39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020699184&doi=10.1109%2fSEAA.2016.39&partnerID=40&md5=c19a175d5aae5f55420833af86c3e203,"Software Projects Management Research Team, ENSIAS, University Mohamed v, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Superieure, Montreal, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781509028191
Scopus,Regression Analysis Based Software Effort Estimation Method,"Estimating the development effort of a software project in the early stages of the software life cycle is a significant task. Accurate estimates help project managers to overcome the problems regarding budget and time overruns. This paper proposes a new multiple linear regression analysis based effort estimation method, which has brought a different perspective to the software effort estimation methods and increased the success of software effort estimation processes. The proposed method is compared with standard Use Case Point (UCP) method, which is a well-known method in this area, and simple linear regression based effort estimation method developed by Nassif et al. In order to evaluate and compare the proposed method, the data of 10 software projects developed by four well-established software companies in Turkey were collected and datasets were created. When effort estimations obtained from datasets and actual efforts spent to complete the projects are compared with each other, it has been observed that the proposed method has higher effort estimation accuracy compared to the other methods. © 2016 World Scientific Publishing Company.",Software effort estimation; software size estimation; use-case point method,"Yücalar F., Kilinc D., Borandag E., Ozcift A.",2016,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194016500261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977489996&doi=10.1142%2fS0218194016500261&partnerID=40&md5=c19f075ce1fd023fdbd4a25b4335ebae,"Department of Software Engineering, Celal Bayar University, Manisa, 45400, Turkey",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Investigation of software maintainability prediction models,"Software must be well developed and maintainable to adapt to the constantly changing requirement of the competitive world. In this article, we distinct different software maintainability prediction models and techniques which can help us to predict the maintainability of software, and can lead us to minimum the effort required to fix the faults in the software and the software will be more maintainable. We have gathered our data from different studies focused on the accuracy of the prediction models as criteria. The results of our study showed that there is a little evidence on the accuracy results of the software maintainability prediction models. © 2016 Global IT Research Institute (GiRI).",metrics; prediction; Software maintainability models; software maintenance; techniques,"Shafiabady A., Mahrin M.N., Samadi M.",2016,Conference,"International Conference on Advanced Communication Technology, ICACT",10.1109/ICACT.2016.7423558,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962784432&doi=10.1109%2fICACT.2016.7423558&partnerID=40&md5=a5ebc2e38c1fcf3d08d297b1617b84fb,"Advanced Informatics School, Universiti Teknologi Malaysia, Jalan Semarak, Kuala Lumpur, Malaysia; Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Jalan Semarak, Kuala Lumpur, Malaysia",Institute of Electrical and Electronics Engineers Inc.,English,17389445,9788996865063
Scopus,Software cost analysis of GPU-accelerated aeroacoustics simulations in C++ with openACC,"Aeroacoustics simulations leverage the tremendous computational power of today’s supercomputers, e.g., to predict the noise emissions of airplanes. The emergence of GPUs that are usable through directive-based programming models like OpenACC promises a costefficient solution for flow-induced noise simulations with respect to hardware expenditure and development time. However, OpenACC’s capabilities for real-world C++ codes have been scarcely investigated so far and software costs are rarely evaluated and modeled for this kind of highperformance projects. In this paper, we present our OpenACC parallelization of ZFS, an aeroacoustics simulation framework written in C++, and its early performance results. From our implementation work, we derive common pitfalls and lessons-learned for real-world C++ codes using OpenACC. Furthermore, we borrow software cost estimation techniques from software engineering to evaluate the development efforts needed in a directive-based HPC environment. We discuss applicability and challenges of the popular COCOMO II model applied to the parallelization of ZFS. © Springer International Publishing AG 2016.",,"Nicolini M., Miller J., Wienke S., Schlottke-Lakemper M., Meinke M., Müller M.S.",2016,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-46079-6_36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992549157&doi=10.1007%2f978-3-319-46079-6_36&partnerID=40&md5=8bb90abd2019d1dc36e64b8caa7712f5,"IT Center, RWTH Aachen University, Aachen, Germany; JARA, High-Performance Computing, Aachen, Germany; Institute of Aerodynamics, RWTH Aachen University, Aachen, Germany",Springer Verlag,English,03029743,9783319460789
Scopus,Using hybrid model of Artificial Bee Colony and Genetic Algorithms in Software Cost Estimation,"The Software Cost Estimation (SCE) is one of most important issues in the cycle of development, management decision, and in the quality of software project. In the case of the lack of certainty of the exact cost for software projects' development, companies encounter with numerous challenges. Also, due to inaccurate cost estimates, making wrong decisions by project managers make irreparable damage. To reach this propos, the effective factors for developing software projects should be evaluated to ensure of the projects success. COCOMO model is the main model for SCE which acts based on criteria and quantities such as number of Line of Code (LOC) or the Function Point Analysis (FPA). Research in recent years has shown that COCOMO model has not a good performance in the SCE. In this paper, we studied SCE by using a hybrid of Genetic Algorithm (GA) and Artificial Bee Colony (ABC) which are Meta-Heuristic Algorithms. Test results show that proposed model, GA and ABC algorithms have less MRE errors values than the COCOMO model. Also, the hybrid model has better convergence comparing with the GA and ABC algorithms. © 2015 IEEE.",Artificial Bee Colony; COCOMO; Genetic algorithm; Software Cost Estimation,"Gharehchopogh F.S., Maleki I., Talebi A.",2015,Conference,"9th International Conference on Application of Information and Communication Technologies, AICT 2015 - Proceedings",10.1109/ICAICT.2015.7338526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960849056&doi=10.1109%2fICAICT.2015.7338526&partnerID=40&md5=84442c40083cf3b6b010a05e104dc9ef,"Department of Computer Engineering, Hacettepe University, Ankara, Turkey",Institute of Electrical and Electronics Engineers Inc.,English,,9781467368551
Scopus,Effort of EAI projects: A repertory grid investigation of influencing factors,"Many companies struggle with effective and efficient accomplishment of enterprise application integration (EAI), resulting in significant time and budget overruns. Concerning project management, a major reason for failure is considered to be effort underestimation. This underestimation is an aftermath of applying estimation methods that do not account for all relevant factors influencing EAI project effort. Applying the Repertory Grid Technique, we explore factors affecting the effort of such projects by conducting 22 semi-structured expert interviews. We provide an extensive overview of 91 effort-influencing factors and their classification in nine categories, which can be used as a checklist in EAI projects. © 2015 by the Project Management Institute.",effort estimation; enterprise application integration; project management; repertory grid,"Wagner H., Pankratz O., Mellis W., Basten D.",2015,Journal,Project Management Journal,10.1002/pmj.21523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942294219&doi=10.1002%2fpmj.21523&partnerID=40&md5=b0f52392a745bc3e4b08b743ac6cf159,"KMA Umwelttechnik GmbH, Königswinter, Germany; Department of Information Systems and Systems Development, University of Cologne, Cologne, Germany",Wiley-Blackwell,English,87569728,
Scopus,Could social factors influence the effort software estimation?,"Effort estimation is often influenced by several factors, including social. This study aims at understanding the interactions between social factors and effort during effort estimation. I want to analyze the dynamics that occur when a developer estimates the effort for a specific task and the influence of the work team and the work conditions. I conducted a semi-structured interview among three different projects with different developers working in Agile and Scrum processes, asking them which factors and social aspects they take in to account when they estimate the effort during the development processes. Results show an important influence of social factors during the effort estimation phase, and call for future works for a large scale Survey for a more accurate identification. © 2015 ACM.",Agile process; Effort estimation; Scrum process; Social factors,Lenarduzzi V.,2015,Conference,"7th International Workshop on Social Software Engineering, SSE 2015 - Proceedings",10.1145/2804381.2804385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975686925&doi=10.1145%2f2804381.2804385&partnerID=40&md5=1f7d1f491b71609f29ad21ccebfca1b1,"University of Bolzano-Bozen, Piazza Domenicani, 3, Bolzano/Bozen, 39100, Italy","Association for Computing Machinery, Inc",English,,9781450338189
Scopus,Lessons Learned from Software Analytics in Practice,"In this chapter, we share our experience and views on software data analytics in practice with a review of our previous work. In more than 10 years of joint research projects with industry, we have encountered similar data analytics patterns in diverse organizations and in different problem cases. We discuss these patterns following a ""software analytics"" framework: problem identification, data collection, descriptive statistics, and decision making. In the discussion, our arguments and concepts are built around our experiences of the research process in six different industry research projects in four different organizations.Methods: Spearman rank correlation, Pearson correlation, Kolmogorov-Smirnov test, chi-square goodness-of-fit test, t test, Mann-Whitney U test, Kruskal-Wallis analysis of variance, k-nearest neighbor, linear regression, logistic regression, naïve Bayes, neural networks, decision trees, ensembles, nearest-neighbor sampling, feature selection, normalization. © 2015 Elsevier Inc. All rights reserved.",Data extraction; Descriptive statistics; Industry research projects; Predictive analytics; Prescriptive analytics; Software analytics framework,"Bener A., Misirli A.T., Caglayan B., Kocaguneli E., Calikli G.",2015,Book Chapter,The Art and Science of Analyzing Software Data,10.1016/B978-0-12-411519-4.00016-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944104894&doi=10.1016%2fB978-0-12-411519-4.00016-1&partnerID=40&md5=f683c71fd3bc4688cd0de51558c018ae,"Mechanical and Industrial Engineering, Ryerson University, Toronto, ON, Canada; Faculty of Computer and Informatics, Istanbul Technical University, Istanbul, Turkey; Microsoft, Seattle, WA, United States; Department of Computing, Open University, Milton Keynes, United Kingdom",Elsevier Inc.,English,,9780124115439; 9780124115194
Scopus,Modeling the productivity of HPC systems on a computing center scale,"In pursue of exaflop computing, the expenses of HPC centers increase in terms of acquisition, energy, employment, and programming. Thus, a quantifiable metric for productivity as value per cost gets more important to make an informed decision on how to invest available budgets. In this work, we model overall productivity from a computing center’s perspective. The productivity model uses as value the number of application runs possible during the lifetime of a given supercomputer. The cost is the total cost of ownership (TCO) of an HPC center including costs for administration and programming effort. For the latter, we include techniques for software cost estimation of large codes taken from the domain of software engineering. As tuning effort increases when more performance is required, we further focus on the impact of the 80-20 rule when it comes to development effort. Here, performance can be expressed with respect to Amdahl’s law. Moreover, we include an asymptotic analysis for parameters like number of compute nodes and lifetime. We evaluate our approach on a real-world case: an engineering application in our integrative hosting environment. © Springer International Publishing Switzerland 2015.",80-20 rule; COCOMO; Computing center; Cost efficiency; Cost-benefit ratio; Development effort; Pareto principle; Productivity; Scalability; TCO,"Wienke S., Iliev H., an Mey D., Müller M.S.",2015,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-20119-1_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984629203&doi=10.1007%2f978-3-319-20119-1_26&partnerID=40&md5=c12885122fed7a364499d8ac02badbed,"IT Center, RWTH Aachen University, Aachen, 52074, Germany; Chair for High Performance Computing, RWTH Aachen University, Aachen, 52074, Germany; JARA – High-Performance Computing, Schinkelstr. 2, Aachen, 52062, Germany",Springer Verlag,English,03029743,
Scopus,Ranking of software reliability growth models using bacterial foraging optimization algorithm,"Developingnew software is an easy task but is really very difficult when it comes to change existing software, because the new changes can affect the old functionalities of the software, hence making the software unreliable. It doesn't matter how much testing is done, but the correct result is known once the product is delivered to the customer. Hence, we need to know whether our products are reliable or not before delivering them to customers. This can be achieved by Software Reliability Models. In this paper, Bacterial Foraging Optimization Algorithm (BFOA) technique is operated on Tandem Computer data set to estimate parameters of thirteen software reliability growth models based on Least Square Estimation fitting technique. These models were then compared against eleven comparison criteria like BIAS, Variance etc. Then Distance Based Approach (DBA) is applied to rank all these models based on the comparison criteria. Among the thirteen models under study inflection s-shaped model and goel-okumoto model were ranked one and two respectively. © 2015 IEEE.",Bacterial Foraging Optimization Algorithm; Distance Based Approach; Least Square Estimation; Rank; Software reliability growth models,"Khalid B., Sharma K.",2015,Conference,"2015 International Conference on Computing for Sustainable Global Development, INDIACom 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960841975&partnerID=40&md5=cf7464db46c80a34d3ba02cf13ace8a9,"Department of Computer Engineering, Delhi Technological University, Delhi, India",Institute of Electrical and Electronics Engineers Inc.,English,,9789380544168
Scopus,An evaluation of functional size measurement methods,"Background: Software size is one of the key factors that has the potential to affect the effort of software projects. Providing accurate software size estimation is a complex task. A number of functional size measurement (FSM) methods have been proposed to quantify the size of software based on functional user requirements (user perspective). Function point analysis (FPA) was the first proposal for a FSM method and it is one of the most accepted FSM methods in the industry. Automated Function Point (AFP) method state the guidelines for automating FPA counting from software source code. Objectives: This paper reports on an experiment that compares FPA and AFP. The goal is to evaluate the measurement process on a range of performance and adoption properties such as accuracy, reproducibility, efficiency, perceived easy to use, usefulness, and intention to use. Methods: A controlled experiment was conducted to compare the two methods. Statistical analyses were conducted to find differences between the methods regarding performance and adoption properties. Results: The functional size results between the FPA and AFP methods were similar (MMRE 6- 8%). Productivity rate was about the same reported for the industry (43.4 FPA/h, 37.8 AFP/h). There were no significant differences between the methods for functional size estimation, reproducibility, and accuracy. Limitations: This is an initial experiment of a work in progress. The limited sample size and nature of the subjects may influence the results. Conclusions: These results support the claim that AFP produces similar measurement results that FPA. The automation of the AFP method could produce more consistent measurement results in conformance with the FPA counting guidelines. An automated and quick FSM counting method will increase the adoption of this metric in industry. Further research is needed to conclude more on some perceived adoption properties. Copyright © 2015 by the authors.",Automated function points AFP; Experimental procedure; Function point analysis FPA; Function points; Functional size measurement,"Quesada-López C., Jenkins M.",2015,Conference,CIBSE 2015 - XVIII Ibero-American Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936105768&partnerID=40&md5=dc4c79ca3a170de1e01ad3b8fc241491,"Center for ICT Research, University of Costa Rica, San Pedro, Costa Rica",Ibero-American Conference on Software Engineering,English,,9789972825804
Scopus,Productivity measurement in software engineering: A study of the inputs and the outputs,"Productivity measurement is constructed by the measure of tree categories of elements: inputs, outputs and factors. This concept, which started being used in the manufacturing industry, has been also a research topic within Software Engineering (SE). In this area, the most used inputs are time and effort and the most used outputs are source code and functionality. Despite of their known limitations, many of the most used productivity measures are still being used due to the information that they provide for management goals. In order to enable the construction of new productivity measures for SE practitioners, the existence of other inputs apart from time and effort, and other outputs, apart from source code and functionality is analyzed in this paper. Moreover, differences in usage of the inputs and production of the outputs among some SE job positions are analyzed and explained. Copyright © 2015, IGI Global.",Job position; Knowledge worker; Measurement; Productivity; Software engineering,"Hernández-López A., Colomo-Palacios R., Soto-Acosta P., Lumberas C.C.",2015,Journal,International Journal of Information Technologies and Systems Approach,10.4018/IJITSA.2015010103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926378964&doi=10.4018%2fIJITSA.2015010103&partnerID=40&md5=ee53f3eae094885ec308c2e0c6e65604,"Department of Computer Science, Universidad Carlos III de Madrid, Madrid, Spain; ØStfold University College, Østfold, Norway; Department of Management and Finance, Universidad de Murcia, Murcia, Spain; Department of Education, Universidad Internacional de la Rioja, Madrid, Spain",IGI Global,English,1935570X,
Scopus,A runtime cloud efficiency software quality metric,"This paper introduces the Cloud Efficiency (CE) metric, a novel runtime metric which assesses how effectively an application uses software-defined infrastructure. The CE metric is computed as the ratio of two functions: i) a benefit function which captures the current set of benefits derived from the application, and ii) a cost function which describes the current charges incurred by the application's resources. We motivate the need for the CE metric, describe in further detail how to compute it, and present experimental results demonstrating its calculation. Copyright © 2014 ACM.",Adaptive systems; Cloud computing; Cost-benefit analysis; Resource-based at-tacks; Software metrics,"Shtern M., Smit M., Simmons B., Litoiu M.",2014,Conference,"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings",10.1145/2591062.2591127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903589888&doi=10.1145%2f2591062.2591127&partnerID=40&md5=2a57d660d8f37835ada2984860463100,"York University, Toronto, Canada; Dalhousie University, Halifax, Canada",Association for Computing Machinery,English,,9781450327688
Scopus,Learning project management decisions: A case study with case-based reasoning versus data farming,"Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool (({\cal W}2)) to software project data. Results: CBR with ({\cal W}2) was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with ({\cal W}2) was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like ({\cal W}2) when data are scarce or noisy or when project data cannot be expressed in the required form of a data farmer. Future Work: This study applied our own CBR tool to several small data sets. Future work could apply other CBR tools and data farmers to other data (perhaps to explore other goals such as, say, minimizing maintenance effort). © 1976-2012 IEEE.",case-based reasoning; COCOMO; data farming; Search-based software engineering,"Menzies T., Brady A., Keung J., Hihn J., Williams S., El-Rawas O., Green P., Boehm B.",2013,Journal,IEEE Transactions on Software Engineering,10.1109/TSE.2013.43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890085180&doi=10.1109%2fTSE.2013.43&partnerID=40&md5=0d426bae5eec6ff6f0c786abc586c08f,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506, United States; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong; Jet Propulsion Laboratory, Caltech, Hong Kong; School of Informatics and Computing Indiana University, Bloomington, IN, United States; University of Southern California, United States",,English,00985589,
Scopus,Cost-benefits of traceability,"Traceability can provide substantial benefits, but many software development projects fail to employ best practise in traceability. A major factor in this is the relatively high cost of traceability and the difficulty of assessing its long term financial benefits. We discuss techniques for implementing requirements traceability in such a way as to maximise the potential benefits whilst minimising the costs, but still ensuring that trace data collected meets the needs of the project. Finally we suggest a simple, practical cost analysis for developing an appropriate traceability strategy, underpinned by value-based software engineering principles. © 2012 Springer-Verlag London Limited. All rights reserved.",,"Ingram C., Riddle S.",2013,Book Chapter,Software and Systems Traceability,10.1007/978-1-4471-2239-5_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929537506&doi=10.1007%2f978-1-4471-2239-5_2&partnerID=40&md5=2e416b49ef7c080f810dfdcb4a6773c4,"Newcastle University, United Kingdom",Springer-Verlag London Ltd,English,,9781447122395; 1447122380; 9781447122388
Scopus,An empirical study into the accuracy of it estimations and its influencing factors,"This paper is the result of two related studies done on the estimation of IT projects at a large Dutch multinational company. The first one is a study about the accuracy of different dimensions of IT project estimating: schedule, budget and effort. [Note: This paper is an extension of the paper published by the authors as ""An analysis of accuracy and learning in software project estimating"" [28].] This study is based on a dataset of 171 projects collected at the IT department of the company. We analyzed the estimation error of budget, effort and schedule. Also, we analyzed whether there is any learning (improvement) effect over time. With the results of the first study we proceeded to research what is causing the current estimation error (inaccuracy). The results of our first study show that there is no relation between accuracy of budget, schedule and effort in the company analyzed. Besides, they show that over time there is no change in the inaccuracy (effectiveness and efficiency of the estimates). In our second study we discovered that the sources of this inaccuracy are: (IT estimation) process complexity, misuse of estimates, technical complexity, requirements redefinition and business domain instability. This paper reflects and provides recommendations on how to improve the learning from historical estimates and how to manage the diverse sources of inaccuracy inside this particular company and also in other organizations. © 2013 World Scientific Publishing Company.",Accuracy of estimation; Budget; Management; Organizational learning; Schedule; Time,"Zapata A.H., Chaudron M.R.V.",2013,Conference,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194013400081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880991147&doi=10.1142%2fS0218194013400081&partnerID=40&md5=494d12a583e1398e49f1427776ca1213,"LIACS, Leiden University, Niels Bohrweg 1, Leiden, 2333 CA, Netherlands; Chalmers University, Goteborg, Sweden",,English,02181940,
Scopus,"A Study of reusability, complexity, and reuse design principles","A study is reported on the relationship of complexity and reuse design principles with the reusability of code components. Reusability of a component is measured as the ease of reuse as perceived by the subjects reusing the component. Thirty-four subjects participated in the study with each subject reusing 5 components, resulting in 170 cases of reuse. The components were randomly assigned to the subjects from a pool of 25 components which were designed and built for reuse. The relationship between the complexity of a component and the ease of reuse was analyzed by a regression analysis. It was observed that the higher the complexity the lower the ease of reuse, but the correlation is not significant. An analysis of the relationship between a set of reuse design principles, used in designing and building the components, and the ease of reuse is also reported. The reuse design principles: well-defined interface, and clarity and understandability significantly increase the ease of reuse, while documentation does not have a significant impact on the ease of reuse. Copyright 2012 ACM.",Empirical study; Reusability; Reuse design; Software reuse,"Anguswamy R., Frakes W.B.",2012,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1145/2372251.2372280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867508448&doi=10.1145%2f2372251.2372280&partnerID=40&md5=7b399cd6a6d18af18d3147fe58b5915c,"Software Reuse Lab, Virginia Tech., 7054 Haycock Road, Falls Church, VA, 22043, United States",,English,19493770,
Scopus,Software effort estimation with a generalized robust linear regression technique,"Background. Outliers and corrupted data points may unduly bias software development effort estimation models. However, given the usually limited size of software engineering data sets, removing too many data points may seriously reduce the power of the statistical tests used and the likelihood of statistically significant result. Also, statistical techniques are typically based on assumptions that are either believed to be true a priori or, at best, checked via statistical tests, without ever achieving 100% certainty on their truthfulness. Estimation models based on less strict assumptions have broader applicability and lower risks of drawing unwarranted conclusions. Aim. We investigate the usefulness of Robust Regression when building effort estimation models, by varying the degree of robustness and, thus, the number of data points that are excluded from the data analysis as outliers. Method. We have used Least Quantile of Squares (LQS) Robust Regression, a generalization of the Least Median of Squares (LMS). LMS builds a regression line by minimizing the median squared residual. LQS minimizes the order statistic of square residuals corresponding to any specified quantile, and not just the median, which is the order statistic corresponding to the 50% quantile. We have extended a statistical significance test for univariate LQS regression models. We have also built a weighted model, obtained from statistically significant LQS models, where each LQS model contributes proportionally to the quantile used. Results. We have applied LQS Linear Regression to estimate development effort on four projects from the PROMISE data set and obtained valid and significant univariate models. Conclusions. LQS may provide a valid alternative to LMS and Ordinary Least Square regressions to build estimation models when (1) balancing the need for excluding outliers and keeping enough data points to build statistically significant models and (2) using less strict assumptions underlying the regression technique.",data analysis; defect prediction; effort prediction; outliers; robust regression; software metrics; statistical significance; weighted models,"Lavazza L., Morasca S.",2012,Conference,IET Seminar Digest,10.1049/ic.2012.0027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865475724&doi=10.1049%2fic.2012.0027&partnerID=40&md5=fb9f7bf1eac3613006088c6bef4626a8,"Dipartimento di Scienze Teoriche e Applicate, Università degli Studi dell'Insubria, Varese, Italy",,English,,9781849195416
Scopus,Size estimation of web applications through Web CMF Object,"This work outlines a new methodology for estimating the size of Web applications developed with a Content Management Framework (CMF). The reason for proposing - through this work - a new methodology for size estimation is the realization of the inadequacy of the RWO method, which we had recently developed, in estimating the effort of the latest Web applications. The size metric used in the RWO method was found not to be well suited for Web applications developed through a CMF. In this work, we present the new key elements for analysis and planning, needed to define every important step in developing a Web application through a CMF. Using those elements, it is possible to obtain the size of such an application. We also present the experimental validation performed on a 7-project dataset, provided by an Italian software company. © 2012 IEEE.",Content Management Framework; experimental validation; Web application size estimation; Web Objects,"Corona E., Marchesi M., Barabino G., Grechi D., Piccinno L.",2012,Conference,"2012 3rd International Workshop on Emerging Trends in Software Metrics, WETSoM 2012 - Proceedings",10.1109/WETSoM.2012.6226986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864145722&doi=10.1109%2fWETSoM.2012.6226986&partnerID=40&md5=9220bd6f64df4c2d92c6df9f5606d74a,"Dept. of Electrical and Electronic Eng., University of Cagliari, Cagliari, Italy; Dept. of Biophysical and Electronic Eng., University of Genova, Genova, Italy; Datasiel s.p.a., Genova, Italy",,English,,9781467317627
Scopus,Methodology for estimating working time effort of the software project,"The precise estimation of the time effort of the project is one of the key limits of its success. One of the ways how to achieve a correct valuation of the project is developing of a detailed analysis, which output is a structured solution that uses use cases. This paper focuses on developing a methodology for estimating working time effort of the project for one particular company. An important part of the methodology is to build up and maintain comparative database of valued use cases and time progress of realized projects. The aim of the methodology is to deliver data for evaluating a new project.",,"Štolfa J., Štolfa S., Koběrský O., Kopka M., Kožusznik J., Snášel V.",2012,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868677946&partnerID=40&md5=5a1fbc3aaf56074df13d9ad27c26a26e,"Department of Computer Science, VŠB - Technical University of Ostrava, Faculty of Electrical Engineering and Computer Science, 17.listopadu 15, Ostrava-Poruba, Czech Republic",CEUR-WS,English,16130073,9788073781712
Scopus,Internal and external software benchmark repository utilization for effort estimation,"Data repositories play critical role in software management practices. Construction of the estimation models, benchmark of software performance indicators, identification of the process improvement opportunities, and quality assessment are major utilization areas of software repositories. This paper presents the observed difficulties in utilization of external and multi-organizational software benchmark repositories for effort estimation model construction for a software organization in finance domain. ISBSG, Albrecht, China, Desharnais, Finnish, Maxwell and Kemerer repositories' data were utilized in this study. The approach was the utilization of these repositories and organization's own repository to estimate the software development effort and evaluate whether external and multi-organizational data be used for effort estimation. © 2011 IEEE.",Benchmark repositories; Effort prediction; Software benchmarking,"Top O.O., Ozkan B., Nabi M., Demirors O.",2011,Conference,"Proceedings - Joint Conference of the 21st International Workshop on Software Measurement, IWSM 2011 and the 6th International Conference on Software Process and Product Measurement, MENSURA 2011",10.1109/IWSM-MENSURA.2011.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856189760&doi=10.1109%2fIWSM-MENSURA.2011.41&partnerID=40&md5=b3cde44cf3bc694e9c787ddec1445412,"Information Systems Department, Middle East Technical University, Ankara, Turkey",,English,,9780769544977
Scopus,An empirical evaluation of outlier deletion methods for analogy-based cost estimation,"Background: Any software project dataset sometimes includes outliers which affect the accuracy of effort estimation. Outlier deletion methods are often used to eliminate them. However, there are few case studies which apply outlier deletion methods to analogy-based estimation, so it is not clear which method is more suitable for analogy-based estimation. Aim: Clarifying the effects of existing outlier deletion methods (Cook's distance based deletion, LTS based deletion, k-means based deletion, Mantel's correlation based deletion, and EID based deletion) and our method for analogy-based estimation. Method: In the experiment, outlier deletion methods were applied to three kinds of datasets (the ISBSG, Kitchenham, and Desharnais datasets), and their estimation accuracy evaluated based on BRE (Balanced Relative Error). Our method eliminates outliers from the neighborhoods of a target project when the effort is extremely different from other neighborhoods. Results: Deletion methods which are designed to apply to analogy-based estimation (i.e. Mantel's correlation based deletion, EID based deletion, and our method) showed stable performance. Especially, only our method showed over 10% improvement of the average BRE on two datasets. Conclusions: It is reasonable to apply deletion methods designed for analogy-based estimation, and more preferable to apply our method to analogybased estimation. Copyright 2011.",Abnormal value; Case based reasoning; Effort prediction; Productivity; Project management,"Tsunoda M., Monden A., Kakimoto T., Matsumoto K.-I.",2011,Conference,ACM International Conference Proceeding Series,10.1145/2020390.2020407,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054075985&doi=10.1145%2f2020390.2020407&partnerID=40&md5=bad572c859f9b130f8d40cf1689151fd,"Nara Institute of Science and Technology, Kansai Science City, 630-0192, Japan; Kagawa National College of Technology, 355 Chokushicho, Takamatsu-shi, Kagawa 761-8058, Japan",,English,,9781450307093
Scopus,Adjusted case-based software effort estimation using bees optimization algorithm,"Case-Based Reasoning (CBR) has achieved a considerable interest from researchers for solving non-trivial or ill-defined problems such as those encountered by project managers including support for software project management in predictions and lesson learned. Software effort estimation is the key factor for successful software project management. In particular, the use of CBR for effort estimation was favored over regression and other machine learning techniques due to its performance in generating reliable estimates. However, this method was subject to variety of design options which therefore has strong impact on the prediction accuracy. Selection of CBR adjustment method and deciding on the number of analogies are such two important decisions for generating accurate and reliable estimates. This paper proposed a new method to adjust the retrieved project efforts and find optimal number of analogies by using Bees optimization algorithm. The Bees algorithm will be used to search for the best number of analogies and features coefficient values that will be used to reduce estimates errors. Results obtained are promising and the proposed method could form a useful extension for Case-based effort prediction model. © 2011 Springer-Verlag.",Bees Algorithm; Case-Based Reasoning; Software Effort Estimation,Azzeh M.,2011,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-23863-5_32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053147050&doi=10.1007%2f978-3-642-23863-5_32&partnerID=40&md5=5064772b07cfbb140509559a1f2c4a1d,"Faculty of Information Technology, Applied Science University, P.O. Box 166, Amman, Jordan",,English,03029743,9783642238628
Scopus,A framework for defect prediction in specific software project contexts,"Software defect prediction has drawn the attention of many researchers in empirical software engineering and software maintenance due to its importance in providing quality estimates and to identify the needs for improvement from project management perspective. However, most defect prediction studies seem valid primarily in a particular context and little concern is given on how to find out which prediction model is well suited for a given project context. In this paper we present a framework for conducting software defect prediction as aid for the project manager in the context of a particular project or organization. The framework has been aligned with practitioners' requirements and is supported by our findings from a systematical literature review on software defect prediction. We provide a guide to the body of existing studies on defect prediction by mapping the results of the systematic literature review to the framework. © 2011 IFIP International Federation for Information Processing.",Metric- based Defect Prediction; Software Defect Prediction; Systematical Literature Review,"Wahyudin D., Ramler R., Biffl S.",2011,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-22386-0_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053135701&doi=10.1007%2f978-3-642-22386-0_20&partnerID=40&md5=2e6242145e743b696f1f1d136f568657,"Institute for Software Technology and Interactive Systems, Vienna University of Technology, Favoritenstr. 9/188, A-1040 Vienna, Austria; Software Competence Center Hagenberg, Softwarepark 21, A-4232 Hagenberg, Austria",,English,03029743,9783642223853
Scopus,A multiple comparative study of test-with development product changes and their effects on team speed and product quality,"Researchers have typically studied the effects of Test-First Development (TFD), compared to Test-Last Development (TLD), across groups or projects, and for relatively short durations. We defined Test-With Development (TWD) as more general than the fine-grained step of TFD, but also in contrast to the large-grained phase of TLD. With our definition, we performed a multiple comparative study to explore and describe TWD product changes, and the effects of those changes on two attributes related to team speed and two attributes related to product quality, within six long-term open-source projects. Our results indicate that when developers exercised some of their changes with automated tests, on average they made significantly larger changes over time while significantly reducing their product's complexity. And, when they exercised all of their changes with tests, on average they made significantly smaller changes over time. We interpret these results to indicate that practicing TWD supports faster simplification of a product. Therefore, we conclude that teams that need to reduce their product's complexity can benefit from practicing TWD. © 2010 Springer Science+Business Media, LLC.",Multiple comparative study; Product quality; Team speed; Test-with development,"Bannerman S., Martin A.",2011,Journal,Empirical Software Engineering,10.1007/s10664-010-9137-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953171863&doi=10.1007%2fs10664-010-9137-5&partnerID=40&md5=925f4a0e1c3c72dd3a2b9dc76ed24f3c,"Computing Laboratory, University of Oxford, Wolfson Building, Parks Road, Oxford, United Kingdom",,English,13823256,
Scopus,A comparison of parametric software estimation models using real project data,"Defense managers and system engineers require estimates of project cost/effort, duration, and quality in order to secure funding and set xpectations with customers, end users, and management teams. Researchers and practitioners of software metrics have developed models to help project anagers and system engineers produce estimates of project effort, duration, and quality. These models generally quantify the project scope using estimated source lines of code or function points, and then require the application of generalized rules-of-thumb to arrive at the needed project estimates of staffing, duration, and quality. Experts agree that for these models to perform at their best, the parameters should be calibrated based on project data from the using organization. Our question was, ""How do parametric models perform out-of-the-box (that is, without calibration)?"" This is analogous to a project team without access to historical data using the models as published. What level of accuracy can they expect? We examined several published models by comparing the predicted values against the actual results from 54 software projects completed by a SEI CMMI Level 3 organization with a mature (and commended) measurement program. This paper evaluated a subset of these approaches - nine simple models (four effort estimation models, three duration estimation models, and two software quality (i.e., defect) models)-using 54 non-trivial commercial projects completed recently by a CMMI Level 3 organization. This certification means that the data was collected in a standard manner and makes sense to use in this study. It does not imply that a defined process level is needed to use the results. For the effort estimation models, we found that the upper bound of the best case model contained 81% of our projects, that is, four out of five of our projects would use less effort than predicted by the best case model, whereas the average effort estimate across all models contained only 54% of our projects, or a little better than a coin flip if we estimate using the average. Duration estimates performed significantly better. In the best case model, the upper bound estimate contained 93% of our projects with the overall model average at 91% and the lower bound estimate exceeded the actual duration more than 70% of the time. This means we can out-perform the project duration seven out of 10 times using the shortest duration estimated using the models out-of-the box. For quality modeling, one of the defect prediction approaches worked quite well, with the upper bound containing 94% of the projects (or 9.4 times out of 10 we will deliver fewer defects than forecast by the model). This information is useful to executives and managers performing early project estimates without detailed analysis of the requirements or architecture as the bounds allow them to quickly respond to customer requests with some level of confidence. So, if you are asked for a project estimate and do not have access to historical data or well-calibrated local estimation models, there is hope. Based on your available sizing information, you can use these models out-of-the-box with some success as long as you keep these things in mind: • Caper's Jones approach was the only one that (relatively) accurately addressed all three project management estimation needs for effort, duration, and quality. • None of the four effort estimation models were particularly effective with our project data, but using the upper bound of the Rone model gives the project team an 80% chance of meeting the effort estimate. • A project should never commit to the lower bound effort estimates from any of the models we evaluated. • The duration estimation models are particularly effective with our project data. Using the upper bound of the Boehm model gives a project team a better than 90% chance of completing the project within the estimated calendar time. • Capers Jones' quality model was the most accurate predictor of quantity of defects in our software development projects. From our analysis, it appears as though duration and quality models are quite useful, but effort estimation is still problematic. We suggest researchers investigate other approaches to effort estimation that are not based on SLOC or Function Points. For example, models that rely on use cases or story points and can estimate all three key parameters (i.e., effort, duration, and quality) may prove valuable in the future. The translation from mission or business need to requirements and architecture is a huge challenge that impacts estimates on each iteration, by developing models to address these early solution descriptions, managers and system engineers can benefit with earlier estimates.",,Stark G.,2011,Journal,CrossTalk,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951524273&partnerID=40&md5=72dc666c142aa9f3af427e6c1b5bc490,"IBM Global Services, Australia",,English,,
Scopus,The role of the measure of functional complexity in effort estimation,"Background. Currently there are several definitions of measures that should represent the size of software functional requirements. These measures have gained a quite relevant role, since they are one of the few basis upon which effort estimation can be based. However, traditional Functional Size Measures do not take into account the amount and complexity of the elaboration required, concentrating instead on the amount of data accessed or moved. This is a problem, when it comes to effort estimation, since the amount and complexity of the required data elaborations affect the implementation effort, but are not adequately represented by the current measures (including the standardized ones). Objective. The paper evaluates different types of functional size measures as effort estimators. Moreover, the consequences of taking into consideration also the amount and complexity of required elaboration in the effort estimation models are evaluated. Methods. In this paper we take into consideration a representative set of functional size measures (namely, function points, COSMIC function points and use case points) and a recently proposed elaboration complexity measure (Paths) and evaluate how well these measures are correlated with the development effort. To this end, we measured a set of 17 projects and analyzed the resulting data. Results. We found that it is possible to build statistically valid models of the development effort that use the functional size and complexity measures as independent variables. In fact, we discovered that using the measure of elaboration complexity in addition to the functional size substantially improves the precision of the fitting. Conclusions. The analysis reported here suggests that a measure of the amount and complexity of elaboration required from a software system should be used, in conjunction with traditional functional size measures, in the estimation of software development effort. Further investigations, involving a greater number of projects, are however needed to confirm these findings.",COSMIC function points; Effort estimation; Function points; Functional complexity measurement; Functional size measurement; Use case points; Use case-based measurement,"Lavazza L., Robiolo G.",2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649762224&doi=10.1145%2f1868328.1868338&partnerID=40&md5=110a16cbdaba99ea218f20f7c80c7e10,"Dipartimento di Informatica e Comunicazione, Università degli Studi dell'Insubria, Via Mazzini, 5, 21100 Varese, Italy; Facultad de Ingeniería, Universidad Austral, Av. Juan de Garay 125, 1063 Bs.As., Argentina",,English,,9781450304047
Scopus,E-cost estimation using expert judgment and COCOMO II,"The accuracy and efficiency of cost estimation methodology for web-based application is very important for software development as it would be able to assist the management team to estimate the cost. Furthermore, it will ensure that the development of cost is within the planned budget and provides a fundamental motivation towards the development of web-based application proíéct. The literature review reveals that COCOMO II provides accurate result because more variables are considered including reuse parameter. The parameter is one of the essential variables in estimating the cost in web-based application development. This research investigates the feasibility to combine and implement COCOMO II and el pert I Udgment technil be in a tool called WebCost. In estimating a cost, the tool considers all variables in COCOMO II and rei bires el pert l udgment to key-in the input of the variables such as pro I éct size, pro I éct type, cost ad i Ustment factor and cost driven factor. I leveloped in JA I A, WebCost is proven able to estimate cost and generate its estimation result. The usability evaluation conducted had shown that WebCost is usable when compared with other tools I lit has its own advantages. WebCost is evidence suitable for everyone especially the prolect managers, software practitioners or software engineering student in handling the cost estimation tasks. © 2010 IEEE.",COCOMO II; Cost estimation; Expert judgment; Web-based application; WebCost,"Mansor Z.B., Kasirun Z.M., Arshad N.H.H., Yahya S.",2010,Conference,"Proceedings 2010 International Symposium on Information Technology - System Development and Application and Knowledge Society, ITSim'10",10.1109/ITSIM.2010.5561466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049393651&doi=10.1109%2fITSIM.2010.5561466&partnerID=40&md5=1a4ea556939d71349d61240d58e9923b,"Department of Computer Science, Faculty of Industrial Information Technology, Universiti Industri Selangor, Selangor, Malaysia; Department of Software Engineering, Faculty of Computer Science and Information Technology, Universiti Malaya, Kuala Lumpur, Malaysia; System Science Studies, Faculty of Computer Science and Mathematics, Universiti Teknologi Mara, Selangor, Malaysia; Computer Technology and Networking Studies, Faculty of Computer Science and Mathematics, Universiti Teknologi Mara, Selangor, Malaysia",,English,,9781424467181
Scopus,Feature subset selection for software cost modelling and estimation,"Feature selection has been recently used in the area of software engineering for improving the accuracy and robustness of software cost models. The idea behind selecting the most informative subset of features from a pool of available cost drivers stems from the hypothesis that reducing the dimensionality of datasets will significantly minimise the complexity and time required to reach to an estimation using a particular modelling technique. This work investigates the appropriateness of attributes, obtained from empirical project databases and aims to reduce the cost drivers used while preserving performance. Finding suitable subset selections that may cater improved predictions may be considered as a pre-processing step of a particular technique employed for cost estimation (filter or wrapper) or an internal (embedded) step to minimise the fitting error. This paper compares nine relatively popular feature selection methods and uses the empirical values of selected attributes recorded in the ISBSG and Desharnais datasets to estimate software development effort. © 2010 CRL Publishing Ltd.",Feature subset selection; Software cost modelling and estimation,"Papatheocharous E., Papadopoulos H., Andreou A.S.",2010,Conference,Engineering Intelligent Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863915960&partnerID=40&md5=438dd5f31308676aa140b36a6e8d40f8,"University of Cyprus, Department of Computer Science, 75 Kallipoleos Street, CY1678 Nicosia, Cyprus; Frederick University, Computer Science and Engineering Department, 7 Y. Frederickou Street, Palouriotisa, 1036 Nicosia, Cyprus; Cyprus University of Technology, Department of Electrical Engineering and Information Technology, 31 Archbishop Kyprianos Street, 3036 Lemesos, Cyprus",,English,14728915,
Scopus,An Overview of Web Effort Estimation,"A cornerstone of Web project management is sound effort estimation, the process by which effort is predicted and used to determine costs and allocate resources effectively, thus enabling projects to be delivered on time and within budget. Effort estimation is a complex domain where the causal relationship among factors is nondeterministic with an inherently uncertain nature. For example, assuming there is a relationship between development effort and developers’ experience using the development environment, it is not necessarily true that higher experience will lead to decreased effort. However, as experience increases so does the probability of decreased effort. The objective of this chapter is to provide an introduction to the process of estimating effort, discuss existing techniques used for effort estimation, and explain how a Web company can take into account the uncertainty inherent to effort estimation when preparing a quote. Therefore, this chapter is aimed to provide Web companies, researchers, and students with an introduction to the topic of Web effort estimation. © 2010 Elsevier Inc.",Bayesian networks; Case-based reasoning; Classification and regression trees; Effort accuracy; Regression analysis; Web effort estimation; Web effort estimation techniques; Web size measures,Mendes E.,2010,Book Chapter,Advances in Computers,10.1016/S0065-2458(10)78005-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869237101&doi=10.1016%2fS0065-2458%2810%2978005-0&partnerID=40&md5=7de31ed08def393e3d4127fe9285819a,"Computer Science Department, The University of Auckland, Auckland, New Zealand",,English,00652458,
Scopus,Selecting optimal maintenance plans based on cost/reliability tradeoffs for software subject to structural and behavioral changes,"Software maintenance is assuming ever more a crucial role in the lifecycle due to the high variability of software requirements and environment. New development paradigms are being defined to support the numerous decisions that have to be taken after the software deployment. On the basis of the increasing request of software quality, nonfunctional attributes should enter in the decisional process to avoid changes that compromise the software quality. In this paper we define an optimization model that drives the choice of a maintenance plan (i.e. a set of maintenance actions to be taken) in correspondence of a certain change scenario. A change scenario is a set of new requirements that induce changes in the structural and behavioral architecture of the software system. The solution of such model, as shown in this paper on a mobile application, provides the set of actions that minimize the maintenance cost while guaranteeing a certain level of software reliability. We also show how this instrument can be used to perform a sensitivity analysis of maintenance plans vs cost/reliability tradeoff. © 2010 IEEE.",Optimization model; Software cost; Software reliability,"Cortellessa V., Mirandola R., Potena P.",2010,Conference,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",10.1109/CSMR.2010.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952652673&doi=10.1109%2fCSMR.2010.15&partnerID=40&md5=23a3868518b10d1a89fb978034231b44,"Dipartimento di Informatica, Università dell'Aquila, Via Vetoio, 1, Coppito (AQ), 67010, Italy; Politecnico di Milano, Dipartimento di Elettronica e Informazione, Piazza Leonardo da Vinci, 32, Milano, 20133, Italy",IEEE Computer Society,English,15345351,9780769543215
Scopus,An empirical comparison of function points and web objects,"This paper compares the effectiveness of Function Points and Web Object analysis to estimate the development effort of Web applications. The empirical study is performed on a set of industrial projects carried on in an Italian software company. Starting from original requirements and estimates, carried on using FP analysis, we compute the Web Objects analysis of the same requirements, and compare the results of the two approaches. We also assess the relative effectiveness of FP and WO using post-mortem data about the actual effort taken to develop the studied systems. ©2009 IEEE.",,"Barabino G., Porruvecchio G., Concas G., Marchesi M., De Lorenzi R., Giaccardi M.",2009,Conference,"Proceedings - 2009 International Conference on Computational Intelligence and Software Engineering, CiSE 2009",10.1109/CISE.2009.5365936,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949713742&doi=10.1109%2fCISE.2009.5365936&partnerID=40&md5=884263d969db702346e072e8480e7344,"Department of Biophysical and Electronic Engineering, University of Genova, 16145 Genova, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, 09123 Cagliari, Italy; Datasiel S.p.a, 16149 Genova, Italy",,English,,9781424445073
Scopus,Quantifying IT estimation risks,"A statistical method is proposed for quantifying the impact of factors that influence the quality of the estimation of costs for IT-enabled business projects. We call these factors risk drivers as they influence the risk of the misestimation of project costs. The method can effortlessly be transposed for usage on other important IT key performance indicators (KPIs), such as schedule misestimation or functionality underdelivery. We used logistic regression as a modeling technique to estimate the quantitative impact of risk factors. We did so because logistic regression has been applied successfully in fields including medical science, e.g. in perinatal epidemiology, to answer questions that show a striking resemblance to the questions regarding project risk management. In our study we used data from a large organization in the financial services industry to assess the applicability of logistic modeling in quantifying IT risks. With this real-world example we illustrated how to scrutinize the quality and plausibility of the available data. We explained how to deal with factors that cannot be influenced, also called risk factors, by project management before or in the early stage of a project, but can have an influence on the outcome of the estimation process. We demonstrated how to select the risk drivers using logistic regression. Our research has shown that it is possible to properly quantify these risks, even with the help of crude data. We discussed the interpretation of the models found and showed that the findings are helpful in decision making on measures to be taken to identify potential misestimates and thus mitigate IT risks for individual projects. We proposed increasing the auditing process efficiency by using the found cost misestimation models to classify all projects as either risky projects or non-risky projects. We discovered through our analyses that projects must not be overstaffed and the ratio of external developers must be kept small to obtain better cost estimates. Our research showed that business units that report on financial information tend to be risk mitigating, because they have more cost underruns in comparison with business units without reporting; the latter have more cost overruns. We also discovered a maturity mismatch: an increase from CMM level 1 to 2 did not influence the disparity between a cost estimate and its actual if the maturity of the business is not also increased. © 2009 Elsevier B.V. All rights reserved.",Cost estimation; Cost overrun; Cost underrun; Data plausibility; Data quality; EQF; Forecasting errors; IT-enabled business investment; Lift chart; Logistic regression; Maturity mismatch; Misestimation; Quantifying IT risks; Risk drivers; Risk factors,"Kulk G.P., Peters R.J., Verhoef C.",2009,Journal,Science of Computer Programming,10.1016/j.scico.2009.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449115569&doi=10.1016%2fj.scico.2009.09.001&partnerID=40&md5=9668c8a3432ae4194a0615b3cc720d61,"VU University Amsterdam, Department of Computer Science, De Boelelaan 1081a, 1081 HV Amsterdam, Netherlands",,English,01676423,
Scopus,Improving the accuracy of effort estimation through fuzzy set combination of size and cost drivers,"In this research, it is investigated the precision of size and cost drivers in the estimation of effort using Constructive Cost Model (COCOMO). It is imperative to stress that uncertainty at the input level of the COCOMO yields uncertainty at the output, which leads to gross estimation error in the effort estimation. Instead of using a single number to represent the size, it can be characterized as a fuzzy value. Cost drivers also expressed through an unclear category which needs subjective assessment. Fuzzy logic has been applied to the COCOMO using the symmetrical triangles and trapezoidal membership functions to represent the cost drivers and size. Using trapezoidal membership function for the size and cost drivers, a few attributes are assigned the maximum degree of compatibility when they should be assigned lower degrees. To overcome the above limitations, in this work, it is concentrated to use Gaussian membership function for the COCOMO parameters. In addition, this paper proposes to incorporate both size and cost drivers together, with a fuzzy set using Gaussian membership function. The present work is based on COCOMO dataset and the experimental part of the study illustrates the approach and compares it with the standard version of the COCOMO. It has been found that the proposed method is performing better than ordinal COCOMO and the achieved results were closer to the actual effort.",COCOMO; Constructive Cost Model; Fuzzy based effort estimation; Gaussian membership function; Software cost estimation; Software effort estimation; Software size and project management,"Reddy Ch.S., Raju K.",2009,Journal,WSEAS Transactions on Computers,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69349083743&partnerID=40&md5=70880dcadf0080de0e9004a382b7ef37,"Department of Computer Science and Systems Engineering, College of Engineering, Andhra University, Visakhapatnam, India",,English,11092750,
Scopus,Application of neural networks in software engineering: A review,"The software engineering is comparatively new and ever changing field. The challenge of meeting tight project schedules with quality software requires that the field of software engineering be automated to large extent and human intervention be minimized to optimum level. To achieve this goal the researchers have explored the potential of machine learning approaches as they are adaptable, have learning capabilities and non-parametric. In this paper, we take a look at how Neural Network (NN) can be used to build tools for software development and maintenance tasks. © 2009 Springer Berlin Heidelberg.",Neural network; Software metrics; Software testing,"Singh Y., Bhatia P.K., Kaur A., Sangwan O.",2009,Journal,Communications in Computer and Information Science,10.1007/978-3-642-00405-6_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66049144557&doi=10.1007%2f978-3-642-00405-6_17&partnerID=40&md5=ab3e8f0eb67daa95e989290f03dfbf42,"University School of Information and Technology, G.G.S. IP University, Delhi, India; Dept. of CSE, GJU of Science and Technology, Hisar, Haryana, India; Amity Institute of Information Technology, Amity University, Uttar Pradesh, India",,English,18650929,9783642004049
Scopus,Quality assurance of agent-based and self-managed systems,"The challenges in implementing intelligent and autonomous software systems remain the development of self-adapting systems, self-healing applications, corporate global creation, and collaborated robotic teams. With software agent technology widely recognized as a key approach in implementing such global infrastructure, the importance of the role of quality assurance of agent-based systems and system development is growing daily. Based on the authors’ more than fifteen years of experience in software agent technology, Quality of Agent-Based and Self-Managed Systems presents the basics principles and structures of agent technology. It covers the main quality issues of software system development and provides examples of agent measurement and evaluation. The authors focus on software agent systems and multi-agent systems (MAS) and discuss the determination of quality properties. They also explain different techniques and approaches to evaluate the development of MAS. The final chapter summarizes quality assurance approaches for agent-based systems and discusses some open problems and future directions. Although often complex and difficult to manage, the applications for software agent systems in essential life systems increase every day. Since the quality of the agent-based self-managing systems is a central point of software risks, analyzing, evaluating, and improving the quality measurement situation will always be a concern when developing these systems. With more than sixty illustrations and twenty tables, this book builds a foundation in quality and quality control for agent-based technology. © 2010 by Taylor and Francis Group, LLC.",,"Dumke R.R., Mencke S., Wille C.",2009,Book,Quality Assurance of Agent-Based and Self-Managed Systems,10.1201/9781439812679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055770953&doi=10.1201%2f9781439812679&partnerID=40&md5=d582453e0da9c5aadd1aa3c4414af623,"University of Magdeburg, Germany; Software Engineering Group, Computer Science Department, University of Magdeburg, Germany; University of Applied Sciences, Bingen, Germany",CRC Press,English,,9781439812679; 9781439812662
Scopus,"A framework for software project estimation based on COSMIC, DSM and rework characterization","Effective software project estimation is one of the most challenging activities in software development. In today's highly competitive world, accurate software estimation can make the difference between successful projects and dismal failures. Proper project planning and control is not possible without a sound and reliable estimate. In this paper we propose a framework, developed by Ericsson R&D Italy, for project time and cost estimation for software development projects in the telecommunications domain. The customization of Design Structure Matrix (DSM), the application of COSMIC and the study of defect complexity curves are the components of this new estimation framework. The joint application of these three components allows all stakeholders interested in the estimation result to have a common view based on objective data and to understand how a change to functional and quality requirements can impact the result. Copyright 2008 ACM.",COSMIC; Defect analysis; DSM; Estimation,"Afsharian S., Giacomobono M., Inverardi P.",2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370837.1370842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049105066&doi=10.1145%2f1370837.1370842&partnerID=40&md5=6b8131d2ddbde9709856761b20077f79,"Computer Science Department, University of L'Aquila, Via Vetoio, Loc. Coppito, I-67100 L'Aquila, Italy; Ericsson R and D Italy, Via Anagnina 203, I-00118 Rome, Italy",,English,02705257,9781605580418
Scopus,Estimate test execution effort at an early stage: An empirical study,"Software testing is becoming more and more important as it is a widely used activity to ensure software quality. Test execution becomes an activity in the critical path of a project. In this case, early estimation of test execution effort can benefit both tester managers and software projects. This paper reports an empirical study on early test execution effort estimation. In the study, we propose an approach which mainly consists of two parts: test case number prediction from use cases and test effort estimation based on the test suite execution vector model which combines test case number, test execution complexity and its tester together. For each part, we evaluate it with the data of real projects from a financial software company. © 2008 IEEE.",Test execution complexity; Test suite execution vector; Use case verification point,"Zhu X., Zhou B., Wang F., Qu Y., Chen L.",2008,Conference,"Proceedings of the 2008 International Conference on Cyberworlds, CW 2008",10.1109/CW.2008.34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949100319&doi=10.1109%2fCW.2008.34&partnerID=40&md5=979f92287e7b6a00d6ff4ed850e168e5,"College of Computer Science, Zhejiang University, Hangzhou, China; State Street Technology Zhejiang, Hangzhou, China",,English,,9780769533810
Scopus,Tests for consistent measurement of external subjective software quality attributes,"One reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate a prediction system for the attribute. However, attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts' intuition about the attribute. These attempts are undertaken without a pre-defined scale on which it is known that the attribute can be measured consistently. Consequently, a valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts' measurement process is not independent of the prediction system's values. Usually, no justification is given for not checking to see if the experts can measure the attribute consistently. It seems to be assumed that: subjective measurement isn't proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated. However, even though the classification of software systems' or software artefacts' quality attributes is subjective, it is possible to quantify experts' measurements in terms of conditional probabilities. It is then possible, using a statistical approach, to assess formally whether the experts' measurements can be considered consistent. If the measurements are consistent, it is also possible to identify estimates of the true values, which are independent of the prediction system. These values can then be used to assess the predictive capability of the prediction system. In this paper we use Bayesian inference, Markov chain Monte Carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes. © 2007 Springer Science+Business Media, LLC.",Bayesian inference; Conditional probability; Consistent measurement; Data imputation; Distribution principle; Error rates; Minimum rejection principle; Multinomial distribution; Subjective software quality attributes,"Moses J., Farrow M.",2008,Journal,Empirical Software Engineering,10.1007/s10664-007-9058-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649113885&doi=10.1007%2fs10664-007-9058-0&partnerID=40&md5=a1bfb8aa7d47a4b4e4c7b1a4f8341d92,"School of Computing and Technology, University of Sunderland, Sunderland SR6 0DD, United Kingdom; Department of Mathematics and Statistics, University of Newcastle-Upon-Tyne, Newcastle-Upon-Tyne NE1 7RU, United Kingdom",,English,13823256,
Scopus,Issues on estimating software metrics in a large software operation,"Software engineering metrics prediction has been a challenge for researchers throughout the years. Several approaches for deriving satisfactory predictive models from empirical data have been proposed, although none has been massively accepted due to the difficulty of building a generic solution applicable to a considerable number of different software projects. The most common strategy on estimating software metrics is the linear regression statistical technique, for its ease of use and availability in several statistical packages. Linear regression has numerous shortcomings though, which motivated the exploration of many techniques, such as data mining and other machine learning approaches. This paper reports different strategies on software metrics estimation, presenting a case study executed within a large worldwide IT company. Our contributions are the lessons learned during the preparation and execution of the experiments, in order to aid the state of the art on prediction models of software development projects. © 2009 IEEE.",Human judgment approaches; Linear regression; Machine learning; Software metrics estimation,"Barros R.C., Ruiz D.D., Tenõrio Jr. N.N., Basgalupp M.P., Becker K.",2008,Conference,"32nd Annual IEEE Software Engineering Workshop, SEW-32 2008",10.1109/SEW.2008.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951157874&doi=10.1109%2fSEW.2008.22&partnerID=40&md5=92a6db26a61fa7c32be93103b25a622c,"Faculty of Informatics, Pontifical Catholic University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Math Sciences and Computing, University of São Paulo, São Carlos, Brazil; Quality Knowledge Ltda., Porto Alegre, Brazil",IEEE Computer Society,English,,9780769536170
Scopus,Improvement of causal analysis using multivariate statistical process control,"Statistical process control (SPC) is a conventional means of monitoring software processes and detecting related problems, where the causes of detected problems can be identified using causal analysis. Determining the actual causes of reported problems requires significant effort due to the large number of possible causes. This study presents an approach to detect problems and identify the causes of problems using multivariate SPC. This proposed method can be applied to monitor multiple measures of software process simultaneously. The measures which are detected as the major impacts to the out-of-control signals can be used to identify the causes where the partial least squares (PLS) and statistical hypothesis testing are utilized to validate the identified causes of problems in this study. The main advantage of the proposed approach is that the correlated indices can be monitored simultaneously to facilitate the causal analysis of a software process. © 2008 Springer Science+Business Media, LLC.",Causal analysis; Multivariate statistical process control; Software process improvement,"Chang C.-P., Chu C.-P.",2008,Conference,Software Quality Journal,10.1007/s11219-007-9042-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48449086197&doi=10.1007%2fs11219-007-9042-3&partnerID=40&md5=623946cf5fdde4737d5e5bd02f373453,"Department of Computer Science and Information Engineering, National Cheng-Kung University, Taiwan, No.1, Ta-Hsueh Road, Tainan 701, Taiwan",Kluwer Academic Publishers,English,09639314,
Scopus,Effective fault localization using BP neural networks,"Fault localization is the most expensive activity of program debugging. It identifies the exact locations of program faults. Finding these faults using an ad-hoc approach or based only on programmers' intuitive guesswork can be very time consuming. A better way is to use a well-justified technique, supported by case studies for its effectiveness, to automatically identify and prioritize suspicious code for an examination of possible fault locations. To do so, we propose the use of a back-propagation (BP) neural network, a machine learning model which has been successful applied to software risk analysis, cost prediction, and reliability estimation, to help programmers effectively locate program faults. A BP neural network is suitable for learning the input-output relationship from a set of data, such as the inputs and the corresponding outputs of a program. We first train a BP neural network with the coverage data (e.g., statement coverage) collected from executing a program, and then we use it to compute the risk of each statement, in terms of its likelihood of containing faults. Suspicious code is ranked in descending order based on its risk. Programmers will examine such code from the top of the rank to identify faults. A case study using the seven programs in the Siemens suite is conducted. Our results suggest that a BP neural network-based fault localization method is effective in locating program faults. Copyright © (2007) by Knowledge Systems Institute (KSI).",BP (back-propagation) neural network; Execution slice; Failed test; Fault localization; Program debugging; Risk of code; Successful test,"Wong W.E., Zhao L., Qi Y., Cai K.-Y., Dong J.",2007,Conference,"19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51949084076&partnerID=40&md5=5c923138411bf0ffae1d2c2f724734d4,"Department of Computer Science, University of Texas, Dallas, United States; Department of Automatic Control, Beijing University of Aeronautics and Astronautics, China",,English,,9781627486613
Scopus,An approach to global sensitivity analysis: FAST on COCOMO,"There are various models in software engineering that are used to predict quality-related aspects of the process or artefacts. The use of these models involves elaborate data collection in order to estimate the input parameters. Hence, an interesting question is which of these input factors are most important. More specifically, which factors need to be estimated best and which might be removedfrom the model? This paper describes an approach based on global sensitivity analysis to answer these questions and shows its applicability in a case study on the COCOMO application at NASA. © 2007 IEEE.",,Wagner S.,2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949087628&doi=10.1109%2fESEM.2007.15&partnerID=40&md5=e9d5766580ddfe7a38c5159b84780665,"Institut für Inforrnatik, Technische Universität München, Boltzmannstr. 3, 85748 Garching B. München, Germany",,English,,0769528864; 9780769528861
Scopus,An approach to probabilistic effort estimation for military avionics software maintenance by considering structural characteristics,"The needs of software maintenance and the importance of maintenance project management increase rapidly in the military avionics industry. Although few previous studies related to the maintenance effort estimation were proposed, they had two drawbacks:(1) the needs of some input parameters which are hard to measure precisely at early stage, (2) the passive support of ""what-if"" analysis by exploring the impact of changes among input parameters for more realistic estimation. In this paper, we suggest probabilistic effort estimation model for military avionics software maintenance by considering structural characteristics. We elicited the four aspects to the maintenance effort including structural characteristics, developed probabilistic maintenance effort estimation model based on Bayesian network with a data set of 76 military avionics software maintenance projects in the Republic Of Korea AirForce(ROKAF). © 2007 IEEE.",,"Song T.-H., Yoon K.-A., Bae D.-H.",2007,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2007.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949232134&doi=10.1109%2fAPSEC.2007.19&partnerID=40&md5=edc95e696b31e9da8e39f19365952665,"Software Engineering Laboratory, Div. of Computer Science, EECS, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Kusong-dong, Yusong-gu, Taejon, 305-701, South Korea",,English,15301362,0769530575; 9780769530574
Scopus,Is this cost estimate reliable? - the relationship between homogeneity of analogues and estimation reliability,"Analogy-based cost estimation provides a useful and intuitive means to support decision making in software project management. It derives a cost estimate required for completing a project from information about similar past projects, namely the analogues. While on average this method provides a relatively accurate cost estimate there remains a possibility of large estimation errors. In this paper, we empirically tested the hypothesis that ""using more homogeneous analogues produces a more reliable cost estimate"" using a software engineering data repository established by the Software Engineering Center (SEC), Information-technology Promotion Agency, Japan. This testing showed that low and high homogeneity projects had a large variation in estimation reliability. For instance, the difference was 22.9% (p = 0.021) in terms of percentage to get accurate estimates (better than Median of Magnitude of Relative Error). © 2007 IEEE.",,"Ohsugi N., Monden A., Kikuchi N., Barker M.D., Tsunoda M., Kakimoto T., Matsumoto K.-I.",2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.61,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649126202&doi=10.1109%2fESEM.2007.61&partnerID=40&md5=d245b48fadb0d0ac5e3c50286dcb7801,"Graduate School of Information Science, Nara Institute of Science and Technology; Software Engineering Center, Information-technology Promotion Agency, Japan",,English,,0769528864; 9780769528861
Scopus,Grey learning based software stage-effort estimation,"Software estimation is an important part of successful software project management. A lot of studies have investigated on whole project effort estimation. However there are only a few studies on stage-effort prediction. The stage-effort prediction can be used to dynamically adjust software project schedule, further to help make project be finished on budget. In this paper, we present a grey learning method base on GM(1,1) for stage-effort prediction during software development. We evaluate the proposed method with a large-scale industry software engineering database. The results are very encouraging and indicate the method has considerable potential. © 2007 IEEE.",Grey model; Machine learning; Software effort estimation; Software project management; Stage-effort estimation,"Wang Y., Song Q.-B., Shen J.-Y.",2007,Conference,"Proceedings of the Sixth International Conference on Machine Learning and Cybernetics, ICMLC 2007",10.1109/ICMLC.2007.4370377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049019135&doi=10.1109%2fICMLC.2007.4370377&partnerID=40&md5=9942d2588d9467181ef162726ac54230,"Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an 710049, China",,English,,142440973X; 9781424409730
Scopus,A fuzzy logic based approach for software testing,"How to provide cost-effective strategies for Software Testing has been one of the research focuses in Software Engineering for a long time. Many researchers in Software Engineering have addressed the effectiveness and quality metric of Software Testing, and many interesting results have been obtained. However, one issue of paramount importance in software testing - the intrinsic imprecise and uncertain relationships within testing metrics - is left unaddressed. To this end, a new quality and effectiveness measurement based on fuzzy logic is proposed. Related issues like the software quality features and fuzzy reasoning for test project similarity measurement are discussed, which can deal with quality and effectiveness consistency between different test projects. Experiments were conducted to verify the proposed measurement using real data from actual software testing projects. Experimental results show that the proposed fuzzy logic based metrics is effective and efficient to measure and evaluate the quality and effectiveness of test projects. © World Scientific Publishing Company.",Fuzzy logic; Fuzzy reasoning; Software engineering; Software testing; Test metric,"Zhang Z., Zhou Y.",2007,Journal,International Journal of Pattern Recognition and Artificial Intelligence,10.1142/S0218001407005636,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250794116&doi=10.1142%2fS0218001407005636&partnerID=40&md5=634ebc9f320b549ab007b086fd653f83,"School of Engineering and Information Technology, Deakin University, Geelong, Vic. 3217, Australia; Intelligent Software and Software Engineering Laboratory, Southwest University, Chongqing 400715, China",,English,02180014,
Scopus,Application of a statistical methodology to simplify software quality metric models constructed using incomplete data samples,"During the construction of a software metric model, incomplete data often appear in the data sample used for the construction. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable ""retrospectively"" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, the authors have earlier derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a ""goodness-of- ftt"" criterion. Whilst the methodology has been applied successfully to software effort metric models, it is applied only recently to software quality metric models which usually suffer from far more serious incomplete data. This paper documents the latter application based on a successful case study. © 2006 IEEE.",,"Chan V.K.Y., Wong W.E., Xie T.F.",2006,Conference,Proceedings - International Conference on Quality Software,10.1109/QSIC.2006.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250775940&doi=10.1109%2fQSIC.2006.13&partnerID=40&md5=6f9008284d3e7261dd45eb88a14f0d72,"School of Business, Macao Polytechnic Institute, Rua de Luis Gonzaga Gomes, Macau; Department of Computer Science, University of Texas at Dallas, Richardson, TX 75083, United States; Department of Mathematics, School of Science, Beijing Institute of Technology, Beijing, 100081, China",,English,15506002,0769527183; 9780769527185
Scopus,Selecting the appropriate machine learning techniques for the prediction of software development costs,"This paper suggests several estimation guidelines for the choice of a suitable machine learning technique for software development effort estimation. Initially, the paper presents a review of relevant published studies, pointing out pros and cons of specific machine learning methods. The techniques considered are Association Rules, Classification and Regression Trees, Bayesian Belief Networks, Neural Networks and Clustering, and they are compared in terms of accuracy, comprehensibility, applicability, causality and sensitivity. Finally the study proposes guidelines for choosing the appropriate technique, based on the size of the training data and the desirable features of the extracted estimation model. © 2006 International Federation for Information Processing.",,"Bibi S., Stamelos I.",2006,Journal,IFIP International Federation for Information Processing,10.1007/0-387-34224-9_62,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749142522&doi=10.1007%2f0-387-34224-9_62&partnerID=40&md5=a1dd3ac8bda1f1a6adcf7a2fc281f861,"Aristotle University of Thessaloniki, Department of Informatics, 54124 Thessaloniki, Greece",,English,15715736,0387342230; 9780387342238
Scopus,Facets of Software Evolution,[No abstract available],Evolution of systems-of-systems; Evolution phenomenon and drivers for evolution; External market factors and internal feedback factors; Lines Of Code (LOC); SESAM/AMEISE system evolution; Software architecture - module and system-of-systems; Software evolution; Software-size strata and complexity; Support mechanism spectrum evolution; User-driven change requests,Mittermeir R.T.,2006,Book Chapter,Software Evolution and Feedback: Theory and Practice,10.1002/0470871822.ch4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882344402&doi=10.1002%2f0470871822.ch4&partnerID=40&md5=751cdd11290965a891a4d989f57f07b9,"Institut für Informatik-Systeme, Universität Klagenfurt, Austria","John Wiley & Sons, Ltd",English,,0470871806; 9780470871805
Scopus,An empirical study of using rule induction and rough sets to software cost estimation,"This paper concerns problems of applying the approach based on rough sets and rule induction to a software engineering data analysis. More precisely, we focus our interest on a software cost estimation problem, which includes predicting the effort required to develop a software system basing on values of cost factors. The case study of analysing the COCOMO data set, containing descriptions of representative historical projects, allows us to discuss how this approach could be used to: identify the most discriminatory cost factors, extract meaningful rule representation of classification knowledge from data, construct accurate rule based classifiers.",COCOMO model; Machine Learning; Preference Modeling; Rough Sets; Software Cost Prediction; Software Engineering,Stefanowski J.,2006,Journal,Fundamenta Informaticae,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744912770&partnerID=40&md5=9dd3f550425943134c6a2aa4cb0b5039,"Institute of Computing Sciences, Poznań University of Technology, ul. Piotrowo 3a, 60-965 Poznań, Poland; Wyzsza Szkola Bankowa W Poznaniu, al. Niepodleglości 2, 61-874 Poznań, Poland",,English,01692968,
Scopus,Portfolio management of software development projects using COCOMO II,"Software development projects are subject to external and internal risks that cause delays, budget overrun and poor quality. Portfolio management can be used to alleviate this problem, as it pools resources together and allows for resource sharing among projects. Consequently, projects are more likely to succeed. However, portfolio management using only deadlines and the number of employees to improve probability of success is still confined. This paper proposes integrating portfolio management with COCOMO II that offers more management flexibility. Managers can adjust other resources, such as tools, staff capability, communication support, etc. to improve the project's success. The proposed method can also be applied despite limited historical data and expert judgment. In addition, this paper introduces time constraints into portfolio management without assuming unrealistic linearity between effort and time.",COCOMO II; Portfolio management; Software development; Software project; Software project risk,"Jiamthubthugsin W., Sutivong D.",2006,Conference,Proceedings - International Conference on Software Engineering,10.1145/1134285.1134443,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247124184&doi=10.1145%2f1134285.1134443&partnerID=40&md5=f7880d1bf29af0bdea0c610f7d6095bc,"Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok 10330, Thailand",IEEE Computer Society,English,02705257,1595933751; 9781595933751
Scopus,Segmented parametric software estimation models: Using the EM algorithm with the ISBSG 8 database,"Parametric software estimation models rely on the availability of historical project databases from which estimation models are derived. In the case of large project databases with data coming from heterogeneous sources, a single mathematical model cannot properly capture the diverse nature of the projects under consideration. In this paper, a clustering algorithm have been used as a tool to produce segmented models. A concrete case study using a modified EM algorithm is reported. In cases in which the clustering algorithm produces an statistical characterization of each of the resulting clusters, as happens with the EM algorithm, such representations can be used as similarity metrics to derive analogical estimates. This can be put in contrast with deriving partial parametric models for each cluster. The paper also provides a comparison of quality of adjustment of both approaches.",Clustering; Effort estimation; EM algorithm; Software Engineering,"Garre M., Cuadrado J.J., Sicilia M.A., Charro M., Rodríguez D.",2005,Conference,"Proceedings of the International Conference on Information Technology Interfaces, ITI",10.1109/ITI.2005.1491119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745261560&doi=10.1109%2fITI.2005.1491119&partnerID=40&md5=d0d65739cbae83101b7cf5b777f40186,"Dept. of Computer Science, University of Alcalá, Ctra. Barcelona km 33.6, 28871 Alcala de Henares, Madrid, Spain; Spanish Air Force Dept., Applied Sciences Torrejón Air Base, Technical School, 28850 - Torrejon de Ardoz, Madrid, Spain; Dept. of Computer Science, University of Reading, PO Box 225, Whiteknights Reading RG6 6AY, United Kingdom",,English,13301012,953713802X; 9789537138028
Scopus,An Intelligent Early Warning System for Software Quality Improvement and Project Management,"One of the main reasons behind unfruitful software development projects is that it is often too late to correct the problems by the time they are detected. It clearly indicates the need for early warning about the potential risks. In this paper, we discuss an intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It helps to assess risks associated with being behind schedule, over budget, and poor quality in software development and maintenance from multiple perspectives. It handles incomplete, inaccurate, and imprecise information, and resolve conflicts in an uncertain environment in its software risk assessment using fuzzy linguistic variables, fuzzy sets, and fuzzy inference rules. Process, product, and organizational metrics are collected or computed based on solid software models. The intelligent risk assessment process consists of the following steps: fuzzification of software metrics, rule firing, derivation and aggregation of resulted risk fuzzy sets, and defuzzification of linguistic risk variables.",,"Liu X., Kane G., Bambroo M.",2003,Conference,Proceedings of the International Conference on Tools with Artificial Intelligence,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344704296&partnerID=40&md5=8f95a0f564eb5ffc8a287a80745a143b,"Dept. of Comp. Sci., Univ. of Missouri-Rolla, United States",,English,10636730,
Scopus,An approach to visualizing empirical software project portfolio data using multidimensional scaling,"Software project portfolio managers and process engineers have to manage increasingly large software project portfolios rather than single projects. Typical portfolio decisions like resource allocation, effort estimation and risk valuation are to be based on complex, high-dimensional software project portfolio data. Handling and understanding this vast amount of data is difficult. Classic analysis approaches-like ABC-analysis for project prioritization, or interactive data mining tools-mostly analyze specific and separate subsets of the information for a given purpose. This paper (i) applies multidimensional scaling methods for visualizing high-dimensional portfolio information, involving data from many different sources, and describes promising applications of the approach in this domain, (ii) proposes a simple method to prepare raw software metric data for MDS analysis and processing with interactive tools, and (in) provides a feasibility study based on a small set of open-source software projects to demonstrate the usability of the proposed approach for software project portfolio decision support. The approach enhances explorative analysis of historic portfolio information, allows for interactive data analysis with intelligent tools, and thus directly supports portfolio assessment and prediction decisions. © 2003 IEEE.",,"Auer M., Graser B., Biffl S.",2003,Conference,"Proceedings of the 2003 IEEE International Conference on Information Reuse and Integration, IRI 2003",10.1109/IRI.2003.1251458,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893277708&doi=10.1109%2fIRI.2003.1251458&partnerID=40&md5=3e0a7e276976fa5f0e6c5756b9ab2169,"Institute of Software Technology, Vienna University of Technology, Austria",Institute of Electrical and Electronics Engineers Inc.,English,,0780382420; 9780780382428
Scopus,Assessing massive maintenance processes: An empirical study,We present an empirical study from the experience of a major international software enterprise in conducting massive adaptive maintenance projects with a close deadline. The adopted process entails the decomposition of the application portfolio into loosely coupled work-packets that can be independently and incrementally worked out by teams distributed on different sites. The study analyzes the correlation between maintenance size and productivity metrics of a large Y2K project. The resulting models allows to estimate the costs of a project conducted according to the adopted massive maintenance process and distribute them among the different phases.,,"De Lucia A., Pannella A., Pompella E., Stefanucci S.",2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972758,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956612300&doi=10.1109%2fICSM.2001.972758&partnerID=40&md5=6190ef8afe1f63adf68cbec40fb8fe90,"Faculty of Engineering, University of Sannio, Palazzo Bosco Lucarelli, Piazza Roma, 82100 Benevento, Italy; EDS Italia Software S.p.A., Viale Edison - Loc. Lo Uttaro, 81100 Caserta, Italy",,English,,
Scopus,Applicability of metrology to information technology,"In 1959 the Director of the National Bureau of Standards declared ""The emergence of science and technology as the paramount concern of the Nation in the 20th century . . . demanded the highest order of measurement competence, in order to provide the standards and measurement techniques on which maintenance of scientific progress depended."" Since 1959, information technology has emerged as having a global impact on all facets of industry. However, the ""standards and measurement techniques"" needed to maintain the scientific progress of information technology into the next century may not be in place. This paper discusses the current state of software metrics.",Function point metrics; Information technology; Software metrics,Gray M.M.,1999,Journal,Journal of Research of the National Institute of Standards and Technology,10.6028/jres.104.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040522305&doi=10.6028%2fjres.104.035&partnerID=40&md5=46e04791de0d568b7995026e4682515e,"Natl. Inst. of Std. and Technology, Gaithersburg, MD 20899-0001, United States",National Institute of Standards and Technology,English,1044677X,
Scopus,Predicting the development effort of multimedia courseware,A recurring problem faced by multimedia courseware developers is the accurate estimation of development effort. This paper outlines a metrics based model for predicting the development effort of multimedia courseware. A composite model of multimedia courseware development effort is proposed which makes use of a Rayleigh curve and cost drivers. Initial analysis of cost drivers and delivery time are described along with future work to develop a rigorous model for multimedia courseware development. © 1994.,courseware; courseware engineering; metrics; multimedia,"Marshall I., Samson W., Dugard P., Scott W.",1994,Journal,Information and Software Technology,10.1016/0950-5849(94)90080-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149148420&doi=10.1016%2f0950-5849%2894%2990080-9&partnerID=40&md5=428b8842665dd1dab98f1d629687e5db,"Department of Mathematical and Computer Sciences, University of Abertay Dundee, Bell Street, Dundee, DD1 1HG, United Kingdom; Marconi Simulation, John Sutcliffe Building, Fulmer Way, Donibristle Industrial Pk., Nr. Dunfermline, Fife KY11 5JX, United Kingdom",,English,09505849,
Scopus,Software Design Smell Detection: a systematic mapping study,"Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18 years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Antipatterns; DesignSmell; Detection tools; Quality models; Systematic mapping study,"Alkharabsheh K., Crespo Y., Manso E., Taboada J.A.",2019,Journal,Software Quality Journal,10.1007/s11219-018-9424-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055955349&doi=10.1007%2fs11219-018-9424-8&partnerID=40&md5=87ebddb6937cfc3d3f4c8bd25358477d,"Universidade de Santiago de Compostela, Santiago de Compostela, Spain; Universidade de Valladolid, Valladolid, Spain",Springer New York LLC,English,09639314,
Scopus,Influential design factors on occupant satisfaction with indoor environment in workplaces,"Occupant satisfaction with IEQ (indoor environmental quality)is influenced by many physical and psychological factors. This paper reports the results of a study that investigate influential office design factors on occupant satisfaction relating environmental dimensions such as thermal and visual comfort in workplaces and predicting which design parameters may bring better satisfaction to occupants. Five office cases in the Netherlands with 579 office occupants were studied using questionnaires, and interviews with facility managers and architects. Different statistical analysis tests were conducted to summarise satisfaction factors. Results show that ‘desk location’ and ‘layout’ contributed most to occupant's satisfaction with thermal and visual comfort regardless of seasons. In summer, ‘orientation’ was exceptionally considered as an important factor for satisfaction with thermal comfort. This study revealed that categorical and regression analyses are required to predict profound outcomes when the data are nominal and categorical variables. This study contributes to develop design solutions, which could improve occupants' environmental satisfaction in workplaces. © 2019 The Authors",Design factors; Indoor environment; Office design; Statistical analysis; User satisfaction; Workplace,"Kwon M., Remøy H., van den Bogaard M.",2019,Journal,Building and Environment,10.1016/j.buildenv.2019.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065546654&doi=10.1016%2fj.buildenv.2019.05.002&partnerID=40&md5=ca1cc33a16cd1d54e19926bd335230a5,"Department of Architectural Engineering + Technology, Faculty of Architecture and the Built Environment, Delft University of Technology, Netherlands; Department of Management in the Built Environment, Faculty of Architecture and the Built Environment, Delft University of Technology, Netherlands; Department of Science Education and Communication, Faculty of Applied Sciences, Delft University of Technology, Netherlands",Elsevier Ltd,English,03601323,
Scopus,Investigating the use of random forest in software effort estimation,"Over the last two decades, there has been an important increase in studies dealing with the software development effort estimation (SDEE) using machine learning (ML) techniques that aimed to improve the accuracy of the estimates and to understand the process used to generate these estimates. Among these ML techniques, decision tree-based models have received a considerable scholarly attention thanks to their generalization ability and understandability. However, very few studies have investigated the use of random forest (RF) in software effort estimation. In this paper, a RF model is designed and optimized empirically by varying the values of its key parameters. The performance of the RF is compared with that of classical regression tree (RT). The evaluation was performed through the 30% hold-out validation method using three datasets: ISBSG R8, Tukutuku and COCOMO. To identify the most accurate techniques, we used three widely known accuracy measures: Pred(0.25), MMRE and MdMRE. The results show that the optimized random forest outperforms the regression trees model on all evaluation criteria. © 2019 The Authors.",accuracy evaluation; random forest; regression trees; Software effort estimation,"Abdelali Z., Mustapha H., Abdelwahed N.",2019,Conference,Procedia Computer Science,10.1016/j.procs.2019.01.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062674268&doi=10.1016%2fj.procs.2019.01.042&partnerID=40&md5=77fb18f8b60e22e8696cc27363acce87,"ENSAM, 150 Boulevard Nile, Casablanca, 20700, Morocco; Faculte des Sciences Ben m'Sik, Boulevard Driss EL Harti, Casablanca, Morocco",Elsevier B.V.,English,18770509,
Scopus,The role and value of replication in empirical software engineering results,"Context: Concerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence. Objective: We aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship. Method: We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals. Results: Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘confirmatory’ result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of effect sizes. Where we are able to compute replication prediction intervals these were surprisingly large. Conclusion: We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is confirmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis. © 2018 Elsevier B.V.",Experiment; Meta-analysis; Reliability; Replication; Software engineering,"Shepperd M., Ajienka N., Counsell S.",2018,Journal,Information and Software Technology,10.1016/j.infsof.2018.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044753804&doi=10.1016%2fj.infsof.2018.01.006&partnerID=40&md5=cede2d48fbb17958b05a80c3206c80dd,"Brunel Software Engineering Lab (BSEL), Department of Computer Science, Brunel University London, United Kingdom; Dept. of Computer Science, Edge Hill University, Ormskirk, L39 4QP, United Kingdom",Elsevier B.V.,English,09505849,
Scopus,A hybrid methodology for effort estimation in agile development: An industrial evaluation,"In agile software development, functionality is added to the system in an incremental and iterative manner. Practitioners often rely on expert-judgment to estimate effort in this context. The impact of a change on the existing system can provide objective information to practitioners to arrive at an informed estimate. In this regard, we have developed an innovative hybrid method that utilizes change impact analysis information for improving effort estimation. Additionally, an estimation model based on boosted trees is also developed. In this study, we evaluate the performance and usefulness of our innovative method and model in the context of agile software development from the perspective of agile development teams. A case study has been conducted with Insiders Technologies, a German software company, where we have applied our proposed method with tool-support in a live iteration and have evaluated it. The results show that the proposed method is useful and effective. It produces more accurate estimates than purely expert-based or purely model-based estimates. © 2018 Association for Computing Machinery.",Agile development; Change impact; Effort estimation; Hybrid estimation,"Tanveer B., Vollmer A.M., Braun S.",2018,Conference,ACM International Conference Proceeding Series,10.1145/3202710.3203152,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048356220&doi=10.1145%2f3202710.3203152&partnerID=40&md5=e95649e643d3c0716bb9bcc28e3ab4f5,"Fraunhofer Institute for Experimental, Software Engineering IESE, Kaiserslautern, Germany; Insiders Technologies GmbH, Kaiserslautern, Germany",Association for Computing Machinery,English,,9781450364591
Scopus,Connecting software metrics across versions to predict defects,"Accurate software defect prediction could help software practitioners allocate test resources to defect-prone modules effectively and efficiently. In the last decades, much effort has been devoted to build accurate defect prediction models, including developing quality defect predictors and modeling techniques. However, current widely used defect predictors such as code metrics and process metrics could not well describe how software modules change over the project evolution, which we believe is important for defect prediction. In order to deal with this problem, in this paper, we propose to use the Historical Version Sequence of Metrics (HVSM) in continuous software versions as defect predictors. Furthermore, we leverage Recurrent Neural Network (RNN), a popular modeling technique, to take HVSM as the input to build software prediction models. The experimental results show that, in most cases, the proposed HVSM-based RNN model has significantly better effort-aware ranking effectiveness than the commonly used baseline models. © 2018 IEEE.",,"Liu Y., Li Y., Guo J., Zhou Y., Xu B.",2018,Conference,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",10.1109/SANER.2018.8330212,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051043151&doi=10.1109%2fSANER.2018.8330212&partnerID=40&md5=474b54dd90256b5b1d1e74cca5052b21,"State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781538649695
Scopus,Addressing problems with replicability and validity of repository mining studies through a smart data platform,"The usage of empirical methods has grown common in software engineering. This trend spawned hundreds of publications, whose results are helping to understand and improve the software development process. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the replicability and validity of approaches. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Furthermore, many studies use small data sets, which comprise of less than 10 projects. This poses a threat especially to the external validity of these studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created SmartSHARK, which implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of it and the mentioned problems. Additionally, we show how we have addressed the issues that we have identified during our work with SmartSHARK. © 2017, Springer Science+Business Media, LLC.",Replicability; Smart data platform; Software analytics; Software mining; Validity,"Trautsch F., Herbold S., Makedonski P., Grabowski J.",2018,Journal,Empirical Software Engineering,10.1007/s10664-017-9537-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027039041&doi=10.1007%2fs10664-017-9537-x&partnerID=40&md5=a685d4065f2bda1cc86f8bdd18090999,"Institute of Computer Science, Georg-August-Universtität Göttingen, Göttingen, Germany",Springer New York LLC,English,13823256,
Scopus,A systematic mapping study of quality assessment models for software products,"Quality model is regarded as a well-accepted approach for assessing, managing and improving software product quality. There are three categories of quality models for software products, i.e., definition model, assessment model, and prediction model. Quality assessment model (QAM) is a metric-based approach to assess the software quality. It is typically regarded as of high importance for its clear method on how to assess a system. However, the current state-of-the-art in QAM research is under limited investigation. To address this gap, the paper provides an organized and synthesized summary of the current QAMs. In detail, we conduct a systematic mapping study (SMS) for structuring the relevant articles. We obtain a total of 716 papers from the five databases, and 31 papers are selected as relevant studies at last. In summary, our work focuses on QAMs from the following aspects: software metrics, quality factors, evaluation methods and tool support. © 2017 IEEE.",Quality assessment model; Software quality; Systematic mapping study,"Yan M., Xia X., Zhang X., Xu L., Yang D.",2017,Conference,"Proceedings - 2017 Annual Conference on Software Analysis, Testing and Evolution, SATE 2017",10.1109/SATE.2017.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043486651&doi=10.1109%2fSATE.2017.16&partnerID=40&md5=d7869d3388503de9c7ee3c10a6cd5cab,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Computer Science, University of British Columbia, Canada; School of Software Engineering, Chongqing University, Chongqing, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781538636879
Scopus,Clustering dycom an online cross-company software effort estimation study,"Background: Software Effort Estimation (SEE) can be formulated as an online learning problem, where new projects are completed over time and may become available for training. In this scenario, a Cross-Company (CC) SEE approach called Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving the high cost of collecting such training projects. However, Dycom relies on splitting CC projects into different subsets in order to create its CC models. Such splitting can have a significant impact on Dycom's predictive performance. Aims: This paper investigates whether clustering methods can be used to help finding good CC splits for Dycom. Method: Dycom is extended to use clustering methods for creating the CC subsets. Three different clustering methods are investigated, namely Hierarchical Clustering, K-Means, and Expectation-Maximisation. Clustering Dycom is compared against the original Dycom with CC subsets of different sizes, based on four SEE databases. A baseline WC model is also included in the analysis. Results: Clustering Dycom with K-Means can potentially help to split the CC projects, managing to achieve similar or better predictive performance than Dycom. However, K-Means still requires the number of CC subsets to be pre-defined, and a poor choice can negatively affect predictive performance. EM enables Dycom to automatically set the number of CC subsets while still maintaining or improving predictive performance with respect to the baseline WC model. Clustering Dycom with Hierarchical Clustering did not offer significant advantage in terms of predictive performance. Conclusion: Clustering methods can be an effective way to automatically generate Dycom's CC subsets. © 2017 Association for Computing Machinery. All rights reserved.",Concept drift; Cross-company learning; Ensembles; Online learning; Software effort estimation,"Minku L.L., Hou S.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3127005.3127007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053194545&doi=10.1145%2f3127005.3127007&partnerID=40&md5=c3c36ee1824b24a3e33a03d47f961c96,"Department of Informatics, University of Leicester, University Road, Leicester, LE1 7RH, United Kingdom; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, 23955-6900, Saudi Arabia",Association for Computing Machinery,English,,9781450353052
Scopus,Effort estimation for agile software development: Comparative case studies using COSMIC functional size measurement and story points,"Agile methodologies have gained significant popularity among software development organizations during the last decade. Although agile methodologies are regarded as minimizing formal processes, they still utilize an estimation methodology for proper management. Story point is the most common input for agile effort estimation. Story point is an arbitrary measure; it reflects experiences of project participants. On the other hand, functional size is an alternative measure used in practice as an input for effort estimation. In this research, we collect and present the outcomes of three case studies which compared the effectiveness of COSMIC-based and story point based effort estimation in agile context. On selected projects of these organizations, software functional size was measured with COSMIC functional size measurement methodology. Effort prediction models were formed by using COSMIC size and actual effort spent; and the models were tested in terms of their effectiveness. The results show controversial outcomes. For all the cases, COSMIC based estimation was more precise. Therefore, COSMIC is an appropriate measure to estimate the effort in organizations that adopt agile software development. It is also observed that COSMIC allowed for computing productivity which has less disperse distribution than the productivity computed with SP. The data is also provided to help other researchers conduct their own studies. © 2017 Association for Computing Machinery.",COSMIC; Effort estimation; Functional size measurement; Story points,"Salmanoglu M., Hacaloglu T., Demirors O.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3143434.3143450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038366174&doi=10.1145%2f3143434.3143450&partnerID=40&md5=a7e27992109cb670012c8194a5cc8171,"Middle East Technical University, Ankara, Turkey; Atilim University, Ankara, Turkey; Izmir Institute of Technology, Izmir, Turkey; University of New South Wales, Sydney, Australia",Association for Computing Machinery,English,,9781450348539
Scopus,Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle,"Many practitioners and academics believe in a delayed issue effect (DIE); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006–2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, DIE is not some constant across all projects. Rather, DIE might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism. © 2016, Springer Science+Business Media New York.",Cost to fix; Phase delay; Software economics,"Menzies T., Nichols W., Shull F., Layman L.",2017,Journal,Empirical Software Engineering,10.1007/s10664-016-9469-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994709940&doi=10.1007%2fs10664-016-9469-x&partnerID=40&md5=c8ae911a0e1c66129a142dc8e1dd79b8,"CS, North Carolina State University, Raleigh, NC, United States; Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Fraunhofer CESE, College Park, MD, United States",Springer New York LLC,English,13823256,
Scopus,"Rapid, evolutionary, reliable, scalable system and software development: The resilient agile process","The increasing pace of change in competition, technology, and complexity of software-intensive systems has increased the demand for rapid, reliable, scalable, and evolvable processes. Agile methods have made significant contributions to speeding up software development, but often encounter problems with reliability, scalability, and evolvability. Over the past 3 years, we have been experimenting with an approach called Resilient Agile (RA), which addresses these problems while also speeding up development by finding enablers for parallel systems engineering, development, and test. This paper summarizes our experience in defining and evolving RA by applying it to three representative emergent-technology applications: Location-Based Advertising, Picture Sharing, and Bad Driver Reporting. In comparison with the mainstream Architected Agile process that we had been using on similar systems, the RA process achieved fewer defects and significant speedups in system development and evolution. The paper summarizes the overall challenge of software schedule compression; identifies managed parallel development as generally the most powerful but least-practiced strategy for schedule compression; summarizes the key elements required to support parallelism, including specific model-driven system development techniques, automatic generation of key elements and realistic schedule and effort estimation. It then summarizes the three successful Resilient Agile projects to date, provides criteria for selecting a Resilient Agile process, and summarizes the key techniques for scaling up Resilient Agile, using a previous million-line command and control project as an example. © 2017 ACM.",Agile Development; Code Generation; Icsm; Microservice Architecture; Mvc; Nosql; Parallel Development; Rapid Delivery; Resilient Software; Rest; Scalable Software Development; Schedule Compression; Uml Modeling; Use Case Driven Development,"Rosenberg D., Boehm B.W., Wang B., Qi K.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3084100.3084107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025822289&doi=10.1145%2f3084100.3084107&partnerID=40&md5=a3f4d02147dc6b0bda0e8f231a7ae00c,"ICONIX Software Engineering, USC, United States; University of Southern California, United States",Association for Computing Machinery,English,,9781450352703
Scopus,Optimizing COCOMO II parameters using particle swarm method,"The estimation of software effort is an essential and crucial activity for the software development life cycle. It is a problem that often appears on the project of making a software. A poor estimate will result in a worse project management. Several software cost estimation models have been introduced to resolve this problem. Constructive Cost Model II (COCOMO II Model) is a most considerable and broadly used model in cost estimation. To estimate the cost of a software project, COCOMO II model uses cost drivers, scale factors and line of code. However, the model is still lacking in terms of accuracy. In this study, we investigate the influence of components and attributes to achieve new better accuracy improvement on COCOMO II model. We introduced the use of Particle Swarm Optimization (PSO) algorithm in optimizing the COCOMO II model parameters. The proposed method is applied on Turkish Software Industry dataset. The method achieves well result and deals proficient with inexplicit data input and further improve a reliability of the estimation method. The optimized MMRE result is 34.1939%. It can reduce 698.9461% and 104.876% errors from the basic COCOMO II model and Tabu Search coefficient significantly. © 2017 IEEE.",cocomo II model; optimization; particle swarm optimization; software effort estimation; swarm intelligence,"Langsari K., Sarno R.",2017,Conference,"Proceeding - 2017 3rd International Conference on Science in Information Technology: Theory and Application of IT for Education, Industry and Society in Big Data Era, ICSITech 2017",10.1109/ICSITech.2017.8257081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046683087&doi=10.1109%2fICSITech.2017.8257081&partnerID=40&md5=42d6d413c3dfdaf158be898ce92327b7,"Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781509058662
Scopus,Improving effort estimation of Fuzzy Analogy using feature subset selection,"Feature selection has been recently used in the area of software development effort estimation for improving the accuracy and robustness of prediction techniques. The idea behind selecting the most informative subset of features from a pool of available effort drivers stems from the hypothesis that reducing the dimensionality of datasets may significantly minimize the complexity and time required to reach to an optimal and accurate estimation. This paper compares two relatively popular feature selection techniques (Forward Subset Selection and Backward Feature Elimination) used with Fuzzy Analogy for software effort estimation. This empirical comparison is done over eight well-known datasets with the Jackknife evaluation method. The results suggest that Fuzzy Analogy using feature subset selection generates more accurate estimates in terms of the Standardized Accuracy (SA) and Pred(p) criteria than Fuzzy Analogy without using feature subset selection regardless of the data set used. Moreover, this study found that the use of Forward Feature Selection, rather than Backward Feature Elimination, may improve the prediction accuracy of Fuzzy Analogy and reduce the number of features selected. © 2016 IEEE.",Feature Subset Selection; Fuzzy Analogy; Software Development Effort Estimation,"Idri A., Cherradi S.",2017,Conference,"2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016",10.1109/SSCI.2016.7849928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016068654&doi=10.1109%2fSSCI.2016.7849928&partnerID=40&md5=b0fa6d032d1a508078369c4630d0c58a,"Software Projects Management Research Team, University Mohamed v in Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042401
Scopus,Evaluating Fuzzy Analogy on incomplete software projects data,"Missing Data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort prediction systems. This paper investigates the use of missing data (MD) techniques with Fuzzy Analogy. More specifically, this study analyze the predictive performance of this analogy-based technique when using toleration, deletion or k-nearest neighbors (KNN) imputation techniques using the Pred(0.25) accuracy criterion and thereafter compares the results with the findings when using the Standardized Accuracy (SA) measure. A total of 756 experiments were conducted involving seven data sets, three MD techniques (toleration, deletion and KNN imputation), three missingness mechanisms (MCAR: missing completely at random, MAR: missing at random, NIM: non-ignorable missing), and MD percentages from 10 percent to 90 percent. The results of accuracy measured in terms of Pred(0.25) confirm the findings of a study which used the SA measure. Moreover, we found that SA and Pred(0.25) measure different aspects of technique performance. Hence, SA is not sufficient to conclude about the technique accuracy and it should be used with other metrics, especially Pred(0.25). © 2016 IEEE.",Analogy-based Software Development Effort Estimation; Fuzzy Analogy; Imputation; Missing Data,"Abnane I., Idri A.",2017,Conference,"2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016",10.1109/SSCI.2016.7849922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016044171&doi=10.1109%2fSSCI.2016.7849922&partnerID=40&md5=9ece9d331d7bf0bbef07ed39c97ca2d1,"Software Project Management Research Team, ENSIAS, University Mohammed v in Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042401
Scopus,Systematic literature review on effort estimation for Open Sources (OSS) web application development,"The development of Web applications has a crucial role as most organizations have their own corporate Web applications to meet the needs of their respective businesses. Different needs create different complexities which represent a new challenge to Web application development. In order to ensure the timely delivery of a project, software providers offering this service choose to use Open Sources (OSS) as an alternative. Since OSS consist of an existing framework that can be implemented directly into the application, how far does this affect the complexity of the effort estimation? A number of research papers have outlined the efforts made to refine the complexity of this field. However, to our best knowledge a systematic overview of the research done on Web application development that involves OSS usage does not appear to exist. Hence, the aim of this paper is to conduct a systematic literature review (SLR) of OSS Web application development. For this purpose, 34 papers from a total of 67 papers were identified and studied. The findings of this study indicate that (a) no research has been carried out on the field mentioned; (b) there is no early effort estimation model for Web projects that involve the usage of OSS. Therefore, this work provides an overview of the field besides identifying future research possibilities. © 2016 IEEE.",Effort estimation; open sources; systematic literature review; web application development,"Lee T.K., Wei K.T., Ghani A.A.A.",2017,Conference,FTC 2016 - Proceedings of Future Technologies Conference,10.1109/FTC.2016.7821748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013680174&doi=10.1109%2fFTC.2016.7821748&partnerID=40&md5=edf2f7353bd76d214a7b14095b6dc465,"Department of Software Engineering and Information System, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, UPM, Serdang, Selangor, 43400, Malaysia",Institute of Electrical and Electronics Engineers Inc.,English,,9781509041718
Scopus,A case study of automated feature location techniques for industrial cost estimation,"We present a case study of feature location in industry. We study two off-the-shelf feature location algorithms for use as input to a software cost estimator. The feature location algorithms that we studied map program requirements to one or more function points. The cost estimator product, which is the industrial context in which we study feature location, transforms the list of function points into an estimate of the resources necessary to implement that requirement. We chose the feature location algorithms because they are simple to explain, deploy and maintain as a project evolves and personnel rotate on and off. We tested both feature location algorithms against a large software system with a development lifespan of over 20 years. We compared both algorithms by surveying our industrial partner about the accuracy of the list of function points produced by each algorithm. To provide further evidence, we compared both algorithms against an open source benchmarking dataset. Finally, we discuss the requirements of the industrial environment and the ways in which it differs from the academic environment. Our industrial partner elected to use Lucene combined with the PageRank algorithm as their feature location algorithm because it balanced accuracy with simplicity. © 2016 IEEE.",,"Armaly A., Klaczynski J., McMillan C.",2017,Conference,"Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",10.1109/ICSME.2016.76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013128181&doi=10.1109%2fICSME.2016.76&partnerID=40&md5=ef2eff8b74c366022e4db4b10cb0a50d,"Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN  46615, United States; SimVentions, Inc., Fredericksburg, VA  22408, United States",Institute of Electrical and Electronics Engineers Inc.,English,,9781509038060
Scopus,Function point structure and applicability: A replicated study,"Background: The complexity of providing accurate functional software size and effort prediction models is well known in the software industry. Function point analysis (FPA) is currently one of the most accepted software functional size metrics in the industry, but it is hardly automatable and generally requires a lengthy and costly process. Objectives: This paper reports on a family of replications carried out on a subset of the International Software Benchmarking Standards Group dataset (ISBSG R12) to evaluate the structure and applicability of function points. The goal of this replication is to aggregate evidence about internal issues of FPA as a metric, and to confirm previous results using a different set of data. Methods: A subset of 202 business application projects from 2005 to 2011 was analyzed. FPA counting was analyzed in order to determine the extent to which the basic functional components (BFC) were independent of each other and thus appropriate for an additive model of size. The correlations among effort and BFCs and unadjusted function points (UFP) were assessed in order to determine whether a simplified sizing metric might be appropriate to simplify effort prediction models. Prediction models were constructed and evaluated in terms of accuracy. Results: The results confirmed that some BFCs of the FPA method are correlated. There is a relationship between BFCs and effort. That suggest that prediction models based on transactional functions (TF) or external inputs (EI) appears to be as good as a model based on UFP in this subset of projects. Conclusions: The results might suggest an improvement in the performance of the measurement process. Simplifying the FPA measurement process based on counting a subset of BFCs could allow savings in measurement effort, preserving the accuracy of effort estimates.",Empirical evaluation; Family of replications; Function point analysis; Software effort prediction,"Quesada-López C., Jenkins M.",2016,Journal,Journal of Object Technology,10.5381/jot.2016.15.3.a2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981165509&doi=10.5381%2fjot.2016.15.3.a2&partnerID=40&md5=1a73209682fd331f863c9a2f301bffbe,"Center for ICT Research (CITIC), University of Costa Rica, San José, Costa Rica; Department of Computer Science, University of Costa Rica, Costa Rica",Association Internationale pour les Technologies Objets,English,16601769,
Scopus,A differential evolution-based model to estimate the software services development effort,"Accurate estimation of software service development effort is a great challenge both in industry and for academia. The concept of effort is an important and effective parameter in process development and software service management. The reliable estimation of effort helps the project managers to allocate the resources better and manage cost and time so that the project will be finished in the determined time and budget. One of the most popular effort estimation methods is analogy-based estimation (ABE) to compare a service with similar historical cases. Unfortunately, ABE is not capable of generating accurate results unless determining weights for service features. Therefore, this paper aims to make an efficient and reliable model through combining ABE method and differential evolution algorithm to estimate the software services development effort. In fact, the differential evolution algorithm was utilized for weighing features in the similarity function of the ABE method. This weighing process could help determining the importance level of the various service features and extracting the best similar historical case. The proposed hybrid model has been evaluated on two real datasets and two artificial datasets. The obtained results were compared with common effort estimation methods. This comparison showed more accuracy, faster convergence, and lower cost of the proposed model. Examine 11 most popular effort-estimation models. Propose new and efficient weighting model. Use many data sets and performance metrics. Help to choice and a better understanding of software-service effort-estimation methods. Copyright © 2015 John Wiley & Sons, Ltd.",analogy based estimation; development effort; differential evolution algorithm; software services,"Khatibi Bardsiri A., Hashemi S.M.",2016,Journal,Journal of Software: Evolution and Process,10.1002/smr.1765,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956590025&doi=10.1002%2fsmr.1765&partnerID=40&md5=baab5687e09e901513096860c878a166,"Computer Engineering Department, Science and Research Branch, Islamic Azad University, Tehran, Iran",John Wiley and Sons Ltd,English,20477481,
Scopus,Dynamic metrics are superior than static metrics in maintainability prediction: An empirical case study,"Software metrics help us to make meaningful estimates for software products and guide us in taking managerial and technical decisions like budget planning, cost estimation, quality assurance testing, software debugging, software performance optimization, and optimal personnel task assignments. Many design metrics have proposed in literature to measure various constructs of Object Oriented (OO) paradigm such as class, coupling, cohesion, inheritance, information hiding and polymorphism and use them further in determining the various aspects of software quality. However, the use of conventional static metrics have found to be inadequate for modern OO software due to the presence of run time polymorphism, templates class, template methods, dynamic binding and some code left unexecuted due to specific input conditions. This gap gave a cue to focus on the use of dynamic metrics instead of traditional static metrics to capture the software characteristics and further deploy them for maintainability predictions. As the dynamic metrics are more precise in capturing the execution behavior of the software system, in the current empirical investigation with the use of open source code, we validate and verify the superiority of dynamic metrics over static metrics. Four machine learning models are used for making the prediction model while training is performed simultaneously using static as well as dynamic metric suite. The results are analyzed using prevalent prediction accuracy measures which indicate that predictive capability of dynamic metrics is more concise than static metrics irrespective of any machine learning prediction model. Results of this would be helpful to practitioners as they can use the dynamic metrics in maintainability prediction in order to achieve precise planning of resource allocation. © 2015 IEEE.",Dynamic metrics; Machine learning; Software maintainability prediction; Software quality; Static metrics,"Sharma H., Chug A.",2015,Conference,"2015 4th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2015",10.1109/ICRITO.2015.7359354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961773074&doi=10.1109%2fICRITO.2015.7359354&partnerID=40&md5=78dea9a505856f1f006eaf315e233c8e,"IT, Lal Bahadur Shastri Institute of Management, New Delhi, India; USICT, GGS IP University, New Delhi, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781467372312
Scopus,Predicting software product quality: A systematic mapping study,"Predicting software product quality (SPQ) is becoming a permanent concern during software life cycle phases. In this paper, a systematic mapping study was performed to summarize the existing SPQ prediction (SPQP) approaches in literature and to organize the selected studies according to seven classification criteria: SPQP approaches, research types, empirical types, data sets used in the empirical evaluation of these studies, artifacts, SQ models, and SQ characteristics. Publication channels and trends were also identified. After identifying 182 documents in ACM Digital Library, IEEE Xplore, ScienceDirect, SpringerLink, and Google scholar, 69 papers were selected. The results show that the main publication source of the papers identified was conference. Data mining techniques are the most frequently SPQP approaches reported in literature. Solution proposal was the main research type identified. The majority of the papers selected were history-based evaluations using existing data which were mainly obtained from open source software projects and domain specific projects. Source code was the main artifact concerned with SPQP approaches. Well-known SQ models were hardly mentioned and reliability is the SQ characteristic through which SPQP was mainly achieved. SPQP-related subject seems to need more investigation from researchers and practitioners. Moreover, SQ models and standards need to be considered more in future SPQP research.",Prediction; Software product quality; Systematic mapping study,"Ouhbi S., Idri A., Ferńandez-Alemán J.L., Toval A.",2015,Journal,Computacion y Sistemas,10.13053/CyS-19-3-1960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943600627&doi=10.13053%2fCyS-19-3-1960&partnerID=40&md5=caa3593757dd9685bb38aa7309228c08,"ENSIAS, Software Project Management Research Team, University Mohammed V, Rabat, Morocco; Department of Informatics and Systems, Faculty of Computer Science, University of Murcia, Murcia, Spain",Instituto Politecnico Nacional,English,14055546,
Scopus,Web framework points: An effort estimation methodology for Web application development using a content management framework,"This work presents the Web Framework Points (WFP) methodology to estimate the effort of Web applications developed with a content management framework. WFP is composed of a sizing phase, and an effort estimation phase, obtained by applying a cost model to the size model of the project to estimate. The sizing of the project takes into account not only usual functional requirements, but also elements specific for developing a Web application. We present the experimental validation of the proposed methodology.Web applications are among the most popular and relevant kinds of application. Most Web applications are developed using a content management framework (CMF). CMF helps to accelerate the publication of large amounts of information and the development of Web applications. However, developing Web applications through CMF is not exempt from cost and time overruns, as in traditional software projects. Currently, there is no estimation model able to adequately measure the effort of developing a Web application. This work presents a new methodology, called web framework points, to estimate the effort of Web applications developed with CMF. Web framework points is a hybrid methodology, composed of a sizing phase, which follows specific guidelines, and an effort estimation phase, obtained by applying a cost model to the size model of the project to estimate. The sizing of the project takes into account not only usual functional requirements, as in function points analysis, but also elements specific for developing a Web application through CMF. We also present the experimental validation of the proposed methodology, performed on a dataset of 29 real-world projects, of which 83% show an estimation error of less than 25%. © 2015 John Wiley & Sons, Ltd.",content management framework; function points; software effort estimation; Web applications; Web objects,"Barabino G., Concas G., Corona E., Grechi D., Marchesi M., Tigano D.",2015,Journal,Journal of Software: Evolution and Process,10.1002/smr.1715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941656524&doi=10.1002%2fsmr.1715&partnerID=40&md5=d71768e9f920c4fc52a1ca4552e11d79,"DITEN, University of Genoa, Genova, Italy; DIEE, University of Cagliari, Cagliari, Italy",John Wiley and Sons Ltd,English,20477481,
Scopus,An empirical validation of function point structure and applicability: A replication study,"Background: The complexity of providing accurate software size estimation and effort prediction models is well known in the software industry. Function point analysis (FPA) is currently one of the most accepted software functional size metric in the industry, but it is hardly automatable and generally requires a lengthy and costly process. Objectives: This paper reports on a family of replications carried out on a subset of the ISBSG R12 dataset to evaluate the structure and applicability of function points. The goal of this replication was to aggregate evidence about internal issues of FPA as a metric, and to confirm previous results using a different set of data. First, FPA counting was analyzed in order to determine the extent to which the base functional components (BFC) were independent of each other and thus appropriate for an additive model of size. Second, the correlation between effort and BFCs and unadjusted function points (UFP) were assessed in order to determine whether a simplified sizing metric might be appropriate to simplify effort prediction models. Methods: A subset of 72 business application projects from 2008 to 2011 was analyzed. BFCs, UFP, and effort correlation were studied. Results: The results aggregated evidence and confirmed that some BFCs of the FPA method are correlated. There is a relationship between BFCs and effort. There are correlations between UFP and inputs, enquiries, and internal files, and between BFCs and effort. Internal files and inputs are found to be correlated always, and external interface files are found to be uncorrelated with the others. A prediction model based on transactions and internal files appear to be as good as a model based on UFP. The use of some contexts attributes may improve effort prediction models. Limitations: This is an initial experiment of a research in progress. The limited size and nature of the dataset may influence the results. Conclusions: Our results might suggest an improvement in the performance of the measurement process. Simplifying FPA measurement procedure based on counting a subset of BFCs could improve measurement process efficiency and simplify prediction models. Copyright © 2015 by the authors.",Effort prediction; Experiment; Family of replications; Function point analysis,"Quesada-López C., Jenkins M.",2015,Conference,CIBSE 2015 - XVIII Ibero-American Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936102909&partnerID=40&md5=8ac3f362ca5b73d51172f7c72380be33,"Center for ICT Research, University of Costa Rica, San Pedro, Costa Rica",Ibero-American Conference on Software Engineering,English,,9789972825804
Scopus,A learning adaptation cases technique for Fuzzy Analogy-based software development effort estimation,"The aim of this paper is to enhance the Fuzzy Analogy technique for software effort development estimation. Fuzzy Analogy selects the similar projects that will be used in the adaptation step according to the definition of the qualification 'closely similar'. The adopted definition consider two projects as closely similar if their similarity is in the vicinity of 1. The qualification 'closely similar' is represented by a fuzzy set defined by a fixed threshold which is obtained experimentally from the environment. However, in many cases the available empirical knowledge may not allow estimators to fit the adequate fuzzy representation of the qualification 'closely similar'. In this study, we propose an approach to learn this fuzzy representation from the similarities obtained in the retrieval step of the Fuzzy Analogy technique. The proposed method provides for each new project, an adequate threshold by using the quasi-arithmetic mean operators. Indeed, the quasi-arithmetic means operators use weighted similarities to calculate the threshold that often ensures the selection of the closest projects in the adaptation step. This paper also presents an empirical validation of the proposed approach based on the COCOMO'81 dataset. © 2014 IEEE.",fuzzy analogy; Minkowski mean; quasi-arithmetic operators; software cost estimation,"Ezghari S., Zahi A., Idri A.",2014,Conference,"2014 2nd World Conference on Complex Systems, WCCS 2014",10.1109/ICoCS.2014.7060958,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988299045&doi=10.1109%2fICoCS.2014.7060958&partnerID=40&md5=9ce612ab475d2c3acd929c1b0c5bb106,"System Intelligent and Application Laboratory (SIA), FST, Fez, Morocco; Software Projects Management Research Team, ENSIAS, Mohamed V University, Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781479946471
Scopus,Mining bug data: A practitioner’s guide,"Although software systems control many aspects of our daily life world, no system is perfect. Many of our day-to-day experiences with computer programs are related to software bugs. Although software bugs are very unpopular, empirical software engineers and software repository analysts rely on bugs or at least on those bugs that get reported to issue management systems. So what makes data software repository analysts appreciate bug reports? Bug reports are development artifacts that relate to code quality and thus allow us to reason about code quality, and quality is key to reliability, end-users, success, and finally profit. This chapter serves as a hand-on tutorial on how to mine bug reports, relate them to source code, and use the knowledge of bug fix locations to model, estimate, or even predict source code quality. This chapter also discusses risks that should be addressed before one can achieve reliable recommendation systems. © Springer-Verlag Berlin Heidelberg 2014.",,"Herzig K., Zeller A.",2014,Book Chapter,Recommendation Systems in Software Engineering,10.1007/978-3-642-45135-5_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948119524&doi=10.1007%2f978-3-642-45135-5_6&partnerID=40&md5=59fef00514a35697f2f76115b10b3946,"Saarland University, Saarbrücken, Germany",Springer Berlin Heidelberg,English,,9783642451355; 9783642451348
Scopus,COSMIC approximate sizing using a fuzzy logic approach: A quantitative case study with industry data,"In software engineering, the standards for functional size measurement require that the functionality to be measured be fully known for accurate measurement results. Therefore, in the early phases of software development, when there is a lack of detail, approximate sizing approaches must be used instead of the standards themselves. Approximate sizing techniques are typically based on the analysis of historical data of the functional size of a number of completed projects within an organization. For organizations without historical data on the functional size of their completed projects, this paper proposes a fuzzy logic model to approximate functional size early in the software development process. It also reports on a quantitative case study of this approximation technique from industry, and compares its performance with the Equal Size Band approximation approach. © 2014 IEEE.",Approximate Sizing; COSMIC ISO 19761; EPCU; FSM; Functional Size; Fuzzy Logic,"Souto F.V., Abran A.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.44,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929649480&doi=10.1109%2fIWSM.Mensura.2014.44&partnerID=40&md5=78a2aa5e175412073719aa905a136d80,"École de Technologie Supérieure, University of Québec, Dept. of Software Engineering, Montréal, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,A fuzzy expert system for cost-effective regression testing strategies,"Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions. © 2013 IEEE.",Adaptive regression testing strategy; AHP; Empirical studies; Fuzzy AHP; Regression testing; Test case prioritization,"Schwartz A., Do H.",2013,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2013.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891669616&doi=10.1109%2fICSM.2013.11&partnerID=40&md5=d3f61a8fdc1ffcdc61185954879b621a,"North Dakota State U., United States",,English,,
Scopus,A proposal for the improvement of project's cost predictability using EVM and historical data of cost,"This paper proposes an extension of the Earned Value Management - EVM technique through the integration of historical cost performance data of processes under statistical control as a means to improve the predictability of the cost of projects. The proposed technique was evaluated through a case-study in industry, which evaluated the implementation of the proposed technique in 22 software development projects Hypotheses tests with 95% significance level were performed, and the proposed technique was more accurate and more precise than the traditional technique for calculating the Cost Performance Index - CPI and Estimates at Completion - EAC. © 2013 IEEE.",Cost Performance Index - CPI; Earned Value Management - EVM; High Maturity; Software Metrics,Diniz De Souza A.,2013,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2013.6606740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886438413&doi=10.1109%2fICSE.2013.6606740&partnerID=40&md5=162c231e6728ef78e8a375e49169e55e,"Universidade Federal Do Rio de Janeiro, UFRJ, COPPE, Rio de Janeiro, Brazil",,English,02705257,9781467330763
Scopus,Models to predict mortality of tribolium castaneum (Coleoptera: Tenebrionidae) exposed to elevated temperatures during structural heat treatments,"Novel thermal death models were developed with certain assumptions, and these models were validated by using actual heat treatment data collected under laboratory conditions at constant temperatures over time and in commercial food-processing facilities where temperatures were dynamically changing over time. The predicted mortalities of both young larvae and adults of the red flour beetle, Tribolium castaneum (Herbst), were within 92-99% of actual measured insect mortalities. There was good concordance between predicted and observed mortalities of young larvae and adults of T. castaneum exposed to constant temperatures in laboratory growth chambers and at variable temperatures during structural heat treatments of commercial food-processing facilities. The models developed in this study can be used to determine effectiveness of structural heat treatments in killing young larvae and adults of T. castaneum and for characterizing insect thermotolerance. © 2013 Entomological Society of America.",food-processing facility; heat treatment; model; mortality; Tribolium castaneum,"Jian F., Subramanyam B., Jayas D.S., White N.D.G.",2013,Journal,Journal of Economic Entomology,10.1603/EC12278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885586453&doi=10.1603%2fEC12278&partnerID=40&md5=7862e6e7fe6adbdb2a56de9b7063bc46,"Department of Biosystems Engineering, 75A Chancellor's Circle, University of Manitoba, Winnipeg, MB, R3T 2N2, Canada; Department of Grain Science and Industry, 201 Shellenbeiger Hall, Kansas State University, Manhattan KS 66506, United States; Agriculture and Agri-Food Canada, Cereal Research Centre, 195 Dafoe Road, Winnipeg, MB, R3T 2M9, Canada",,English,00220493,
Scopus,An algorithmic approach to missing data problem in modeling human aspects in software development,"Background: In our previous research, we built defect prediction models by using confirmation bias metrics. Due to confirmation bias developers tend to perform unit tests to make their programs run rather than breaking their code. This, in turn, leads to an increase in defect density. The performance of prediction model that is built using confirmation bias was as good as the models that were built with static code or churn metrics. Aims: Collection of confirmation bias metrics may result in partially ""missing data"" due to developers' tight schedules, evaluation apprehension and lack of motivation as well as staff turnover. In this paper, we employ Expectation-Maximization (EM) algorithm to impute missing confirmation bias data. Method: We used four datasets from two large-scale companies. For each dataset, we generated all possible missing data configurations and then employed Roweis' EM algorithm to impute missing data. We built defect prediction models using the imputed data. We compared the performances of our proposed models with the ones that used complete data. Results: In all datasets, when missing data percentage is less than or equal to 50% on average, our proposed model that used imputed data yielded performance results that are comparable with the performance results of the models that used complete data. Conclusions: We may encounter the ""missing data"" problem in building defect prediction models. Our results in this study showed that instead of discarding missing or noisy data, in our case confirmation bias metrics, we can use effective techniques such as EM based imputation to overcome this problem.",Confirmation bias; Expectation Maximisation (EM) algorithm; Handling missing data; Software defect prediction,"Calikli G., Bener A.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2499393.2499394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969289901&doi=10.1145%2f2499393.2499394&partnerID=40&md5=e458cb6afb989c80cc852bdd077edceb,"Data Science Laboratory, Dept. of Mechanical and Industrial Engineering, Ryerson University, Canada",Association for Computing Machinery,English,,9781450320160
Scopus,Using knowledge management and aggregation techniques to improve web effort estimation,"Effort estimation is one of the main pillars of sound project management as its accuracy can affect significantly whether projects will be delivered on time and within budget. However, due to being a complex domain, there are countless examples of companies that underestimate effort, and such estimation error can be of 30%-40% on average, thus leading to serious project management problems (Jørgensen & Grimstad, 2009). The contribution of this chapter is twofold: 1) to explain the knowledge management methodology employed to build industrial expert-based Web effort estimation models, such that other companies willing to develop such models can do so and 2) to provide a wider understanding on the fundamental factors affecting Web effort estimation and their relationships via a mechanism that partially aggregates the expert-based Web effort estimation models built. © 2013 by IGI Global. All rights reserved.",,"Mendes E., Baker S.",2013,Book Chapter,Knowledge-Based Processes in Software Development,10.4018/978-1-4666-4229-4.ch005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944724154&doi=10.4018%2f978-1-4666-4229-4.ch005&partnerID=40&md5=cef4d282ecce13b2db6f762dbb634ac6,"Blekinge Institute of Technology, School of Computing, Karlskrona, Sweden; Cambridge University, Computer Laboratory, United Kingdom",IGI Global,English,,9781466642300; 1466642297; 9781466642294
Scopus,Realising Web effort estimation: A qualitative investigation,"Context: Reliable effort estimation is essential for better management of Web projects, hence the need to identify what are the key factors that affect effort estimates for new Web projects and how they are inter-related. Objective: This paper improves our understanding of Web effort estimation using as basis the knowledge of Web effort estimation experts. Method: We employed qualitative research with the participation of four different Web development companies in Manaus (Brazil) using semi-structured interviews for data collection; our data analysis was carried out using Grounded Theory-based procedures to identify and combine factors affecting the estimation effort of Web projects. Results: We identified four main groupings (categories) of factors - Web project, Web development complexity, Web development team, and Clients. Each of these groupings contains a set of factors that impact upon Web effort estimation. Conclusions: This is the first time that qualitative research is employed in the field of Web effort estimation to further understand and help improve this process. In addition, some of the factors found had never been identified in any of the previous studies in this field, thus suggesting that the use of Grounded Theory-based procedures may provide a way to enrich our understanding of the phenomenon under investigation via the identification of factors that overlap and also complement those from previous studies. Copyright 2013 ACM.",Grounded-theory; Qualitative studies; Web effort estimation; Web effort predictors; Web project management,"Matos O., Fortaleza L., Conte T., Mendes E.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2460999.2461002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877264660&doi=10.1145%2f2460999.2461002&partnerID=40&md5=5f3764f837ed9d54883918214e86ceb0,"Institute of Computing, Federal University of Amazonas, Brazil; School of Computing, Blekinge Institute of Technology, Brazil",,English,,9781450318488
Scopus,"Software engineering productivity: Concepts, issues and challenges","Software engineering productivity has been widely studied, but there are many issues that remain unsolved. Interesting works related to new metrics and more replications of past productivity analysis have emerged, however, in order to fulfill these unsolved issues, a consensus about influencing factors and well recognized and useful sets of inputs and outputs for using in measurements must be reached. In this regard, a clear state of the art may shed light on further research in software engineering productivity, which remains a promising research area. In this paper, general concepts of software engineering productivity along with general issues and recent challenges that need further attention from the research community are presented. © 2013, IGI Global.",,"Hernández-López A., Colomo-Palacios R., García-Crespo A., Cabezas-Isla F.",2013,Book Chapter,Perspectives and Techniques for Improving Information Technology Project Management,10.4018/978-1-4666-2800-7.ch006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944405261&doi=10.4018%2f978-1-4666-2800-7.ch006&partnerID=40&md5=d85313c58864ddecfeb26ad9b6f059ad,"Computer Science Department, Universidad Carlos III, Madrid, Spain",IGI Global,English,,9781466628014; 1466628006; 9781466628007
Scopus,Productivity in software engineering: A study of its meanings for practitioners - Understanding the concept under their standpoint,"Productivity is a complex concept to be measured or even defined. In software engineering, productivity measurement have focused on the productivity of product delivery, perhaps influenced in part by the formulas used to estimate software projects. These measures have been used in various levels of measurement, from high levels such as organization level to the lowest level: the software engineer. In this paper, the results of an exploratory study about the definition of productivity from the point of view of software engineers are presented. Thus, a definition close to what software engineers do in their day to day is obtained. Furthermore, this definition is compared with the definitions of productivity as defined and used in the scientific literature so far. © 2012 AISTI.",measurement; productivity; software engineer,"Hernández-López A., Colomo-Palacios R., García-Crespo Á.",2012,Conference,"Iberian Conference on Information Systems and Technologies, CISTI",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869066350&partnerID=40&md5=72b15cf80c092d6fa936c5cd61ea159b,"Departamento Informática, Universidad Carlos III de Madrid, Madrid, Spain",,Portuguese,21660727,9789899624771
Scopus,Software cost modelling and estimation using artificial neural networks enhanced by input sensitivity analysis,"This paper addresses the issue of Software Cost Estimation (SCE) providing an alternative approach to modelling and prediction using Artificial Neural Networks (ANN) and Input Sensitivity Analysis (ISA). The overall aim is to identify and investigate the effect of the leading factors in SCE, through ISA. The factors identified decisively influence software effort in the models examined and their ability to provide sufficiently accurate SCEs is examined. ANN of variable topologies are trained to predict effort devoted to software development based on past (finished) projects recorded in two publicly available historical datasets. The main difference with relevant studies is that the proposed approach extracts the most influential cost drivers that describe best the effort devoted to development activities using the weights of the network connections. The approach is validated on known software cost data and the results obtained are assessed and compared. The ANN constructed generalise efficiently the knowledge acquired during training providing accurate effort predictions. The validation process included predictions with only the most highly ranked attributes among the original cost attributes of the datasets and revealed that accuracy performance was maintained at same levels. The results showed that the combination of ANN and ISA is an effective method for evaluating the contribution of cost factors, whereas the subsets of factors selected did not compromise the accuracy of the prediction results. © J.UCS.",Artificial neural networks; Input sensitivity analysis; Software cost estimation,"Papatheocharous E., Andreou A.S.",2012,Journal,Journal of Universal Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868293985&partnerID=40&md5=25af3cd97037fb4edfa1307a6b8a1c5a,"University of Cyprus, Nicosia, Cyprus; Cyprus University of Technology, Lemesos, Cyprus",,English,0948695X,
Scopus,Alternative methods using similarities in software effort Estimation,"A large variety of methods has been proposed in the literature about Software Cost Estimation, in order to increase accuracy when predicting the effort of developing new projects. Estimation by Analogy is one of the most studied techniques in this area the last 20 years. The popularity of the methodology can be explained by its accordance to human problem thinking and solving, the straightforward interpretation and the usually comparable accuracy to other methodologies. Furthermore, the methodology is essentially a special case of non-parametric regression, easily implementable and free of theoretical assumptions, based on the notion of ""similarity"" which is used to define ""neighbors"". All of these reasons led us to study the technique in more depth, considering alternative ways to exploit similarities, in order to assign weights to neighbors. In this paper, our aim is to review the existing weighting practices and explore some new iterative procedures from matrix algebra, which transform a similarity matrix to a bi-stochastic matrix (a matrix with row and column summing to 1). Specifically, we apply algorithms such as the Sinkhorn-Knopp and the Bregmanian Bi-Stochastication to similarity matrices of well-known software cost datasets in order to derive matrices that assign weights to the neighbors used for effort estimates. We investigate the sensitivity of the results with respect to the similarity function, focusing on a Gaussian kernel matrix with a tuning parameter. The promising results show that the new methods deserve a more thorough investigation and can be considered as generalization of the Estimation by Analogy method. Copyright © 2012 ACM.",Bregmanian bi-stochastication; Estimation by analogy; Iterative algorithms; Keywords software effort estimation; Sinkhorn - knopp,"Kosti M.V., Mittas N., Angelis L.",2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867698238&doi=10.1145%2f2365324.2365333&partnerID=40&md5=c0f404484ca89d6682c99fab8f92d232,"Department of Informatics, Aristotle University, Thessaloniki, 54124, Greece",,English,,9781450312417
Scopus,Discretization methods for NBC in effort estimation: An empirical comparison based on ISBSG projects,"Background: Bayesian networks have been applied in many fields, including effort estimation in software engineering. Even though there are Bayesian inference algorithms than can handle continuous variables, performance tends to be better when these variables are discretized that when they are assumed to follow a specific distribution. On the other hand, the choice of the discretization method and the number of discretized intervals may lead to significantly different estimating results. However, discretization issues are seldom mentioned in software engineering effort estimation models. Aim: This paper seeks to show that discretization issues are important in terms of prediction accuracy while building a Naive Bayes Classifier (NBC) for estimating software effort. Method: For this purpose, a NBC model has been developed for software effort estimation based on ISBSG projects applying different discretization schemes (equal width intervals, equal frequency intervals, and k-means clustering) and using different number of intervals. Results: Regarding the NBC model built, the estimation accuracy of equal frequency discretization is only improved by k-means clustering with respect to Pred(0.25), although it reflects better the original distribution. Conclusions: Further experimentation should determine the potential of clustering methods already highlighted in other fields. Copyright 2012 ACM.",Bayesian networks; Discretization methods; Effort estimation; ISBSG; Naive bayes classifier; Software projects,"Fernández-Diego M., Torralba-Martínez J.-M.",2012,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1145/2372251.2372268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867564079&doi=10.1145%2f2372251.2372268&partnerID=40&md5=b54f72fec6eb9a12f65ba9f032c00c04,"Department of Business Administration, Universitat Politècnica de València, 46022 Valencia, Spain",,English,19493770,
Scopus,Using unreliable data for creating more reliable online learners,"Some machine learning applications involve the question of whether or not to use unreliable data for the learning. Previous work shows that learners trained using unreliable data in addition to reliable data present either similar or worse performance than learners trained solely on reliable data. Such learners frequently use unreliable data as if they were reliable and consider only the offline learning scenario. The present paper shows that it is possible to use unreliable data to improve the performance in online learning scenarios with a pre-existing set of unreliable data. We propose an approach called Dynamic Un+Reliable data learners (DUR) able to determine when unreliable data could be useful by maintaining a fixed size weighted memory of unreliable data learners. The weights represent how well learners perform for the current concept and are updated throughout DUR's lifetime. This approach manages not only to outperform an approach which uses only reliable data, but also an approach which uses unreliable data as if they were reliable. Moreover, the variance in performance is reduced in comparison to the approach which uses only reliable data. In other words, DUR is a more reliable learner. © 2012 IEEE.",,"Minku L.L., Yao X.",2012,Conference,Proceedings of the International Joint Conference on Neural Networks,10.1109/IJCNN.2012.6252711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865095163&doi=10.1109%2fIJCNN.2012.6252711&partnerID=40&md5=0973fa011f63d3901d93aa62510b1a58,"Centre of Excellence for Research in Computational Intelligence and Applications, School of Computer Science, University of Birmingham Edgbaston, Birmingham, B15 2TT, United Kingdom",,English,,9781467314909
Scopus,A proposal for simplified model-based cost estimation models,"Most cost estimation models require a measure of the functional size of the application to be developed. To this end, FPA (Function Point Analysis) is one of the most used functional size measurement methods. FPA was originally proposed for traditional data processing systems, but it has been successfully adapted also to measure real-time and embedded systems. Since functional size measurement according to FPA can be quite expensive and time consuming, researchers have proposed ""simplified"" processes, which are expected to provide reasonably accurate measures, but require less effort and time. In this paper, we illustrate the application of these simplified techniques to UML models of software, via a precise mapping between UML elements and the so-called Basic Functional Components, upon which FPA measurement is based. As a result, it is possible to decrease the cost of modeling, and consequently the cost of measurement and estimation. The relatively low cost of the estimation models also allows developers to build different alternative models, to perform what-if analyses and choose the most economically sensible option. © 2012 Springer-Verlag.",Function Point Analysis; Functional Size Measurement; Measurementoriented modeling; Model-based measurement; Simplified functional size measurement,"Del Bianco V., Lavazza L., Morasca S.",2012,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-31063-8_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862205307&doi=10.1007%2f978-3-642-31063-8_6&partnerID=40&md5=52c937505a1ba590dc1e664cb665366b,"Università degli Studi dell'Insubria, Dipartimento di Informatica e Comunicazione, Via Mazzini, 5, Varese 21100, Italy",,English,03029743,9783642310621
Scopus,A hybrid software cost estimation approach utilizing decision trees and fuzzy logic,"Software cost estimation (SCE) is one of the critical activities in software project management. During the past decades various models have been proposed for SCE. However, developing accurate and useful models is limited in practice despite the considerable financial gain they could offer to software stakeholders. Traditional techniques, such as regression, by-analogy and machine learning, face the difficulty of handling the dynamic nature of the software process and the problematic nature of the public data available. This paper addresses the issue of SCE proposing an alternative approach that combines robust decision tree structures with fuzzy logic. Fuzzy decision trees are generated using the CHAID and CART algorithms in a systematic manner, while development effort is treated as the dependent variable against two subsets of factors: The first contains selected attributes from the ISBSG, COCOMO and DESHARNAIS datasets and the second contains a subset of the available factors that can be measured early in the development cycle. The association rules obtained from the trees are then merged and defuzzified through a Fuzzy Implication System (FIS). The fuzzy framework is utilized to perform effort estimations. Experimental results indicate that the proposed approach is promising as it yields quite accurate estimations in most dataset cases considered. Finally, our evaluation suggests that accurate estimations may be produced, even when using only a small set of factors that can be measured early in the development cycle, thus increasing the practical value of the proposed cost model. © 2012 World Scientific Publishing Company.",decision trees; fuzzy implication systems; fuzzy logic; Software cost estimation,"Papatheocharous E., Andreou A.S.",2012,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194012500106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863442324&doi=10.1142%2fS0218194012500106&partnerID=40&md5=9e22f7120cf0caf12147a1331fea6371,"Department of Computer Science, University of Cyprus, 75 Kallipoleos Street, CY1678, Nicosia, Cyprus; Department of Computer Engineering and Informatics, Cyprus University of Technology, 31 Archbishop Kyprianos Street, 3036 Lemesos, Cyprus",,English,02181940,
Scopus,Confirming distortional behaviors in software cost estimation practice,"Cost estimation of software projects is an important management activity. Despite research efforts the accuracy of estimates does not seem to improve. In this paper we confirm intentional distortions of estimates reported in a previous study. This study is based on questionnaire responses from 48 software practitioners from eight different companies. The results of the questionnaire suggest that prevalence of intentional distortions is affected by the organizational type and the development process in use. Further, we extend the results with information about three companies' estimation practices and related distortions collected in interviews with three managers. Lastly, based on these results and additional organizational politics theory we describe organizational politics tactics that affect cost estimates. © 2011 IEEE.",,"Magazinius A., Feldt R.",2011,Conference,"Proceedings - 37th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2011",10.1109/SEAA.2011.61,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955178155&doi=10.1109%2fSEAA.2011.61&partnerID=40&md5=1ff1b183158d3f114a6af5e651743667,"Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden",,English,,9780769544885
Scopus,Search-Based approaches for software development effort estimation,"In the last years the use of Search-Based techniques has been suggested to estimate software development effort. These techniques are meta-heuristics able to find optimal or near optimal solutions to problems characterized by large space. In the context of effort estimation Search-Based approaches can be exploited to build estimation models or to enhance the effectiveness of other methods. The preliminary investigations carried out so far have provided promising results. Nevertheless, the capabilities of these approaches have not been fully explored and the empirical analyses carried out so far have not considered the more recent recommendations on how to perform this kind of empirical assessment in the effort estimation context and in Search-Based Software Engineering. The main aim of the PhD dissertation is to provide an insight on the use of Search-Based techniques for effort estimation trying to highlight strengths and weaknesses. Copyright 2010 ACM.",Empirical study; Search-Based Software Engineering; Software development effort estimation,Sarro F.,2011,Conference,ACM International Conference Proceeding Series,10.1145/2181101.2181111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865618736&doi=10.1145%2f2181101.2181111&partnerID=40&md5=4f68fab28353e3d1377f64cdb2d75542,"University of Salerno, Via Ponte don Melillo, 8404 Fisciano (SA), Italy",,English,,9781450307833
Scopus,Economies and diseconomies of scale in software development,"Economies and diseconomies of scale-gains and losses in productivity arising from increases in project size-are of considerable, practical importance in software engineering. However, there is no consensus as to whether, in general, economies or diseconomies exist with respect to project size; neither is there any consensus on the relationship among project size, team size, and overall productivity. Using evidence from a large database of projects, this paper derives a simple parametric model for estimating development effort in terms of size and productivity factors. The model is compared with popular estimation models-the Putnam model and COCOMO II-and a possible explanation is provided for the lack of consensus. By considering also the value of the software produced, the model provides a basis for the calculation of the optimal team size for a given task, with respect to the anticipated net benefit from the development activity. © 2011 John Wiley & Sons, Ltd.",scale economies; software economics; team size,"Comstock C., Jiang Z., Davies J.",2011,Journal,Journal of Software Maintenance and Evolution,10.1002/smr.526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155135084&doi=10.1002%2fsmr.526&partnerID=40&md5=773793ec8cbfb45e0049ea05fd390117,"University of Oxford, Oxford, United Kingdom; Birmingham Business School, Birmingham, United Kingdom",,English,1532060X,
Scopus,Prediction of software project effort using fuzzy logic,"Software development effort estimation is a branch of forecasting that has received increased interest in academia as well as in the field of research and development. Predicting software effort with any acceptable degree remains challenging. In this paper we have developed 2 different linear regression models using fuzzy function point (FFP) and non fuzzy function point in order to predict the software project effort and further we have also considered that the entire projects are organic in nature i.e. the project size lies between 2 to 50 KLOC. After obtaining the software effort, project manager can control the cost and ensures the quality more accurately. © 2011 IEEE.",Function Point; Fuzzy Function Point; Mean Relative Error; Membership Function; Software Effort,"Sadiq Mohd., Mariyam F., Ali A., Khan S., Tripathi P.",2011,Conference,ICECT 2011 - 2011 3rd International Conference on Electronics Computer Technology,10.1109/ICECTECH.2011.5941919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961236647&doi=10.1109%2fICECTECH.2011.5941919&partnerID=40&md5=9856612a90248bd927e7024042efe73a,"Computer Engineering Laboratory, University Polytechnic Faculty of Engineering and Technology, Jamia Millia Islamia (A Central University), New Delhi-110025, India; Department of Computer Science and Engineering, AL-Falah School of Engineering and Technology, Maharshi Dayanand University, Rohtak, Haryana, India; Department of Computer Science and Information Technology, Sunder Deep College of Engineering and Technology, Dasna, Ghaziabad, U.P., India; U.P. Technical University, Lucknow, U.P., India",,English,,9781424486779
Scopus,Handling estimation uncertainty with bootstrapping: Empirical evaluation in the context of hybrid prediction methods,"Reliable predictions are essential for managing software projects with respect to cost and quality. Several studies have shown that hybrid prediction models combining causal models with Monte Carlo simulation are especially successful in addressing the needs and constraints of today's software industry: They deal with limited measurement data and, additionally, make use of expert knowledge. Moreover, instead of providing merely point estimates, they support the handling of estimation uncertainty, e.g., estimating the probability of falling below or exceeding a specific threshold. Although existing methods do well in terms of handling uncertainty of information, we can show that they leave uncertainty coming from imperfect modeling largely unaddressed. One of the consequences is that they probably provide over-confident uncertainty estimates. This paper presents a possible solution by integrating bootstrapping into the existing methods. In order to evaluate whether this solution does not only theoretically improve the estimates but also has a practical impact on the quality of the results, we evaluated the solution in an empirical study using data from more than sixty projects and six estimation models from different domains and application areas. The results indicate that the uncertainty estimates of currently used models are not realistic and can be significantly improved by the proposed solution. © 2011 IEEE.",CoBRA; Defect prediction; Effort estimation; Empirical study; HyDEEP; Monte carlo simulation,"Kläs M., Trendowicz A., Ishigai Y., Nakao H.",2011,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/esem.2011.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858737754&doi=10.1109%2fesem.2011.33&partnerID=40&md5=7f563ec1a0599f75c988fa7bc716328a,"Fraunhofer IESE, Kaiserslautern, Germany; Mitsubishi Research Institute, Tokyo, Japan; Japan Manned Space Systems Corporation, Tsukuba, Japan",IEEE Computer Society,English,19493770,
Scopus,Formalization studies in functional size measurement,"Functional size has been favored as a software size attribute that can be measured early in the software development cycles. Its characteristics of being independent of implementation language, technique and technology promoted the use in software cost estimation and other project management practices. It has been about three decades since Albrecht introduced the concept of functional size and a variety of measurement methods have been developed, some of which have been published by International Organization for Standardization (ISO). Although the concept is recognized in the software community, and there is a growing interest in Functional Size Measurement (FSM), the applications in software organizations have not been common as expected. The problems with FSM method structures and practices have been discussed to be the major factors to explain this situation. This chapter reviews the research papers that propose solutions to the problems with FSM via formalizations in FSM practices or related concept definitions. The associations of the formalization ideas to the abstract software models that represent the view of functionality for FSM methods are of particular interest of the chapter. © 2011, IGI Global.",,"Özkan B., Demirörs O.",2010,Book Chapter,Modern Software Engineering Concepts and Practices: Advanced Approaches,10.4018/978-1-60960-215-4.ch010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898558226&doi=10.4018%2f978-1-60960-215-4.ch010&partnerID=40&md5=34b9a1ff2355dc162e04b3f370720b0b,"Information Systems Department, Middle East Technical University, Turkey",IGI Global,English,,9781609602154
Scopus,Knowledge representation in pattern management,[No abstract available],,"Kamthan P., Pai H.-I.",2010,Book Chapter,Encyclopedia of Knowledge Management,10.4018/978-1-59904-931-1.ch085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898243909&doi=10.4018%2f978-1-59904-931-1.ch085&partnerID=40&md5=e2b725a91f46f7f6100419686b49276b,"Concordia University, Canada",IGI Global,English,,9781599049311
Scopus,Aggregating expert-driven causal maps for web effort estimation,"Reliable Web effort estimation is one of the cornerstones of good Web project management. Hence the need to fully understand which factors affect a project's outcome and their causal relationships. The aim of this paper is to provide a wider understanding towards the fundamental factors affecting Web effort estimation and their causal relationships via combining six different Web effort estimation causal maps from six independent local Web companies, representing the knowledge elicited from several domain experts. The methodology used to combine these maps extended previous work by adding a mapping scheme to handle complex domains (e.g. effort estimation), and the use of an aggregation process that preserves all the causal relations in the original maps. The resultant map contains 67 factors, and also commonalities amongst Web companies relating to factors and causal relations, thus providing the means to better understand which factors have a causal effect upon Web effort estimation. © 2010 Springer-Verlag Berlin Heidelberg.",causal maps; map aggregation; Web effort estimation; Web effort prediction,"Baker S., Mendes E.",2010,Conference,Communications in Computer and Information Science,10.1007/978-3-642-17578-7_27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651071886&doi=10.1007%2f978-3-642-17578-7_27&partnerID=40&md5=17f54f2b485428647c4761cc22badbea,"Computer Science Department, University of Auckland, 92019, Auckland, New Zealand",,English,18650929,3642175775; 9783642175770
Scopus,The COTECOMO: COnstractive Test Effort COst MOdel,"The primary purpose of Software Testing Process and Evaluation (STP&E) is to reduce risk. While there exists extensive literature on software cost estimation techniques, industry practice continues to rely upon standard regression-based algorithms. These software effort models are typically calibrated or tuned to local conditions using local data. This paper cautions that current approaches to model calibration often produce sub-optimal models because of the large variance problem which is inherent in cost data and by including far more effort multipliers than the data supports. Building optimal models requires that a wider range of models be considered while correctly calibrating these models requires rejection rules that prune variables and records and use multiple criteria for evaluating model performance. This article compares the approaches taken by three (COCOMO II, FP, UCP) widely used models for software cost and schedule estimation to develop COTECOMO (COnstractive Test Effort COst MOdel). It also documents what we call the large variance problem that is a leading cause of cost model brittleness or instability. This paper proposes Software/System Test Point (STP), a new metric for estimating overall software testing process. STP covers so-called black-box testing; an estimate for the test activities, which precede scenarios (threads) testing (white-box testing included), will already have been included in the estimate produced by function point analysis. Software test point is a useful metric for test managers interested in estimating software test effort, and the metric aids in the precise estimation of project effort and addresses the interests of metric group. © 2009 Springer Science+Business Media, LLC.",,"Lazić L., Mastorakis N.",2009,Conference,Lecture Notes in Electrical Engineering,10.1007/978-0-387-85437-3_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651545783&doi=10.1007%2f978-0-387-85437-3_9&partnerID=40&md5=d9089c0502002a933dee76a0a895de42,"School of Electrical Engineering, Vojvode Stepe 283, Belgrade, Serbia",,English,18761100,9780387848181
Scopus,Analysis of the functional size measurement methods usage by Polish business software systems providers,"This paper analyses the level of using the software functional size measurement methods by the Polish providers of dedicated business software systems as well as the reasons behind this status quo. The surveys were conducted against a background of author's own research concerning the usage of software development and enhancement projects effort estimation methods. The use of both types of methods was examined in two cycles: at the turn of the year 2005/2006, being the time of economic prosperity, and next at the turn of the year 2008/2009, that is in the initial stage of crisis and increased investment uncertainty associated with it. This paper presents the most significant conclusions coming from the results of both surveys as well as from comparative analysis of the two. © Springer-Verlag Berlin Heidelberg 2009.",Benchmarking data; Business software systems; Cosmic method; Effort estimation methods; IFPUG method; ISO/IEC standards; Software development and enhancement projects; Software engineering; Software functional size measurement methods,Czarnacka-Chrobot B.,2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650642047&doi=10.1007%2f978-3-642-05415-0_2&partnerID=40&md5=9bec75ee7bd0b79afc3ae11b90fdda05,"Faculty of Business Informatics, Warsaw School of Economics, Al. Niepodleglosci 164, 02-554 Warsaw, Poland",,English,03029743,3642054145; 9783642054143
Scopus,"Assessing and estimating corrective, enhancive, and reductive maintenance tasks: A controlled experiment","This paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The goal of the study is to assess the maintenance size, effort, and effort distribution of three different maintenance types and to describe estimation models to predict the programmer's effort on maintenance tasks. The results of our study suggest that corrective maintenance is much less productive than enhancive and reductive maintenance. Our study also confirms the previous results which conclude that corrective and reductive maintenance requires large proportions of effort on program comprehension activity. Moreover, the best effort model we obtained from fitting the experiment data can estimate the time of 79% of the programmers with the error of 30% or less. © 2009 IEEE.",COCOMO; Maintenance experiment; Maintenance size; Software estimation; Software maintenance,"Nguyen V., Boehm B., Danphitsanuphan P.",2009,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2009.49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76349110752&doi=10.1109%2fAPSEC.2009.49&partnerID=40&md5=fa6691f9d8ed28c5b7cdc7eef7d929ab,"Computer Science Department, University of Southern California, Los Angeles, CA, United States; Computer Science Department, King Mongkut's University of Technology North Bangkok, Bangkok, Thailand",,English,15301362,9780769539096
Scopus,Classification and prediction of software cost through fuzzy decision trees,"This work addresses the issue of software effort prediction via fuzzy decision trees generated using historical project data samples. Moreover, the effect that various numerical and nominal project characteristics used as predictors have on software development effort is investigated utilizing the classification rules extracted. The approach attempts to classify successfully past project data into homogeneous clusters to provide accurate and reliable cost estimates within each cluster. CHAID and CART algorithms are applied on approximately 1000 project cost data records which were analyzed, pre-processed and used for generating fuzzy decision tree instances, followed by an evaluation method assessing prediction accuracy achieved by the classification rules produced. Even though the experimentation follows a heuristic approach, the trees built were found to fit the data properly, while the predicted effort values approximate well the actual effort. © 2009 Springer Berlin Heidelberg.",CART; CHAID; Classification; Fuzzy decision trees; Software cost estimation,"Papatheocharous E., Andreou A.S.",2009,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-642-01347-8_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65949122342&doi=10.1007%2f978-3-642-01347-8_20&partnerID=40&md5=cd8c91b7f2fc83d594b02e5b92d1b8fd,"Department of Computer Science, University of Cyprus, 75 Kallipoleos str., Nicosia CY1678, Cyprus",Springer Verlag,English,18651348,9783642013461
Scopus,How to use COSMIC functional size in effort estimation models?,"Although Functional Size Measurement (FSM) methods have become widely used by the software organizations, the functional size based effort estimation still needs further investigation. Most of the studies on effort estimation consider total functional size of the software as the primary input to estimation models and they mostly focus on identifying the project parameters which might have a significant effect on the size-effort relationship. This study brings suggestions on how to use COSMIC functional size as an input for effort estimation models and explores whether the productivity values for developing different functionality types deviate significantly from a total average productivity value computed from total functional size and effort figures. The results obtained after conducting a multiple case study in which COSMIC method was used for size measurement are discussed as well. © 2008 Springer Berlin Heidelberg.",Base functional component; COSMIC; Effort estimation; Functional size measurement; Functionality,Gencel C.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-89403-2-17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049109155&doi=10.1007%2f978-3-540-89403-2-17&partnerID=40&md5=832a404c63fcfdde5d9e80e19ef26a40,"Department of Systems and Software Engineering, Blekinge Institute of Technology, Ronneby, Sweden",,English,03029743,3540894020; 9783540894025
Scopus,Theoretical maximum prediction accuracy for analogy-based Software cost estimation,"Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignore the importance of the dataset quality. What is more, they implicitly assume the prediction model to be able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models only provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. It is therefore important to realize the theoretical maximum prediction accuracy (TMPA) for the given model with a given dataset. In this paper, we first discuss the practical importance of this notion, and propose a novel method for the determination of TMPA in the application of analogy-based software cost estimation. Specifically, we determine the TMPA of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models. © 2008 IEEE.",Analogy; K-NN; MMRE; Software cost estimation; Software metrics and measurement,Keung J.W.,2008,Conference,"Neonatal, Paediatric and Child Health Nursing",10.1109/APSEC.2008.43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749137668&doi=10.1109%2fAPSEC.2008.43&partnerID=40&md5=7349aab98036cf65ba52c794386b8b13,"NICTA Ltd., Sydney, Australia; CSE, UNSW, Sydney, Australia",,English,14416638,
Scopus,Calibrating function point backfiring conversion ratios using neuro-fuzzy technique,"Software size estimation is an important aspect in software development projects because poor estimations can lead to late delivery, cost overruns and possibly project failure. Backfiring is a popular technique for sizing and predicting the volume of source code by converting the function point metric into source lines of code mathematically using conversion ratios. While this technique is popular and useful, there is a high margin of error in backfiring. This research introduces a new method to reduce this margin of error. Neural networks and fuzzy logic in software prediction models have been demonstrated in the past to have improved performance over traditional techniques. For this reason, a neuro-fuzzy approach is introduced to the backfiring technique to calibrate the conversion ratios. This paper presents the neuro-fuzzy calibration solution and compares the calibrated model against the default conversion ratios currently used by software practitioners. © 2008 World Scientific Publishing Company.",Backfiring; Function point; Lines of code; Neuro-fuzzy; Sizing; Software estimation,"Wong J., Ho D., Capretz L.F.",2008,Journal,"International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems",10.1142/S0218488508005650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149242551&doi=10.1142%2fS0218488508005650&partnerID=40&md5=c4d81480689e148de0339687c7b89350,"Department of Electrical and Computer Engineering, University of Western Ontario, London, ON N6A 5B9, Canada; NFA Estimation Inc., 32-538 Platts Lane, London, ON N6G 3A8, Canada; 4823 Dundas Street, Burnaby, BC V5C 1B8, Canada",,English,02184885,
Scopus,Exponential effort estimation model using unadjusted function points,"In this study, we present a window-based exponential effort estimation model to predict the effort required in terms of man days by using Unadjusted Function Point (UFP) size measure and eliminate the usage of General Systems Characteristics (GSCs). A very comprehensive statistical analysis and test was carried out on large amount of quality project data in the International Software Benchmarking Standard Group (ISBSG) Release 9 dataset, which were collected by the International Function Points User Group (IFPUG) count approach in the process of model development. The effectiveness of the model was examined and reported in this study. © 2008 Asian Network for Scientific Information.",Effort estimation model; Functional size measure; Unadjusted function point regression analysis,"Koh T.W., Selamat M.H., Ghani A.A.A.",2008,Journal,Information Technology Journal,10.3923/itj.2008.830.839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749122043&doi=10.3923%2fitj.2008.830.839&partnerID=40&md5=b4c88622bff672ee3c9da11c88b976a9,"Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400 UPM Serdang, Selangor Darul Ehsan, Malaysia",,English,18125638,
Scopus,Assessing the usability of a visual tool for the definition of e-learning processes,"In this paper, we present a usability study aiming at assessing a visual language-based tool for developing adaptive e-learning processes. The tool implements the adaptive self-consistent learning object SET (ASCLO-S) visual language, a special case of flow diagrams, to be used by instructional designers to define classes of learners through stereotypes and to specify the more suited adaptive learning process for each class of learners. The usability study is based on the combined use of two techniques: a questionnaire-based survey and an empirical analysis. The survey has been used to achieve feedbacks from the subjects' point of view. In particular, it has been useful to capture the perceived usability of the subjects. The outcomes show that both the proposed visual notation and the system prototype are suitable for instructional designers with or without experience on the computer usage and on tools for defining e-learning processes. This result is further confirmed by the empirical analysis we carried out by analysing the correlation between the effort to develop adaptive e-learning processes and some measures suitable defined for those processes. Indeed, the empirical analysis revealed that the effort required to model e-learning processes is not influenced by the experience of the instructional designer with the use of e-learning tools, but it only depends on the size of the developed process. © 2008 Elsevier Ltd. All rights reserved.",Adaptive learning processes; e-Learning; Effort estimation; Usability survey; Visual languages,"Costagliola G., De Lucia A., Ferrucci F., Gravino C., Scanniello G.",2008,Journal,Journal of Visual Languages and Computing,10.1016/j.jvlc.2008.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54949095150&doi=10.1016%2fj.jvlc.2008.01.003&partnerID=40&md5=06fb85b0db5bdac642493a2f27caeed9,"Dipartimento di Matematica e Informatica, Università di Salerno, Via Ponte Don Melillo, 84084 Fisciano, SA, Italy; Dipartimento di Matematica e Informatica, Università della Basilicata, Viale dell'Ateneo Macchia Romana, 85100 Potenza, Italy",,English,1045926X,
Scopus,Effects of software process maturity on COCOMO II's effort estimation from CMMI perspective,"The software cost estimation model, COnstructive COst MOdel (COCOMO), in its last update (COCOMO II) has a set of seventeen cost drivers as well as a set of five scale factors. Process Maturity, PMAT, is one of that five scale factors and its ratings are based on the Software Capability Maturity Model, SW-CMM, which is used to evaluate an organization's process maturity. In this paper, an investigation into the effect of process maturity on software development effort is presented by deriving the new set of COCOMO II's PMAT rating values based on the most recent version of SW-CMM, i.e. Capability Maturity Model Integration (CMMI). COCOMO II's data collection questionnaire was prepared and distributed to a group of selected software development organizations. The precise data for the analysis was collected from the record of 30 completed projects which spanned the range of CMMI Levels, from Level 1 (Level 1 Lower half and Level 1 Upper half) to Level 4, where 6 data points were collected from each level. The Ideal Scale Factor (ISF) method was used in order to withhold the effect of the COCOMO II's PMAT scale factor. All prediction accuracies were measured using PRED (30). The study shows that our proposed model (with the new PMAT rating values) yields better estimates comparing with the Generic COCOMO II model's estimates. ©2008 IEEE.",CMMI; COCOMO; Cost driver; Effort estimation; Scale factor; Software engineering; SW-CMM,"Yahya M.A., Ahmad R., Lee S.P.",2008,Conference,"RIVF 2008 - 2008 IEEE International Conference on Research, Innovation and Vision for the Future in Computing and Communication Technologies",10.1109/RIVF.2008.4586364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51949119398&doi=10.1109%2fRIVF.2008.4586364&partnerID=40&md5=cd4cdd60b32a3230ebebd08253ca6383,"Department of Software Engineering, FCSIT, University of Malaya, Kuala Lumpur, Malaysia",,English,,9781424423798
Scopus,Exploiting a goal-decomposition technique to prioritize non-functional requirements,"Business stakeholders need to have clear and realistic goals if they want to meet commitments in application development. As a consequence, at early stages they prioritize requirements. However, requirements do change. The effect of change forces the stakeholders to balance alternatives and reprioritize requirements accordingly. In this paper we discuss the problem of priorities to non-functional requirements subjected to change. We, then, propose an approach to help smooth the impact of such changes. Our approach favors the translation of nonoperational specifications into operational definitions that can be evaluated once the system is developed. It uses the goal-question-metric method as the major support to decompose non-operational specifications into operational ones. We claim that the effort invested in operationalizing NFRs helps dealing with changing requirements during system development. Based on this transformation and in our experience, we provide guidelines to prioritize volatile non-functional requirements.",,"Daneva M., Kassab M., Ponisio M.L., Wieringa R.J., Ormandjieva O.",2007,Conference,"Proceedings of the 10th Workshop on Requirements Engineering, WER 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051974591&partnerID=40&md5=63013b120dfed6d63ac074bd763c1009,,,English,,9781550144833
Scopus,Comparing between web application effort estimation methods,"Effort estimation consist in predict how many hours of work and how many workers are needed to develop a project. This is important in the process of project management. As far as estimation and prediction are concerned there are still a number of unsolved problems and errors. To obtain good results it is essential to take into consideration any previous projects. Estimation the project manager has not yet been solved and even the project manager has to deal with it since the beginning. Measurement can play a significant role in the effective management and development of web applications. It provides the scientific basis for web engineering to become true engineering discipline effective web project estimation is one of the most important activities in web development. Proper project planning and control is not possible without a sound and reliable estimate. Although estimating the effort required in developing web applications is a difficult task, accurate estimates of development effort have an important role to play in the successful management of web development projects. This paper is a review about web effort estimation entities, attributes, and methods from the other earlier researches and at last compare these methods to each other. © 2007 IEEE.",,"Moayed M.J., Ghani A.A.A., Seyedzadegan M.",2007,Conference,"Proceedings - The 2007 International Conference on Computational Science and its Applications, ICCSA 2007",10.1109/ICCSA.2007.22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48049095399&doi=10.1109%2fICCSA.2007.22&partnerID=40&md5=5ec28f2bf5139fb09d6f317c094b83e4,"Department of Computer Science and Information Technology, Putra University, Malaysia",,English,,0769529453; 9780769529455
Scopus,An alternative method employing uses cases for early effort estimation,"Function points are a standard method frequently used to calculate size and to estimate effort in applications. Although it has been used with good results in the development of industrial software, it is still necessary to improve some aspects such as: the time at which the estimation of effort is performed and the margin of error in the effort estimation. In this article an alternative method for early effort estimation based on use cases is presented. Two new alternative notions of size are used for effort estimation: Transactions and Entity Objects. A case study developed in order to compare the estimations obtained with the two methods for actual projects is described. The result shows that by using number of transactions as a notion of size and the Technique Mean Productivity Value to perform the estimation, the current way of estimating effort can be improved.",,"Robiolo G., Orosco R.",2007,Conference,Proceedings - International Conference on Software Engineering,10.1109/SEW.2007.91,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749093535&doi=10.1109%2fSEW.2007.91&partnerID=40&md5=092e7fab59f0aebe9eebd54e64585d7e,Universidad Austral; Universidad Argentina de la Empresa,,English,02705257,0769528627; 9780769528625
Scopus,Component-based software certification based on experimental risk assessment,"Third-party software certification should attest that the software product satisfies the required confidence level according to certification standards such as ISO/TEC 9126, ISO/IEC 14598 or ISO/IEC 25051. In many application areas, especially in mission-critical applications, certification is essential or even mandatory. However, the certification of software products using common off-the-shelf (COTS) components is difficult to attain, as detailed information about COTS is seldom available. Nevertheless, software products are increasingly being based on COTS components, which mean that traditional certification processes should be enhanced to take COTS into account in an effective way. This paper proposes a mean to help in the certification of component-based systems through an experimental risk assessment methodology based on fault injection and statistical analysis. Using the proposed methodology the certification authority or the system integrator can compare among components available the one that best fit for the system that is assembling a component that provides a specific functionality. Based on the results it is also possible to decide whether a software product may be considered certified or not in what concerns the risk of using a COTS into the system. The proposed approach is demonstrated and evaluated using a space application running on top of two alternative COTS real-time operating systems: RTEMS and RTLinux. © Springer-Verlag Berlin Heidelberg 2007.",Component-based system certification; Experimental risk assessment; Fault injection,"Moraes R., Durães J., Martins E., Madeira H.",2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-75294-3_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149087639&doi=10.1007%2f978-3-540-75294-3_14&partnerID=40&md5=b928c3683c913fb81d96bbdb722fb9c9,"State University of Campinas, UNICAMP São Paulo, Brazil, 13.084-971 - Campinas - SP, Brazil; CISUC, University of Coimbra, Portugal, 3030-290 - Coimbra, Portugal; CISUC, ISEC, 3030-290 - Coimbra, Portugal",Springer Verlag,English,03029743,3540752935; 9783540752936
Scopus,Regression analisys of segmented parametric software cost estimation models using recursive clustering tool,"Parametric software effort estimation models rely on the availability of historical project databases from which estimation models are derived. In the case of large project databases with data coming from heterogeneous sources, a single mathematical model cannot properly capture the diverse nature of the projects under consideration. Clustering algorithms can be used to segment the project database, obtaining several segmented models. In this paper, a new tool is presented, Recursive Clustering Tool, which implements the EM algorithm to cluster the projects, and allows use different regression curves to fit the different segmented models. This different approaches will be compared to each other and with respect to the parametric model that is not segmented. The results allows conclude that depending on the arrangement and characteristics of the given clusters, one regression approach or another must be used, and in general, the segmented model improve the unsegmented one. © Springer-Verlag Berlin Heidelberg 2006.",Clustering; Effort estimation; EM algorithm; Recursive clustering tool (RCT); Segmented parametric model; Software engineering,"Garre M., Sicilia M.A., Cuadrado J.J., Charro M.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11875581_102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750554887&doi=10.1007%2f11875581_102&partnerID=40&md5=7811a07d57ae8e497aca529bdd7c935d,"University of Alcalá, Ctra. Barcelona km 33.6 - 28871, Alcala de Henares, Madrid, Spain; Dept. Applied Sciences, Torrejón Air Base - Technical School, 28850 - Torrejon de Ardoz, Madrid, Spain",Springer Verlag,English,03029743,3540454853; 9783540454854
Scopus,Software Development Productivity,"This chapter explains what software development productivity is and why it is important. It discusses the various ways of measuring software size and project effort using examples from past research. An overview of productivity factors considered in prior research is also presented. A methodology for determining influential productivity factors in software project data is briefly described. This is followed by a case study that shows what one bank learned from the productivity analysis of their software project data. Finally, the author shares her practical real-life experiences of benchmarking software development productivity. © 2003 Elsevier Science (USA). All rights reserved.",,Maxwell K.D.,2003,Review,Advances in Computers,10.1016/S0065-2458(03)58001-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957119735&doi=10.1016%2fS0065-2458%2803%2958001-9&partnerID=40&md5=8e9e39a927b59334018d98a72cec4323,"Datamax, 7 bis bld. Foch, 77300 Fontainebleau, France",,English,00652458,
Scopus,Measuring system and software architecture complexity,"Conventional measures of software code complexity, specifically code length, Halstead difficulty: and McCahe cyclomatic complexity, are adapted to measure architecture complexity. These measures are applied to architecture models based on Unified Modeling Language (UML) class, sequence, and state diagrams by redefming measure factors in terms of UML elements. This approach is illustrated using architecture diagrams Gom a real-time communication control system. This technique enables architects to consistently quantify system complexity early in and throughout the engineering lifecycle. This is an advance over both current software engineering practices that measure complexity after implementation and commercially available modeling tools that measure sohare code, not architectures. © 2003 IEEE.",,Lankford J.,2003,Conference,IEEE Aerospace Conference Proceedings,10.1109/AERO.2003.1235569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049158947&doi=10.1109%2fAERO.2003.1235569&partnerID=40&md5=83f05f8d4c23d9e259e5788c60c25c16,"Aerospace Corporation, 2350 E. El Segundo Blvd., El Segundo, CA 90245-4691, United States",,English,1095323X,078037651X; 9780780376519
Scopus,Analogy based prediction of work item flow in software projects: A case study,"A software development project coordinates work by using work items that represent customer, tester and developer found defects, enhancements, and new features. We set out to facilitate software project planning by modeling the flow of such work items and using information on historic projects to predict the work flow of an ongoing project. The history of the work items is extracted from problem tracking or configuration management databases. The Web-based prediction tool allows project managers to select relevant past projects and adjust the prediction based on staffing, type, and schedule of the ongoing project. We present the workflow model, and briefly describe project prediction of a large software project for customer relationship management (CRM). © 2003 IEEE.",Computer aided software engineering; Customer relationship management; Data mining; History; Predictive models; Programming; Project management; Resource management; Software testing; Spatial databases,Mockus A.,2003,Conference,"Proceedings - 2003 International Symposium on Empirical Software Engineering, ISESE 2003",10.1109/ISESE.2003.1237970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348963606&doi=10.1109%2fISESE.2003.1237970&partnerID=40&md5=c98ae1dfea7a18a8bd369a4fb774553c,"Avaya Labs. Research, Department of Software Technology Research, 233 Mt Airy Rd., Basking Ridge, NJ  07920, United States",Institute of Electrical and Electronics Engineers Inc.,English,,0769520022; 9780769520025
Scopus,A procedure for assessing the influence of problem domain on effort estimation consistency,"By and large, given the inherent subjectivity in defining and measuring factors used in algorithmic effort estimation methods, when algorithmic methods produce consistent estimates it seems reasonable to assume that this is in part due to estimator experience. Further, software development factors are usually assumed to have different degrees of influence on actual effort. For example, no specific allowances for program language or problem domain were made in the original COCOMO model or in Albrecht's Function Points, whilst allowances for development mode in COCOMO and function type complexity for Albrecht's Function Points are crucial. However, work has been conducted that concluded that 4GLs are associated with higher productivity than 3GLs. Clearly, we can support such conclusions about productivity, since, for example, it usually requires less effort to develop a database using a purposely designed DBMS product than it does using a 3GL. However, in general, for a given problem domain an appropriate development language and platform will be selected. Hence, we might feel that an appropriate development language will not be a factor that influences estimate consistency unduly, given that an estimator has experience of the problem domain. However, algorithmic methods usually require calibration to different problem domains. Calibration may be needed because the method was originally designed using data from another type of domain. Furthermore, estimators' estimation consistency within problem domains may be affected for one or more reasons. Intuitively, reasons might include: estimators lack estimation experience in some domains; or the development team(s) may have different levels of experience in different domains, which the estimator finds difficult to take into account. We demonstrate how, in general, the influence of problem domain may be assessed using a Hierarchical Bayesian inference procedure. We also show how values can be derived to account for variations in estimate consistency in problem domains.",COCOMO; Dependence; Deviance statistic; Effort estimates; Hierarchical Bayesian inference; Hyperparameters; Informative and non-informative priors; Interactions with random effects; Linear regression models; Model fit and checking,"Moses J., Farrow M.",2003,Conference,Software Quality Journal,10.1023/A:1025861011126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543050661&doi=10.1023%2fA%3a1025861011126&partnerID=40&md5=6fbdc851f38d66e06ea692814dfe776e,"School of Computing and Technology, University of Sunderland, SR6 0DD, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Integrating dynamic models for CMM-based software process improvement,"During the last decade software process simulation has been used to address a wide diversity of management problems. Some of these problems are related to strategic management, technology adoption, understanding, training and learning, and risk management, among others. In this work a dynamic integrated framework for software process improvement is presented. This framework combines traditional estimation static models with an intensive utilization of dynamic simulation models of the software process. The aim of this framework is to support a qualitative and quantitative assessment for software process improvement and decision making to achieve a higher software development process capability according to the Capability Maturity Model. The paper describes the concepts underlying this framework, its implementation, the dynamic approach followed to systematically develop the dynamic modules, and an example of its potential use and benefits. © Springer-Verlag Berlin Heidelberg 2002.",,"Ruiz M., Ramos I., Toro M.",2002,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-36209-6_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048875243&doi=10.1007%2f3-540-36209-6_8&partnerID=40&md5=f64eaf3ee7937e622c461f649e4f2114,"Department of Computer Languages and Systems, Escuela Superior de Ingeniería, University of Cádiz, C/ Chile, nº1, Cádiz, 11003, Spain; Department of Computer Languages and Systems, Escuela Técnica Superior de Ingeniería Informática, University of Seville, Avda. Reina Mercedes, s/n, Seville, 41012, Spain",Springer Verlag,English,03029743,3540002340; 9783540002345
Scopus,Bayesian analysis of software cost and quality models,"Due to the pervasive nature of software, software-engineering practitioners have continuously expressed their concerns over their inability to accurately predict the cost, schedule and quality of a software product under development. Thus, one of the most important objectives of the software engineering community has been to develop useful models that constructively explain the software development life-cycle and accurately predict the cost, schedule and quality of developing a software product. Most of the existing parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. This approach imposes a few restrictions often violated by software engineering data and has resulted in the development of inaccurate empirical models that do not perform very well. The focus of this dissertation is to explain the drawbacks of the multiple regression approach for software engineering data and discuss the Bayesian approach which alleviates a few of the problems faced by the multiple regression approach.",,Chulani S.,2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972773,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956616202&doi=10.1109%2fICSM.2001.972773&partnerID=40&md5=d630e04567e9688300358acd8817508b,,IEEE Computer Society,English,,
Scopus,Software metrics for small database applications,"Known software metrics for estimating complexity and effort are usually based on lines of code or the program's flowgraph. Such metrics are suitable for large-scale procedural or object-oriented software applications. In this work, we propose a new complexity metric, called DataBase Points (DBP), that is suitable for small-scale relational database business applications developed in the MS-ACCESS (ACCESS is a trademark of Microsoft Corporation) or similar environments. DBP is constructed from components that are derived from typical ACCESS design. Further, DBP is used to estimate the effort needed to develop such software. The results of applying this new metric to a number of applications show that it is promising and that it captures the complexity features of small database applications. © 2000 ACM.","Complexity metric.; Effort estimation, and; Function points; Software metrics","Abiad S., Haraty R.A., Mansour N.",2000,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/338407.338579,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954767979&doi=10.1145%2f338407.338579&partnerID=40&md5=b42e4cfc0329c4854768497bdeb055f9,"Lebanese American University, P.O. Box 13-5053, Beirut, Lebanon",,English,,1581132409; 9781581132403
Scopus,Statistical analysis of deviation of actual cost from estimated cost using actual project data,"This paper analyzes an association of a deviation of the actual cost (measured by person-month) from the estimated cost with the quality and the productivity of software development projects. Although the obtained results themselves may not be new from the academic point of view, they could provide motivation for developers to join process improvement activities in a software company and thus become a driving force for promoting the process improvement. We show that if a project is performed faithfully under a well-organized project plan (i.e. the plan is first constructed according to the standards of good writing, and then a project is managed and controlled to meet the plan), the deviation of the actual cost from the estimated one becomes small. Next we show statistically that projects with small deviation of the cost estimate tend to achieve high quality of final products and high productivity of development teams. In this analysis, the actual project data on 37 projects at a certain company are extensively applied.",,"Mizuno O., Kikuno T., Inagaki K., Takagi Y., Sakamoto K.",2000,Journal,Information and Software Technology,10.1016/S0950-5849(00)00092-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033751262&doi=10.1016%2fS0950-5849%2800%2900092-6&partnerID=40&md5=e6dd99a5af0d6864fcecd92cbc940dbc,"Grad. School of Engineering Science, Osaka Univ., 1-3 Machikaneyama, T., Osaka, Japan; OMRON Corporation Kusatsu, 525-0035, Shiga, Japan","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,Five Principles for the Formal Validation of Models of Software Metrics,"The goal of software metrics is the improvement of the software process. Five principles are fundamental to the formal validation of models of software metrics:--Attribute Type--what attribute1993 of software behavior are we measuring?;The mathematical postulates of a measure function--a model must have the utility for comparison; this unique criterion must be passed by every effective model of measure;Metrical Vindication--applying the model directly to measure actual software module(s); how consistent and reliable are the results of the measures?;Feedback Effect--how does the metric help us to identify faults and errors (of design) or improve testing and maintenance; what is a metric if it is just a number?;Units of Measure--what are we measuring with respect to dimensions: size, length/depth, extent, degree of variation, degradation, etc.?; a measure without unit(s) is like a building without a roof.Without principles, there is nothing to validate. Every good science is a body of relevant principles. Some worked examples on some of these principles are presented. Some of these examples in a way, expose the failures or frustrations resulting from the applications of some models of measure presently being used in industry and science. Not every mathematical function can be used as a model of measurement. © 1993, ACM. All rights reserved.",,Ejiogu L.O.,1993,Journal,ACM SIGPLAN Notices,10.1145/163114.163123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041680601&doi=10.1145%2f163114.163123&partnerID=40&md5=23714c93dff092d98d791447e222811e,"Software Engineering, Softmetrix, INC., 2540 N. Kedzie, Chicago, II 6064, United States",,English,03621340,
Scopus,An object‐oriented tool for function point analysis,"Abstract: Function points have become an accepted measure of software size and are becoming an industry standard. However, the application of function point analysis is fairly complex and requires experience and a good understanding to apply it in a consistent manner. This paper describes the development of a knowledge‐based, object‐oriented system to assist an analyst in performing function point analysis. The objective of the function point analysis (FPA) tool is to allow an analyst to estimate system size in function points without having extensive training or experience using the function point method. The FPA tool uses information available in a functional specification that is a product of the requirements analysis phase of the software development life cycle. An object‐oriented model was used to represent the functional requirements of a software system. Copyright © 1993, Wiley Blackwell. All rights reserved",,"Matson J.E., Mellichamp J.M.",1993,Journal,Expert Systems,10.1111/j.1468-0394.1993.tb00298.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027544154&doi=10.1111%2fj.1468-0394.1993.tb00298.x&partnerID=40&md5=5ff39e75091c2fdd231894d29608f630,"University of Alabama, Tuscaloosa, Alabama, 35487, United States",,English,02664720,
Scopus,A tool for discriminant analysis and classification of software metrics,"The potential of software complexity measurements has not been fully explored for lack of a systematic approach to collect, evaluate and use these measurements. The goal of the discriminant analysis tool and classification model for software complexity metrics is to automate the process of collecting software measurements. It also develops a methodology to determine the robustness of several combined metrics and offers a norm against which measurements can be compared. The model acts as an evaluator of the many metrics that have been and may be suggested in the literature and offers programmers a systematic approach to choose the ones that best fit their particular type of system, problem domain and environment. © 1987.",methodologies; software metrics; software techniques,"Rodríguez V., Tsai W.",1987,Journal,Information and Software Technology,10.1016/0950-5849(87)90133-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023330360&doi=10.1016%2f0950-5849%2887%2990133-9&partnerID=40&md5=50bb15898342f08a056c5daacba76ab9,"Arden Hills Programming Division, Control Data Corporation, Arden Hills, MN 55112, United States; Computer Science Department, University of Minnesota, Minneapolis, MN 55455, United States",,English,09505849,
Scopus,Software project scheduling problem in the context of search-based software engineering: A systematic review,"This work provides a systematic literature review of the software project scheduling problem, in the context of search-based software engineering, and summarizes the main models, techniques, search algorithms and evaluation criteria applied to solve this problem. We also discuss trends and research opportunities. Our keyword search found 438 papers, published in the last 20 years. After considering the inclusion and exclusion criteria and performing the snowballing procedure, we have analyzed 37 primary studies. The results show the predominance of the use of evolutionary algorithms. The static model, in which the scheduling is performed once during the project, is considered in the majority of the papers. Synthetic instances are commonly used to validate the heuristic and hypervolume and execution time are the mostly applied evaluating criteria. © 2019 Elsevier Inc.",Search-based software engineering; Software project scheduling problem; Systematic review,"Rezende A.V., Silva L., Britto A., Amaral R.",2019,Journal,Journal of Systems and Software,10.1016/j.jss.2019.05.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065781168&doi=10.1016%2fj.jss.2019.05.024&partnerID=40&md5=2b45831474f938f0d09efbd4323ffd36,"Computer Science Department, Federal University of Sergipe (UFS), São Cristóvão, CEP: 49100-000, Brazil",Elsevier Inc.,English,01641212,
Scopus,Internal and external quality in the evolution of mobile software: An exploratory study in open-source market,"Context: Mobile applications evolve rapidly and grow constantly to meet user requirements. Satisfying these requirements may lead to poor design choices that can degrade internal quality and performance, and consequently external quality and quality in use. Therefore, monitoring the characteristics of mobile applications through their evolution is important to facilitate maintenance and development. Objective: This study aims to explore internal quality, external quality and the relation between these two by carrying out an embedded, multiple case study that includes two cases in different functional domains. In each case study, the evolution of three open-source mobile applications having similar features in the same domain and platform is investigated with the analysis of a number of code-based and community-based metrics, to understand whether they are significantly related to quality characteristics. Method: A total of 105 releases of the six mobile applications are analyzed to understand internal quality, where code-based characteristics are employed in the light of Lehman's Increasing Complexity, Continuous Growth, and Decreasing Quality laws. External quality is explored by adapting DeLone and McLean model of information system success and using community-based metrics, when data is available for the included releases, to derive a corresponding success index. Finally, internal and external quality relationship is investigated by applying Spearman's correlation analysis on metrics data from 91 corresponding releases. Results: The analysis of Lehman's laws shows that only the law of Continuous Growth is validated for the selected mobile applications in both case studies. Spearman's analysis results indicate that the internal quality attribute of ‘Understandability’ is negatively related to ‘Success Index’ for Case Study A and ‘LCOM’ is negatively related to ‘Success Index’ for Case Study B. No other significant relationship between the internal quality attributes and the Success Index is observed; but specific to community-based metrics, some significant relationships with code-based attributes were determined. Conclusion: Our exploratory study is unique for the method it employs for exploring the relationship between internal and external quality in the evolution of mobile applications. Yet, our findings should be used with caution as they are derived from a limited number of applications. Therefore, this study should be considered to provide initial evidence for applicability of the method and a degree of confidence for repeating similar studies in wider contexts. © 2019",C & K metric set; DeLone and McLean model; External quality; Internal quality; ISO/IEC 25010; Lehman laws; Mobile software; Open source; OSS; Quality in use; Software evolution; Software quality,"Gezici B., Tarhan A., Chouseinoglou O.",2019,Journal,Information and Software Technology,10.1016/j.infsof.2019.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064813178&doi=10.1016%2fj.infsof.2019.04.002&partnerID=40&md5=9af9e6162f0fc62ef2d390bbfc2b1e6d,"Computer Eng. Dept., Hacettepe University, Software Engineering Research Group (HUSE), Turkey; Industrial Eng. Dept., Hacettepe University, Software Engineering Research Group (HUSE), Turkey",Elsevier B.V.,English,09505849,
Scopus,Dynamic selection of fitness function for software change prediction using Particle Swarm Optimization,"Context: Over the past few years, researchers have been actively searching for an effective classifier which correctly predicts change prone classes. Though, few researchers have ascertained the predictive capability of search-based algorithms in this domain, their effectiveness is highly dependent on the selection of an optimum fitness function. The criteria for selecting one fitness function over the other is the improved predictive capability of the developed model on the entire dataset. However, it may be the case that various subsets of instances of a dataset may give best results with a different fitness function. Objective: The aim of this study is to choose the best fitness function for each instance rather than the entire dataset so as to create models which correctly ascertain the change prone nature of majority of instances. Therefore, we propose a novel framework for the adaptive selection of a dynamic optimum fitness function for each instance of the dataset, which would correctly determine its change prone nature. Method: The predictive models in this study are developed using seven different fitness variants of Particle Swarm Optimization (PSO) algorithm. The proposed framework predicts the best suited fitness variant amongst the seven investigated fitness variants on the basis of structural characteristics of a corresponding instance. Results: The results of the study are empirically validated on fifteen datasets collected from popular open-source software. The proposed adaptive framework was found efficient in determination of change prone classes as it yielded improved results when compared with models developed using individual fitness variants and fitness-based voting ensemble classifiers. Conclusion: The performance of the models developed using the proposed adaptive framework were statistically better than the models developed using individual fitness variants of PSO algorithm and competent to models developed using machine learning ensemble classifiers. © 2019 Elsevier B.V.",Change proneness; Empirical validation; Fitness function; Particle Swarm Optimization,"Malhotra R., Khanna M.",2019,Journal,Information and Software Technology,10.1016/j.infsof.2019.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064712715&doi=10.1016%2fj.infsof.2019.04.007&partnerID=40&md5=d5695a072720fa27361fabda76c6950d,"Discipline of Software Engineering, Department of Computer Science & Engineering, Delhi Technological University, Delhi, India; Discipline of Software Engineering, Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Sri Guru Gobind Singh College of Commerce, University of Delhi, Delhi, India",Elsevier B.V.,English,09505849,
Scopus,Effort estimation model for software development projects based on use case reuse,"This paper describes a new effort estimation model based on use case reuse, called the use case reusability (UCR), intended for the projects that are reusing artifacts previously developed in past projects with similar scope. Analysis of the widely spread effort estimation techniques for software development projects shows that these techniques were primarily intended for the development of new software solutions. The baseline for the new effort estimation model is the use case points model. The UCR model introduces new classification of use cases based on their reusability, and it includes only those technical and environmental factors that according to the effort estimation experts have significant impact on effort for the target projects. This paper also presents a study which validates the usage of UCR model. The study is conducted within industry and academic environments using industry project teams and postgraduate students as subjects. The analysis results show that UCR model can be applied in different project environments and that according to the observed mean magnitude relative error, it produced very promising effort estimates. © 2018 The Authors. Journal of Software: Evolution and Process Published by John Wiley & Sons Ltd.",effort estimation; reusability; software development; use case; use case points,"Rak K., Car Ž., Lovrek I.",2019,Journal,Journal of Software: Evolution and Process,10.1002/smr.2119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061603844&doi=10.1002%2fsmr.2119&partnerID=40&md5=203e355b19610cfdacc80791036489e0,"IBM Croatia, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia",John Wiley and Sons Ltd,English,20477481,
Scopus,A novel online supervised hyperparameter tuning procedure applied to cross-company software effort estimation,"Software effort estimation is an online supervised learning problem, where new training projects may become available over time. In this scenario, the Cross-Company (CC) approach Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving their collection cost. However, Dycom requires CC projects to be split into subsets. Both the number and composition of such subsets can affect Dycom’s predictive performance. Even though clustering methods could be used to automatically create CC subsets, there are no procedures for automatically tuning the number of clusters over time in online supervised scenarios. This paper proposes the first procedure for that. An investigation of Dycom using six clustering methods and three automated tuning procedures is performed, to check whether clustering with automated tuning can create well performing CC splits. A case study with the ISBSG Repository shows that the proposed tuning procedure in combination with a simple threshold-based clustering method is the most successful in enabling Dycom to drastically reduce (by a factor of 10) the number of required WC training projects, while maintaining (or even improving) predictive performance in comparison with a corresponding WC model. A detailed analysis is provided to understand the conditions under which this approach does or does not work well. Overall, the proposed online supervised tuning procedure was generally successful in enabling a very simple threshold-based clustering approach to obtain the most competitive Dycom results. This demonstrates the value of automatically tuning hyperparameters over time in a supervised way. © 2019, The Author(s).",Concept drift; Cross-company learning; Hyperparameter tuning; Online learning; Software effort estimation; Transfer learning,Minku L.L.,2019,Journal,Empirical Software Engineering,10.1007/s10664-019-09686-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596441&doi=10.1007%2fs10664-019-09686-w&partnerID=40&md5=4479fee57d19f1f80c4d3126a2a5b327,"School of Computer Science, The University of Birmingham, Edgbaston, Birmingham, B15 2TT, United Kingdom",Springer New York LLC,English,13823256,
Scopus,The state-of-the-art in software development effort estimation,"The software developers and researchers have been facing difficulties regarding software development effort estimation (SDEE) since 1960s. Both overestimation and underestimation are problematic for future software development. The software engineering field is continuously adapting new technologies and development methodologies, so there is always a requirement to have an accurate SDEE method that can cater the needs of continually growing software industry. The major purpose of this state-of-the-art review is to provide an additional insight of existing SDEE studies while considering five points of reference: techniques used to construct models, strengths and weaknesses of different models, availability of benchmark data sets, data set characteristics, generalization ability of models. We have performed a comprehensive review of SDEE studies published in the period 1981-2016. We have defined a new scheme of categorizing existing SDEE models. We have found that a majority of available data sets do not include complete information of projects, which misleads the direction of research. To compare SDEE models, we recommend to use same data sets while focusing on specific aspects of accuracy as none of SDEE studies has yet been able to compare all the existing models over same data sets while considering same aspects of accuracy. © 2018 John Wiley & Sons, Ltd.",effort estimation; software development; software engineering; software estimation,"Gautam S.S., Singh V.",2018,Journal,Journal of Software: Evolution and Process,10.1002/smr.1983,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058495419&doi=10.1002%2fsmr.1983&partnerID=40&md5=79272d2ebab15cafa74532d5487e40c0,"Department of Information Technology, Indian Institute of Information Technology, Allahabad, 211015, India",John Wiley and Sons Ltd,English,20477481,
Scopus,Developing and using checklists to improve software effort estimation: A multi-case study,"Expert judgment based effort estimation techniques are widely used for estimating software effort. In the absence of process support, experts may overlook important factors during estimation, leading to inconsistent estimates. This might cause underestimation, which is a common problem in software projects. This multi-case study aims to improve expert estimation of software development effort. Our goal is two-fold: 1) to propose a process to develop and evolve estimation checklists for agile teams, and 2) to evaluate the usefulness of the checklists in improving expert estimation processes. The use of checklists improved the accuracy of the estimates in two case companies. In particular, the underestimation bias was reduced to a large extent. For the third case, we could not perform a similar analysis, due to the unavailability of historical data. However, when checklist was used in two sprints, the estimates were quite accurate (median Balanced Relative Error (BRE) bias of -0.05). The study participants from the case companies observed several benefits of using the checklists during estimation, such as increased confidence in estimates, improved consistency due to help in recalling relevant factors, more objectivity in the process, improved understanding of the tasks being estimated, and reduced chances of missing tasks. © 2018 Elsevier Inc.",Agile software development; Case study; Checklist; Expert judgment based effort estimation,"Usman M., Petersen K., Börstler J., Santos Neto P.",2018,Journal,Journal of Systems and Software,10.1016/j.jss.2018.09.054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054338217&doi=10.1016%2fj.jss.2018.09.054&partnerID=40&md5=7fc66958e0bfd3e598a4d0b730493570,"Department of Software Engineering, Blekinge Institute of Technology, 371 79, Karlskrona, Sweden; Federal University of Piaui, Teresina, Brazil",Elsevier Inc.,English,01641212,
Scopus,Influence Factors in Software Productivity - A Tertiary Literature Review,"Software organizations need to increase their productivity to stay competitive. Although there is a lot of research on productivity in software development, software organizations still do not know what are the most significant productivity factors in which they should invest. This paper presents a Tertiary Literature Review (TLR) that aims to identify and analyze Systematic Literature Reviews (SLR) on the influence factors of software productivity reported in the scientific literature. We extracted and classified the influence factors into organizational factors (organizational-dependent factors) and human factors (people-dependent factors). The relevance of the factors was extracted according to the amount of references found in the secondary studies. Using this information, software organizations can improve the productivity of their projects by evaluating the influence factors that best fit their context. © 2018 World Scientific Publishing Company.",productivity influence factors; software productivity; Tertiary literature review,"Oliveira E., Conte T., Cristo M., Valentim N.",2018,Conference,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194018400296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059932552&doi=10.1142%2fS0218194018400296&partnerID=40&md5=8fd20aa025440a694ed054ec488dcd36,"Institute of Computing, Federal University of Amazonas, UFAM, Manaus, Amazonas, Brazil; Department of Informatics, Federal University of Paraná, UFPR, Curitiba, Paraná, Brazil",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Investigating the Significance of the Bellwether Effect to Improve Software Effort Prediction: Further Empirical Study,"Context: In addressing how best to estimate how much effort is required to develop software, a recent study found that using exemplary and recently completed projects [forming Bellwether moving windows (BMW)] in software effort prediction (SEP) models leads to relatively improved accuracy. More studies need to be conducted to determine whether the BMW yields improved accuracy in general, since different sizing and aging parameters of the BMW are known to affect accuracy. Objective: To investigate the existence of exemplary projects (Bellwethers) with defined window size and age parameters, and whether their use in SEP improves prediction accuracy. Method: We empirically investigate the moving window assumption based on the theory that the prediction outcome of a future event depends on the outcomes of prior events. Sampling of Bellwethers was undertaken using three introduced Bellwether methods (SSPM, SysSam, and RandSam). The ergodic Markov chain was used to determine the stationarity of the Bellwethers. Results: Empirical results show that 1) Bellwethers exist in SEP and 2) the BMW has an approximate size of 50 to 80 exemplary projects that should not be more than 2 years old relative to the new projects to be estimated. Conclusion: The study's results add further weight to the recommended use of Bellwethers for improved prediction accuracy in SEP. © 1963-2012 IEEE.",Bellwether effect; Bellwether moving window (BMW); growing portfolio (GP); Markov chains; software effort prediction (SEP),"Mensah S., Keung J., MacDonell S.G., Bosu M.F., Bennin K.E.",2018,Journal,IEEE Transactions on Reliability,10.1109/TR.2018.2839718,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048670006&doi=10.1109%2fTR.2018.2839718&partnerID=40&md5=77c9c6194d852febf840bf8ddbf133d8,"Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Department of Information Science, University of Otago, Dunedin, 9016, New Zealand; Department of IT and Software Engineering, Auckland University of Technology, Auckland, 1010, New Zealand; Centre for Business, Information Technology and Enterprise, Wintec, Hamilton, 3200, New Zealand",Institute of Electrical and Electronics Engineers Inc.,English,00189529,
Scopus,Evaluating System Architecture Quality and Architecting Team Performance Using Information Quality Theory,"As engineering projects grow in complexity, estimating the required engineering effort during the development phase of a project has become more art than science. Predictions of required engineering effort are based on empirical models fitted to historical data with additional subjective factors, such as ""team cohesion,"" applied based on the judgment of the user of the model. Recent work has shown that information quality theory can be used to estimate the engineering effort necessary to develop system requirements by evaluating the change in requirements uncertainty during the requirements engineering process. This paper builds upon that foundation by generalizing the information quality theory to apply to the variety of system engineering artifacts generated during the development phase of a program. The generalized form of information quality theory is implemented in a model for application to the system architecture definition process. Engineering effort required for system architecture definition on four programs is used to evaluate both the performance of the model and the performance of the system architecture definition teams. The results of the model and program evaluations show clear benefits in reducing the engineering effort required to define a system's architecture when there is reuse from previous programs. The results of the evaluation also show the architecture definition team benefits from ""momentum"" and performs more efficiently if the team has completed a system's architecture definition on a recent program. © 2017 IEEE.",Design synthesis process; engineering effort estimation; engineering team efficiency; information quality theory; system architecture quality,"Corbets J.B., Willy C.J., Bischoff J.E.",2018,Journal,IEEE Systems Journal,10.1109/JSYST.2017.2647980,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011673127&doi=10.1109%2fJSYST.2017.2647980&partnerID=40&md5=4452c7806001b13b06a21b82fa677033,"Lockheed Martin Corporation, Palmdale, CA  93599, United States; George Washington University, Washington, DC  20052, United States; Department of Engineering Management and Systems Engineering, George Washington University, Washington, DC  20052, United States",Institute of Electrical and Electronics Engineers Inc.,English,19328184,
Scopus,Nature inspired algorithm,"Nature-inspired algorithms are a set of novel problem-solving methodologies and approaches and have been attracting considerable attention for their good performance. Representative examples of nature-inspired algorithms include artificial neural networks (ANN), fuzzy systems (FS), evolutionary computing (EC), and swarm intelligence (SI), and they have been applied to solve many real-world problems. Despite the popularity of nature-inspired algorithms, many challenges remain which require further research efforts. The contributions presented in this special issue include some latest developments of nature-inspired algorithms, such as genetic algorithm, particle swarm optimization, ant colony optimization, migrating birds optimization, neural networks, gravitational search algorithm, and their applications. Several real-world optimization problems have been studied by several nature-inspired algorithms. In this paper, we are going to see Firefly and Particle swarm optimization. © 2017 IEEE.",bio-inspired algorithm; Firefly; Particle swarm optimization,"Ajay Adithyan T., Sharma V., Gururaj B., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046652445&doi=10.1109%2fICOEI.2017.8300889&partnerID=40&md5=b45b9c5685cbc93b7728a04cd417eba0,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Pearson Correlation Coefficient Analysis (PCCA) on Adenoma carcinoma cancer,"Adenoma is a benign (non-dangerous) tumor that structures from the cells covering the inward surface or body. Here we have made a Pearson Correlation Coefficient Analysis (PCCA) on the Adenoma attributes to find its strength of association. From this result, one can make wise decision to predict the Adenoma cancer. © 2017 IEEE.",Adenoma; Cancer; Carcinoma; Pearson Correlation,"Mujahid A.K.R., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300976,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046642858&doi=10.1109%2fICOEI.2017.8300976&partnerID=40&md5=423074704dae0c6b67e923526ba91450,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,A new combinatorial framework for software services development effort estimation,"Since software services have become one of the most important parts of companies and organizations in recent years, estimation of the effort required for their development is a critical necessity. Increased accuracy of an estimation model is important for both industry and academia. Different methods and models have been proposed for estimating the effort, but none are suitable for all services and different types of data-sets. In fact, the performance of each model can be right or wrong based on conditions. In this paper, a new method is presented based on a combination of different estimation models and mathematical assembling of their answers. The new method was evaluated on two quite different real data-sets. The results show that the proposed method has improved the estimation accuracy and performance. The proposed method flexibly estimates the effort without any additional overhead. It is completely independent of methods employed in the combination so that any new method can be smoothly added. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",Development effort; Estimation; Software services,Bardsiri A.K.,2018,Journal,International Journal of Computers and Applications,10.1080/1206212X.2017.1395103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045107175&doi=10.1080%2f1206212X.2017.1395103&partnerID=40&md5=faa9bd8219bb683d48c9261f53f2e7c5,"Computer Engineering Department, Islamic Azad University, Bardsir, Iran",Taylor and Francis Ltd.,English,1206212X,
Scopus,Performance analysis of FCM based ANFIS and ELMAN neural network in software effort estimation,"One of the major challenges confronted in the software industry is the software cost estimation. It is very much related to, the decision making in an organization to bid, plan and budget the system that is to be developed. The basic parameter in the software cost estimation is the development effort. It tend to be less accurate when computed manually. This is because, the requirements are not specified accurately at the earlier stage of the project. So several methods were developed to estimate the development effort such as regression, iteration etc. In this paper a soft computing based approach is introduced to estimate the development effort. The methodology involves an Adaptive Neuro Fuzzy Inference System (ANFIS) using the Fuzzy C Means clustering (FCM) and Subtractive Clustering (SC) technique to compute the software effort. The methodology is compared with the effort estimated using an Elman neural network. The performance characteristics of the ANFIS based FCM and SC are verified using evaluation parameters. © 2018, Zarka Private University. All rights reserved.",ANFIS; Cost; Effort estimation; Process planning; Software development,"Edinson P., Muthuraj L.",2018,Journal,International Arab Journal of Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040927694&partnerID=40&md5=d81c2c1200f73fbeeccd1007ee839297,"Department of Electronics and Communication Engineering, V V College of Engineering, India; Department of Computer Science and Engineering, Government College of Engineering, India",Zarka Private University,English,16833198,
Scopus,Software defect prediction: A comparison between artificial neural network and support vector machine,"Software industry has stipulated the need for good quality software projects to be delivered on time and within budget. Software defect prediction (SDP) has led to the application of machine learning algorithms for building defect classification models using software metrics and defect proneness as the independent and dependent variables, respectively. This work performs an empirical comparison of the two classification methods: support vector machine (SVM) and artificial neural network (ANN), both having the predictive capability to handle the complex nonlinear relationships between the software attributes and the software defect. Seven data sets from the PROMISE repository are used and the prediction models’ are assessed on the parameters of accuracy, recall, and specificity. The results show that SVM is better than ANN in terms of recall, while the later one performed well along the dimensions of accuracy and specificity. Therefore, it is concluded that it is necessary to determine the evaluation parameters according to the criticality of the project, and then decide upon the classification model to be applied. © Springer Nature Singapore Pte Ltd. 2018.",Artificial neural networks; Back propagation; Software quality; Supervised learning; Support vector machine,"Arora I., Saha A.",2018,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-10-4603-2_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034082083&doi=10.1007%2f978-981-10-4603-2_6&partnerID=40&md5=0082c89cbec075d302d9da889755753c,"Northern India Engineering College, FC-26, Shastri Park, Delhi, 110053, India; University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Sector-16C, Dwarka, Delhi, 110078, India",Springer Verlag,English,21945357,9789811046025
Scopus,An empirical study of the impact of bad designs on defect proneness,"To reduce loss from software defects, in the past decades, a number of software engineering researchers have proposed many software defect prediction techniques, which mainly focus on predicting the defect prone software modules, source code files, or code changes. Prior research have identified software design has significant impacts on software quality, especially the bad designs, e.g., anti-patterns, high dependency design, and large source code files, have made various software engineering tasks more difficult. Given these prior works, various bad designs indicators have been widely considered as the fundamental defect prediction metrics in various defect prediction models. Even though the performance of these techniques have been investigated empirically, researchers have not yet gained a clear understanding of correlation between these design metrics and defects proneness. To bridge this gap, in this paper, we investigate the impact of the three kinds of bad design indicators on software defect proneness by conducting a comprehensive empirical study on 18 release versions of the Apache Commons series. In details, we discuss the defect proneness on the file level of three kinds of bad designs, corresponding to seven defect proneness metrics, including various types of well defined code smells, high method dependency, and the files of large size. Furthermore, we investigate the performance of each defect proneness metrics and the overlap between the file sets involved in the bad designs. The experiment results indicate that the three types of bad designs do have impact on defect proneness, the files participating in some special code smell, the large number of code calls to other modules and the large number of lines of code are significantly more likely to be faulty. Moreover, the overlaps of three types of bad designs are relatively small, which means that each group of defect proneness metrics is independent of each other. © 2017 IEEE.",,"Zhang X., Zhou Y., Zhu C.",2017,Conference,"Proceedings - 2017 Annual Conference on Software Analysis, Testing and Evolution, SATE 2017",10.1109/SATE.2017.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043475991&doi=10.1109%2fSATE.2017.9&partnerID=40&md5=83410ccb723a2f94ee10574d12ffd9b6,"School of Computer Science and Technology, Soochow University, Suzhou, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781538636879
Scopus,The Impact of Software Development Process on Software Quality: A Review,Quality of software products depends upon various phase of software development process. Process of software development is used to create and achieve quality in software products. Software development process uses four main phases which have its own importance for development. Software quality is a conformance to requirements which is divided into functional and non-functional requirements. The objective of this paper is to present a review on the impact of software development process on software quality. In this paper various quality attributes have been considered during analysis of development process to see the impact on software quality. During analysis we observe that software architecture is more important phase than other phase because it provides abstract representation of overall structure of software. © 2016 IEEE.,software coding/implementation; software design; software development process; software quality; software requirement; software testing,"Singh B., Gautam S.",2017,Conference,"Proceedings - 2016 8th International Conference on Computational Intelligence and Communication Networks, CICN 2016",10.1109/CICN.2016.137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040032605&doi=10.1109%2fCICN.2016.137&partnerID=40&md5=76d202ee37155441403e06424a3165dc,"ICT Research Lab, Department of Computer Science, University of Lucknow, Lucknow, 226007, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509011445
Scopus,An investigation of effort distribution among development phases: A four-stage progressive software cost estimation model,"Software cost estimation is a key process in project management. Estimations in the initial project phases are made with a lot of uncertainty that influences estimation accuracy which typically increases as the project progresses in time. Project data collected during the various project phases can be used in a progressive time-dependent fashion to train software cost estimation models. Our motivation is to reduce uncertainty and increase confidence based on the understanding of patterns of effort distributions in development phases of real-world projects. In this work, we study effort distributions and suggest a four-stage progressive software cost estimation model, adjusting the initial effort estimates during the development life-cycle based on newly available data. Initial estimates are reviewed on the basis of the experience gained as development progresses and as new information becomes available. The proposed model provides an early, a post-planning, a post-specifications, and a post-design estimate, while it uses industrial data from the ISBSG (R10) dataset. The results reveal emerging patterns of effort distributions and indicate that the model provides effective estimations and exhibits high explanatory value. Contributions in lessons learned and practical implications are also provided. Copyright © 2017 John Wiley & Sons, Ltd.",four-stage progressive model; project management; software cost estimation,"Papatheocharous E., Bibi S., Stamelos I., Andreou A.S.",2017,Journal,Journal of Software: Evolution and Process,10.1002/smr.1881,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021258815&doi=10.1002%2fsmr.1881&partnerID=40&md5=6f859f5724b47e949414513bedbdebd5,"University of Cyprus, Department of Computer Science, 75 Kallipoleos Street, P.O. Box 20537, Nicosia, CY1678, Cyprus; Swedish Institute of Computer Science, Isafjordsgatan 22/Kistagången 16, Box 1263, Kista, Stockholm  SE-164 29, Sweden; Department of Informatics & Telecommunications Engineering, University of Western Macedonia, Kozani, Greece; Department of Informatics, Aristotle University Campus, Aristotle University, PO 54124, Thessaloniki, Greece; Department of Electrical Engineering/Computer Engineering and Informatics, Cyprus University of Technology, 31 Archbishop Kyprianos Street, Limassol, 3036, Cyprus",John Wiley and Sons Ltd,English,20477481,
Scopus,Effort estimation for ERP projects - A systematic review,"Enterprise Resource Planning (ERP) systems are large scale integrated systems covering most of the business processes of an enterprise. ERP projects differ from software projects with customization, modification, integration and data conversion phases. Most of the time effort and time estimations are performed in an ad-hoc fashion in ERP projects and as a result they frequently suffer from time and budget overruns. Although there is no consensus on a methodology to estimate size, effort and cost of ERP projects there are various research studies in the field. The purpose of this paper is to review the literature on effort estimation methods for ERP projects, their validations and limitations. The systematic literature review used online journal indexes between January 2000 and December 2016. Studies focusing on effort estimation for ERP projects were selected. Two reviewers assessed all studies and 41 were shortlisted. In most of the studies, cost factors for ERP projects were investigated and validated. Our findings showed that effort estimation methods have mostly used function points as an input. Validations of these methods were mostly done by using history-based validation approaches. © 2017 IEEE.",Effort estimation; Enterprise resource planning; Systematic literature review,"Omural N.K., Demirors O.",2017,Conference,"Proceedings - 43rd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2017",10.1109/SEAA.2017.68,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034431001&doi=10.1109%2fSEAA.2017.68&partnerID=40&md5=29ad5c6e7317d6c3b480980a3df06e8a,"Informatics Institute, Middle East Technical University, Ankara, Turkey; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia",Institute of Electrical and Electronics Engineers Inc.,English,,9781538621400
Scopus,Effort and cost in software engineering: A comparison of two industrial data sets,"Context: The research literature on software development projects usually assumes that effort is a good proxy for cost. Practice, however, suggests that there are circumstances in which costs and effort should be distinguished. Objectives: We determine similarities and differences between size, effort, cost, duration, and number of defects of software projects. Method: We compare two established repositories (ISBSG and EBSPM) comprising almost 700 projects from industry. Results: We demonstrate a (log)-linear relation between cost on the one hand, and size, duration and number of defects on the other. This justifies conducting linear regression for cost. We establish that ISBSG is substantially different from EBSPM, in terms of cost (cheaper) and duration (faster), and the relation between cost and effort. We show that while in ISBSG effort is the most important cost factor, this is not the case in other repositories, such as EBSPM in which size is the dominant factor. Conclusion: Practitioners and researchers alike should be cautious when drawing conclusions from a single repository. © 2017 Association for Computing Machinery.",Benchmarking; Cost prediction; EBSPM; Evidence-based software portfolio management; ISBSG; Software economics,"Huijgens H., Van Deursen A., Minku L.L., Lokan C.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3084226.3084249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025451477&doi=10.1145%2f3084226.3084249&partnerID=40&md5=850dd599bc7741ae10ad6677bc945003,"Delft University of Technology, Delft, Netherlands; Department of Informatics, University of Leicester, United Kingdom; University of New South Wales, Canberra, Australia",Association for Computing Machinery,English,,9781450348041
Scopus,Effort estimation in co-located and globally distributed agile software development: A comparative study,"Context: Agile methods are used both by both co-located and globally distributed teams. Recently separate studies have been conducted to understand how effort estimation is practiced in Agile Software Development (ASD) in co-located and distributed contexts. There is need to compare the findings of these studies. Objectives: The objective of this comparative study is to identify the similarities and differences in how effort estimation is practiced in co-located and globally distributed ASD. Method: We combined the data of the two surveys to conduct this comparative study. First survey was conducted to identify the state of the practice on effort estimation in co-located ASD, while the second one identified the same in globally distributed ASD context. Results: The main findings of this comparative study are: 1) Agile practitioners, both in co-located and distributed contexts, apply techniques that use experts' subjective assessment to estimate effort. 2) Story points are the most frequently used size metrics in both co-located and distributed agile contexts 3) Team's prior experience and skill level are leading cost drivers in both contexts. Distributed agile practitioners cited additional cost drivers related to the geographical distance between distributed teams. 4) In both co-located and distributed agile context, effort is estimated mainly at iteration and release planning levels 5) With regard to the accuracy of effort estimates, underestimation is the dominant for both co-located and distributed agile software development. Conclusions: Similar techniques and size metrics have been used to estimate effort by both co-located and distributed agile teams. The main difference is with regard to the factors that are considered as important cost drivers. Global barriers due to cultural, geographical and temporal differences are important cost and effort drivers for distributed ASD. These additional cost drivers should be considered when estimating effort of a distributed agile project to avoid gross underestimation. © 2016 IEEE.",,"Usman M., Britto R.",2017,Conference,"Proceedings - 26th International Workshop on Software Measurement, IWSM 2016 and the 11th International Conference on Software Process and Product Measurement, Mensura 2016",10.1109/IWSM-Mensura.2016.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011957133&doi=10.1109%2fIWSM-Mensura.2016.042&partnerID=40&md5=8bfb600695692887758e495e70ef7723,"Department of Systems Engineering, Okayama Prefectural University, Soja, Okayama, 719-1197, Japan; School of Engineering and Information Technology, UNSW Canberra, Canberra, ACT  2600, Australia",Institute of Electrical and Electronics Engineers Inc.,English,,9781509041473
Scopus,An Empirical Validation of Mobile Application Effort Estimation Models,"Software effort and cost estimation are necessary for the software project manager to be able to successfully plan for the software project. At present, the number of the mobile applications, such as smartphones and tablets, is increasing. The planning and development environment of such mobile applications is different from the traditional information system development. It is asserted that traditional effort estimation models may not be appropriate for the mobile application development project. Therefore new approaches specially designed to fit for mobile application effort estimation in the new environment have been suggested [1], [2]. This research empirically validated and compared the accuracy between a traditional effort estimation model i.e. Function Points Analysis method and a proposed method especially design for mobile application effort estimation, in order to find out which software effort estimation model is more appropriate for mobile development environment. The findings of this study show high percentage errors in term of MRE percentages and very low on the measure of prediction level or PRED (p) for both estimation models. The statistical test also indicates that there is no statistical different for the accuracy of both models.",Effort estimation accuracy; Effort estimation model validation; Mobile application effort estimation; Mobile application effort estimation accuracy; Mobile effort estimation validation,"Arnuphaptrairong T., Suksawasd W.",2017,Conference,Lecture Notes in Engineering and Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041219855&partnerID=40&md5=d519e62e9aa427334fd8fc6a4051c7e2,"Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok, 10250, Thailand; Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok, 10250, Thailand",Newswood Limited,English,20780958,9789881404770
Scopus,Software Development Effort Estimation based-on multiple classifier system and Lines of Code,"The development effort estimation is one of the most difficult problems in software project management. It is one of the most critical aspects in the early stages of the software project. Several software development effort estimation models have been proposed, however, these models are not able to obtain more than a 25 percent of accuracy, neither provide an understandable model for experts in the application area. Therefore, in this paper, we present EEpred, an explanatory model to estimate the development effort based on data of known software projects. It is a serial multiple classifier system based-on several decision trees. The model performance was evaluated by an internal validation procedure, analyzing their robustness and predictive performance. This procedure demonstrates that EEpred is able to estimate the software development effort with a 71 percent of precision. The main advantage of EEpred, regarding to other algorithms, is its ability to translate the process into a collection of simple decision rules, providing more easily interpretable knowledge that can help software engineer to improve decision-making on development planning. © 2003-2012 IEEE.",Decision Trees; Effort estimation; Explanatory models; Multiple Classifiers; Regression Trees; Software metrics; Software projects,"Velarde H., Santiesteban C., Garcia A., Casillas J.",2016,Journal,IEEE Latin America Transactions,10.1109/TLA.2016.7786379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007495794&doi=10.1109%2fTLA.2016.7786379&partnerID=40&md5=a8cb1ec2d8fbe611c55a0f4bba808f0a,"Facultad de Ciencias e Ingenierías. Físicas y Formales, Universidad Católica de Santa María, Arequipa, Peru; Centro de Bioplantas, Universidad de Ciego de Ávila, Cuba; División Territorial Villa Clara, DESOFT, Cuba; Departamento de Ciencias de la Computación e Inteligencia Artificial, Universidad de Granada, Spain",IEEE Computer Society,Spanish,15480992,
Scopus,"A framework for capturing, statistically modeling and analyzing the evolution of software models","This paper presents a new methodological framework for capturing and statistically modeling the evolution of models in model-driven software development. The framework captures the changes between revisions of models in terms of both low-level (internal) and high-level (developer-visible) edit operations applied between revisions. In our approach, evolution is modeled statistically by using ARMA, GARCH and mixed ARMA-GARCH models. Forecasting and simulation aspects of these time series models are thoroughly assessed. The suitability of the framework is shown by applying it to a large set of design models of real Java systems. Our analysis shows that mixed ARMA-GARCH models are superior to ARMA models. A main motivation for, and application of, the resulting statistical models is to control the generation of realistic model histories which are intended to be used for testing model versioning tools. We present the architecture of the model generator and show how to generate random sequences from the statistical models which control the generation process. Further usages of the statistical models include various forecasting and simulation tasks. © 2016 Elsevier Inc. All rights reserved.",Forecasting; Model driven engineering; Simulation; Software model evolution analysis; Test model generation; Time series analysis,"Shariat Yazdi H., Angelis L., Kehrer T., Kelter U.",2016,Journal,Journal of Systems and Software,10.1016/j.jss.2016.05.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969190349&doi=10.1016%2fj.jss.2016.05.010&partnerID=40&md5=b1964cd7dcad36eed7033a55fa3dc980,"Software Engineering Group, University of Siegen, Germany; Department of Informatics, Aristotle University of Thessaloniki, Greece; Department of Electronics, Informatics and Bioengineering, Politecnico di Milano, Italy",Elsevier Inc.,English,01641212,
Scopus,A Bayesian Prediction Model for Risk-Based Test Selection,"In industry, testing is commonly performed under severe pressure due to limited resources. Therefore, risk-based testing, which uses predicted risks to guide the test process, is employed to select test cases. To this end, risks have so far mainly been estimated ad hoc, but not systematically predicted on the basis of the defect history and defect costs. In this paper, we present a novel approach to risk-based test selection, which employs a comprehensive and versatile Bayes risk model taking defect probabilities and costs into account. It enables the prediction of a risk decrement that could potentially be used for test selection. We first define a generic Bayes risk decision criterion for test selection, and then implement and evaluate it in an industrial software development project, where it is intended to support decisions steering the quality assurance process. © 2015 IEEE.",Bayesian Reasoning; Prediction; Risk Analysis; Risk-Based Testing; Test Management; Test Selection,"Adorf H.-M., Felderer M., Varendorff M., Breu R.",2015,Conference,"Proceedings - 41st Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2015",10.1109/SEAA.2015.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958267972&doi=10.1109%2fSEAA.2015.37&partnerID=40&md5=53601457b6eb54e6742f525d5f13734e,"Mgm Technology Partners, Munich, Germany; University of Innsbruck, Innsbruck, Austria",Institute of Electrical and Electronics Engineers Inc.,English,,9781467375856
Scopus,Functional and non-functional size measurement with IFPUG FPA and SNAP — case study,"Software size measures are probably the most frequently used metrics in software development projects. One of the most popular size measurement methods is the IFPUG Function Point Analysis (FPA), which was introduced by Allan Albrecht in the late-1970’s. Although the method proved useful in the context of cost estimation, it focuses only on measuring functional aspects of software systems. To address this deficiency, a complementary method was recently proposed by IFPUG, which is called Software Non-functional Assessment Process (SNAP). Unfortunately, the method is still new and we lack in-depth understanding of when and how it should be applied. The goal of the case study being described in the paper was to investigate how FPA and SNAP measurement methods relate to each other, and provide some early insights into the application of SNAP to measure the non-functional size of applications. The results of the study show that SNAP could help mitigating some well-known deficiencies of the FPA method. However, we have also identified some potential problems related to applying SNAP in a price-persize- unit pricing model. © Springer International Publishing Switzerland 2015.",FPA; Function Points; IFPUG; Size measurement; SNAP,"Ochodek M., Ozgok B.",2015,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-319-18473-9_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942777718&doi=10.1007%2f978-3-319-18473-9_3&partnerID=40&md5=077af6727d9f86e41d168ab77f120fef,"Poznan University of Technology, Institute of Computing Science, ul. Piotrowo 2, Poznań, 60-965, Poland",Springer Verlag,English,21945357,9783319184722
Scopus,Function point analysis using NESMA: simplifying the sizing without simplifying the size,"This paper examines the trade-off between the utility of outputs from simplified functional sizing approaches, and the effort required by these sizing approaches, through a pilot study. The goal of this pilot study was to evaluate the quality of sizing output provided by NESMA’s simplified size estimation methods, adapt their general principles to enhance their accuracy and extent of relevance, and empirically validate such an adapted approach using commercial software projects. A dataset of 11 projects was sized using this adapted approach, and these results compared with those of the established Indicative, Estimated and Full NESMA method approaches. The performances of these adaptations were evaluated against the NESMA approaches in three ways: (1) effort to perform; (2) the accuracy of the total function counts produced; and (3) the accuracy of the profiles of the function counts for each of the base functional component types. The adapted approach outperformed the Indicative NESMA in terms of sizing accuracy and generally performed as well as the Estimated NESMA across both datasets, and required only ~ 50 % of the effort incurred by the Estimated NESMA. This adapted approach, applied to varying levels of information presented in commercial requirements documentation, overcame some of the limitations of simplified functional sizing methods by providing more than simply the simplified ‘indication’ of overall functional size. The provision and refinement of the more detailed function profile enable a greater degree of validation and utility for the size estimate. © 2013, Springer Science+Business Media New York.",Commercial projects; Function point analysis; NESMA; Simplified estimation; Software size estimation,"Morrow P., Wilkie F.G., McChesney I.R.",2014,Journal,Software Quality Journal,10.1007/s11219-013-9215-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919951229&doi=10.1007%2fs11219-013-9215-1&partnerID=40&md5=a2b7d5e168095b934cd8f5a0c447a4d9,"School of Computing and Mathematics, University of Ulster, Newtownabbey, Co-Antrim, BT37 0QB, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Cost prediction and software project management,"This chapter reviews the background and extent of the software project cost prediction problem. Given the importance of the topic, there has been a great deal of research activity over the past 40 years, most of which has focused on developing formal cost prediction systems. The problem is that presently there is limited evidence to suggest formal methods outperform experts, therefore detailed consideration is given to the available empirical evidence concerning expert performance. This shows that software professionals tend to be biased (optimistic) and over-confident, and there are a number of deep cognitive biases which help us understand why this is so. Finally, the chapter describes how this might best be tackled through a range of simple, practical and evidence-based methods. © 2014 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,Shepperd M.,2014,Book Chapter,Software Project Management in a Changing World,10.1007/978-3-642-55035-5_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930745654&doi=10.1007%2f978-3-642-55035-5_3&partnerID=40&md5=5ac856948f544778f39e9b52e0622f0a,"Department of Computer Science, Brunel University, Middlesex, UB83PH, United Kingdom",Springer-Verlag Berlin Heidelberg,English,,9783642550355; 3642550347; 9783642550348
Scopus,The characteristics of information system maintenance: An empirical analysis,"With the advancement in information technology, enterprises have invested vast resources in implementing information systems that undoubtedly come with higher expectations and hopes of improving the operational efficiency of, and bringing competitive advantages to, enterprises. From the viewpoint of quality management, the quality of information systems, or software, used by enterprises also emerges as one important part of various enterprises' quality management considerations. As more and more enterprises have started to implement/adopt business software packages provided by specialised software developers, the focus of the software quality management issue in enterprises has virtually shifted from the topic of software development to software maintenance, with the aim of prolonging the lifespan and enhancing the functionalities of enterprise information systems. Through a questionnaire survey for enterprises and the use of statistical analysis, the findings obtained from this study can be employed to enhance the understanding of the characteristics of enterprise information systems in each phase of software maintenance, and thereby to provide useful suggestions for administrators seeking to improve their software maintenance and also software quality and productivity. © 2013 Taylor & Francis.",information system maintenance; software maintenance; software maintenance life cycle; software quality,"Li S.-H., Yen D.C., Lu W.-H., Chen T.-Y.",2014,Journal,Total Quality Management and Business Excellence,10.1080/14783363.2013.807679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893643895&doi=10.1080%2f14783363.2013.807679&partnerID=40&md5=813f7fd9b74967e9fd9f0953316b3a8f,"Department of Information Management, Tatung University, Taipei, Taiwan; Department of Information Systems and Analytics, Miami University, Oxford, OH, United States; Department of Computer Science and Engineering, Tatung University, Taipei, Taiwan",,English,14783363,
Scopus,Field Studies: A methodology for construction and evaluation of recommendation systems in software engineering,"One way to implement and evaluate the effectiveness of recommendation systems in software engineering is to conduct field studies. Field studies are important as they are the extension of laboratory experiments into real-life situations of organizations and/or society. They bring greater realism to the phenomena that are under study. However, field studies require following a rigorous research approach with many challenges attached, such as difficulties in implementing the research design, achieving sufficient control, replication, validity, and reliability. In practice, another challenge is to find organizations who are prepared to be studied. In this chapter, we provide a step-by-step process for the construction and deployment of recommendation systems in software engineering in the field. We also emphasize three main challenges (organizational, data, design) encountered during field studies, both in general and specifically with respect to software organizations. © Springer-Verlag Berlin Heidelberg 2014.",,"Mısırlı A.T., Bener A., Çağlayan B., Çalıklı G., Turhan B.",2014,Book Chapter,Recommendation Systems in Software Engineering,10.1007/978-3-642-45135-5_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948077984&doi=10.1007%2f978-3-642-45135-5_13&partnerID=40&md5=ecaa37ec786cbc332073a1504aacd15a,"University of Oulu, Oulu, Finland; Ryerson University, Toronto, ON, Canada; Boğaziçi University, Istanbul, Turkey",Springer Berlin Heidelberg,English,,9783642451355; 9783642451348
Scopus,Applications of case-based reasoning in software engineering: A systematic mapping study,"Domain knowledge for various decision-making activities of Software Engineering (SE) is rarely available in a structured or well-formalised form. Owing to lack of the well-informed knowledge, decision making for different kinds of predictions and estimations in SE domain is a challenge. Maintenance and elicitation of domain knowledge is an overwhelming task and causes the knowledge acquisition bottleneck. Most of the artificial intelligence techniques of prediction and estimation do not work in absence of complete and structured knowledge. Case-based reasoning (CBR) is a lazy learning paradigm of artificial intelligence which takes care of this challenge and helps to reduce the knowledge availability bottleneck. This technique exploits the similar experience of past which may be available in unstructured form, and improves its learning curve with passage of time. In literature, CBR has been successfully applied in various areas of SE, but there is lack of single systematic panoramic picture which might have highlighted the potential research questions in this direction. In this study, the author has presented a comprehensive and panoramic systematic mapping study of various CBR applications in SE domain, and identified some promising future research directions. © The Institution of Engineering and Technology 2014.",,Khan M.J.,2014,Journal,IET Software,10.1049/iet-sen.2013.0127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919643219&doi=10.1049%2fiet-sen.2013.0127&partnerID=40&md5=930af8f8b0f3fb3c05a1c66ab0b7d92d,"Department of Computer Science, Namal College, Mianwali, Pakistan",Institution of Engineering and Technology,English,17518806,
Scopus,Conversion from IFPUG FPA to COSMIC: Within-vs without-company equations,"Companies have employed for years 1st generation Functional Size Measurement (FSM) methods, e.g., IF PUG Function Points Analysis (FPA), collecting IF PUG-based historical data useful for benchmarking and estimation purposes. With the advent of 2nd generation FSM methods (e.g., COSMIC) the need for resizing past projects utilizing these new measures arises. The adoption of 2nd generation FSM methods has been limited both by the costs for acquiring new know-how and the need for resizing the historical data. Conversion equations represent a useful mean to facilitate the migration towards 2nd generation FSM methods. Previous works showed a significant correlation between the COSMIC and the FPA sizes. In our study we applied the conversion equations found in those works (i.e., Without-company equations) to resize 25 IF PUG-based projects coming from a single software company. We compared their use with respect to two conversion equations built by using two small subsets (i.e., 5 and 10 projects) from that company data (i.e., Within-company equations). We aimed to verify whether the use of within-company equations built using few projects could provide more accurate conversions than those achieved by applying without-company equations. Our analysis revealed that the within-company equations performed significantly better than the without-company ones. Thus, companies should develop their own equations rather than using without-company conversions. © 2014 IEEE.",Conversion equations; COSMIC; IFPUG,"Ferrucci F., Gravino C., Sarro F.",2014,Conference,"Proceedings - 40th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2014",10.1109/SEAA.2014.76,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916596179&doi=10.1109%2fSEAA.2014.76&partnerID=40&md5=a134a333376782d591abc6c604d74691,"Department of Management and Information Technology, University of Salerno, Italy; CREST, Department of Computer Science, University College London, United Kingdom",Institute of Electrical and Electronics Engineers Inc.,English,,9781479957941
Scopus,Is there a place for qualitative studies when identifying effort predictors? A case in web effort estimation,"Background: Effort estimation is the key for efficiently managing Web projects and achieving their success. In order to correctly estimate, it is necessary to have a broad knowledge of the factors that influence effort estimation in Web projects. Aim: In this research we aim to increase the understanding of Web effort estimation by using a set of factors identified in literature along with the knowledge from experts in effort estimation. Method: We have gathered data from two different sources: (a) our previous work, in which we applied Grounded Theory procedures to identify factors that influence Web effort estimation from the point of view of Web project estimation experts; and (b) a Systematic Literature Review (SLR) extension, in which we identified factors reported in research papers. We have used the qualitative results from these sources to make comparisons and draw conclusions on factors affecting Web effort estimation. Results: We identified a total of 90 factors that influence effort estimation in Web projects. From this set, 30 factors were identified only in the qualitative study with experts in effort estimation, not being present in the SLR extension. Conclusions: By integrating the factors found in both our qualitative study with effort estimation experts and the SLR extension, we managed to create a comprehensive list of factors influencing effort estimation. Also, this set can be a starting point in the proposal of effort estimation models. Finally, the results from our comparison can be considered an indication that it is necessary to increase the employment of qualitative research to capture evidences regarding the current state of practice in Software Engineering. Copyright 2014 ACM.",Qualitative study; Systematic literature review; Web effort estimation; Web effort predictor; Web project management,"Matos O., Conte T., Mendes E.",2014,Conference,ACM International Conference Proceeding Series,10.1145/2601248.2601281,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905463405&doi=10.1145%2f2601248.2601281&partnerID=40&md5=77da7cbd43682e87b64d4e3d097748d9,"Institute of Computing, Federal University of Amazonas, Brazil; School of Computing, Blekinge Institute of Technology, Brazil",Association for Computing Machinery,English,,9781450324762
Scopus,Improving the accuracy of CBSD effort estimation using fuzzy logic,"One of the most important issues in effort estimation is accuracy of size measure methods, because accuracy of estimation depends upon the accurate prediction of size. Prediction of size is depends upon project data,Most of the time in initial stages project data is imperfect and ambiguous this leads to imprecision in its output thereby resulting in erroneous effort estimation using Constructive Cost Model (COCOMO-II) Model. Today's software development is component based and that makes effort estimation process difficult due to the black box nature of component. Also traditional method does not support the component based software development effort estimation. Now the method which support accurate size prediction in component based software development is too much important for accurate effort estimation. Fuzzy logic based cost estimation model address the imperfect and ambiguousness present in Constructive Cost Model (COCOMO-II) models to make reliable and accurate estimation of effort. Component point method supports the accurate size prediction for component based software development which leads to accurate effort estimation in CBSD. The first aim of this paper is to show with comparisons the importance of size measure methods for accurate effort estimation. Paper shows component point is the best method for accurate size prediction in component black box nature. The second aim of this paper is to analyze the use of fuzzy logic in COCOMO-II model to address the imprecision present in its input and suggested four new cost drivers to improve the accuracy of effort estimation. © 2014 IEEE.",Component Based Software Development (CBSD); Component Point (CP); Effort Estimation; Function Point (FP); Fuzzy Logic (FL),"Patil L.V., Shivale N.M., Joshi S.D., Khanna V.",2014,Conference,"Souvenir of the 2014 IEEE International Advance Computing Conference, IACC 2014",10.1109/IAdCC.2014.6779529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899106609&doi=10.1109%2fIAdCC.2014.6779529&partnerID=40&md5=a0477d90219a013dc36cee8c1bbd70d8,"Bharath University, Chennai, India; Dept. of IT., SKNCOE, Pune, India; Dept. of Comp., BVDUCOE, Pune, India; Dept. of I.T., Bharath University, Chennai, India",IEEE Computer Society,English,,
Scopus,Cost estimate of power line projects based on grey relational analysis and neural networks,"To accurately estimate the cost of a power line project, a method based on grey relational analysis (GRA) and neural networks (NN) is presented and studied. Grey relational analysis technologies are used to analyze the features of the transmission line project and ten main features which affect the project cost most are selected. Then, the main features are used as input neural cell of neural networks, and a model of GRA-ANN is built. To verify the method, the cost data of a 110 kV power construction project are used to train and test the model. Results show the model's maximum relative error of static investment is 3.72% and the minimum is 1.85%, and its accuracy is high. The LM-BP algorithm and the traditional BP algorithm are used respectively to train the GRA-ANN network, and results show the error declining rate of LM-BP algorithm is faster and the overall error is lower.",Artificial neural networks; Cost estimate; Grey relational analysis; LM-BP algorithm,"Yang Y., Wang Y., Fan X., Liu C.",2013,Journal,Chongqing Daxue Xuebao/Journal of Chongqing University,10.11835/j.issn.1000-582X.2013.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891638313&doi=10.11835%2fj.issn.1000-582X.2013.11.003&partnerID=40&md5=44f9bbdeb7f2801234c62cca60e564b7,"State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing 400044, China; Electric Power Design Institute of Chongqing, Chongqing 400030, China",,Chinese,1000582X,
Scopus,Software development cost estimation using similarity difference between software attributes,"Although software industry has seen a tremendous growth and expansion since its birth, it is continuously facing problems in its evolution. The major challenge for this industry is to produce quality software which is timely designed and build with proper cost estimates. Thus the techniques for controlling the quality and predicting cost of software are in the center of attention for many software firms. In this paper, we have tried to propose a cost estimation model based on Multiobjective Particle Swarm Optimization (MPSO) to tune the parameters of the famous COstructive COst MOdel (COCOMO). This cost estimation model is integrated with Quality Function Deployment (QFD) methodology to assist decision making in software designing and development processes for improving the quality. This unique combination will help the project managers to efficiently plan the overall software development life cycle of the software product. © 2013 ACM.",analogy and similarity difference; cost estimation; k-nearest neighbor classifier; software attributes; software development cost,"Kashyap D., Misra A.K.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2503859.2503860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883715599&doi=10.1145%2f2503859.2503860&partnerID=40&md5=359f64ad54ff343ce1ee1e0c7779a68a,"Department of Computer Science and Engineering, MNNIT, Allahabad, India",,English,,9781450322997
Scopus,Enhancing the accuracy of case-based estimation model through early prediction of error patterns,"The paper tries to explore the importance of software fault prediction and to minimize them thoroughly with the advanced knowledge of the error-prone modules, so as to enhance the software quality. For estimating a new project effort, case-based reasoning is used to predict software quality of the system by examining a software module and predicting whether it is faulty or non faulty. In this research we have proposed a model with the help of past data which is used for prediction. Two different similarity measures namely, Euclidean and Manhattan are used for retrieving the matching case from the knowledge base. These measures are used to calculate the distance of the new record set or case from each record set stored in the knowledge base. The matching case(s) are those that have the minimum distance from the new record set. This can be extended to variety of system like web based applications, real time system etc. In this paper we have used the terms errors and faults, and no explicit distinction made between errors and faults. In order to obtain results we have used MATLAB 7.10.0 version as an analyzing tool. © 2013 IEEE.",Analogy; CBR; Error; Similarity measure; Software fault prediction,"Rashid E., Patnaik S., Bhattacharya V.",2013,Conference,"Proceedings - 2013 International Symposium on Computational and Business Intelligence, ISCBI 2013",10.1109/ISCBI.2013.49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894463159&doi=10.1109%2fISCBI.2013.49&partnerID=40&md5=cc3aa894c59801dc017899fc46a7775b,"Department of Comp. Sc. and Engg., C.I.T, Tatisilwai, Ranchi, India; Department of Comp. Sc. and Engg., Siksha 'O' Anusandhan University, Bhubaneshwar, Orissa, India; Department of Comp. Sc. and Engg, Birla Institute of Technology, Mesra, Ranchi, India",IEEE Computer Society,English,,9780769550664
Scopus,A cost-benefit model for software quality assurance activities,"Software project managers must schedule quality assurance activities. This is difficult because not enough information is available. Therefore, we developed and validated the quantitative model CoBe. It is based on detailed relationships and is quantified with historical data. It allows to decide which reviews and tests have to be conducted, how they are conducted, and how corrected defects are retested. The results are costs and benefits for quality assurance activities during development and after delivery. Results are given in terms of effort, time, and staff. They are summed up and weighted financially so that an optimal trade-off between costs and benefits can be found. The model is validated with realworld data: Detailed relationships and the complete model are validated with data from 21 student projects. A sensitivity analysis was conducted. CoBe was also validated with data of two industry projects. Overall, the model results are sufficiently accurate. But a calibration is necessary for applying the model in a specific environment. For this, only a few parameters must be set. Their values can be obtained from data that is available frequently from past projects. Copyright © 2012 ACM.",Cost-benefit model; Software quality assurance,Hampp T.,2012,Conference,ACM International Conference Proceeding Series,10.1145/2365324.2365337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867704001&doi=10.1145%2f2365324.2365337&partnerID=40&md5=bdba8a0881a3926c31ec5ba8b4f66231,"Institut für Softwaretechnologie, Universität Stuttgart, Universitätsstraße 38, Stuttgart, 70569, Germany",,English,,9781450312417
Scopus,A framework for the development of measurement and quality assurance in software-based medical rehabilitation systems,"The field of computer and robot-assisted rehabilitation system is rooted in the principle that software must be largely errorless, userfriendly, robust, accurate with respect to data, respond in a timely manner, and yet inexpensive, which lead to enhanced patient outcomes. In this digitized age, computerized and robotic rehabilitation systems act as a vital support for disabled individuals. Till today, different types of software for medical rehabilitation systems have been developed and applied to the rehabilitation process successfully, but improvement in quality and measurement of rehabilitation software is continuously in progress. Some ways of the software production have been established but further measurement process has always been a necessity. This paper presents the framework and recommends establishment of software quality measurement in computer- and robot-assisted automated medical rehabilitation system. Also, a brief discussion of rehabilitation technique and their software quality is also included. Lastly, we include its importance in medical technology and quality. To satisfy the end user, vendor satisfaction, software measurement and quality assurance are important components in software-based medical rehabilitation systemsy. © 2012 The Authors.",Measurement; Medical; Quality; Rehabilitation; Robot; Software,"Ahamed N.U., Sundaraj K., Ahmad R.B., Rahman M., Ali A.",2012,Conference,Procedia Engineering,10.1016/j.proeng.2012.07.142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901009661&doi=10.1016%2fj.proeng.2012.07.142&partnerID=40&md5=3a9c1fd720cc9c28f2f661100a5ed08d,"School of Mechatronic Engineering, Universiti Malaysia Perlis, 01000 Kangar, Perlis, Malaysia; School of Computer and Communication Engineering, Universiti Malaysia Perlis, 01000 Kangar, Perlis, Malaysia; College of Computer Science and Information System, Najran University, 61441 Najran, Saudi Arabia",Elsevier Ltd,English,18777058,
Scopus,Software effort estimation using NBC and SWR: A comparison based on ISBSG projects,"There are many quantitative estimation methods, e.g. linear regression, neural networks, regression trees. Compared to traditional methods, Bayesian networks are being increasingly used in software engineering because their use opens many possibilities. A main feature of Bayesian networks is their capability to combine data and expert knowledge. This paper seeks to reinforce the hypothesis that Bayesian networks are a competitive method for estimating software effort in terms of prediction accuracy. For this purpose a Naive Bayes Classifier (NBC) and a forward Stepwise Regression (SWR) models have been developed from a subset of the ISBSG dataset. Under homogeneous conditions we found similar results provided that the discretization of the continuous variables is thin enough. © 2012 IEEE.",Bayesian networks; Discretization; Effort estimation; Forward stepwise regression; ISBSG; Naive Bayes Classifier; Software projects,"Fernández-Diego M., Elmouaden S., Torralba-Martínez J.-M.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900862610&doi=10.1109%2fIWSM-MENSURA.2012.28&partnerID=40&md5=7b61f6c54b61570417d271ba9bbdb81d,"Department of Business Administration, Universitat Politècnica de València, Valencia, Spain",IEEE Computer Society,English,,
Scopus,CPN-a hybrid model for software cost estimation,"One of the challenges faced by the managers in the software industry today is the ability to accurately define the requirements of the software projects early in the software development phase. The cost-benefit analysis forms the basis of the planning and decision making throughout the software development lifecycle. As such there is a need for efficient software cost estimation techniques for making any endeavor viable. Software cost estimation is the process of prognosticating the amount of effort required to build a software project. In this paper we have proposed a Particle Swarm Optimization (PSO) technique which operates on data sets clustered using the K-means clustering algorithm. PSO is employed to generate parameters of the COCOMO model for each cluster of data values. The clusters and effort parameters are then trained to a Neural Network by using Back propagation technique, for classification of data. Here we have tested the model on the COCOMO 81 dataset and also compared the obtained values with standard COCOMO model. By making use of the experience from Neural Networks and the efficient tuning of parameters by PSO operating on clusters, the proposed model is able to generate better results and it can be applied efficiently to larger data sets. © 2011 IEEE.",Back propagation algorithm; COnstructive COst MOdel(COCOMO); CPN:Clustering-PSO-Neural Networks; K-Means algorithm; Particle Swarm Optimization (PSO); Software Cost estimation,"Hari C.V.M.K., Sethi T.S., Kaushal B.S.S., Sharma A.",2011,Conference,"2011 IEEE Recent Advances in Intelligent Computational Systems, RAICS 2011",10.1109/RAICS.2011.6069439,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355163536&doi=10.1109%2fRAICS.2011.6069439&partnerID=40&md5=83f7fbcbdc14e01c43edcab98153f41a,"Department of Information Technology, GITAM Institute of Technology, GITAM University, Visakhapatnam, India; Department of Computer Science Engineering, GITAM Institute of Technology, GITAM University, Visakhapatnam, India",,English,,9781424494774
Scopus,Exploring the effort of general software project activities with data mining,"Software project effort estimation requires high accuracy, but accurate estimations are difficult to achieve. Increasingly, data mining is used to improve an organization's software process quality, e.g. the accuracy of effort estimations. Data is collected from projects, and data miners are used to discover beneficial knowledge. This paper reports a data mining experiment in which we examined 32 software projects to improve effort estimation. We examined three major categories of software project activities, and focused on the activities of the category which has got the least attention in research so far, the non-construction activities. The analysis is based on real software project data supplied by a large European software company. In our data mining experiment, we applied a range of machine learners. We found that the estimated total software project effort is a predictor in modeling and predicting the actual quality management effort of the project. © 2011 World Scientific Publishing Company.",data mining; Effort estimation; project behavior,"Haapio T., Menzies T.",2011,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194011005438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053503288&doi=10.1142%2fS0218194011005438&partnerID=40&md5=f9a4fca7f9e1c91884c9b4aee7408074,"Lane Department of Computer Science, West Virginia University, Morgantown, WV 26506-610, United States",,English,02181940,
Scopus,Human judgement and software metrics: Vision for the future,"Background: There has been much research into building formal (metrics-based) prediction systems with the aim of improving resource estimation and planning of software projects. However the 'objectivity' of such systems is illusory in the sense that many inputs need themselves to be estimated by the software engineer. Method: We review the uptake of past software project prediction research and identify relevant cognitive psychology research on expert behaviour. In particular we explore potential applications of recent metacognition research. Results: We find the human aspect is largely ignored, despite the availability of many important results from cognitive psychology. Conclusions: In order to increase the actual use of our metrics research e.g. effort prediction systems we need to have a more integrated view of how such research might be used and who might be using it. This leads to our belief that future research must be more holistic and inter-disciplinary. © 2011 ACM.",accuracy; effort prediction; metacognition; software project management,"Mair C., Shepperd M.",2011,Conference,Proceedings - International Conference on Software Engineering,10.1145/1985374.1985393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959848312&doi=10.1145%2f1985374.1985393&partnerID=40&md5=f7f100475a804949526d78e5a5e1ac36,"Dept. of Psychology, Southampton Solent Unversity, Southampton, SO14 0YN, United Kingdom; Dept. of IS and Computing, Brunel University, Uxbridge, UB8 3PH, United Kingdom",,English,02705257,9781450305938
Scopus,A revised web objects method to estimate web application development effort,"We present a study of the effectiveness of estimating web application development effort using Function Points and Web Objects methods, and a method we propose - the Revised Web Objects (RWO). RWO is an upgrading of WO method, aimed to account for new web development styles and technologies. It also introduces an up-front classification of web applications according to their size, scope and technology, to further refine their effort estimation. These methods were applied to a data-set of 24 projects obtained by Datasiel spa, a mid-sized Italian company, focused on web application projects, showing that RWO performs statistically better than WO, and roughly in the same way as FP. © 2011 ACM.",function points; software metrics; web development; web objects,"Folgieri R., Barabino G., Concas G., Corona E., De Lorenzi R., Marchesi M., Segni A.",2011,Conference,Proceedings - International Conference on Software Engineering,10.1145/1985374.1985388,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959842827&doi=10.1145%2f1985374.1985388&partnerID=40&md5=89a5818caeef83e2759a07277ba0dbf9,"DEAS - Department of Economics, Business and Statistics, University of Milan, Milan, Italy; DIBE - Department of Biophysical and Electronic Engineering, University of Genova, Genova, Italy; DIEE - Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy; Datasiel Spa, Genova, Italy",,English,02705257,9781450305938
Scopus,Uncertain context factors in ERP project estimation are an asset: Insights from a semi-replication case study in a financial services firm,"This paper reports on the findings of a case study in a company in the financial services sector in which we replicated the use of a previously published approach to systematically balance the contextual uncertainties in the estimation of Enterprise Resource Planning (ERP) projects. The approach is based on using three techniques, a parametric model, namely COCOMO II, a portfolio management model, and Monte Carlo simulations. We investigated (i) whether the adjustment of uncertain cost drivers in the COCOMO II model increases the chance of project success in a portfolio of ERP projects, (ii) which cost drivers of the COCOMO II model can be adjusted in a way that maximized the chance of portfolio success under time constraints, and (iii) which cost drivers of the COCOMO II model can be adjusted in a way that maximized the chance of portfolio success under effort constraints. We found that 11 COCOMO II cost drivers can be changed so that the change impacts the project outcomes under both time and effort constraints. This result is different from the result in the first case study in which 13 such factors were found. © 2011 World Scientific Publishing Company.",empirical evaluation; enterprise systems; project effort estimation; Project portfolio management; requirements-based estimation,Daneva M.,2011,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194011005335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960760498&doi=10.1142%2fS0218194011005335&partnerID=40&md5=a26be291f8264d34950c67cfb202215b,"University of Twente, Computer Science Department, Drienerlolaan 5, 7500 Enschede, Netherlands",,English,02181940,
Scopus,US DoD application domain empirical software cost analysis,"General software cost parameters such as size, effort distribution, and productivity are necessarily imprecise due to variations by domain. To improve this situation, empirical software cost analysis using the primary US DoD cost database has been segmented by domain. This analysis supports a software cost estimation metrics manual for improvements in acquisition policies, procedures and tools. We have addressed the challenges of consistent data definitions and taxonomies across diverse stakeholder communities, data integrity, data formats, and others. We highlight example analysis results from an application domain demonstrating cost estimating relationships, benchmarks on reuse parameters and effort distributions for estimators to use. © 2011 IEEE.",Department of defense; Software cost estimation; Software cost models; Software metrics; Software productivity,"Madachy R., Boehm B., Clark B., Tan T., Rosa W.",2011,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/esem.2011.56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858732077&doi=10.1109%2fesem.2011.56&partnerID=40&md5=8a9116a39fc3759fafd9d02f75cc49e1,"Department of Systems Engineering, Naval Postgraduate School, Monterey, CA, United States; USC Center for Systems and Software Engineering, University of Southern California, Los Angeles, CA, United States; Information Technology Division, Air Force Cost Analysis Agency, Arlington, VA, United States",IEEE Computer Society,English,19493770,
Scopus,SMARtS: Software metric analyzer for relational database systems,"Software complexity has a strong impact on development effort and maintainability. Measurement of complexity can help managers in project planning and cost estimation. A number of metrics exist to measure the complexity for procedural and object oriented applications. In this paper we present a model to compute the complexity for small scale relational database applications. Our model is based on different relational database objects. Complexity of each database object is computed by assigning suitable weights to its complexity determining factors. To evaluate our model, we have applied correlation analysis on computed complexity and actual effort. The results indicate a strong correlation between the effort and complexity computed by our model. ©2010 IEEE.","Categorization ranges; Complexity determining factors; Complexity weights, database application complexity estimation","Jamil B., Batool A.",2010,Conference,"2010 International Conference on Information and Emerging Technologies, ICIET 2010",10.1109/ICIET.2010.5625716,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650571252&doi=10.1109%2fICIET.2010.5625716&partnerID=40&md5=4a62e20b7e382e5bea0215a352494696,"Department of Computer Science, University of Sargodha, Sargodha, Pakistan",,English,,9781424480012
Scopus,Intelligently predict project effort by reduced models: Based on multiple regressions and genetic algorithms with neural networks,"Estimating the amount of effort required for developing a software system is one of the most important project management concerns. This study successfully produces an optimal reduced linear model for software cost estimation by employing a series of methods of multiple regressions to identify the most significant explanatory variables of the fifteen COCOMO cost drivers. The results yielded by the linear models are then compared with their counterparts obtained from the simulation using genetic algorithms with feed-forward neural networks (NN) with back-propagation learning algorithms. The performance of the resulted optimal reduced linear model is very close to that of the full regression and neural network models, and is also comparable to that of the COCOMO '81 intermediate in terms of MMRE and Pred (25). As both linear and nonlinear reduction methods described in this paper are applied and the most significant nine explanatory variables selected among the fifteen COCOMO cost drivers in these reduced models are the identical and their effort estimation accuracy is highly acceptable, the reduced models can be concluded as accurate and robust. © 2010 IEEE.",Effort estimation; Genetic algorithms; Multiple regressions; Neural networks; Project management,Li Z.,2010,Conference,"Proceedings of the International Conference on E-Business and E-Government, ICEE 2010",10.1109/ICEE.2010.390,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649661116&doi=10.1109%2fICEE.2010.390&partnerID=40&md5=21b761bfdbb21b223d0430e23ed87cf2,"Business School, Central South University, Changsha, 410083, China",,English,,9780769539973
Scopus,An improved method to simplify software metric models constructed with incomplete data samples,"Software metric models are useful in predicting the target software metric(s) for any future software project based on the project's predictor metric(s). Obviously, the construction of such a model makes use of a data sample of such metrics from analogous past projects. However, incomplete data often appear in such data samples. Worse still, the necessity to include a particular continuous predictor metric or a particular category for a certain categorical predictor metric is most likely based on an experience-related intuition that the continuous predictor metric or the category matters to the target metric. However, in the presence of incomplete data, this intuition is traditionally not verifiable ""retrospectively"" after the model is constructed, leading to redundant continuous predictor metric(s) and/or excessive categorization for categorical predictor metrics. As an improvement of the author's previous work to solve all these problems, this paper proposes a methodology incorporating the k-nearest neighbors (k-NN) multiple imputation method, kernel smoothing, Monte Carlo simulation, and stepwise regression. This paper documents this methodology and one experiment on it. ©2010 IEEE.",Missing data; Model simplification; Multiple imputation; Software metrics; Stepwise regression,"Xie T., Wong W.E.",2010,Conference,"Proceedings - 2010 7th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2010",10.1109/FSKD.2010.5569384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649291527&doi=10.1109%2fFSKD.2010.5569384&partnerID=40&md5=eb5290da6e0e8ba9ee7b0747a3c1958b,"College of Applied Sciences, Beijing University of Technology, Beijing 100124, China; Department of Computer Science, University of Texas at Dallas, Richardson, TX 75083, United States",,English,,9781424459346
Scopus,Experimental study using functional size measurement in building estimation models for software project size,"This paper reports on an experiment that investigates the predictability of software project size from software product size. The predictability research problem is analyzed at the stage of early requirements by accounting the size of functional requirements as well as the size of non-functional requirements. The experiment was carried out with 55 graduate students in Computer Science from Concordia University in Canada. In the experiment, a functional size measure and a project size measure were used in building estimation models for sets of web application development projects. The results show that project size is predictable from product size. Further replications of the experiment are, however, planed to obtain more results to confirm or disconfirm our claim. © 2010 IEEE.",Empirical software engineering; Experiment; Functional size measurement; Software project estimation,"Condori-Fernandez N., Daneva M., Buglione L., Ormanjieva O.",2010,Conference,"8th ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2010",10.1109/SERA.2010.42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955334325&doi=10.1109%2fSERA.2010.42&partnerID=40&md5=519094118c2d751c2e531c533dceb64f,"Universitad Politecnica de Valencia, Valencia, Spain; University of Twente, Enschede, Netherlands; ETS/Engineering.IT, Rome, Italy; Concordia University, Montreal, Canada",,English,,9780769540757
Scopus,Multi-instance learning for software quality estimation in object-oriented systems: A case study,"We investigate a problem of object-oriented (OO) software quality estimation from a multi-instance (MI) perspective. In detail, each set of classes that have an inheritance relation, named 'class hierarchy', is regarded as a bag, while each class in the set is regarded as an instance. The learning task in this study is to estimate the label of unseen bags, i.e., the fault-proneness of untested class hierarchies. A fault-prone class hierarchy contains at least one fault-prone (negative) class, while a non-fault-prone (positive) one has no negative class. Based on the modification records (MRs) of the previous project releases and OO software metrics, the fault-proneness of an untested class hierarchy can be predicted. Several selected MI learning algorithms were evaluated on five datasets collected from an industrial software project. Among the MI learning algorithms investigated in the experiments, the kernel method using a dedicated MI-kernel was better than the others in accurately and correctly predicting the fault-proneness of the class hierarchies. In addition, when compared to a supervised support vector machine (SVM) algorithm, the MI-kernel method still had a competitive performance with much less cost. © Zhejiang University and Springer-Verlag Berlin Heidelberg 2010.",Kernel methods; Multi-instance (MI) learning; Object-oriented (OO) software; Software quality estimation,"Huang P., Zhu J.",2010,Journal,Journal of Zhejiang University: Science C,10.1631/jzus.C0910084,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951794995&doi=10.1631%2fjzus.C0910084&partnerID=40&md5=221befacf183d5c82f32646e8068429b,"Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai 200240, China",,English,18691951,
Scopus,Wrapper-based feature ranking for software engineering metrics,"The application of feature ranking to software engineering datasets is rare at best. In this study, we consider wrapper-based feature ranking where nine performance metrics aided by a particular learner are evaluated. We consider five learners and take two different approaches, each in conjunction with one of two different methodologies: 3-fold Cross-Validation (CV) and 3-fold Cross-Validation Risk Impact (CV-R). The classifiers are Naïve Bayes (NB), Multi Layer Perceptron (MLP), k-Nearest Neighbors (kNN), Support Vector Machines (SVM), and Logistic Regression (LR). The performance metrics used as ranking techniques are Overall Accuracy (OA), F-Measure (FM), Geometric Mean (GM), Arithmetic Mean (AM), Area under ROC (AUC), Area under PRC (PRC), Best F-Measure (BFM), Best Geometric Mean (BGM), and Best ArithmeticMean (BAM). To evaluate the classifier performance after feature selection has been applied, we use AUC as the performance evaluator. This paper represents a preliminary report on our proposed wrapper-based feature ranking approach to software defect prediction problems. © 2009 IEEE.",Feature selection; Performance metrics; Software engineering; Wrapper-based feature ranking,"Altidor W., Khoshgoftaar T.M., Napolitano A.",2009,Conference,"8th International Conference on Machine Learning and Applications, ICMLA 2009",10.1109/ICMLA.2009.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950856684&doi=10.1109%2fICMLA.2009.17&partnerID=40&md5=6bb19e433564a586ccce0b08fd33d689,"Florida Atlantic University, Boca Raton, FL 33431, United States",,English,,9780769539263
Scopus,Estimating web application development effort using COSMIC-FFP method,"In the last few years, some researchers have proposed the use of COSMIC-FFP for effort prediction of Web applications. It is widely recognized, that a measure can be accepted only if its usefulness has been proved through some empirical studies. In this paper, we reported on an empirical study carried out using an industrial dataset and compared the results obtained with a previous analysis based on Web applications developed by academic students. We used an adaptation of COSMIC-FFP specifically conceived for Web applications as size measure and the Ordinary Least Square Regression as modelling technique. This analysis had a twofold goal: to verify whether COSMIC-FFP can provide good estimations and to analyse possible differences/similarities in the empirical results obtained with the two different datasets.",Effort estimation; Empirical studies; Size measures; Web applications; Web engineering,"Di Martino S., Gravino C.",2009,Journal,International Journal of Computers and Applications,10.2316/Journal.202.2009.3.202-2962,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950834644&doi=10.2316%2fJournal.202.2009.3.202-2962&partnerID=40&md5=6d33835cfb68ec8d4f665231bd1f8f51,"Dipartimento di Scienze Fisiche, Sezione Informatica, University of Naples Federico II, Via Cinthia, 80128 Napoli, Italy; Dipartimento di Matematica e Informatica, University of Salerno, Via Ponte Don Melillo, 84084 Fisciano (SA), Italy",,English,1206212X,
Scopus,Bootstrap prediction intervals for a semi-parametric software cost estimation model,"The vital task of accurate Software Cost Estimation predictions remains a challenging problem attracting the interest of researchers and practitioners. Although Least Squares (LS) regression and Estimation by Analogy (EbA) are two of the most widely applied methods, there seems to be a discrepancy in choosing the best prediction technique. In this paper, we further extend our previous work on the utilization of a semi-parametric model, called LSEbA that achieves to combine the abovementioned methods. More precisely, we present a method of constructing prediction intervals by the bootstrap resampling technique. The prediction intervals obtained for LSEbA are compared with those of LS and EbA separately, with the aid of a new methodology that takes into account not only the ability of comparative intervals to capture the actual cost, but also their similarity and their width. © 2009 IEEE.",Bootstrap; Estimation by analogy; Prediction interval; Regression; Semiparametric model,"Mittas N., Angelis L.",2009,Conference,Conference Proceedings of the EUROMICRO,10.1109/SEAA.2009.49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549217855&doi=10.1109%2fSEAA.2009.49&partnerID=40&md5=cc1a8bd6f4336ec4c04f0028bc8a1de8,"Department of Informatics, Aristotle University of Thessaloniki, 54124, Thessaloniki, Greece",,English,10896503,9780769537849
Scopus,Accuracy and efficiency comparisons of single- and multi-cycled software classification models,"Software classification models have been regarded as an essential support tool in performing measurement and analysis processes. Most of the established models are single-cycled in the model usage stage, and thus require the measurement data of all the model's variables to be simultaneously collected and utilized for classifying an unseen case within only a single decision cycle. Conversely, the multi-cycled model allows the measurement data of all the model's variables to be gradually collected and utilized for such a classification within more than one decision cycle, and thus intuitively seems to have better classification efficiency but poorer classification accuracy. Software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs. However, this important topic is not adequately explored in software measurement and analysis literature. By using an industrial software measurement dataset of NASA KC2, this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis (DA)- and logistic regression (LR)-based single-cycled models and the decision tree (DT)-based (C4.5 and ECHAID algorithms) multi-cycled models. The experimental results suggest that the re-appraisal cost of the Type I MR, the software failure cost of Type II MR and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model. © 2008 Elsevier B.V. All rights reserved.",Classification accuracy and efficiency; Multi-cycle; Single-cycle; Software classification model; Software measurement and analysis,"Chen L.-W., Huang S.-J.",2009,Journal,Information and Software Technology,10.1016/j.infsof.2008.03.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649117572&doi=10.1016%2fj.infsof.2008.03.004&partnerID=40&md5=f283e941887cde86d5d256613cd4ea33,"Department of Information Management, National Taiwan University of Science and Technology, 43, Sec. 4, Keelung Road, Taipei, 106, Taiwan",,English,09505849,
Scopus,Complementing approaches in ERP effort estimation practice: An industrial study,"Projects implementing enterprise resource planning (ERP) solutions are characterized by specific context factors such as high level of reuse, scope of the ERP modules, interdependent functionality, and use of vendor-specific standard implementation method, all of which impose risks known to cause various degrees of project failure. We suggest a remedy to this issue by tackling it from a portfolio management perspective. Our solution rests on earlier work by other authors and is a combination of a classic cost estimation method (COCOMO II), a Monte Carlo simulation process, and a portfolio management model. We report on the results of a case study done in a company site in the telecommunication sector. Copyright 2008 ACM.",Effort estimation; Enterprise resource planning; Portfolio management,Daneva M.,2008,Conference,Proceedings - International Conference on Software Engineering,10.1145/1370788.1370808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049163741&doi=10.1145%2f1370788.1370808&partnerID=40&md5=961feecd01d6a30e7f34f38dd842d970,"University of Twente, Drienerlolaan 5, Enschede 7500, Netherlands",,English,02705257,9781605580364
Scopus,Early estimate the size of test suites from use cases,"Software quality becomes an increasingly important factor in software marketing. It is well known that software testing is an important activity to ensure software quality. Despite the important role that software testing plays, little is known about the prediction of test suites size. Estimation of testing size is a crucial activity among the tasks of testing management. Work plan and subsequent estimations of the effort required are made based on the estimation of test suites size. The earlier test suites size estimation we do, the more benefit we will get in the process of testing. This paper presents an experience-based approach for the test suites size estimation. The main findings are: (1) Model of use case verification points. (2) Linear relationship between use case verification points and test case number. The test case number prediction model deduced from the data of real projects in a financial software company.",,"Yi Q., Bo Z., Xiaochun Z.",2008,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650519128&partnerID=40&md5=6af7707e603c67df5ece8285a9c155c5,"College of Computer Science, Zhejiang University, Hangzhou, China",,English,15301362,9780769534466
Scopus,Jidoka in software development,"Lean management is based on two concepts: the elimination of ""Muda"", the waste, from the production process, and ""Jidoka"", the continuous quality inspection inside the production process. In software production, the elimination of Muda received significant attention, while Jidoka has not yet been fully exploited. In this work we want to propose a holistic approach to insert Jidoka in software production.We depict the architecture of a tool to support Jidoka and describe the components that are part of it.",Jidoka; Quality assurance,"Danovaro E., Janes A., Succi G.",2008,Conference,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",10.1145/1449814.1449874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63349083946&doi=10.1145%2f1449814.1449874&partnerID=40&md5=356b7764439d3faee6ff2b3eeac76fdb,"Center for Applied Software Engineering, Free University of Bozen, Italy",,English,,
Scopus,At what level of granularity should we be componentizing for software reliability?,"In Component-Based Software Systems (CBSSs), software designers need to decide about decomposition level (level of granularity) which involves component sizes and the number of components. In these systems, decomposition level is important due to its major impacts on reliability. However, the basis to choose the decomposition level of a CBSS has not been addressed adequately in the existing research. On the other hand, software system components may vary with respect to their criticalities to different failures. The knowledge about component failure criticalities are currently not incorporated in the architectural design decisions of these systems. As a result, these systems consider different failures equally and disregard the various severities of different failures. In this paper, we study the level of decomposition of CBSSs with respect to its impact on their reliabilities based on various component failure criticalities. We discuss the level of decomposition impacts on CBSS architectures with respect to the architectural attributes and component failure criticalities. We derive the reliability of these systems and show the level of decomposition impacts on these system reliabilities. © 2008 IEEE.",Architectural design decisions; Component reliability; Componentization; Failure criticalities; Level of decomposition,"Mohamed A., Zulkernine M.",2008,Conference,Proceedings of IEEE International Symposium on High Assurance Systems Engineering,10.1109/HASE.2008.14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449086167&doi=10.1109%2fHASE.2008.14&partnerID=40&md5=8fed66f60f97e5b5bdcf80f9fc8c70c6,"School of Computing, Queen's University, Kingston, ON K7L 3N6, Canada",,English,15302059,9780769534824
Scopus,An integrated approach for identifying relevant factors influencing software development productivity,"Managing software development productivity and effort are key issues in software organizations. Identifying the most relevant factors influencing project performance is essential for implementing business strategies by selecting and adjusting proper improvement activities. There is, however, a large number of potential influencing factors. This paper proposes a novel approach for identifying the most relevant factors influencing software development productivity. The method elicits relevant factors by integrating data analysis and expert judgment approaches by means of a multi-criteria decision support technique. Empirical evaluation of the method in an industrial context has indicated that it delivers a different set of factors compared to individual data- and expert-based factor selection methods. Moreover, application of the integrated method significantly improves the performance of effort estimation in terms of accuracy and precision. Finally, the study did not replicate the observation of similar investigations regarding improved estimation performance on the factor sets reduced by a data-based selection method. © 2008 Springer-Verlag Berlin Heidelberg.",Development productivity; Effort estimation; Factor selection; Influencing factors; Software,"Trendowicz A., Ochs M., Wickenkamp A., Münch J., Ishigai Y., Kawaguchi T.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85279-7_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949092695&doi=10.1007%2f978-3-540-85279-7_18&partnerID=40&md5=883c9a0e7a1417cc9432c0f2b6512189,"Fraunhofer IESE, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; IPA-SEC, 2-28-8 Honkomagome, Bunkyo-Ku, Tokyo 113-6591, Japan; Research Center for Information Technology, Mitsubishi Research Institute, Inc., 3-6, Otemachi 2-Chome, Chiyoda-Ku, Tokyo 100-8141, Japan; Toshiba Information Systems (Japan) Corporation, 7-1 Nissin-Cho, Kawasaki-City 210-8540, Japan",,English,03029743,3540852786; 9783540852780
Scopus,Functional equivalence between radial basis function neural networks and Fuzzy analogy in software cost estimation,"We show in this paper the equivalence between the radial basis function networks and Fuzzy analogy in the field of software cost estimation. We prove that under weak conditions, the three layers of RBFN are functionally equivalent to the three steps of fuzzy analogy. This functional equivalence implies that advances in each literature, such new learning rules or new similarity measures, can be applied to both models directly. Furthermore, this equivalence can help us to provide a natural interpretation of cost estimation models based on RBFN.",Equivalence; Fuzzy analogy; Radial basis function networks; Software cost estimation,"Idri A., Zakrani A., Abran A.",2008,Conference,"2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications, ICTTA",10.1109/ICTTA.2008.4530015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49149083210&doi=10.1109%2fICTTA.2008.4530015&partnerID=40&md5=662843c9cdc63dbc94ac6b4d87307bb2,"Department of Software Engineering, ENSIAS, Mohamed V University, Rabat, Morocco; École de Technologie Supérieure, 1180 Notre-Dame Ouest, Montreal, H3C 1K3, Canada",,English,,1424417511; 9781424417513
Scopus,A hands-on approach for teaching systematic review,"An essential part of a software engineering education is technology innovation. Indeed software engineers, as future practitioners, must be able to identify the most appropriate technologies to adopt in projects. As so, it is important to develop the skills that will allow them to evaluate and make decisions on tools, technologies, techniques and methods according to the available empirical evidence reported in literature. In this sense, a rigorous manner for analyzing and critically addressing literature is Systematic Review. It requires formalizing an answerable research question according to the problem or issues to face; search the literature for available evidence according to a systematic protocol and retrieve data from the identified sources; analyze the collected evidence and use it to support decision making and conclusions. In this paper we report on how Systematic Review has been integrated in the ""Empirical Software Engineering Methods"" course that is taught at the Department of Informatics at the University of Bari, and how students have been introduced to this type of literature review through a hands-on approach. As far as we know, it is the first attempt of including a complex topic like systematic review in a university course on empirical software engineering. We have no empirical evidence on the effectiveness of the approach adopted, other than practice-based experience that we have acquired. Nonetheless, we have collected qualitative data through a questionnaire submitted to the students of the course. Their positive answers and impressions are a first informal confirmation of the successful application of our strategy. © 2008 Springer-Verlag Berlin Heidelberg.",Empirical Software Engineering; Evidence Based Software Engineering; Statistical Process Control; Systematic Review,"Baldassarre M.T., Boffoli N., Caivano D., Visaggio G.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349133033&doi=10.1007%2f978-3-540-69566-0_33&partnerID=40&md5=6d2e51b3a96bd3ad98926a8109c9fc68,"Department of Informatics, University of Bari, RCOST Bari",,English,03029743,3540695648; 9783540695646
Scopus,Predicting software metrics at design time,"How do problem domains impact software features? We mine software code bases to relate problem domains (characterized by imports) to code features such as complexity, size, or quality. The resulting predictors take the specific imports of a component and predict its size, complexity, and quality metrics. In an experiment involving 89 plug-ins of the ECLIPSE project, we found good prediction accuracy for most metrics. Since the predictors rely only on import relationships, and since these are available at design time, our approach allows for early estimation of crucial software metrics. © 2008 Springer-Verlag Berlin Heidelberg.",,"Holz W., Premraj R., Zimmermann T., Zeller A.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249112918&doi=10.1007%2f978-3-540-69566-0_6&partnerID=40&md5=51d0e15d0facc532d0f9ebf068c4b10e,"Saarland University, Germany; University of Calgary, Canada",,English,03029743,3540695648; 9783540695646
Scopus,Agent-based and discrete-event modeling: A quantitative comparison,"Although agent-based modeling is widely regarded as a concept well-suited to the simulation of logistics systems, there has not been any quantitative comparison with discreteevent modeling or other more established concepts. As a timely check on the rising popularity of this paradigm for modeling complex systems, this paper describes a study comparing an agent-based model with a traditional discrete-event model, each of which has been implemented to a common specification. The model scenario is based on an existing application - the global repair operation of a fleet of civil aircraft high-bypass turbofan engines. Initial results, including metrics for runtime, and software size, structure, and complexity highlight the basic similarities and differences between the models. Future estimation of model maintainability will further quantify their benefits and drawbacks. It is hoped that characterizing the paradigms more fully will enable a modeler to match various parts of a model more appropriately to its problem domain.",,"Yu T.-T., Scanlan J.P., Wills G.B.",2007,Conference,"Collection of Technical Papers - 7th AIAA Aviation Technology, Integration, and Operations Conference",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249043249&partnerID=40&md5=c43f6866774e8d93c475c7433b704077,"University of Southampton, Southampton, Hampshire, SO17 1BJ, United Kingdom; Computational Engineering and Design Group, School of Engineering Sciences, University of Southampton, University Road, United Kingdom; Department of Design, School of Engineering Sciences, University of Southampton, University Road, United Kingdom; Learning Societies Lab., School of Electronics and Computer Science, University of Southampton, University Road, United Kingdom",,English,,1563479087; 9781563479083
Scopus,Dynamic measurement of polymorphism,"Measuring ""reuse"" and ""reusability"" is difficult because there are so many different facets to these concepts. Before we can effectively measure reuse and reusability, we must first be able to effectively measure these different facets. One such facet is the programming language constructs that are available. For example whether or not a language supports polymorphism is believed to affect how reusable a developer can make a code artifact. Effectively measuring polymorphism is a challenge because its behaviour is only observable at run-time. In this paper, we present a metric for polymorphism based on the dynamic behaviour of the code. We evaluate the usefulness of the metric through two case studies. Copyright © 2007, Australian Computer Society, Inc.",Dynamic profiling; Inheritance; Polymorphism; Software metrics,"Choi K.H.T., Tempero E.",2007,Conference,Conferences in Research and Practice in Information Technology Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455243927&partnerID=40&md5=361ee11cbf9100515fe9031c1091829d,"Department of Computer Science, University of Auckland, Auckland, New Zealand",,English,14451336,1920682430; 9781920682439
Scopus,Software cost estimation using artificial neural networks with inputs selection,"Software development is an intractable, multifaceted process encountering deep, inherent difficulties. Especially when trying to produce accurate and reliable software cost estimates, these difficulties are amplified due to the high level of complexity and uniqueness of the software process. This paper addresses the issue of estimating the cost of software development by identifying the need for countable entities that affect software cost and using them with artificial neural networks to establish a reliable estimation method. Input Sensitivity Analysis (ISA) is performed on predictive models of the Desharnais and ISBSG datasets aiming at identifying any correlation present between important cost parameters at the input level and development effort (output). The degree to which the input parameters define the evolution of effort is then investigated and the selected attributes are employed to establish accurate prediction of software cost in the early phases of the software development life-cycle.",Artificial neural networks; Input sensitivity analysis; Software cost estimation,"Papatheocharous E., Andreou A.S.",2007,Conference,"ICEIS 2007 - 9th International Conference on Enterprise Information Systems, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649104534&partnerID=40&md5=5cb6508051702c0f8c929f9e8ce5d95d,"University of Cyprus, Dept. of Computer Science, 75 Kallipoleos str, CY1678 Nicosia, Cyprus",,English,,
Scopus,Cognitive limits of software cost estimation,"This paper explores the cognitive limits of estimation in the context of software cost estimation. Two heuristics, representativeness and anchoring, motivate two experiments involving psychology students, engineering students, and engineering practitioners. The first experiment, designed to determine if there is a difference in estimating ability in everyday quantities, demonstrates that the three populations estimate with relatively equal accuracy. The results shed light on the distribution of estimates and the process of subjective judgment. The second experiment, designed to explore abilities for estimating the cost of software-intensive systems given incomplete information, shows that predictions by engineering students and practitioners are within 3-12% of each other. The value of this work is in helping better understand how software engineers make decisions based on limited information. The manifestation of the two heuristics is discussed together with the implications for the development of software cost estimation models in light of the findings from the two experiments.",,Valerdi R.,2007,Conference,"Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007",10.1109/ESEM.2007.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949099831&doi=10.1109%2fESEM.2007.30&partnerID=40&md5=157e1f55b0e346323cb4312d635c54a0,Massachusetts Institute of Technology,,English,,0769528864; 9780769528861
Scopus,Fuzzy logic based group maturity rating for software performance prediction,"Driven by market requirements, software services organizations have adopted various software engineering process models (such as capability maturity model (CMM), capability maturity model integration (CMMI), ISO 9001:2000, etc.) and practice of the project management concepts defined in the project management body of knowledge. While this has definitely helped organizations to bring some methods into the software development madness, there always exists a demand for comparing various groups within the organization in terms of the practice of these defined process models. Even though there exist many metrics for comparison, considering the variety of projects in terms of technology, life cycle, etc., finding a single metric that caters to this is a difficult task. This paper proposes a model for arriving at a rating on group maturity within the organization. Considering the linguistic or imprecise and uncertain nature of software measurements, fuzzy logic approach is used for the proposed model. Without the barriers like technology or life cycle difference, the proposed model helps the organization to compare different groups within it with reasonable precision. © 2007 Institute of Automation, Chinese Academy of Sciences.",Fuzzy logic; Fuzzy sets; Group maturity rating; Historical data; Software projects,"Verma A.K., Anil R., Jain O.P.",2007,Journal,International Journal of Automation and Computing,10.1007/s11633-007-0406-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148873018&doi=10.1007%2fs11633-007-0406-8&partnerID=40&md5=4681773dcb30bca037a03091d1f459f1,"Indian Institute of Technology Bombay, Powai, Mumbai 400 076, India; Larsen and Toubro Infotech Limited, Powai, Mumbai 400 072, India",,English,14768186,
Scopus,Sizing maintenance tasks for web applications,"Web applications are fast becoming the new legacy systems of today. While there is considerable similarity between traditional software systems and Web-based systems, there are also significant differences between them. One area that illustrates this dual nature is cost and effort estimation. There exists a mature body of knowledge for performing such estimates on traditional software systems, but such methods may not appear to be directly applicable to Web applications. This paper presents an effort estimation technique for maintaining a large-scale Web application by measuring and tracking the size and complexity of the Web-based system. Specifically, a combination of function-points and static impact analysis is used, tracing the change requests to different components of the Web application, and then measuring their size and complexity to aid the cost estimation for that particular change request based on function point productivity measurements. To illustrate the use of this technique, a case study from a real-world industrial product is presented. © 2007 IEEE.",Cost estimation; Impact analysis; Metrics; Software maintenance; Web; Web size measurement,"Sneed H.M., Shihong H.",2007,Conference,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",10.1109/CSMR.2007.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547653827&doi=10.1109%2fCSMR.2007.41&partnerID=40&md5=1a343f94cb0bc549f1e9059c9dde5f5e,"Anecon GmbH, Vienna, Austria; Computer Science and Engineering, Florida Atlantic University, United States",,English,15345351,
Scopus,Fuzzy radial basis function neural networks for web applications cost estimation,"The Fuzzy Radial basis function Neural Networks (FRBFN) for software cost estimation is designed by integrating the principles of RBFN and the fuzzy C-means clustering algorithm. The architecture of the network is suitably modified at the hidden layer to realise a novel neural implementation of the fuzzy clustering algorithm. Fuzzy set-theoretic concepts are incorporated at the hidden layer, enabling the model to handle uncertain and imprecise data, which can improve greatly the accuracy of obtained estimates. MMRE and Pred are used as measures of prediction accuracy for this comparative study. The results show that an RBFN using fuzzy C-means performs better than an RBFN using hard C-means. This study uses data on web applications from the Tukutuku database. ©2008 IEEE.",,"Idri A., Zakrani A., Elkoutbi M., Abran A.",2007,Conference,"Innovations'07: 4th International Conference on Innovations in Information Technology, IIT",10.1109/IIT.2007.4430367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249168929&doi=10.1109%2fIIT.2007.4430367&partnerID=40&md5=ee74c556489187d35257cf7c8b48c2c0,"ENSIAS, Mohammed V University, Rabat, Morocco; École de Technologie Supérieure, Montreal, QC, Canada",IEEE Computer Society,English,,9781424418411
Scopus,A new regression based software cost estimation model using power values,"The paper aims to provide for the improvement of software estimation research through a new regression model. The study design of the paper is organized as follows. Evaluation of estimation methods based on historical data sets requires that these data sets be representative for current or future projects. For that reason the data set for software cost estimation model the International Software Benchmarking Standards Group (ISBSG) data set Release 9 is used. The data set records true project values in the real world, and can be used to extract information to predict new projects cost in terms of effort. As estimation method regression models are used. The main contribution of this study is the new cost production function that is used to obtain software cost estimation. The new proposed cost estimation function performance is compared with related work in the literature. In the study same calibration on the production function is made in order to obtain maximum performance. There is some important discussion on how the results can be improved and how they can be applied to other estimation models and datasets. © Springer-Verlag Berlin Heidelberg 2007.",Regression analysis; Software cost estimation; Software cost models,"Adalier O., Uǧur A., Korukoglu S., Ertas K.",2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-77226-2_34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38449087623&doi=10.1007%2f978-3-540-77226-2_34&partnerID=40&md5=13960074a87d0ff58ee289e1252f898e,"TUBITAK-UEKAE, National Research Institute of Electronics and Cryptology, PK 74 Gebze Kocaeli, Turkey; Ege University, Department of Computer Engineering, Bornova Izmir, Turkey; Dokuz Eylül University, Department of Econometrics, Buca, Izmir, Turkey",Springer Verlag,English,03029743,9783540772255
Scopus,Enhanced software development effort and cost estimation using fuzzy logic model,"The development of software has always been characterized by parameters that possess certain level of fuzziness. This requires that some degree of uncertainty be introduced in the models, in order to make the models realistic. Fuzzy logic fares well in this area. Many of the problems of the existing effort estimation models can be solved by incorporating fuzzy logic. Besides, fuzzy logic had been combined with algorithmic, non-algorithmic effort estimation models as well as a combination of them to deal with the inherent uncertainty issues. This paper also described an enhanced fuzzy logic model for the estimation of software development effort. The model (FLECE) possesses similar capabilities as the previous fizzy logic model. In addition to that, the enhancements done in FLECE improved the empirical accuracy of the previous model in terms of MMRE (Mean Magnitude of Relative Error) and threshold-oriented prediction measure or prediction quality (pred).",Cost estimation; Fuzzy logic,"Su M.T., Ling T.C., Phang K.K., Liew C.S., Man P.Y.",2007,Journal,Malaysian Journal of Computer Science,10.22452/mjcs.vol20no2.7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049125927&doi=10.22452%2fmjcs.vol20no2.7&partnerID=40&md5=1124e1ea8755a0905fed578252d1d1db,"Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia",Faculty of Computer Science and Information Technology,English,01279084,
Scopus,COCOMO-SCORM: Interactive courseware project cost modeling,"The U.S. Department of Defense has made significant investments in interactive courseware designed to train personnel distributed around the world. As these projects have multiplied, so too have the different approaches to estimating the schedule and staffing to create them. This paper presents the current status of a project that is creating a cost estimation algorithm for interactive courseware based on the COCOMO II family of software cost models. Our project focuses on specializing the COCOMO algorithm to account for the important variables to courseware development projects. We also will calibrate the algorithm to match historical data available from completed SCORM courseware projects. Copyright © 2006 by Roger Smith & Lacey Edwards.",,"Smith R., Edwards L.",2006,Conference,"16th Annual International Symposium of the International Council on Systems Engineering, INCOSE 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878790892&partnerID=40&md5=a1e47076e1ecad4da34e29db7d2b5c10,"SPARTA Inc., 13501 Ingenuity Drive, Orlando, FL 32826, United States",,English,,9781622769292
Scopus,Design systems engineering of software products: Implementation of a software estimation model,"In this paper, we present the implementation of a software estimation model to evaluate effort, schedule and cost of software development during all the phases of a project. An approach based on COCOMO II model is used, and we propose an adaptation to a particular context of an organization. Both human and cultural aspects of the company are considered in order to mitigate the problem of acceptability. Our model is based on the work of Dr. Allan Albrecht on Function Points (FP) and Dr. Barry Boehm on COCOMO II model.",CMMI; COCOMO II; Function points; Project evaluation; Software estimation,"Ferchichi A., Bourey J.P., Bigand M., Barron M.",2006,Conference,"IMACS Multiconference on ""Computational Engineering in Systems Applications"", CESA",10.1109/CESA.2006.313501,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51749116214&doi=10.1109%2fCESA.2006.313501&partnerID=40&md5=e76db69728931eaed78d3f658dc2d8e8,"Laboratoire de Génie Industriel de Lille (LGIL), Ecole Centrale de Lille, B.P. 48, F-59651 Villeneuve d'Ascq, France; Recherche Opérationnelle Innovation, Sylis, Parc du Pont Royal, Bat. G., 251 Avenue du Bois, 59831 Lambersart, France",,English,,7302139229; 9787302139225
Scopus,Distribution of cost over the application lifecycle -A multi-case study,"IT management focuses on planning and developing new IT solutions. The importance of production (operation, support, maintenance) and further development of existing solutions is often neglected, although these tasks are responsible for the majority of today's IT costs. The paper presents the results of a survey of the life cycle costs of 30 IT application systems. Within the survey, the distribution of costs over the application life cycle was recorded and evaluated. The results show the central importance of recurring costs for production and further development. For a production time of 5 years these costs amounted to 79% of all life cycle costs, whereas only 21% of the costs were incurred during the planning and initial development stages. Further findings include an evaluation of the poor quality of the cost data and the important role of business units as service providers.",IT application systems; IT management; IT production; Life cycle costs; Life cycle management,"Zarnekow R., Brenner W.",2005,Conference,"Proceedings of the 13th European Conference on Information Systems, Information Systems in a Rapidly Changing Economy, ECIS 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871001310&partnerID=40&md5=668c77e9547741be84c3c59f2ab2e463,"University of St. Gallen, Institute of Information Management, Mueller-Friedberg-Strasse 8, 9000 St. Gallen, Switzerland",,English,,3937195092; 9783937195094
Scopus,Optimizing and simplifying software metric models constructed using maximum likelihood methods,"A software metric model can be used to predict a target metric (e.g., the development work effort) for a future release of a software system based on the project's predictor metrics (e.g., the project team size). However, missing or incomplete data often appear in the data samples used to construct the model. So far, the least biased and thus the most recommended software metric models for dealing with the missing/incomplete data are those constructed by using the maximum likelihood methods. It is true that the inclusion of a particular predictor metric in the model construction is initially based on an intuitive or experience-based assumption that the predictor metric impacts significantly the target metric. Nevertheless, this assumption has to be verified. Previous research on metric models constructed by using the maximum likelihood methods simply took this verification for granted. This can result in probable inclusion of superfluous predictor metric(s) and/or unnecessary predictor metric complexity. In this paper, we propose a methodology to optimize and simplify such models based on the results of appropriate hypothesis tests. An experiment is also reported to demonstrate the use of our methodology in trimming redundant predictor metric(s) and/or unnecessary predictor metric complexity. © 2005 IEEE.",Maximum likelihood method; Modeling; Software metrics,"Chan V.K.Y., Wong W.E.",2005,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/COMPSAC.2005.116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751058682&doi=10.1109%2fCOMPSAC.2005.116&partnerID=40&md5=e8faf3adaac0457c72ca7272d81a4581,"Macao Polytechnic Institute, Macau; University of Texas, Dallas, United States",,English,07303157,0769522092; 9780769522098
Scopus,Software Quality Estimation with Case-Based Reasoning,"The software quality team of a software project often strives to predict the operational quality of software modules prior to software deployment. A timely software quality prediction can be used for enacting any preventive actions so as to reduce software faults from occurring during system operations. This is especially important for high-assurance systems where software reliability is very critical. The two most commonly used models for software quality estimation are, software fault prediction and software quality classification. Generally, such models use software metrics as predictors of a software module's quality, which is either represented by the expected number of faults or a class membership to quality-based groups. This study presents a comprehensive methodology for building software quality estimation models with case-based reasoning (cbr), a computational intelligence technique that is suited for experience-based analysis. A cbr system is a practical option for software quality modeling, because it uses an organization's previous experience with its software development process to estimate the quality of a currently under-development software project. In the context of software metrics and quality data collected from a high-assurance software system, software fault prediction and software quality classification models are built. The former predicts the number of faults in software modules, while the latter predicts the class membership of the modules into the fault-prone and not fault-prone groups. This study presents in-depth details for the cbr models so as to facilitate a comprehensive understanding of the cbr technology as applied to software quality estimation. © 2004 Elsevier Inc. All rights reserved.",,"Khoshgoftaar T.M., Seliya N.",2004,Review,Advances in Computers,10.1016/S0065-2458(03)62006-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951136732&doi=10.1016%2fS0065-2458%2803%2962006-1&partnerID=40&md5=b3061e9d56075a6292f8fce737514c84,"Florida Atlantic University Boca Raton, FL, United States",,English,00652458,
Scopus,Improving software size estimates by using probabilistic pairwise comparison matrices,"The Pairwise Comparison technique is a general purpose estimation approach for capturing expert judgment. This approach can be generalized to a probabilistic version using Monte Carlo methods to produce estimates of size distributions. The probabilistic pairwise comparison technique enables the estimator to systematically incorporate both estimation uncertainty as well as any uncertainty that arises from using multiple historical analogies as reference modules. In addition to describing the methodology, the results of the case study are also included. This paper is an extension of the work presented in [1] and will show how the original software size estimates compared to the actual delivery size. It will also describe the techniques used to modify the approach based on lessons learned. The results because they are based on only one case do not validate the effectiveness of the proposed approach but are suggestive that the technique can be effective and support the conclusion that further research is worth pursuing. © 2004 IEEE.",,"Hihn J., Lum K.T.",2004,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRIC.2004.1357898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844328036&doi=10.1109%2fMETRIC.2004.1357898&partnerID=40&md5=4d8ecbafa22836e2d863b90993e506a2,"Jet Propulsion Laboratory, California Institute of Technology, United States",,English,15301435,0769521290
Scopus,Decision support for using software inspections,"In support of decision-making for planning the effort to be allocated to inspections in different software development phases, we propose combining empirical studies with process modeling and simulation. We present the simulator developed for answering questions and running ""what-if"" scenarios specific to NASA software development projects. © 2004 IEEE.",,"Rus I., Shull F., Donzelli P.",2004,Conference,"Proceedings - 28th Annual NASA Goddard Software Engineering Workshop, SEW 2003",10.1109/SEW.2003.1270720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954454816&doi=10.1109%2fSEW.2003.1270720&partnerID=40&md5=f5581c5e24f5ab2d827998285c191e78,"Fraunhofer CenterMD, United States; University of Maryland, Computer Science Department, United States",Institute of Electrical and Electronics Engineers Inc.,English,,0769520642; 9780769520643
Scopus,Prediction of costs and duration of software reengineering projects [Aufwandsschätzung von Software-Reengineering-Projekten],"Costs and duration of projects to reengineer existing software are much easier to predict than the respective figures for software development projects. Moreover, in reengineering projects costs, risks and duration are much smaller. This article presents a tool-based method to calculate the three most important reengineering project types: encapsulation projects, renovation projects, and conversion projects. The method comprises eight consecutive steps - starting from reengineering requirements and ending with risk assessments and adjustments. A set of well-known prediction methods are incorporated, among them COCOMO, component analysis, function point analysis, data point analysis, and object point analysis. The method lays particular focus on the size of the projected software as well as on its complexity and quality.",COCOMO; Data point analysis; Duration; Effort; Function point analysis; Object point analysis; Project planning; Software engineering; Software reengineering,Sneed H.M.,2003,Journal,Wirtschaftsinformatik,10.1007/bf03250923,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347599077&doi=10.1007%2fbf03250923&partnerID=40&md5=ffa63ce8c4c5ab86ffc3d82ef010d139,"Prellerweg 5, 82054 Arget, Germany",Friedr. Vieweg und Sohn Verlags GmbH,German,09376429,
Scopus,New directions in measurement for software quality control,"Assessing and controlling software quality is still an immature discipline. One of the reasons for this is that many of the concepts and terms that are used in discussing and describing quality are overloaded with a history from manufacturing quality. We argue in this paper that a quite distinct approach is needed to software quality control as compared with manufacturing quality control. In particular, the emphasis in software quality control is in design to fulfill business needs, rather than replication to agreed standards. We will describe how quality goals can be derived from business needs. Following that, we will introduce an approach to quality control that uses rich causal models, which can take into account human as well as technological influences. A significant concern of developing such models is the limited sample sizes that are available for eliciting model parameters. In the final section of the paper we will show how expert judgment can be reliably used to elicit parameters in the absence of statistical data. In total this provides an agenda for developing a framework for quality control in software engineering that is freed from the shackles of an inappropriate legacy. © 2002 IEEE.",Failure Analysis; Process Control; Quality Control; System Reliability,"Krause P., Freimut B., Suryn W.",2002,Conference,"Proceedings - 10th International Workshop on Software Technology and Engineering Practice, STEP 2002",10.1109/STEP.2002.1267623,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961749812&doi=10.1109%2fSTEP.2002.1267623&partnerID=40&md5=4050ba10df4a9500996b41f0452e494f,"Surrey University, Guildford, Surrey, KT21 2TY, United Kingdom; Philips Electronics, Crossoak Lane, Redhill, RH1 5HA, United Kingdom; Fraunhofer Institute for Experimental Software Engineering, Sauerwiesen 6, Kaiserslautern, D-67661, Germany; Department of Electrical Engineering, École de Technologie Supérieure (ÉTS), 1100 Notre Dame St. West, Montréal, QC  H3C 1K3, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,0769518788; 9780769518787
Scopus,Measuring Effort Estimation Uncertainty to Improve Client Confidence,"A description and a theoretical discussion are given of an approach that it is proposed will improve client confidence in estimates produced using algorithmic software estimation methods, such as function Point or COCOMO estimation. The approach utilises Bayes Theorem and Bayesian inference. The underlying theory has been successfully used in other arenas of subjective measurement to help improve measurement consistency. It is also proposed that the approach can be used to improve estimation consistency during the estimation of the effort required to develop software development artefacts (e.g. project effort, milestone effort, requirements changes). Software developers will also be able to measure uncertainty in their estimates of artefacts using Bayesian inference. Outsourcers can use the approach to provide statements for client companies about the confidence they have in their estimates. The statements of confidence can then be used to assist outsourcers and clients during project negotiations. Examples are provided to show how the method can be used to measure estimate uncertainty, how estimators can be supported during their estimation procedures, and what kinds of statement can be made to aid project negotiations.",Albrecht's and MK II Function Points; Bayes theorem; Bayesian inference; COCOMO; Effort estimation; Function type complexity measurement; Outsourcer and client negotiations,Moses J.,2002,Journal,Software Quality Journal,10.1023/A:1020523923715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842867200&doi=10.1023%2fA%3a1020523923715&partnerID=40&md5=ab7f589b561e7e0f60b6169756f9c6ac,"University of Sunderland, Sch. of Comp., Eng. and Technology, St. Peter's Campus, Sunderland SR6 0DD, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Reliability modeling incorporating error processes for Internet-distributed software,"This paper proposes several improvements on the conventional software reliability growth models (SRGMs) to describe actual software development process by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the ""delay-effect factor"", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included in the actual correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect human learning process in our proposed model Experiments on a real data set for internet-distributed software has been performed, and the results show that the proposed new model gives better performance in estimating the number of initial faults than previous approaches.",Delay-Effect Factor; Delayed-Time NHPP Model; Fault Detection/Correction Processes; SRGM.,"Lo J.-H., Kuo S.-Y., Hung C.-Y.",2001,Conference,IEEE Region 10 International Conference on Electrical and Electronic Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035786581&partnerID=40&md5=e5d593ad97952911af03ae956f6aeaf3,"Dept. of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Central Bank of China, Taipei, Taiwan",,English,,0780371011
Scopus,Back-to-front programming effort prediction,"This paper considers metrics for estimating software development effort. We are attempting to develop a family of metrics that will explain effort after the completion of a project and predict remaining effort when applied at some milestone prior to completion. We suggest that a milestone is meaningful only if it is early enough so that effort prediction is worthwhile and only if it is clearly identifiable. We analyzed the construction process for twenty-seven Fortran programs. The total programming time for each program was divided into two components, the design/coding time and the debug time. We chose the FCC (first clean compiled, i.e. first syntax-error-free) version as a meaningful milestone. In our data we found that the actual effort spent before this milestone was about 70% of the total effort, with 30% remaining thereafter. Furthermore, our results suggest that if an effort model performs well in estimating total programming effort at the end of development, it will also perform well at milestone FCC. But, the metrics we examined were not very useful in predicting the amount of effort remaining after the FCC milestone-even though each predicted total effort accurately enough. More work must be done order to find or create such an estimator. © 1984.",,"Wang A.S., Dunsmore H.E.",1984,Journal,Information Processing and Management,10.1016/0306-4573(84)90045-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020230019&doi=10.1016%2f0306-4573%2884%2990045-1&partnerID=40&md5=67ea4a700efcab3d17d049eda662e530,"Department of Computer Sciences, Purdue University, West Lafayette, IN 47907, United States",,English,03064573,
Scopus,An extensive evaluation of ensemble techniques for software change prediction,"Predicting the areas of the source code having a higher likelihood to change in the future represents an important activity to allow developers to plan preventive maintenance operations. For this reason, several change prediction models have been proposed. Moreover, research community demonstrated how different classifiers impact on the performance of devised models as well as classifiers tend to perform similarly even though they are able to correctly predict the change proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we deeper investigated whether the use of ensemble approaches, ie, machine learning techniques able to combine multiple classifiers, can improve the performances of change prediction models. Specifically, we built three change prediction models based on different predictors, ie, product-, process- metrics-, and developer-related factors, comparing the performances of four ensemble techniques (ie, Boosting, Random Forest, Bagging, and Voting) with those of standard machine learning classifiers (ie, Logistic Regression, Naive Bayes, Simple Logistic, and Multilayer Perceptron). The study was conducted on 33 releases of 10 open-source systems, and the results showed how ensemble methods and in particular Random Forest provide a significant improvement of more than 10% in terms of F measure. Indeed, the statistical analyses conducted confirm the superiority of this ensemble technique. Moreover, the model built using developer-related factors performed better than the other models that exploit product and process metrics and achieves an overall median of F measure around 77%. © 2019 John Wiley & Sons, Ltd.",change prediction; empirical study; ensemble techniques; machine learning,"Catolino G., Ferrucci F.",2019,Conference,Journal of Software: Evolution and Process,10.1002/smr.2156,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070074608&doi=10.1002%2fsmr.2156&partnerID=40&md5=51e62ad6dbffedf34e6dadd380bf4901,"Department of Computer Science, University of Salerno, Fisciano, Italy",John Wiley and Sons Ltd,English,20477481,
Scopus,Software quality assessment model: a systematic mapping study,"Quality model is regarded as a well-accepted approach for assessing, managing and improving software product quality. There are three categories of quality models for software products, i.e., definition model, assessment model, and prediction model. Quality assessment model (QAM) is a metric-based approach to assess the software quality. It is typically regarded as of high importance for its clear method on how to assess a system. However, the current state-of-the-art in QAM research is under limited investigation. To address this gap, the paper provides an organized and synthesized summary of the current QAMs. In detail, we conduct a systematic mapping study (SMS) for structuring the relevant articles. We obtain a total of 716 papers from the five databases, and 31 papers are selected as relevant studies at last. In summary, our work focuses on QAMs from the following aspects: software metrics, quality factors, aggregation methods, evaluation methods and tool support. According to the analysis results, our work discovers five needs that researchers in this area should continue to address: (1) new method and criteria to tailor a quality framework (i.e., structure of software metrics and quality factors) according to different specifics, (2) systematic investigations on the effectiveness, strength and weakness of different aggregation methods to guide the method selection in different context, (3) more investigations on evaluating QAMs in the context of industrial cases, (4) further investigations or real-world case studies on the QAMs related tools, and (5) building a public and diverse software benchmark which can be adopted in different application context. © 2019, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.",aggregation method; quality assessment model; software quality; systematic mapping study,"Yan M., Xia X., Zhang X., Xu L., Yang D., Li S.",2019,Review,Science China Information Sciences,10.1007/s11432-018-9608-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069895120&doi=10.1007%2fs11432-018-9608-3&partnerID=40&md5=a2fbcdb0b67be249834e19658827f8c7,"College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Faculty of Information Technology, Monash University, Melbourne, 3800, Australia; School of Software Engineering, Chongqing University, Chongqing, 401331, China",Science in China Press,English,1674733X,
Scopus,Search Strategy to Update Systematic Literature Reviews in Software Engineering,"[Context] Systematic Literature Reviews (SLRs) have been adopted within the Software Engineering (SE) domain for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now outdated, and there are no standard proposals on how to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on how to best to search for evidence when updating SLRs in SE. [Method] To achieve our goal, we compare and discuss outcomes from applying different search strategies to identifying primary studies in a previously published SLR update on effort estimation. [Results] The use of a single iteration forward snowballing with Google Scholar, and employing the original SLR and its primary studies as a seed set seems to be the most cost-effective way to search for new evidence when updating SLRs. [Conclusions] The recommendations can be used to support decisions on how to update SLRs in SE. © 2019 IEEE.",Searching for evidence!; Snowballing; Software Engineering; Systematic Literature Review Update; Systematic Literature Reviews,"Mendes E., Felizardo K., Wohlin C., Kalinowski M.",2019,Conference,"Proceedings - 45th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2019",10.1109/SEAA.2019.00061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076019859&doi=10.1109%2fSEAA.2019.00061&partnerID=40&md5=c3f3f9bc9b854ad539ca79a4568bf97d,"Department of Computer Science, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computing, Federal Technological University of Paraná, Cornélio Procópio, Brazil; Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Informatics, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil",Institute of Electrical and Electronics Engineers Inc.,English,,9781728132853
Scopus,An ensemble of optimal trees for software development effort estimation,"Accurate estimation of software development effort plays a pivotal role in managing and controlling the software development projects more efficiently and effectively. Several software development effort estimation (SDEE) models have been proposed in the literature including machine learning techniques. However, none of these models proved to be powerful in all situation and their performance varies from one dataset to another. To overcome the weaknesses of single estimation techniques, the ensemble methods have been recently employed and evaluated in SDEE. In this paper, we have developed an ensemble of optimal trees for software development effort estimation. We have conducted an empirical study to evaluate and compare the performance of this optimal trees ensemble using five popular datasets and the 30% hold-out validation method. The results show that the proposed ensemble outperforms regression trees and random forest models in terms of MMRE, MdMRE and Pred(0.25) in all datasets used in this paper. © Springer Nature Switzerland AG 2019.",Accuracy evaluation; Optimal trees ensemble; Random forest; Regression trees; Software effort estimation,"Abdelali Z., Hicham M., Abdelwahed N.",2019,Book Chapter,Lecture Notes in Networks and Systems,10.1007/978-3-030-11914-0_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062921259&doi=10.1007%2f978-3-030-11914-0_6&partnerID=40&md5=3033e09fa6b87d5ac287e642756fa80d,"ENSAM, 150 Boulevard Nile, Casablanca, Morocco; Faculté des Sciences Ben M’sik, Bd Driss El Harti, Casablanca, Morocco",Springer,English,23673370,
Scopus,Support vector regression-based imputation in analogy-based software development effort estimation,"Missing data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort estimation (SDEE) techniques. To deal with this challenge, several imputation techniques have been investigated in SDEE and k-nearest neighbors (KNN)-based imputation is still the most frequently used. To the best of our knowledge, no study has used support vector regression (SVR)-based imputation to construct accurate estimation techniques, in particular those based on analogy. This paper introduces a new imputation technique based on SVR for handling MD in two analogy-based SDEE techniques: classical analogy and fuzzy analogy. More specifically, we investigate whether the use of SVR instead of KNN in imputing MD improves the predictive performance of these two analogy-based techniques. A total of 1134 experiments were conducted involving seven datasets, SVR/KNN MD imputation techniques (KNN with Euclidean and Manhattan distances), three missingness mechanisms (missing completely at random, missing at random, non-ignorable missing), and MD percentages from 10% to 90%. The results suggest that the use of SVR imputation, rather than KNN imputation, may improve the prediction performance of both analogy-based techniques. Furthermore, we found that the impact of MD percentage upon effort prediction performance is reduced when using SVR rather than KNN. Moreover, fuzzy analogy generates better estimates in terms of the standardized accuracy measure than classical analogy regardless of the MD technique, the dataset used, the missingness mechanism, or the MD percentage. © 2018 John Wiley & Sons, Ltd.",analogy-based software development effort estimation; imputation; k-nearest neighbors; missing data; support vector machine,"Idri A., Abnane I., Abran A.",2018,Journal,Journal of Software: Evolution and Process,10.1002/smr.2114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058493764&doi=10.1002%2fsmr.2114&partnerID=40&md5=09059bd4e9087fda5b7225f1c8aeea9d,"Software Project Management Research Team, ENSIAS, University Mohammed V, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",John Wiley and Sons Ltd,English,20477481,
Scopus,Model-based software engineering: A multiple-case study on challenges and development efforts,"A recurring theme in discussions about the adoption of Model-Based Engineering (MBE) is its effectiveness. This is because there is a lack of empirical assessment of the processes and (tool-)use of MBE in practice. We conducted a multiple-case study by observing 2 two-month MBE projects from which software for a Mars rover were developed. We focused on assessing the distribution of the total software development effort over different development activities. Moreover, we observed and collected challenges reported by the developers during the execution of projects. We found that the majority of the effort is spent on the collaboration and communication activities. Furthermore, our inquiry into challenges showed that tool-related challenges are the most encountered. © 2018 Association for Computing Machinery.",Case Study Design; Effort Distribution; MBE Challenges; Model-Based Engineering; Modeling Tools; Software Engineering,"Jolak R., Ho-Quang T., Michel R.V., Schiffelers R.R.H.",2018,Conference,"Proceedings - 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2018",10.1145/3239372.3239404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056818993&doi=10.1145%2f3239372.3239404&partnerID=40&md5=7752cb9290019fe84007586273693bb7,"Chaudron Chalmers, Univesity of Gothenburg, Gothenburg, Sweden; Technische Universiteit Eindhoven, TUE and ASML Netherlands B.V, Eindhoven, Netherlands","Association for Computing Machinery, Inc",English,,9781450349499
Scopus,Prediction of benign and malignant tumor,In the season of unbelievable advancement from climate news to online request is known with the assistance of employments in the light weight systems. Through this paper we built up a strategy which helps in yearning of benign and malignant tumor of a patient by metric assessment. By utilizing this procedure we diminish the credits required to expect the breast disease in the light weight structures which has less figuring power. © 2017 IEEE.,Data Analysis; part; Pearson; Regression,"Sharma K., Muktha B., Rani A., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300871,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046674440&doi=10.1109%2fICOEI.2017.8300871&partnerID=40&md5=9658f496f435eb1642d30241ff6b29e8,"MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Prediction of diabetes disease using control chart and cost optimization-based decision,"In this paper, a prediction of diabetes disease for people of various age groups and genders is implemented using cost optimization and control chart. Generally, Diabetes is one of the metabolic diseases caused due to increase in the sugar levels of blood over a prolonged period. Diabetes will be confirmed by examining albumin and bilirubin levels and protein content in blood. This paper discusses on predicting diabetes disease among various persons and existence of disease among them using Cost optimization method. © 2017 IEEE.",Attributes; Control Chart; Optimization; Prediction,"Thirumalai C., Saisharan G.V., Krishna K.V., Senapathi K.J.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300857,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046648229&doi=10.1109%2fICOEI.2017.8300857&partnerID=40&md5=8fcc8013f9d94bee041879b7d92a00c3,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Calculating the user-item similarity using Pearson's and cosine correlation,"In this paper, we look into three different techniques for computing similarities for obtaining a recommendation for them. There are a number of different mathematical formulations that can be used to calculate the similarity between two items. On the basis of various parameters, we conclude Pearson's Correlation Coefficient provides better quality than the others. © 2017 IEEE.",Cosine Correlation; Pearson's Correlation,"Dharaneeshwaran N., Nithya S., Srinivasan A., Senthilkumar M.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046641985&doi=10.1109%2fICOEI.2017.8300858&partnerID=40&md5=88807453f1f28a808e5040c3e461e794,"MS Software Engineering, VIT University, Vellore, India; VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,"Software development measurement programs: Development, management and evolution","This book seeks to promote the structured, standardized and accurate use of software measurement at all levels of modern software development companies. To do so, it focuses on seven main aspects: Sound scientific foundations, cost-efficiency, standardization, value-maximization, flexibility, combining organizational and technical aspects, and seamless technology integration. Further, it supports companies in their journey from manual reporting to automated decision support by combining academic research and industrial practice. When scientists and engineers measure something, they tend to focus on two different things. Scientists focus on the ability of the measurement to quantify whatever is being measured; engineers, however, focus on finding the right qualities of measurement given the designed system (e.g. correctness), the system’s quality of use (e.g. ease of use), and the efficiency of the measurement process. In this book, the authors argue that both focuses are necessary, and that the two are complementary. Thus, the book is organized as a gradual progression from theories of measurement (yes, you need theories to be successful!) to practical, organizational aspects of maintaining measurement systems (yes, you need the practical side to understand how to be successful). The authors of this book come from academia and industry, where they worked together for the past twelve years. They have worked with both small and large software development organizations, as researchers and as measurement engineers, measurement program leaders and even teachers. They wrote this book to help readers define, implement, deploy and maintain company-wide measurement programs, which consist of a set of measures, indicators and roles that are built around the concept of measurement systems. Based on their experiences introducing over 40,000 measurement systems at over a dozen companies, they share essential tips and tricks on how to do it right and how to avoid common pitfalls. © Springer International Publishing AG, part of Springer Nature 2018.",Software Creation and Management; Software Development; Software Engineering; Software Functional Properties; Software Measurement,"Staron M., Meding W.",2018,Book,"Software Development Measurement Programs: Development, Management and Evolution",10.1007/978-3-319-91836-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060056515&doi=10.1007%2f978-3-319-91836-5&partnerID=40&md5=2989aeba549e92a0f3eda6862bc9d072,"Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden; Ericsson AB, Gothenburg, Sweden",Springer International Publishing,English,,9783319918365; 9783319918358
Scopus,Iagile: Mission critical military software development,"This paper reports the experience of applying agile methods in the defense sector, characterized mostly by embedded and mission critical software. We describe the experience of creating a Command and Control system for the 4th Logistic Department of the Italian Army's General Staff. The project was approved by the Army as a pilot to determine whether it could be possible to reduce development costs and at the same time produce a product better responsive to the changing conditions in the theatre of operations, where often the confrontation has become asymmetric and requires reaction times much faster than the conventional approach. After 13 five-week long sprints, we were able to deliver a complete product that met all user requirements and satisfied regulatory Army requirements. Achieving this result required a concerted effort to change the development culture, but even when counting this effort as part of the development costs, the total development costs were lower than the costs of using the traditional development method. This paper summarizes the experience trying, whenever possible, to quantify the results, and to support the observed positive results with appropriate data. © 2017 IEEE.",Agile Processes; Defense Software; Empirical Software Engineering; Mission critical software; Non-invasive Measurement Systems; Scrum,"Benedicenti L., Messina A., Sillitti A.",2017,Conference,"Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017",10.1109/HPCS.2017.87,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032331583&doi=10.1109%2fHPCS.2017.87&partnerID=40&md5=7679a9f8ca07344bbd095f089d5c4b38,"Faculty of Engineering, University of Regina, Regina, Canada; Institute of Information Systems, Innopolis University, Innopolis, Russian Federation",Institute of Electrical and Electronics Engineers Inc.,English,,9781538632505
Scopus,Optimizing effort and time parameters of cocomo ii estimation using fuzzy multi-objective pso,"The estimation of software effort is an essential and crucial activity for the software development life cycle. Software effort estimation is a challenge that often appears on the project of making a software. A poor estimate will produce result in a worse project management. Various software cost estimation model has been introduced to resolve this problem. Constructive Cost Model II (COCOMO II Model) create large extent most considerable and broadly used as model for cost estimation. To estimate the effort and the development time of a software project, COCOMO II model uses cost drivers, scale factors and line of code. However, the model is still lacking in terms of accuracy both in effort and development time estimation. In this study, we do investigate the influence of components and attributes to achieve new better accuracy improvement on COCOMO II model. And we introduced the use of Gaussian Membership Function (GMF) Fuzzy Logic and Multi-Objective Particle Swarm Optimization method (MOPSO) algorithms in calibrating and optimizing the COCOMO II model parameters. The proposed method is applied on Nasa93 dataset. The experiment result of proposed method able to reduce error down to 11.891% and 8.082% from the perspective of COCOMO II model. The method has achieved better results than those of previous researches and deals proficient with inexplicit data input and further improve reliability of the estimation method. © 2018, Institute of Advanced Engineering and Science. All rights reserved.",Effort Estimation; Fuzzy; Optimizaton; Time Development Estimation; —COCOMO II Model,"Langsari K., Sarno R.",2017,Conference,"International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",10.11591/eecsi.4.1047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044850474&doi=10.11591%2feecsi.4.1047&partnerID=40&md5=20e86de371eef4664d243401b2be6b6a,"Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",Institute of Advanced Engineering and Science,English,2407439X,
Scopus,Review on intelligent and soft computing techniques to predict software cost estimation,"This article, presents the Machine Learning, Intelligent and Soft computing techniques were applied to predict the estimated cost, quality, accuracy of software developed during 2000-2017. This article, gives a 95% amount of work brought from highly reputed conferences and journals viz. IEEE, Elsevier, Springer, ACM and 5% other journals, and conferences on the application of intelligent techniques. The review is broadly categorized according to the type of technique applied viz. (1) Neural networks (NNs), (2) fuzzy logic, (3) genetic algorithm, (4) decision tree, (5) case base reasoning and (6) soft computing. This article basically attention on the importance of benchmark datasets were used for training and testing of all intelligence techniques, type of techniques were applied, and which are the best evaluation metrics. After doing tremendous analysis, we observed that the benchmark data sets are COCOMO, NASA, ISBSG, DEHANAIS prominent datasets, and the evaluation metrics are MMRE, PRED is prominent which is based on total count is presented in Table 8 and 9. Furthermore, we found that Neural Networks technique was used recurrently by the authors and at the same time the techniques applied non regular intervals such as Hybrid, fuzzy logic, decision tree and evolutionary computation. This review is going to be useful for researchers as beginners as and it provides future directions. This would eventually be led to better predict, in the field of Software Cost Estimation. © Research India Publications.",Decision tree (dT); Fuzzy logic(fL); Genetic algorithms (GA); Neural networks (NN); Soft computing (SC); Software cost estimation (SCE),"Venkataiah V., Mohanty R., Nagaratna M.",2017,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057642787&partnerID=40&md5=1a0b3cdde43eba42fe00a61b8c570425,"Computer Science and Engineering, CMR College of Engineering & Technology, Medchal, Hyderabad, 500055, India; Computer Science and Engineering, Keshav Memorial Institute of Technology, Narayaguda, Hyderabad, -500011, India; Computer Science and Engineering, Jawaharlal Nehru Technological University Hyderabad- College of Engineering, Kukatpally, Hyderabad, 500085, India",Research India Publications,English,09734562,
Scopus,Portability of executable service-oriented processes: metrics and validation,"A key promise of process languages based on open standards, such as the Web Services Business Process Execution Language, is the avoidance of vendor lock-in through the portability of processes among runtime environments. Despite the fact that today various runtimes claim to support this language, every runtime implements a different subset, thus hampering portability and locking in their users. It is our intention to improve this situation by enabling the measurement of the portability of executable service-oriented processes. This helps developers to assess their implementations and to decide if it is feasible to invest in the effort of porting a process to another runtime. In this paper, we define several software quality metrics that quantify the degree of portability of an executable, service-oriented process from different viewpoints. When integrated into a development environment, such metrics can help to improve the portability of the outcome. We validate the metrics theoretically with respect to measurement theory and construct validity using two validation frameworks. The validation is complemented with an empirical evaluation of the metrics using a large set of processes coming from several process libraries. © 2016, Springer-Verlag London.",Metrics; Portability; Process; SOA; Software quality,"Lenhard J., Wirtz G.",2016,Journal,Service Oriented Computing and Applications,10.1007/s11761-016-0195-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978083260&doi=10.1007%2fs11761-016-0195-4&partnerID=40&md5=7ea37d00ef3718627fb172b8630044ca,"Department of Mathematics and Computer Science, Karlstad University, Karlstad, 65188, Sweden; Distributed Systems Group, University of Bamberg, Bamberg, 96045, Germany",Springer London,English,18632386,
Scopus,Evaluation of estimation models using the Minimum Interval of Equivalence,"This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals. © 2016 Elsevier B.V.",Bootstrap; Credible intervals; Equivalence Hypothesis Testing; Soft computing; Software estimations,"Dolado J.J., Rodriguez D., Harman M., Langdon W.B., Sarro F.",2016,Journal,Applied Soft Computing Journal,10.1016/j.asoc.2016.03.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964595263&doi=10.1016%2fj.asoc.2016.03.026&partnerID=40&md5=d8cf7b0b03d186ac65967a33b9a5706d,"Facultad de Informática, UPV/EHU, University of the Basque Country, Spain; Dept. of Computer Science, University of Alcalá28871, Spain; CREST, University College LondonWC1E 6BT, United Kingdom",Elsevier Ltd,English,15684946,
Scopus,Using Software Non-Functional Assessment Process to Complement Function Points for Software Maintenance,"Context: Most widely used cost models use source lines of code (SLOC) as the software size input measure, due to its quantifiability and high correlation with effort. Estimating the SLOC of a project is very difficult in early stages of the software lifecycle, especially for software maintenance tasks. Depending on the reuse model being used, one would need to size the existing code that needs modifications and the size of the changes being made in SLOC. Functional size measures, such as Function Points (FPs) and the Software Non-functional Assessment Process (SNAP), have been developed to improve the ability to estimate project size early in the lifecycle for both development and maintenance projects. While FPs represent software size by functions; SNAP complements FPs by sizing non-functional requirements, such as data operations and interface design. Goal: SNAP complements Function Points by sizing non-functional requirements, such as data operations and interface design. Through an empirical analysis, the authors want to determine whether SNAP might be an effective software size measure individually or in conjunction with FPs to improve effort estimation accuracy. Method: The empirical analysis will be run on Unified Code Count (UCC)'s dataset, a software tool maintained by University of Southern California (USC). Results: The analyses found that separating projects adding new functions from those modifying existing functions resulted in improved estimation models using SNAP. The effort estimation model for projects modifying functions in UCC had high prediction accuracy statistics, but less impressive results for projects adding existing functions to UCC. The effort estimation accuracy were satisfactory when using SNAP in conjunction with FPs for both groups of projects. Conclusions: SNAP, indeed, complements FPs in terms of the requirements that are considered and sized. Both size metrics should be treated as individual metrics, but can be used together for acceptably accurate cost models in UCC's development environment. © 2016 ACM.",Cost Estimation; Effort Estimation; Function Point Analysis; Local Calibration; Project Management; SNAP; Software Maintenance; Software Non-Functional Assessment Process,"Hira A., Boehm B.",2016,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1145/2961111.2962615,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991705903&doi=10.1145%2f2961111.2962615&partnerID=40&md5=80df0aca7491b47c35f5e70f2a09e737,"University of Southern California, Los Angeles, CA  90089, United States",IEEE Computer Society,English,19493770,9781450344272
Scopus,A novel adaptive structure for SOA system effort estimation,"This paper addresses a novel adaptive structure to solve the problem of service-oriented architecture (SOA) system effort estimation. The objective of our work is to combine the insights of signals theory with empirical research to find a recommendation scheme for the problem of SOA system effort estimation. The main motivation for using this structure is to enhance the principle of self-adaptability to the situation at hand. The proposed structure consists of an adaptive filter composed of linear combiner of filter weights, input and desired values. Additionally, a gradient steepest descent method is used to adjust the filter parameters (training process) using the least mean square algorithm. Furthermore, an experimental analysis is conducted with the proposed structure using the data of 10 past SOA system industrial applications, and in the empirical analysis, two performance measurement metrics and an evaluation function are used to assess the performance in terms of predictive accuracy. The experimental analysis and comparison study are helpful to demonstrate the effectiveness of an estimation technique. The obtained results indicate that an improved predictive accuracy for the problem of SOA system effort estimation has been achieved using the proposed structure when compared with support vector machines, linear, stepwise and ordinary least square regression techniques. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.",,"Mishra S., Kumar C.",2016,Journal,Transactions on Emerging Telecommunications Technologies,10.1002/ett.3053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980320700&doi=10.1002%2fett.3053&partnerID=40&md5=82f2b2065ccf11a1316687e9b9a91a31,"Department of Computer Science and Engineering, Indian School of Mines (ISM), Dhanbad, 826004, India",Wiley Blackwell,English,21615748,
Scopus,Analyzing the non-functional requirements to improve accuracy of software effort estimation through case based reasoning,"Producing accurate software effort estimation is essential for effective software project management that remains a considerable challenge to software engineering and software industry in general. Many methods have been proposed to increase the accuracy of estimating the software project size, effort, or cost. However, the primary focus has been on functional requirements FRs. We are convinced that the rigorous estimation requires a thorough knowledge of all the requirements of the software to be measured. Consequently, a clear identification of the FRs and NFRs as well as a strong understanding of the relationships existing between them is crucial to get measurements closer to reality. In this paper, we propose an early software size and effort estimation method based on a combination of COSMIC and case based reasoning that uses individual requirement measurements as a solution to improve the performance of CBR and to increase the precision of the estimations. This hybrid technique consists in adjusting the FRs measurements by the effect of NFRs with which they are connected. A new link requirements model is proposed in which the possible relationships existing between FRs and NFRs are expressed. This combination will help to efficiently include NFRs, and their relations with FRs, earlier in the measurement process and throughout the life cycle of the software development project. © 2015 IEEE.",case based reasoning (CBR); COSMIC; FR; NFR; Software effort estimation,"Fellir F., Nafil K., Touahni R.",2015,Conference,"2015 10th International Conference on Intelligent Systems: Theories and Applications, SITA 2015",10.1109/SITA.2015.7358402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476367&doi=10.1109%2fSITA.2015.7358402&partnerID=40&md5=54fe6407a64e0ae0acea87193e1e7906,"Ibn Tofail University, Faculty of Sciences, LASTID Laboratory, Kenitra, Morocco; Mohamed v University, Faculty of Law, Economics and Social Sciences, Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781509002207
Scopus,Software effort estimation using functional link neural networks tuned with active learning and optimized with particle swarm optimization,"This paper puts forward a new learning model based on the collaborative effort of active learning and particle swarm optimization (PSO) in functional link artificial neural networks (FLANNs) to estimate software effort. The active learning uses quick algorithm to detect the essential content of the datasets by which the dataset is reduced and are processed through PSO optimized FLANN. The PSO uses the inertia weight, which is an important parameter in PSO that significantly affects the convergence and exploration-exploitation in the search space while training FLANN. The Chebyshev polynomial has been used for mapping the original feature space from lower to higher dimensional functional space. The method has been evaluated exhaustively on different test suits of PROMISE repository to study the performance. The computational results show that the active learning along with PSO optimized FLANN greatly improves the performance of the model and its variants for software development effort estimation. © Springer International Publishing Switzerland 2015.",Active learning and FLANN; PSO; Software effort estimation,"Benala T.R., Mall R., Dehuri S., Swetha P.",2015,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-20294-5_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946146323&doi=10.1007%2f978-3-319-20294-5_20&partnerID=40&md5=e608e66b7324033abd88a9e9600eba9b,"Department of Information Technology, Jawaharlal Nehru Technological University Kakinada, University College of Engineering, Vizianagaram, 535003, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Information and Communication Technology, Fakir Mohan University, Vyasa Vihar,Balasore,Odisha, 756019, India",Springer Verlag,English,03029743,9783319202938
Scopus,An Impact of Linear Regression Models for Improving the Software Quality with Estimated Cost,"Developing organizations are spends lots of money to finding the Errors and bugs. In this article, an application of defects removal effectiveness to improve the software quality and fault prone analysis, methods are finding the solution of parameters in linear regression models with cost estimating method. It describes the approach of quantitative quality management through defect removal effectiveness and statistical process control of cost analysis with historical project data. Software quality is going continuously monetary benefit to perform well management planning, and achieve a new height. In this methodology, Software quality model can make timely predictions of reliability indications; it's enabling to improve software development processes by target reducing the estimated cost for software products and improve the techniques for more effectively and efficiently. © 2015 The Authors.",Cost estimation; Defect injection; Defect removal; Linear regression model; Quality management,"Marandi A.K., Khan D.A.",2015,Conference,Procedia Computer Science,10.1016/j.procs.2015.06.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944064163&doi=10.1016%2fj.procs.2015.06.039&partnerID=40&md5=7d5afa5083b6c389d1bc8340aba6a2ec,"Department of Computer Applications, National Institute of Technology, Jamshedpur, 831 014, India",Elsevier,English,18770509,
Scopus,Shaping the effort of developing secure software,"Effort estimation is extremely challenging for developing secure software systems. Two major challenges are: (1) lack of validated methods or models, (2) large variation in existing security standards that limits applicability of existing methods. This paper reports an exploratory study in establishing effort estimation model for secure operating system software development in China. More specifically, we investigate the existing cost estimation relationships in the domain of secure software systems, then conduct a comparative analysis of existing Chinese IT security standards and the corresponding international standards, and build a customized estimation model to leverage cost estimation relationships with the most similar security requirements, with appropriate adjustment to reflect the differences in standards. The resultant model is evaluated through an example project and results show encouraging improvement in estimation accuracy. © 2015 Published by Elsevier B.V.",COCOMO; Common Criteria; COSECMO; Effort estimation; Information security; Operating system,"Yang Y., Du J., Wang Q.",2015,Conference,Procedia Computer Science,10.1016/j.procs.2015.03.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938602918&doi=10.1016%2fj.procs.2015.03.041&partnerID=40&md5=cbd2dae7853da4baacc605b9505f6b94,"Stevens Institute of Technology, Hoboken, 07030, United States; Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China",Elsevier B.V.,English,18770509,
Scopus,A proposal for the improvement of project's cost predictability using earned value management and historical data of cost - An empirical study,"Although the Earned Value Management (EVM) technique has been used by several companies in various industrial sectors (software development, construction, aerospace, aeronautics, among others) for over 35 years to predict time and cost outcomes, many studies have found vulnerabilities, including: (i) cost performance data do not always have normal distribution, which makes reliable projections difficult; (ii) instability of cost performance indexes during the execution of projects, (iii) there is a worsening tendency in cost performance indexes when project approaches termination. This paper proposes an extension of the EVM technique through the integration of historical cost performance data of processes as a means to improve the project's cost predictability. The proposed technique was evaluated through an empirical study, which evaluated the implementation of the proposed technique in 22 software development projects. The proposed technique has been applied in real projects with the aim of evaluating the accuracy and variation compared to the traditional technique. Hypotheses tests with 95% significance level were performed, and the proposed technique was more accurate and more precise than the traditional technique for calculating the Cost Performance Index (CPI) and Estimates at Completion (EAC). © 2015 World Scientific Publishing Company.",Cost estimation; performance measures; productivity,"De Souza A.D., Da Rocha A.R.C., Dos Santos D.C.S.",2015,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194015400021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929575744&doi=10.1142%2fS0218194015400021&partnerID=40&md5=3088fc9b3735d1a5c20fa48491540e40,"Universidade Federal Do Rio de Janeiro (UFRJ), Programa de Engenharia de Sistemas e Computação (PESC), Av. Horácio Macedo, 2030, Rio de Janeiro, CEP: 21941-914, RJ, Brazil; Universidade Federal de Itajubá (UNIFEI), Departamento de Matemática e Computação (DMC), Avenida BPS, Caixa Postal:50, Itajubá, MG, CEP:37500-903, Brazil",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Assessing software project management complexity: PMCAT tool,"Software projects are complex endeavors that quite often fail to satisfy their initial objectives. As such the need to systematically study and assess the complexity of software projects is quite important. This study presents a systematic framework for assessing complexity of software projects that is based on the study of project management subject areas as defined in Project Management Body of Knowledge (PMBOK). The presented framework is based on a model that combines the concepts of project, complexity model, complexity factor etc. is an attempt to systematically assess and compare the complexity of software projects. The whole concept has been implemented within Project Management Complexity Assessment Tool (PMCAT) and it is available as a software service over the web. © Springer International Publishing Switzerland 2015.",Complexity measurement; Project complexity; Project management; Software projects,"Damasiotis V., Fitsilis P.",2015,Journal,Lecture Notes in Electrical Engineering,10.1007/978-3-319-06764-3_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919359591&doi=10.1007%2f978-3-319-06764-3_30&partnerID=40&md5=f921ae91529eeb7d78dc8b2d457e7621,"TEI Thessaly, Larissa, 41110, Greece",Springer Verlag,English,18761100,
Scopus,The issues of solving staffing and scheduling problems in software development projects,"Search-Based Software Engineering (SBSE) applies search-based optimization techniques in order to solve complex Software Engineering problems. In the recent years there has been a dramatic increase in the number of SBSE applications in areas such as Software Test, Requirements Engineering, and Project Planning. Our focus is on the analysis of the literature in Project Planning, specifically the researches conducted in software project scheduling and resource allocation. SBSE project scheduling and resource allocation solutions basically use optimization algorithms. Considering the results of a previous Systematic Literature Review, in this work, we analyze the issues of adopting these optimization algorithms in what is considered typical settings found in software development organizations. We found few evidence signaling that the expectations of software development organizations are being attended. © 2014 IEEE.",Literature review; Project management; Resource allocation; Scheduling; Search-Based Software Engineering; Staffing,"Peixoto D.C.C., Mateus G.R., Resende R.F.",2014,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/COMPSAC.2014.96,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928598392&doi=10.1109%2fCOMPSAC.2014.96&partnerID=40&md5=3e5dc8a94e593bdce6c7d39b8f441d30,"Department of Computer, Centro Federal de Educação Tecnológica, Belo Horizonte, Brazil; Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil",IEEE Computer Society,English,07303157,
Scopus,Software cost estimation using the improved fuzzy logic framework,"Software cost estimation is the process of predicting effort required to develop a software system. This effort may be in terms of number of hours of work or number of workers. Precise effort estimation with a high grade of reliability is an indispensable part of effectively software management. Software project costs include the cost incurred in all the expenses, i.e. the cost of project from initiation, development to test, software management, quality management and contingent rework, etc. The imprecision inculcated from the inputs utilized in algorithmic models like constructive cost model COCOMO results in imprecise outputs which leads to erroneous effort estimation. In this paper, a software cost estimation model has been proposed based on fuzzy logic. The fuzzy logic model fuzzifies the two parts of the COCOMO model i.e. nominal effort prediction and the effort adjustment factor. The analysis shows that the performance of the FIS enhanced by increasing the number of membership functions. Validation experiment was carried out on NASA 93 and COCOMO81 public database. © 2014 IEEE.",COCOMO; Cost Drivers; EAF; Effort Estimation; Fuzzy Logic; NASA93 Data Set; Soft Computing,"Kushwaha N., Suryakant",2014,Conference,"Proceedings of the 2014 Conference on IT in Business, Industry and Government: An International Conference by CSI on Big Data, CSIBIG 2014",10.1109/CSIBIG.2014.7056959,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988253868&doi=10.1109%2fCSIBIG.2014.7056959&partnerID=40&md5=ff0e3052fb6f902a0b25f103dce052f7,"Dept of C.S.E, Galgotias University, India; Dept of C.S.E, Graphic Era Hill University, Dehradun, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781479930630
Scopus,Model-based quality management of software development projects,"Managing the quality of artifacts created during the development process is an integral part of software project management. Software quality models capture the knowledge and experience regarding the quality characteristics of interest, the measurement data that can help to reason about them, and the mechanisms to use for characterizing and assessing software quality. They are the foundation for managing software quality in projects in an evidence-based manner. Nowadays, coming up with suitable quality models for an organization is still a challenging endeavor. This chapter deals with the definition and usage of software quality models for managing software development projects and discusses different challenges and solutions in this area. The challenges are: (1) There is no universal model that can be applied in every environment because quality is heavily dependent on the application context. In practice and research, a variety of different quality models exists. Finding the right model requires a clear picture of the goals that should be obtained from using the model. (2) Quality models need to be tailored to company specifics and supported by corresponding tools. Existing standards (such as the ISO/IEC 25000 series) are often too generic and hard to fully implement in an organization. (3) Practitioners require a comprehensive set of techniques, methods, and tools for systematically specifying, adapting, and applying quality models in practice. (4) In order to create sustainable quality models, their contribution to the organizational goals must be clarified, and the models need to be integrated into the development and decision-making processes. © 2014 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,"Heidrich J., Rombach D., Kläs M.",2014,Book Chapter,Software Project Management in a Changing World,10.1007/978-3-642-55035-5_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930729598&doi=10.1007%2f978-3-642-55035-5_6&partnerID=40&md5=3ccf66e9a603cd95a572717800e1ce87,"Fraunhofer IESE, Fraunhofer-Platz 1, Kaiserslautern, 67663, Germany; Technische Universität Kaiserslautern, Kaiserslautern, Germany; Fraunhofer Institute for Experimental Software Engineering (IESE), Fraunhofer-Platz 1, Kaiserslautern, 67663, Germany",Springer-Verlag Berlin Heidelberg,English,,9783642550355; 3642550347; 9783642550348
Scopus,A statistical study of the relevance of lines of code measures in software projects,"Lines of code metrics are routinely used as measures of software system complexity, programmer productivity, and defect density, and are used to predict both effort and cost. The guidelines for using a direct metric, such as lines of code, as a proxy for a quality factor such as complexity or defect density, or in derived metrics such as cost and effort are clear. Amongst other criteria, the direct metric must be linearly related to, and accurately predict, the quality factor and these must be validated through statistical analysis following a rigorous validation methodology. In this paper, we conduct such an analysis to determine the validity and utility of lines of code as a measure using the ISBGS-10 data set. We find that it fails to meet the specified validity tests and, therefore, has limited utility in derived measures. © 2014, Springer-Verlag London.",Data mining; ISBSG-10; Linear models; Lines of code; Software estimation,"Barb A.S., Neill C.J., Sangwan R.S., Piovoso M.J.",2014,Journal,Innovations in Systems and Software Engineering,10.1007/s11334-014-0231-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939872935&doi=10.1007%2fs11334-014-0231-5&partnerID=40&md5=4e2e2fda590ee2c8d1fb7873a79f23a0,"Penn State University, 30 E Swedesford Rd, Malvern, PA  19355, United States",Springer-Verlag London Ltd,English,16145046,
Scopus,Towards improvement of analogy-based software development effort estimation: A review,"In this paper a systematic review is conducted to investigate the structure, components, techniques, evaluation procedure, and comparison scope related to prior ABE-based studies. The undeniable role of accurate development effort estimation in the success of software project management has attracted the attention of researchers over the past few years. Among various algorithmic and non-Algorithmic estimation methods, analogy based estimation (ABE) is a widely accepted method due to its simplicity and estimation capability. This paper investigates the improvement process of ABE method during 2000 to 2012. Six research questions are defined to be addressed through evaluation of prior ABE-based studies. The review domain includes 24 papers selected through a tough filtration process. The results show that improvement of ABE can be performed through adjustment, grey theory, attribute weighting and attribute selection techniques. Moreover, ABE configurations can significantly affect the results. © World Scientific Publishing Company.",analogy-based estimation; Soft computing; software development effort; systematic review,"Bardsiri V.K., Jawawi D.N.A., Khatibi E.",2014,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194014500351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928634823&doi=10.1142%2fS0218194014500351&partnerID=40&md5=dd37afdf3f3c3fc5593c10fd566854fe,"Department of Software Engineering, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor Bahru, Malaysia; Department of Computer Engineering, Bardsir Branch, Islamic Azad University, Kerman, Iran",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Hierarchical cluster generation for software quality: A comparative approach,Clustering is a powerful technique of data mining for extracting useful information from a set of data and classifies the data into several clusters based on similarity of the pattern. This paper presents the quality estimation for students' projects data based on hierarchical clustering and fuzzy clustering using Min-Max method. From the experimental results it is seen the fuzzy clustering and hierarchical clustering technique prove to be useful tools in obtaining clusters which can be meaning fully interpreted.,Cluster generation; Comparative approach; Fuzzy clustering; Hierarchical cluster; Software quality,"Pal J., Bhattacherjee V.",2014,Journal,International Journal of Engineering and Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908889497&partnerID=40&md5=c8dea18603aa241e38a657845ce39120,"Department of CS and E, Birla Institute of Technology, Ranchi, India; Department of CS and E, Birla Institute of Technology, Ranchi, India",Engg Journals Publications,English,23198613,
Scopus,Benchmarking effort estimation models using Archetypal Analysis,"The research on software cost estimation has resulted not only to a large number of prediction methodologies and improvement techniques, but also to numerous methods for evaluating and comparing them. The identification of the best prediction model for a specific dataset is still an open issue since the evaluation of candidate models is essentially a multi-criteria problem. Model comparison usually involves statistical hypothesis tests with respect to a single criterion, while for multiple criteria, aggregating methods are usually employed. In the current study, we investigate the alternative approach of benchmarking, which is different from model comparison. The general idea is first to choose among the competitors few ""reference models"" with special, preferably divergent performance characteristics with respect to multiple criteria and then to examine the placement of all the other models in relation to the reference ones. For solving this problem, we utilize a multivariate statistical method, known as Archetypal Analysis (AA), which provides an appealing and intuitive approach for the identification of the reference models and the subsequent benchmarking of all the competitors. The competitor models are considered as points in a multi-dimensional space, defined by the prediction performance criteria, while AA locates the archetypes, i.e. the reference models which determine the convex hull of the swarm of all points (competitors). Apart from identifying reference models for benchmarking with superior or inferior predictive power according to several accuracy measures, the proposed methodology utilizes the similarity of a subset of models to a ""superior"" archetype in order to provide a mechanism for building ensembles. The proposed methodology is applied to a dataset containing performance measures of seventy five models which were initially trained and tested on 195 Web projects of the TUKUTUKU database. The application illustrates the straightforwardness and the intuitively attractive interpretation of the derived results. Copyright 2014 ACM.",Archetypal analysis; Benchmarking; Effort estimation; Prediction models,"Mittas N., Karpenisi V., Angelis L.",2014,Conference,ACM International Conference Proceeding Series,10.1145/2639490.2639502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905663220&doi=10.1145%2f2639490.2639502&partnerID=40&md5=a4d174badf2d210bee8e70ef2e805805,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece",Association for Computing Machinery,English,,9781450328982
Scopus,An approach for OO software size estimation using Predictive Object Point Metrics,"For estimating software, system size is the main parameter of the system development effort. It affects substantially on accurate estimation of effort of development. The Predictive Object Point (POPs) input gives an estimate of the size of the software for which the estimation is required. POPs are a metric suitable for estimating the size of object oriented software, based on the behaviors that each class is delivering to the system along with top level inputs describing the structure of a system. However there is no real mapping of Source Lines of Code (SLOC) to POPs exists. This paper is an attempt to map the Predictive Object Point Metrics with software size which may help in further prediction of effort. This may also help in estimation of cost as well as schedule measurement of an OO system. The proposed method of mapping between POP and software size has been empirically investigated. KLOC has been estimated in terms of EKLOC through POP count using the linear regression equation. The results are presented here to show that how POP Count may be mapped to corresponding software size (KLOC) of an object oriented system. © 2014 IEEE.",Estimated KLOC; KLOC; Object-Oriented Measurement; Predictive Object Point; SLOC; Software Estimation Technique; Software Metrics; Software Sizing,"Jain S., Yadav V., Singh R.",2014,Conference,"2014 International Conference on Computing for Sustainable Global Development, INDIACom 2014",10.1109/IndiaCom.2014.6828172,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903848746&doi=10.1109%2fIndiaCom.2014.6828172&partnerID=40&md5=8c4b9a8bc975bd024cfdcf89619e4ab4,"Department of Computer Science and Engineering, Kanpur Institute of Technology, Kanpur, India; Computer Science and Engineering Department, Harcourt Butler Technological Institute, Kanpur, India",IEEE Computer Society,English,,9789380544120
Scopus,Combining evidence and meta-analysis in software engineering,"Recently there has been a welcome move to realign software engineering as an evidence-based practice. Many research groups are actively conducting empirical research e.g. to compare different fault prediction models or the value of various architectural patterns. However, this brings some challenges. First, for a particular question, how can we locate all the relevant evidence (primary studies) and make sense of them in an unbiased way. Second, what if some of these primary studies are inconsistent? In which case how do we determine the 'true' answer? To address these challenges, software engineers are looking to other disciplines where the systematic review is normal practice (i.e. systematic, objective, transparent means of locating, evaluating and synthesising evidence to reach some evidence-based answer to a particular question). This chapter examines the history of empirical software engineering, overviews different meta-analysis methods and then describe the process of systematic reviews and conclude with some future directions and challenges for researchers. © Springer-Verlag 2013.",,Shepperd M.,2013,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-36054-1_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893754621&doi=10.1007%2f978-3-642-36054-1_2&partnerID=40&md5=da6034a708fd9e7f31ae6b7e007be007,"Brunel University, London, United Kingdom",,English,03029743,9783642360534
Scopus,Towards apriori and posteriori estimation models for renewable energy projects in software engineering,"Currently, cost estimation for renewable energy projects is one of the important issues in software engineering projects. It has been observed in the industry that cost estimates of renewable energy projects often differ from the final costs by a factor of two or more; such large overestimates impact on process integrity and, ultimately, on final quality. RE development organizations are expected to deliver projects on time and on budget in a context of fixed-price contracts. To meet these market demands, managers must determine well in advance such estimates which called is the apriori cost estimation. These organizations typically use an estimation techniques based either on informal personal or organizational experience, or on aposteriori estimation models developed internally or developed by outside consultancy. This paper collects most of the common used models of the cost estimation models throughout the apriori and aposteriori contexts, and we highlights some of their mixed uses between these estimating models in software engineering development projects for RE and lists some of the estimating techniques used in A priori and A posteriori contexts. © 2013 IEEE.",a posteriori context; a priori context; cost estimation models; renewable energy,"Al-Sarayreh K.T., Meridji K.",2013,Conference,"2013 1st International Conference and Exhibition on the Applications of Information Technology to Renewable Energy Processes and Systems, IT-DREPS 2013",10.1109/IT-DREPS.2013.6588161,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884571787&doi=10.1109%2fIT-DREPS.2013.6588161&partnerID=40&md5=426fdb4b466bc242048e2b46505666f1,"Department of Software Engineering, Hashemite University, Zarqa 13115, Jordan; Department of Software Engineering, University of Petra, Amman 11196, Jordan",,English,,9781479907106
Scopus,FAHSCEP: Fuzzy and analogy based hybrid software cost estimation process,"Estimation models in software engineering are used to predict some important attributes of future entities such as software development effort, software reliability and programmers productivity. Among these models, those estimating software effort have motivated considerable research in recent years. Estimation by analogy is one of the attractive techniques in software effort estimation field. However, the procedure used in estimation by analogy is not yet able to handle correctly categorical data. In this paper, we propose a new approach based on reasoning by analogy, fuzzy logic and linguistic quantifiers to estimate effort when the software project is described either by categorical or numerical data. Fuzzy logic-based cost estimation models are more appropriate when vague and imprecise information is to be accounted for. Fuzzy systems try to behave just like the processes of the brain with a rule base. In our proposed method the effort and the cost can be estimated using the Fuzzy logic based analogy method and the cost can be calculated. The performance of the proposed system is calculated using MARE (Mean Absolute Relative Error). © 2013 Praise Worthy Prize S.r.l. - All rights reserved.",Analogy; COCOMO (COstructive COst Model); Cost Estimation; Effort Estimation; Fuzzy Logic; MARE (Mean Absolute Relative Error),"Shanker Ganesh M.K., Thanushkodi K.",2013,Journal,International Review on Computers and Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884232901&partnerID=40&md5=0f94e0fafe23a29bf6b6d800f209a37d,"Software Engineering, ERP(SAP), ATOS, India; Akshaya College of Engineering and Technology, Coimbatore, India",,English,18286003,
Scopus,A grammatical evolution approach for software effort estimation,"Software effort estimation is an important task within software engineering. It is widely used for planning and monitoring software project development as a means to deliver the product on time and within budget. Several approaches for generating predictive models from collected metrics have been proposed throughout the years. Machine learning algorithms, in particular, have been widely-employed to this task, bearing in mind their capability of providing accurate predictive models for the analysis of project stakeholders. In this paper, we propose a grammatical evolution approach for software metrics estimation. Our novel algorithm, namely SEEGE, is empirically evaluated on public project data sets, and we compare its performance with state-of-the-art machine learning algorithms such as support vector machines for regression and artificial neural networks, and also to popular linear regression. Results show that SEEGE outperforms the other algorithms considering three different evaluation measures, clearly indicating its effectiveness for the effort estimation task. Copyright © 2013 ACM.",Evolutionary algorithms; Genetic programming; Grammatical evolution; Software effort estimation; Software metrics,"Barros R.C., Basgalupp M.P., Cerri R., Da Silva T.S., De Carvalho A.C.P.L.F.",2013,Conference,GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference,10.1145/2463372.2463546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883103940&doi=10.1145%2f2463372.2463546&partnerID=40&md5=10b562cd2ce733e194288be3e66a315e,"Institute of Mathematical Sciences and Computing, University of São Paulo, São Carlos, SP, Brazil; Institute of Science and Technology, Federal University of SP, S. J. dos Campos, SP, Brazil",,English,,9781450319638
Scopus,How to treat timing information for software effort estimation?,"Software development effort estimation is an essential aspect of software project management. An effort estimation model expresses relationships between effort and factors such as organizational and project features (e.g. software functional size, and the programming language used in a project). However, software development practices and tools change over time, to environmental changes. This can affect some relationships assumed in an effort estimation model. A moving windows method (a method for treating the timing information of projects), has thus been proposed for estimation models. The moving windows method uses data from a fixed number of the most recent projects data for model construction. However, it is not clear that moving windows is the best way to handle the timing information in an estimation model. The goal of our research is to determine how best to treat timing information in constructing effort estimation models. To achieve the goal, we compared six different methods (moving windows, dummy variable of moving windows, dummy variables of equal bins, dummy variables of year, year predictor, and serial number) for treating timing data, in terms of estimation accuracy. In the experiment, we use three software development project datasets. We found that moving windows is best when the number of projects included in the dataset is not small, and dummy variable of moving windows is the best when the number is small. Copyright 2013 ACM.",Interaction; Model-based effort estimation; Moving windows; Process changes; Time series,"Tsunoda M., Amasaki S., Lokan C.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2486046.2486051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878471195&doi=10.1145%2f2486046.2486051&partnerID=40&md5=ab4694c142fc4c19841d9de1b7ed4e91,"Toyo University, 2100 Kujirai, Kawagoe, Saitama 350-8585, Japan; Okayama Prefectural University, 111 Kuboki, Soja, Okayama 719-1197, Japan; School of Engineering and IT, UNSWADFA, Canberra, Australia",,English,,9781450320627
Scopus,Using CBR and CART to predict maintainability of relational database-driven software applications,"Background: Relational database-driven software applications have gained significant importance in modern software development. Given that software maintainability is an important quality attribute, predicting these applications' maintainability can provide various benefits to software organizations, such as adopting a defensive design and more informed resource management. Aims: The aim of this paper is to present the results from employing two well-known prediction techniques to estimate the maintainability of relational database-driven applications. Method: Case-based reasoning (CBR) and classification and regression trees (CART) were applied to data gathered on 56 software projects from software companies. The projects concerned development and/or maintenance of relational database-driven applications. Unlike previous studies, all variables (28 independent and 1 dependent) were measured on a 5-point bi-polar scale. Results: Results showed that CBR performed slightly better (at 76.8% correct predictions) in terms of prediction accuracy when compared to CART (67.8%). In addition, the two important predictors identified were documentation quality and understandability of the applications. Conclusions: The results show that CBR can be used by software companies to formalize and improve their process of maintainability prediction. Future work involves gathering more data and also employing other prediction techniques. Copyright 2013 ACM.",Case-based reasoning; Classification trees; Maintainability; Prediction; Relational database-driven software applications,"Riaz M., Mendes E., Tempero E., Sulayman M.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2460999.2461019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877268215&doi=10.1145%2f2460999.2461019&partnerID=40&md5=527732f9fed53581146c04cc664b14b4,"Department of Computer Science, University of Auckland, New Zealand; School of Computing, Blekinge Institute of Technology, Sweden",,English,,9781450318488
Scopus,Vertical software industry evolution: The impact of software costs and limited customer base,"Context: Software systems are commonly used in a variety of industries as a means of automating organizational business processes. Initially, such software is often developed in-house by the vertical organizations possibly with the support of professional IT service providers; however, in many cases, internally developed software is eventually replaced with the software products provided by independent software vendors. These vendors often use license fees to recover their software development investments, as well as to gain some margin. However, if the vendor's customer base for a specific type of software is limited, then either the license fees are too high and hence the customers may prefer to develop the software internally, or the margin has to be decreased. As a result, the market for software products of that type may not materialize. Objective: The paper introduces an analytical model that defines the minimum number of customers that the software vendor should have for its software to be less expensive as compared to the in-house software. Method: Following a conceptual-analytical approach, a model is constructed wherein the minimum number of a vendor's customers is represented as a function of other factors affecting software development costs. This model is verified by applying it to estimate the minimum customer base in the segment of telecommunications billing mediation software. Results: Using the proposed analytical model, the minimum number of customers and the maximum number of software vendors in this segment are evaluated. The obtained results are found to be in line with the information available from a telecommunications software market database. Conclusions: Based on the model, a preliminary conclusion is made that in industries with high software development costs, heterogeneous legacy systems to integrate with, and a limited pool of potential customers, the number of software vendors is unlikely to be significant, and hence the in-house or custom-made software is unlikely to be superseded by the software products. © 2012 Elsevier B.V. All rights reserved.",Customer base; Software development costs; Software industry; Telecommunications OSS/BSS software,"Mazhelis O., Tyrväinen P., Frank L.",2013,Conference,Information and Software Technology,10.1016/j.infsof.2012.10.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875222060&doi=10.1016%2fj.infsof.2012.10.006&partnerID=40&md5=06857cc92df5dbae4a5524df654f01e5,"Department of Computer Science and Information Systems, University of Jyväskylä, P.O. Box 35, FI-40014 Jyväskylä, Finland",,English,09505849,
Scopus,Software cost estimation using Particle Swarm Optimization in the light of Quality Function Deployment technique,"Although software industry has seen a tremendous growth and expansion since its birth, it is continuously facing problems in its evolution. The major challenge for this industry is to produce quality software which is timely designed and build with proper cost estimates. Thus the techniques for controlling the quality and predicting cost of software are in the center of attention for many software firms. In this paper, we have tried to propose a cost estimation model based on Multi-objective Particle Swarm Optimization (MPSO) to tune the parameters of the famous COstructive COst MOdel (COCOMO). This cost estimation model is integrated with Quality Function Deployment (QFD) methodology to assist decision making in software designing and development processes for improving the quality. This unique combination will help the project managers to efficiently plan the overall software development life cycle of the software product. © 2013 IEEE.",,"Kashyap D., Misra A.K.",2013,Conference,"2013 International Conference on Computer Communication and Informatics, ICCCI 2013",10.1109/ICCCI.2013.6466263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874673682&doi=10.1109%2fICCCI.2013.6466263&partnerID=40&md5=e7cd0c782a412264b850e5c21d3260b2,"Department of Computer Science and Engineering, Motilal Nehru National Institute of Technology, Allahabad, India",,English,,9781467329057
Scopus,A metric of software size as a tool for IT governance,"This paper proposes a new metric for software functional size, which is derived from Function Point Analysis (FPA), but overcomes some of its known deficiencies. The statistical results show that the new metric, Functional Elements (EF), and its sub metric, Functional Elements of Transaction (EFt), have higher correlation with the effort in software development than FPA in the context of the analyzed data. The paper illustrates the application of the new metric as a tool to improve IT governance specifically in assessment, monitoring, and giving directions to the software development area. © 2013 IEEE.",Function Points; IT governance; IT performance; Software engineering; Software metrics,"De Castro M.V.B., Hernandes C.A.M.",2013,Conference,"Proceedings - 2013 27th Brazilian Symposium on Software Engineering, SBES 2013",10.1109/SBES.2013.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900335850&doi=10.1109%2fSBES.2013.13&partnerID=40&md5=0e8ac9ad4804d33967cbce0e9fa54be4,"Tribunal de Contas da União (TCU), Brasília, Brazil",IEEE Computer Society,English,,
Scopus,Measurement and assessment of software: State of technology and outlook [Messung und Bewertung von Software: Stand der Technik und Ausblick],[No abstract available],,"Dumke R., Ebert C., Heidrich J., Wille C.",2013,Journal,Informatik-Spektrum,10.1007/s00287-013-0723-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896533824&doi=10.1007%2fs00287-013-0723-y&partnerID=40&md5=98d88e3fa9a4ab777088620bd821f6bb,"Otto-Von-Guericke-Universität Magdeburg, Postfach 4120, 39016 Magdeburg, Germany; Vector Consulting Services Ingersheimer, Straße 24, 70499 Stuttgart, Germany; Fraunhofer IESE, Fraunhofer Platz 1, 67663 Kaiserslautern, Germany; Fachhochschule Bingen, Berlinstr. 109, 55411 Bingen Am Rhein, Germany",Springer Verlag,German,01706012,
Scopus,FHSWebEE: An effort estimation model for web application,"Effort estimation becomes a crucial part in software development process because false effort estimation result can lead to delayed project and affect the success of a project. This research proposed a model of effort estimation for web application developed using object oriented approach. In the proposed model, researcher combined method for functional size measurement of object oriented based web application named OOmFPWeb by Abrahao and web metric by Mendes for web application size measurement. The estimation process was done by using CBR method with considering 3 projects that are most similar to the project being estimated. To evaluate the proposed model, comparison between OOmFPWeb as the only variable that affect web effort estimation, web metric by Mendes and the proposed model were done. The evaluation result showed that effort estimation for web application with the proposed model is better than just using OOmFPWeb or web metric from Mendes. © 2012 Elsevier B.V.",FHSWebEE; Functional size estimation; Hypermedia size estimation; Web effort estimation,"Rosmina, Suharjito",2012,Conference,Procedia Engineering,10.1016/j.proeng.2012.10.067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891754107&doi=10.1016%2fj.proeng.2012.10.067&partnerID=40&md5=3db306e8c8e91c9f3c3cebadd261a286,"Information Technology, Bina Nusantara University, Jl. Kebon Jeruk Raya 27, Jakarta Barat 11530, Indonesia",,English,18777058,
Scopus,Software mining and fault prediction,"Mining software repositories (MSRs) such as source control repositories, bug repositories, deployment logs, and code repositories provide useful patterns for practitioners. Instead of using these repositories as record-keeping ones, we need to transform them into active repositories that can guide the decision processes inside the company. By MSRs with several data mining algorithms, effective software fault prediction models can be built and error-prone modules can be detected prior to the testing phase. We discuss numerous real-world challenges in building accurate fault prediction models and present some solutions to these challenges.© 2012 Wiley Periodicals, Inc.",,Catal C.,2012,Journal,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,10.1002/widm.1067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873116329&doi=10.1002%2fwidm.1067&partnerID=40&md5=460d8d5ad1291f9cadc1c635ea20cbf1,"Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey",,English,19424787,
Scopus,Automated trendline generation for accurate software effort estimation ?,"It is well-known that accurate effort estimation is one of the key factors in deciding the success of a software project. However, as any project manager knows, generating accurate estimates has proven to be extremely difficult in practice. Even well-known estimation techniques such as COCOMO or SLIM are not guaranteed to work all the time. One key issue in estimation is the selection of the appropriate historical project data set as a frame of reference against which the estimation can be generated. In our experience in working with software projects in IBM, we have found this to be the most crucial deciding factor for the success of a software estimate; indeed, choosing the wrong project data set during estimation could be disastrous for the software project in question. This is because the trendlines (charts of effort vis-a-vis size) generated from the historical data determine the estimate for the software project, and wrong trendlines could result in wrong estimates.To that end, in this paper, we present an automated trendline generation technique for improving effort estimation in software projects. Our technique makes use of a novel data structure that we have designed called Estimation Key-Map, which represents project data in a multi-dimensional format. This format enables dynamic analysis and clustering of project data into appropriate subsets that can be selected as historical data for estimation of the software project in question. We present the results of validation of our technique against reported actual data, by evaluating it against a large project data set from IBM; therein, we show how our technique enables the selection of the appropriate trendline, thereby enabling more accurate effort estimates.",Custom Application Development; Software Estimation; Trendlines,"Ponnalagu K., Narendra N.C.",2012,Conference,"SPLASH'12 - Proceedings of the 2012 ACM Conference on Systems, Programming, and Applications: Software for Humanity",10.1145/2384716.2384774,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869784347&doi=10.1145%2f2384716.2384774&partnerID=40&md5=d91d1a3c0f6bea46a86daffa1e70ff09,"IBM Research India, Bangalore, India",,English,,9781450315630
Scopus,ProPRED: A probabilistic model for the prediction of residual defects,"In this paper, we propose ProPRED, a probabilistic model for predicting residual defects based on Bayesian Networks (BN) in the software development lifecycle. With the chain rule for BN, ProPRED can be used to take the evidence of the influential factors to the activities (Analyze and Design, Development, Maintain, and Review and Test) that bring about the defects introduction and removal to reason and predict the probable residual defects. We refine and classify the influential factors to the four basic activities, and construct the ProPRED. Giving a case study, we conclude that the ProPRED improve its performance in reasoning under uncertainty and convenience in decision-making and quality control. © 2012 IEEE.",Bayesian Network; defect prediction method; influential factor; probabilistic model; residual defect,"Ba J., Wu S.",2012,Conference,"Proceedings of 2012 8th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications, MESA 2012",10.1109/MESA.2012.6275569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867458824&doi=10.1109%2fMESA.2012.6275569&partnerID=40&md5=125248190484b950c0fdea77bdf3e762,"School of Computer Science and Technology, Beijing University of Aeronautics and Astronautics, Beijing, China; Department of Technical Support Engineering, Academy of Armored Forces Engineering, Beijing, China",,English,,9781467323475
Scopus,Similarity measurement for data with high-dimensional and mixed feature values through fuzzy clustering,"For data with high-dimensional and mixed feature values, traditional similarity measurement becomes no longer applicable. In this paper, a new similarity measurement is proposed by designing a high dimension FCM clustering algorithm. Firstly, an initialization of ordinal-numerical mappings is given; secondly, new ordinal-numerical mappings are learned from the iterative high dimension FCM clustering algorithm and the clustering effect becomes optimized at the same time; finally, a new similarity measurement for data with high-dimensional and mixed feature values is proposed with the fuzzy partition matrix. Experimental results show that the similarity measurement improves the precision of estimation. © 2012 IEEE.",high dimensionality; nominal feature; ordinal feature; similarity; similarity measurement,"Liu H., Wei R.-X., Jiang G.-P.",2012,Conference,"CSAE 2012 - Proceedings, 2012 IEEE International Conference on Computer Science and Automation Engineering",10.1109/CSAE.2012.6273028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867077802&doi=10.1109%2fCSAE.2012.6273028&partnerID=40&md5=14dc9417c8cb2eaac6d9d75fce21fc04,"Department of Equipment Economics and Management, Naval University of Engineering, Wuhan, China; College of Science, Naval University of Engineering, Wuhan, China",,English,,9781467300865
Scopus,Satisfaction and motivation: IT practitioners' perspective,"Job satisfaction and motivation are traditional areas of organizational psychology research. Nevertheless, within Software Engineering (SE), these concepts continue to be in the research agenda. These concepts are interrelated between them and with other important management aspects within SE: productivity measurement. In this paper, the job satisfaction and the motivation of software development team members are analyzed using a qualitative exploratory approach. Results are presentedunderthe point of view of Maslow's hierarchy of needs and Herzberg 's dual factor theory. Moreover, the factors that participants considered as improvers of their productivity are also analyzed. And also, links between productivity factors and motivation are analyzed. Finally, results points to a high presence of hygienic factors that should be covered if organizations what to improve the job satisfaction of software project team workers. Copyright © 2012, IGI Global.",Job satisfaction; Motivation; Practitioner; Productivity; Software Engineering (SE),Hernández-López A.,2012,Journal,International Journal of Human Capital and Information Technology Professionals,10.4018/jhcitp.2012100104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877296360&doi=10.4018%2fjhcitp.2012100104&partnerID=40&md5=cf0c1ed759bdf5ed15ed2bc3b043fa55,"Computer Science Department, Universidad Carlos III de Madrid, Madrid, Spain",,English,19473478,
Scopus,Quality of estimations - How to assess reliability of cost predictions,"Software Project Cost Prediction is one of the unresolved problems of mankind. While today's civil engineering work is more or less under control, software projects are not. Cost overruns are so frequent that it is wise never trusting any initial cost estimate but take precaution for higher cost. Nevertheless, finance managers need reliable estimates in order to be able to fund software and ICT projects without running risks. Estimates are usually readily available - for instance based on functional size and benchmarking. However, the question how reliable these estimations are is often left out, or answered in a purely statistical manner that gives no clue to practitioners what these overall statistical variations means for them. This paper explains how to make use of Six Sigma's transfer functions that map cost defined by a committee of GUFPI-ISMA onto project cost. Transfer functions reverse the process of estimation: they show how much a project costs under suitable assumptions for the cost drivers. If cost drivers can be measured, and transfer functions can be determined with known accuracy, not only project cost can be predicted but also the range and probability for such cost to occur. © 2012 IEEE.",Cost Drivers; Lean Six Sigma; Project Cost Estimation; Soft Skills; Transfer Function,"Fehlmann T., Kranich E.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900861054&doi=10.1109%2fIWSM-MENSURA.2012.11&partnerID=40&md5=90fab21648eca7c5345f5247927e63cb,"Euro Project Office AG, Zurich, Switzerland; Processes, Quality and IT (PQIT), T-Systems International GmbH, Bonn, Germany",IEEE Computer Society,English,,
Scopus,Assessing modifiability in application services using enterprise architecture models - A case study,"Enterprise architecture has become an established discipline for business and IT management. Architecture models constitute the core of the approach and serve the purpose of making the complexities of the real world understandable and manageable to humans. EA ideally aids the stakeholders of the enterprise to effectively plan, design, document, and communicate IT and business related issues, i.e. they provide decision support for the stakeholders. However, few initiatives explicitly state how one can analyze the EA models in order to aid decision-making. One approach that does focus on analysis is the Enterprise Architecture Modifiability Analysis Tool. This paper suggests changes to this tool and presents a case study in which these have been tested. The results indicate that the changes improved the tool. Also, based on the outcome of the case study further improvement possibilities are suggested. © 2012 Springer-Verlag.",Decision-making; Enterprise architecture; Modeling; Modifiability analysis,"Österlind M., Lagerström R., Rosell P.",2012,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-642-34163-2_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868315586&doi=10.1007%2f978-3-642-34163-2_10&partnerID=40&md5=8fbac5c2587fd6e9b20361aae1df6faa,"Industrial Information and Control Systems, KTH, Royal Institute of Technology, Osquldas väg 10, 10044 Stockholm, Sweden",Springer Verlag,English,18651348,9783642341625
Scopus,Development of intelligent effort estimation model based on fuzzy logic using Bayesian networks,"Accuracy gain in the software estimation is constantly being sought by researchers. On the same time new techniques and methodologies are being employed for getting capability of intelligence and prediction in estimation models. Today the target of estimation research is not only the achievement of accuracy but also fusion of different technologies and introduction of new factors. In this paper we advise improvement in some existing work by introducing mechanism of gaining accuracy. The paper focuses on method for tuning the fuzziness function and fuzziness value. This document proposes a research for development of intelligent Bayesian Network which can be used independently to calculate the estimated effort for software development, uncertainty, fuzziness and effort estimation. The comparison of relative error and magnitude relative error bias helps the selection of parameters of fuzzy function; however the process can be repeated n-times to get suitable accuracy. We also present an example of fuzzy set development for ISBSG data set in order to elaborate working of proposed system. © 2011 Springer-Verlag.",Bayesian networks; Effort estimation; ISBSG data set; New development; Re development; statistical analysis,"Khan J., Shaikh Z.A., Nauman A.B.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-27207-3_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755225179&doi=10.1007%2f978-3-642-27207-3_9&partnerID=40&md5=3281a2346da405d1a5680b716b46eefd,"Department of Computer Science and IT, Sarhad University of Science and Information Technology, Peshawar 25000, Pakistan; Department of Computer Science, FAST-National University of Computer and Emerging Sciences, Karachi, Pakistan",,English,18650929,9783642272066
Scopus,Applying systems engineering modeling language(sysml) to system effort estimation utilizing use case points,"This paper proposes that as model based systems engineering begins to play a larger role in system development it may be utilized in estimating systems effort, making model based systems engineering more valuable to system engineering. This paper proposes to extend the Use Case Points (UCP) estimating approach, used by some software engineers, to SysML for estimating the systems engineering effort. The paper reviews literature that encourages the notion of a UCP method for Systems Engineering (UCPSE). Finally the paper maps out the research plan to develop and validate the UCPSE method.©2011 by Mary A. Bone. Published and used by INCOSE with permission.",,"Bone M.A., Cloutier R.",2011,Conference,"21st Annual International Symposium of the International Council on Systems Engineering, INCOSE 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879312164&partnerID=40&md5=2d5c2d445a868b20a2d3c980468a2551,"Stevens Institute of Technology, Castle Point on Hudson, Hoboken, NJ 07030, United States; Stevens Institute of Technology, Castle Point on Hudson, Hoboken, NJ 07030, United States",,English,,9781618391155
Scopus,Genetic algorithm for optimizing neural network based software cost estimation,"Software engineering cost models and estimation techniques are used for number of purposes. These include budgeting, tradeoff and risk analysis, project planning and control, software improvement and investment analysis. The proposed work uses neural network based estimation, which is essentially a machine learning approach, is one of the most popular techniques. In this paper the author has proposed a 2 step process for software effort prediction. In first phase known as training phase neural network selects the matching class (datasets) for the given input, which is improved by optimizing the parameters of each individual dataset by Genetic algorithm. In second step known as testing phase, the prediction process is done by adaptive neural networks. The proposed method uses COCOMO-II as base model. The experimental results show that our method could significantly improve prediction accuracy of conventional Artificial Neural Networks (ANN) and has potential to become an effective method for software cost estimation. © 2011 Springer-Verlag.",and COCOMO-II; ANN; BP-Learning; Genetic algorithm; Software cost estimation,"Benala T.R., Dehuri S., Satapathy S.C., Raghavi Ch.S.",2011,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-27172-4_29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555196719&doi=10.1007%2f978-3-642-27172-4_29&partnerID=40&md5=90c29c88c011343e2da1f02ec303b195,"Anil Neerukonda Institute of Technology and Sciences, Sangivalasa, Visakhapatnam, Andhra Pradesh, India; Department of Information and Communication Technology, Fakir Mohan University, Vyasa Vihar, Balasore-756019, India",,English,03029743,9783642271717
Scopus,Monte Carlo simulation based estimations: Case from a global outsourcing company,Effort estimation of various software project life cycle phases have been traditionally performed using deterministic methods. Deterministic methods fail to account for the variability inherent in the software engineering processes deployed in executing the project. We have developed stochastic notations and models that can represent these process variations. We have also constructed a Monte Carlo simulation tool that works using the stochastic model we have developed for software development projects. This tool has been tested in a mid-sized software services outsourcing (SSO) organization. The tool and results are described and reported in this paper. © 2011 IEEE.,,"Dasgupta J., Sahoo G., Mohanty R.P.",2011,Conference,"Proceedings of the 1st International Technology Management Conference, ITMC 2011",10.1109/ITMC.2011.5996034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053006003&doi=10.1109%2fITMC.2011.5996034&partnerID=40&md5=b46b22132e500e7c46c5d8f0da36045e,"Computer Society, Patni Computer Systems Limited, Airoli, Navi Mumbai 400 708, India; Department of Information Technology, Birla Institute of Technology, Mesra, Ranchi 835 215, India; ITM Group of Institutions, B.S.E.L. Tech. Park, 704-712, Vashi, Navi Mumbai 400 705, India",,English,,9781612849522
Scopus,Software effort estimation: Harmonizing algorithms and domain knowledge in an integrated data mining approach,"Software development effort estimation is important for quality management in the software development industry, yet its automation still remains a challenging issue. Applying machine learning algorithms alone often cannot achieve satisfactory results. This paper presents an integrated data mining framework that incorporates domain knowledge into a series of data analysis and modeling processes, including visualization, feature selection, and model validation. An empirical study on the software effort estimation problem using a benchmark dataset shows the necessity and effectiveness of the proposed approach. Copyright © 2011, IGI Global.",Data analysis; Data mining; Domain knowledge; Modelling; Software effort estimation,"Deng J.D., Purvis M.K., Purvis M.A.",2011,Review,International Journal of Intelligent Information Technologies,10.4018/jiit.2011070104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052861775&doi=10.4018%2fjiit.2011070104&partnerID=40&md5=7e1c349c6d9b93aef0d841e154a2bc20,"University of Otago, New Zealand",,English,15483657,
Scopus,Defect Cost Flow Model - A Bayesian network for predicting defect correction effort,"Background. Software defect prediction has been one of the central topics of software engineering. Predicted defect counts have been used mainly to assess software quality and estimate the defect correction effort (DCE). However, in many cases these defect counts are not good indicators for DCE. Therefore, in this study DCE has been modeled from a different perspective. Defects originating from various development phases have different impact on the overall DCE, especially defects shifting from one phase to another. To reduce the DCE of a software product it is important to assess every development phase along with its specific characteristics and focus on the shift of defects over phases. Aims. The aim of this paper is to build a model for effort prediction at different development stages. Our model is mainly focused on a dynamic DCE changing from one development phase to another. It reflects the increasing cost of correcting defects which are introduced in early, but found in later development phases. Research Method. The modeling technique used in this study is a Bayesian network which, among many others, has three important capabilities: reflecting causal relationships, combining expert knowledge with empirical data and incorporating uncertainty. The procedure of model development contains a set of iterations including the following steps: problem analysis, data analysis, model enhancement with simulation runs and model validation. Results. The developed Defect Cost Flow Model (DCFM) reflects the widely used V-model, an international standard for developing information technology systems. It has been pre-calibrated with empirical data from past projects developed at Robert Bosch GmbH. The analysis of evaluation scenarios confirms that DCFM correctly incorporates known qualitative and quantitative relationships. Because of its causal structure it can be used intuitively by end-users. Conclusion. Typical cost benefit optimization strategies regarding the optimal effort spent on quality measures tend to optimize locally, e.g. every development phase is optimized separately in its own domain. In contrast to that, the DCFM demonstrates that even cost intensive quality measures pay off when the overall DCE of specific features is considered.",Bayesian network; Correction effort; Decision support; Defect Flow; Process modeling; Software process,"Schulz T., Radlinski Ł., Gorges T., Rosenstiel W.",2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868353,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649855353&doi=10.1145%2f1868328.1868353&partnerID=40&md5=208b415fdbd4f684f8ce15e19871c17c,"Robert Bosch GmbH, CC/ESR2, Postfach 16 61, 71226 Leonberg, Germany; University of Szczecin, ul. Mickiewicza 64, 71-101 Szczecin, Poland; Eberhard Karls Universität Tübingen, Sand 13, B 207, 72076 Tübingen, Germany",,English,,9781450304047
Scopus,Modeling the relationship between software effort and size using Deming regression,"Background: The relation between software effort and size has been modeled in literature as exponential, in the sense that the natural logarithm of effort is expressed as a linear function of the logarithm of size. The common approach to estimate the parameters of the linear model is ordinary least squares regression which has been extensively applied to various datasets. The least squares estimation takes into account only the error arising from the dependent variable (effort), while the measurement of independent variable (size) is considered free of errors. Aims: The basis of the study is that in practice the assumption of measuring the size without error is hardly true, since the size of a software project depends on the precision of the tool of measurement and often by the subjectivity of the rater. Moreover, the sizes of projects comprising a dataset have been measured by different measurement tools and this adds another source of variability in the independent variable. Method: In this paper, we consider a regression technique, known as Deming regression, which takes into account the error in measurement of the independent variable, the size. Deming regression is applied to four publically available datasets in order to model the linear relationship between effort and size and to compare it with ordinary least squares. Results: Accuracy measures of fitting (MAE, MdAE, MMRE, MdMRE, pred25) are improved by the Deming regression. Comparison of Absolute Errors (AE) by the Wilcoxon test shows significant difference at <0.001 level of significance. Conclusions: Deming regression is appropriate for datasets where the size is subject to measurement error. However some assumptions on the variances of the measurement errors are arbitrary and need to be studied. Further work is needed for using the Deming regression for effort prediction.",Deming regression; Error-in-variables model; Ordinary least squares regression; Software cost estimation,"Mittas N., Kosti M.V., Argyropoulou V., Angelis L.",2010,Conference,ACM International Conference Proceeding Series,10.1145/1868328.1868339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649770315&doi=10.1145%2f1868328.1868339&partnerID=40&md5=6dcf06450bc0e5f048d1992d3e80743d,"Department of Informatics, Aristotle University, 54124 Thessaloniki, Greece",,English,,9781450304047
Scopus,Applying neural networks to software reliability assessment,"We adapt concepts from the field of neural networks to assess the reliability of software, employing cumulative failures, reliability, remaining failures, and time to failure metrics. In addition, the risk of not achieving reliability, remaining failure, and time to failure goals are assessed. The purpose of the assessment is to compare a criterion, derived from a neural network model, for estimating the parameters of software reliability metrics, with the method of maximum likelihood estimation. To our surprise the neural network method proved superior for all the reliability metrics that were assessed by virtue of yielding lower prediction error and risk. We also found that considerable adaptation of the neural network model was necessary to be meaningful for our application only inputs, functions, neurons, weights, activation units, and outputs were required to characterize our application. © 2010 World Scientific Publishing Company.",neural network; reliability prediction; Software reliability; software testing,Schneidewind N.,2010,Journal,"International Journal of Reliability, Quality and Safety Engineering",10.1142/S0218539310003834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249289151&doi=10.1142%2fS0218539310003834&partnerID=40&md5=a1eafa68d15157a3d21552d07d7e0c2e,"Department of Information Science, Graduate School of Operation and Information Sciences, Monterey, CA, United States",,English,02185393,
Scopus,Two novel effort estimation models based on quality metrics in Web projects,"Web development projects are certainly different from traditional software development projects and, hence, require differently tailored measures for accurate effort estimation. Effort estimation accuracy will affect the availability of resource allocation and task scheduling. In this paper, we investigate the suitability of a newly proposed quality metrics and models to estimate the effort and duration for small or medium-size Web development projects. It then describes a new size metrics, presents two novel methods (WEBMO+ and VPM+), based on WEB model (WEBMO) using Web objects instead of SLOC and Vector Prediction Model (VPM), to fast estimate the development effort of Web-based information systems. We also empirically validate the approach with a four projects study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life cycle to within +/-20 percent across a range of application types. In contrast with other existing methods, WEBMO+ and VPM+ uses raw historical information about development capability and high granularity information about the system to be developed, in order to carry out such estimations. This method is simple and specially suited for small or medium-size Web based information systems.",Effort Estimation Method; Sizing Metric; Web Engineering; Web Quality; Web-based Metrics,"Lazić L., Mastorakis N.E.",2010,Journal,WSEAS Transactions on Information Science and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955440090&partnerID=40&md5=1a8626944ee2c092bd8170563fe64c32,"Department for Mathematics and Informatics, State University of Novi Pazar, Serbia; Technical University of Sofia, English Language Faculty of Engineering Industrial Engineering, Sofia, Bulgaria",,English,17900832,
Scopus,Using Support Vector Regression for web development effort estimation,"The objective of this paper is to investigate the use of Support Vector Regression (SVR) for Web development effort estimation when using a crosscompany data set. Four kernels of SVR were used, linear, polynomial, Gaussian and sigmoid and two preprocessing strategies of the variables were applied, namely normalization and logarithmic. The hold-out validation process was carried out for all the eight configurations using a training set and a validation set from the Tukutuku data set. Our results suggest that the predictions obtained with linear kernel applying a logarithmic transformation of variables (LinLog) are significantly better than those obtained with the other configurations. In addition, SVR has been compared with the traditional estimation techniques, such as Manual StepWise Regression, Case-Based Reasoning, and Bayesian Networks. Our results suggest that SVR with LinLog configuration can provide significantly superior prediction accuracy than other techniques. © Springer-Verlag Berlin Heidelberg 2009.",Effort estimation; Empirical web engineering; Support vector regression,"Corazza A., Di Martino S., Ferrucci F., Gravino C., Mendes E.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650663568&doi=10.1007%2f978-3-642-05415-0_19&partnerID=40&md5=fb5338ac74d0cd533a2a831a8714812f,"University of Napoli Federico II, Via Cinthia, I, 80126 Napoli, Italy; University of Salerno, Via Ponte Don Melillo, I, 84084 Fisciano (SA), Italy; University of Auckland, 92019 Auckland, New Zealand",,English,03029743,3642054145; 9783642054143
Scopus,Software estimation: Universal models or multiple models?,"In the field, there is a very large diversity of development processes in use, and various mixes of costs drivers, each with a different impact depending on the context. The classical approach to building estimation models in software engineering is to build a single estimation model and include within it as many cost factors (i.e. independent variables) as possible. In this paper, we do not postulate that there exists a single estimation model that is ideal in all circumstances, but rather we report on exploratory research conducted over the past few years looking at relevant concepts from the field of economics and from discussions with organizations attempting to understand the data that they have collected on their projects. The purpose of exploratory research is not to demonstrate a hypothesis, but to identify new potentially relevant concepts to develop hypotheses to be tested later on with empirical or experimental data.",Estimation; Process measurement; Productivity measurement,"Abran A., Cuadrado Gallego J.J.",2009,Conference,"Proceedings of the 21st International Conference on Software Engineering and Knowledge Engineering, SEKE 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149323426&partnerID=40&md5=cdfde82b457856708d449a15352b5be8,"École de Technologies Supérieure, Université du Québec, Montréal, QC, Canada; Universidad de Alcalà de Henarès, Madrid, Spain",,English,,1891706241; 9781891706240
Scopus,Gauging the differences between expectation and systems support: The managerial approach of adaptive and perfective software maintenance,"Differences in the type and nature of tasks in software development and maintenance require a careful selection of separate methods and procedures to handle each task. A great deal of academic and practical attention has been devoted to studying methods of developing software; considerably less attention has been devoted to studying the management of software once implemented. Also, current research has been focused on the technical side of the software maintenance (e.g., development of models and tools) and completely ignores the managerial side. This research gap initiates the knowledge niche for researchers and practitioners to further investigate the management issues in the software maintenance process. In this paper, we assimilate previous research in the software maintenance management area and address three major management challenges associated with the software maintenance. © 2009 IEEE.",Maintenance process; Software maintenance management,"Rashid A., Wang W.Y.C., Dorner D.",2009,Conference,"4th International Conference on Cooperation and Promotion of Information Resources in Science and Technology, COINFO 2009",10.1109/COINFO.2009.53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951464716&doi=10.1109%2fCOINFO.2009.53&partnerID=40&md5=dce336fa091655cb68b312b616113157,"School of Information Management, Victoria University of Wellington, PO Box 600, Wellington 6015, New Zealand",,English,,9780769538983
Scopus,An empirical study on the use of web-cobra and web objects to estimate web application development effort,"We have performed a replication of a previous study in order to further assess the effectiveness of Web-COBRA method, with the Web Objects measure, in predicting Web application development effort. The results of the empirical analysis confirm the interesting results of the previous study. © 2009 Springer Berlin Heidelberg.",Effort estimation method; Web applications; Web-COBRA,"Di Martino S., Ferrucci F., Gravino C.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-02818-2_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350239874&doi=10.1007%2f978-3-642-02818-2_17&partnerID=40&md5=9d283c177c798c9e639797fd074db5e1,"University of Napoli Federico II, Via Cinthia, I-80126 Napoli, Italy; University of Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy",,English,03029743,3642028179; 9783642028175
Scopus,Measures and techniques for effort estimation of web applications: An empirical study based on a single-company dataset,"Effort estimation is a key management activity which goes on throughout a software project being fundamental for accurate project planning and for allocating resources adequately. Thus, it is important to identify techniques and measures that can support such project management activity during the development of Web applications. To this aim, empirical investigations should be performed using data coming from the industrial world. To address this issue, this paper reports on an empirical study based on data from 15 Web applications developed by an Italian software company. The objective of the study was two-fold. The first goal was to verify whether or not some size measures were good indicators of the effort spent to develop the Web applications taken into account. The second goal was to compare the effectiveness of some techniques to establish the relationships between the employed size measures and the development effort of the Web applications. The measures were organized in two sets, where the first one included some length measures while the second one consisted of the nine components which are used to estimate the Web Objects measure. The techniques taken into account were Stepwise Regression, Case- Based Reasoning, and Regression Tree. The results indicated that both the sets of size measures were good indicators of the effort for the analyzed dataset. Furthermore, the analysis also revealed that the first set presented significantly superior performance than the second set when using Stepwise Regression. No significant differences between the two sets of size measures were highlighted when using Case-Based Reasoning and Regression Tree. © Rinton Press.",Effort estimation; Empirical validation communicated by: D. Lowe & O. Pastor; Size measures; Web applications,"Martino S.D.I., Ferrucci F., Gravino C., Mendes E.",2009,Journal,Journal of Web Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549099732&partnerID=40&md5=39f2dc7fbab7ee76c7b63bc2757e360c,"Dipartimento di Matematica e Informatica, University of Salerno, Italy; University of Auckland, Private Bag 92019, New Zealand",,English,15409589,
Scopus,Function point size estimation for object oriented software based on use case model,"Precise size estimation earlier in the software development life cycle has always been a challenge for the software industry. In the context of object oriented software, Use Case Model (UCM) is widely used to capture the functionality addressed in the software. Existing size estimation techniques such as use case points and use case size points do not adhere to any standard. Consequently, lots of variations are possible, leading to inaccurate size estimation. On the other hand, Function Point Analysis (FPA) has been standardized. However, the current estimation approaches based on FPA employ object modeling that happens later in the software development life cycle rather than the UCM. In order to gain the advantages of FPA as well as UCM, this paper proposes a new approach for size estimation of object oriented software. This approach is based on the UCM by adapting it to FPA. Mapping rules are proposed for proper identification and classification of various components from UCM to FPA. Estimation results obtained using the proposed approach are compared with those using finer granular level object model which adapts FPA at design phase. The close agreement between these two results indicates that the proposed approach is suitable for accurate software size estimation earlier in the software development life cycle.",Function point; Object model; Object-oriented software; Use case model,"Chamundeswari A., Babu C.",2008,Conference,ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649155775&partnerID=40&md5=28aac6980a8e85e1b5815afdabf2f5b0,"SSN College of Engineering, Rajiv Gandhi Salai, SSN Nagar - 603110, India",,English,,9789898111524
Scopus,Software estimation tool based on three -layer model for software engineering metrics,"In today's competitive world of software, cost estimation of software plays a major role because it determines the effort, time, quality etc. Lines of codes and Function Point are traditional method for estimating the various software metrics. But in today's world of object oriented programming Function Point calculation for C++, Java is a big challenge. We have developed a tool based on function point analysis which has three layer architecture model to calculate various software metrics project, especially for Java program. This tool has been used for estimating of various projects developed by students. Finally as a result we have shown the attributes of three layer models i.e. FP, LOC, productivity and Time etc.",Function point; Line of codes; Three layer model,"Gupta D., Kaushal S.J., Sadiq M.",2008,Conference,"Proceedings of the 4th IEEE International Conference on Management of Innovation and Technology, ICMIT",10.1109/ICMIT.2008.4654437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749156979&doi=10.1109%2fICMIT.2008.4654437&partnerID=40&md5=5c6c25af7719ce97c98b30c878269c93,"Department of Computer Engineering, Delhi College of Engineering, Delhi, India; Department of Computer Engineering, Jamia Millia Islamia, Delhi, India",,English,,9781424423309
Scopus,Patterns for improving the pragmatic quality of Web information systems,"The significance of approaching Web information systems (WIS) from an engineering viewpoint is emphasized. A methodology for deploying patterns as means for improving the quality of WIS as perceived by their stakeholders is presented. In doing so, relevant quality attributes and corresponding stakeholder types are identified. The role of a process, feasibility issues, and the challenges in making optimal use of patterns are pointed out. Examples illustrating the use of patterns during macro- and micro-architecture design of a WIS, with the purpose of the improvement of quality attributes, are given. © 2008, IGI Global.",,Kamthan P.,2008,Book Chapter,Handbook of Research on Web Information Systems Quality,10.4018/978-1-59904-847-5.ch003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48949113814&doi=10.4018%2f978-1-59904-847-5.ch003&partnerID=40&md5=298fe8f173afb03400efaf4149302f0e,"Concordia University, Canada",IGI Global,English,,9781599048475
Scopus,Application of genetic programming in software engineering empirical data modelling,"Research in software engineering data analysis has only recently incorporated computational intelligence methodologies. Among these approaches, genetic programming retains a remarkable position, facilitating symbolic regression tasks. In this paper, we demonstrate the effectiveness of the genetic programming paradigm, in two major software engineering duties, effort estimation and defect prediction. We examine data domains from both the commercial and the scientific sector, for each task. The proposed model is proved superior to past literature works.",Data mining; Defect prediction; Effort estimation; Genetic programming; Software engineering,"Tsakonas A., Dounias G.",2008,Conference,ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849120898&partnerID=40&md5=5c7c6066ade74017e49f855665febfd8,"Department of Financial and Management Engineering, University of the Aegean, 31 Fostini St., Chios, Greece",,English,,9789898111517
Scopus,A model for software rework reduction through a combination of anomaly metrics,"Analysis of anomalies reported during testing of a project can tell a lot about how well the processes and products work. Still, organizations rarely use anomaly reports for more than progress tracking although projects commonly spend a significant part of the development time on finding and correcting faults. This paper presents an anomaly metrics model that organizations can use for identifying improvements in the development process, i.e. to reduce the cost and lead-time spent on rework-related activities and to improve the quality of the delivered product. The model is the result of a four year research project performed at Ericsson. © 2008 Elsevier Inc. All rights reserved.",Defects; Faults; Process improvement; Software metrics; Testing strategies,"Damm L.-O., Lundberg L., Wohlin C.",2008,Journal,Journal of Systems and Software,10.1016/j.jss.2008.01.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52049096177&doi=10.1016%2fj.jss.2008.01.017&partnerID=40&md5=093d8e87de88fc27e4d8a56ac38d60d2,"Ericsson AB, Ölandsgatan 1 -3, P.O. Box 518, 37123 Karlskrona, Sweden; School of Engineering, Blekinge Institute of Technology, Sweden",,English,01641212,
Scopus,A perspective on software engineering education with open source software,"As the development and use of open source software (OSS) becomes prominent, the issue of its outreach in an educational context arises. The practices fundamental to software engineering, including those related to management, process, and workflow deliverables, are examined in light of OSS. Based on a pragmatic framework, the prospects of integrating OSS in a traditional software engineering curriculum are outlined, and concerns in realizing them are given. In doing so, the cases of the adoption of an OSS process model, the use of OSS as a computer-aided software engineering (CASE) tool, OSS as a standalone subsystem, and open source code reuse are considered. The role of openly accessible content in general is discussed briefly. © 2007, IGI Global.",,Kamthan P.,2007,Book Chapter,"Handbook of Research on Open Source Software: Technological, Economic, and Social Perspectives",10.4018/978-1-59140-999-1.ch054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898180231&doi=10.4018%2f978-1-59140-999-1.ch054&partnerID=40&md5=2941cd1eb4d0a2b8b417eafb8a0a86b7,"Concordia University, Canada",IGI Global,English,,9781591409991
Scopus,A framework for improving effort management in software projects,"Rapid changes in information technology (IT) set challenges to software project effort estimation. Besides effort estimation, software projects involve various other effort-related functions which have an effect on the effort estimation. Some of these functions are required by the capability maturity models such as Capability Maturity Model Integration (CMMI). However, both the capability maturity model requirements and research on software project effort is very fragmented. This article proposes a framework for improving effort management in software projects. The framework assists both the total management and, on the other hand, concentration on selected areas of effort-related functions. The framework has been applied at TietoEnator Telecom & Media. Copyright © 2007 John Wiley & Sons, Ltd.",Effort management; Framework; Project management; SPI,Haapio T.,2007,Conference,Software Process Improvement and Practice,10.1002/spip.350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37749032247&doi=10.1002%2fspip.350&partnerID=40&md5=28dc67b3eb3549e8de9583287a01f2c0,"TietoEnator Telecom. and Media, FI-70601 Kuopio, Finland; TietoEnator Telecom. and Media, P.O. Box 1779, FI-70601 Kuopio, Finland",,English,10774866,
Scopus,Design optimization metrics for UML based object-oriented systems,"Design plays a key role in the development of software. The quality of design is crucial and is a fundamental decision element in assessing the software product. The early availability of design quality evaluation provides a better way to decide the quality of the final product. This avoids presumption in the quality evaluation process. Hence Software Metrics provide a valuable and objective insight of enhancing each of the software quality characteristics. This paper proposes a quality model to assess the design phase of any object-oriented system based on the works of Chidamber, Kemrer and Basili and suggests two new metrics. The research focuses on analyzing a set of metrics, which has direct influence on the quality of the software and creating a metrics tool based on Java that can be used to validate the object-oriented projects against these metrics. The analysis is carried out on a set of real world projects designed using Unified Modeling Language, which are used as test cases. These metrics and models are proposed to add more quality information in refining any object-oriented system during the early stages of design itself. © World Scientific Publishing Company.",Class; Cohesion; Coupling; Design phase metrics; Inheritance; Methods; Object; Object-oriented programming; Response for class; UML; Weighted method for a class,"Ramaraj E., Duraisamy S.",2007,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194007003252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547414449&doi=10.1142%2fS0218194007003252&partnerID=40&md5=d0a53366a9e9c89f748f5cfca26b3955,"Department of Computer Science and Engineering, Alagappa University, Karaikudi, Tamil Nadu, India; Department of Computer Applications, Sri Krishna College of Engineering and Technology, Coimbatore, Tamil Nadu, India",,English,02181940,
Scopus,Development of an imputation technique - INI for software metric database with incomplete data,"Software metrics are numerical data that provides a quantitative basis for the development and validation of models, and effective measurement of the software development process. Gathering software engineering data can be expensive. Such precious and costly data cannot afford to be missing. However missing data is a common problem and software engineering database is not an exception. Though there are many algorithms to solve problem of incomplete data, unfortunately few have been developed in the field of Software Engineering. Missing data causes significant problem. With inaccurate data or missing data, it is very difficult to know how much a project will cost or worth. Missing data leads to loss of information, causes biasness in data analysis and hence results to inaccurate decision-making for project management and implementation. In this paper, an imputation technique for imputing missing data based on global-local Modified Singular Value Decomposition (MSVD) algorithm, INI was proposed. This technique was used for estimating missing data in a software engineering database (PROMISE). Its performance was evaluated and compared with two existing imputation techniques, Expectation Maximization (EM) and Mean Imputation (MI). Varying percentages of missings, (1%, 10%, 15%, and 20% 25%) were introduced in the original dataset in order to have an incomplete dataset for imputation. Simulations were carried for comparative purposes. Imputation Error (IE) was use as an evaluation criterion. Study results showed that, the only method that consistently outperformed other methods (EM and MI), guarantee a higher accuracy of imputed data, prompt and less bias at all level of missings is the global-local MSVD, INI. It maintained consistency at all level of missings compared to EM and MI. It was found that EM is not suitable for data with missing proportion greater than 20%. While MI lost in all count to EM and INI. © 2006 IEEE.",Data imputation; k-NN; Missing data; MSVD based imputation; Software engineering database,"Olanrewaju R.F., Ito W.",2006,Conference,"SCOReD 2006 - Proceedings of 2006 4th Student Conference on Research and Development ""Towards Enhancing Research Excellence in the Region""",10.1109/SCORED.2006.4339312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46849122876&doi=10.1109%2fSCORED.2006.4339312&partnerID=40&md5=c201fdb87762786e1c3125454b08c19b,"Dept. of Electrical and Computer Engineering, Kulliyyah of Engineering, International Islamic University, Jalan Gombak, 50728 Kuala Lumpur, Malaysia",,English,,1424405270; 9781424405275
Scopus,Fuzzy logic systems for software development effort estimation based upon clustering of programs segmented by personal practices,"The most common application of software metrics is to develop models that predict the effort that will be required to complete certain stages of a software development. There are two main stages for using an estimation model (1) it must be determined whether the model is adequate to describe the observed (actual) data, that is, the model adequacy checking; if it resulted adequate then (2) the estimation model is validated in its environment using new data. In this paper, an investigation aimed to compare personal Fuzzy Logic Systems (FLS) with linear regression is presented. The evaluation criteria is based upon ANOVA of MRE and MER, as well as MMRE, MMER and pred(25). Ninety-nine programs were developed by eighteen programmers. From these programs four FLS are generated for estimating the effort of sixty programs developed by a group of ten developers. Results show that a FLS can be used as an alternative for estimating the development effort at personal level. © 2006 IEEE.",Clustering of pprograms; Correlation; Fuzzy logic; Linear regression; Software effort estimation,"Lopez-Martin C., Yañez-Marquez C., Gutierrez-Tornes A.",2006,Conference,"Proceedings - Electronics, Robotics and Automotive Mechanics Conference, CERMA 2006",10.1109/CERMA.2006.114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547473732&doi=10.1109%2fCERMA.2006.114&partnerID=40&md5=be1b4e82b72da7c1f17dd905b5ed5cd8,"Center for Computing Research, National Polytechnic Institute, Mexico; Banamex, Mexico, D.F., Mexico; ITESM, Mexico",,English,,0769525695; 9780769525693
Scopus,An unadjusted size measurement of embedded software system families and its validation,"Embedded software systems have become the driving force in many areas of technology, like the automotive industry. Functions for the control of cars, driver assistance as well as systems for information and entertainment are accomplished by software driven control units. Owing to the high complexity and development effort of embedded systems, these resources have to be reused. Software system families (SSF) are a promising solution to achieve cost reduction by reusing common software assets in different variants of an automobile. To support the economic management of this developmental approach, we need software metrics to estimate the effort of building embedded software system families. Techniques of size measurement and cost estimation for software system families are highly insufficient, in general, and do not exist for the automotive domain. Therefore, this article describes a conglomeration of innovative metrics to measure the size of a system family oriented software development. These size metrics analyze a real-time and a process focused perspective of embedded software system families in the automotive domain. A combination of both viewpoints describes the unadjusted size of software driven control units to indicate and estimate their development costs. Copyright © 2006 John Wiley & Sons, Ltd.",Embedded systems; Measurement; Metrics; Process; Software system families,"Kiebusch S., Franczyk B., Speck A.",2006,Conference,Software Process Improvement and Practice,10.1002/spip.285,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747117093&doi=10.1002%2fspip.285&partnerID=40&md5=195cdf7f0b6e203cc23b8e1a1a68a977,"Information Systems Institute, Faculty of Economics and Management, University of Leipzig, Germany; Commercial Information Systems, Faculty of Economics and Business Administration, University of Jena, Germany",,English,10774866,
Scopus,The quality of design team factors on software effort estimation,"Over the past ten couple of years, there is a variety of effort models proposed by academicians and practitioners at early stage of software development life cycle. Some addressed that efforts could be predicted using Lines of Codes(LOC) and COCOMO, others emphasized that it could be made using Function Point Analysis(FPA) or others. The study seeks to develop a model that estimates software effort by studying and analyzing small and medium scale application software. To develop such a model, 50 completed software projects are collected from a software company. With the sample data, design team factors are identified and extracted. By applying them to simple regression analyses, a prediction of software of effort estimates with accuracy of MMRE = 9% was constructed. The results give several benefits. First, the estimation problems are minimized due to the simple procedure used in identifying those factors. Second, the predicted software projects are only limited to a specific environment rather than being based upon industry environment. We believe the accuracy of effort estimates can be improved. According to the results analyzed, the work shows that it is possible to build up simple and useful prediction model based on data extracted at the early stage of software development life cycle. We hope this model can provide valuable ideas and suggestions for project designers for planning and controlling software projects in near future. © 2006 IEEE.",Application software; Design team factors; Effort estimation; Software projects,Wu S.I.K.,2006,Conference,"2006 IEEE International Conference on Service Operations and Logistics, and Informatics, SOLI 2006",10.1109/SOLI.2006.237145,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048930810&doi=10.1109%2fSOLI.2006.237145&partnerID=40&md5=4682b9462e2ebee499350ae17f7c01f4,"Faculty of Business Administration, University of Macau, Macau, Macau",IEEE Computer Society,English,,1424403189; 9781424403189
Scopus,A real time measure of software system families,"Software systems with inherent real time characteristics have become the driving force in many areas of technology like the automotive sector. Control functions of cars, driver assistance as well as systems for information and entertainment are accomplished by software driven control units. Due to the high complexity and development effort of real time systems, these resources have to be reused. Software system families are a promising solution to gain a cost reduction by reusing common software assets in different variants of an automobile. To support the economic management of this development approach we need software metrics to estimate the effort of building embedded software system families. Techniques of size measurement and cost estimation for software system families are highly insufficient in general and do not exist for the automotive domain. Therefore this article describes a conglomerate of innovative metrics to analyze a realtime perspective of embedded software system families in the automotive domain. These size metrics calculate an unadjusted measure of software driven control units to indicate and estimate their development costs. © 2005 ACM.",,"Kiebusch S., Franczyk B., Speck A.",2005,Conference,Proceedings - International Conference on Software Engineering,10.1145/1083292.1083307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885897472&doi=10.1145%2f1083292.1083307&partnerID=40&md5=8fcbd24fee7fdd26729f5459dbaba121,"Information Systems Institute Faculty of Economics and Management, University of Leipzig, Germany; Commercial Information Systems, Faculty of Economics and Business Administration, University of Jena, Germany",,English,02705257,1595931228; 9781595931221
Scopus,Survival analysis for the duration of software projects,"In the area of software engineering various methods have been proposed in order to predict the cost of a software project in terms of the effort or of the productivity, An important feature which is closely related to the cost is the duration of a software project. In this paper we deal with the problem of studying and modeling the distribution of the time from specification until delivery of a software product. Specifically, we investigate the use of a statistical methodology known from biostatistics as survival analysis. The purpose of such an analysis is to describe the distribution of the duration and also to identify important factors that affect it. The great advantage of survival analysis is that we can utilize information not only from the completed projects in a dataset but also from ongoing projects. The general principles of the methodology are described with examples from applications to known data sets. © 2005 IEEE.",,"Sentas P., Angelis L.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749070787&doi=10.1109%2fMETRICS.2005.45&partnerID=40&md5=525212a04ecae581535656faab770369,"Department of Informatics, Aristotle University of Thesaloniki, 54124, Greece",,English,15301435,0769523714; 9780769523712
Scopus,Empirical studies of software cost estimation: Training of effort estimation uncertainty assessment skills,"This research abstract describes my proposed doctorial work within the field of software project cost and effort estimation. My work focuses on assessment of uncertainty of software development cost or effort estimates. In particular, the work focuses on to which degree this assessment is a skill that can be improved with better training. Work completed includes one small scale experiment with student participants. A follow-up, larger, experiment with professional software developers is currently in progress. Studies targeted towards better understanding of the mental processes of development of work effort estimates and uncertainty assessments will be the next step. This work aims at the development of effective training processes of estimation and uncertainty assessment skills. Through the METRICS05 dissertation forum I hope to receive feedback on the viability and relevance of the proposed work within the software cost and effort estimation field. © 2005 IEEE.",,Gruschke T.,2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749068440&doi=10.1109%2fMETRICS.2005.19&partnerID=40&md5=75c3bf6c33fca5db20b0856bb52898db,"University of Oslo, Norway",,English,15301435,0769523714; 9780769523712
Scopus,Using use case models to generate object points,"Project managers are required to provide size and effort estimates at several milestones throughout the software development life cycle (SDLC), which is most challenging in the early stages when no detailed information is readily available about the project. Object points (OPs) is one of the size metrics that can be used the early prototyping stages of software development. Use case (UC) elicitation and modeling technique has been extensively used, despite its deficiencies, to model and specify system requirements and system prototypes. A UC meta-model is proposed to overcome most of the UC model's deficiencies. Project managers will now be supported with a new method that utilizes the proposed UC meta-model so as to bridge the gap between UC models and the OPs counting technique. This method automatically counts the system OPs from UC models at early stages of the SDLC. Finally, the initial quantitative results of this method confirm our hypothesis which believes that the more detailed a UC model is, the more accurate the derived OPs count.",Object points; Software cost estimation; Software metrics; Use case model,"Issa A., Odeh M., Coward D.",2005,Conference,"Proceedings of the IASTED International Conference on Software Engineering: part of the 23rd IASTED International Multi-Conference on Applied Informatics, SE 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746223229&partnerID=40&md5=5165d209a9e4bea44c515af4817fae84,"Centre for Complex Cooperative Systems, CEMS Faculty, University of West of England (UWE), Coldharbour Lane, Frenchay, Bristol BS16 1QY, United Kingdom",,English,,0889864640; 9780889864641
Scopus,Component integration metrics,"We propose two suite of metrics in this paper for the measurement of the complexity and integration difficulty in Component Based Software Engineering (CBSE). The metrics accommodate both static and dynamic aspects of the problem. Static metrics measure complexity and criticality of integrated components. Complexity metrics, namely Component Packing Density and Component interaction Density, are collected during the design phase. Dynamic metrics namely, number of cycles and average active components are gathered after the first version of running the software. Both static and dynamic metrics are evaluated using Weyuker's set of properties. The paper also shows how these metrics could be profitably used over real systems.",CBSE; Component Integration; Dynamic Metrics; Static Metrics,"Narasimhan V.L., Hendradjaya B.",2004,Conference,"Proceedings of the International Conference on Software Engineering Research and Practice, SERP'04",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344282057&partnerID=40&md5=cfa22bd6aed9b0947edede0cfa04b93b,"Fac. of Elec. Eng. and Comp. Science, University of Newcastle, NSW 2308, Australia",,English,,1932415300; 9781932415308
Scopus,Measuring requirement evolution - A case study in the E-commerce domain,"Although requirement evolution is a widely recognized phenomenon, there are only a few approaches for measuring it. These existing approaches are based on the assumption that all the requirements exist and can be seen in the requirement elicitation and analysis phases. They do not include provisions for the emergence during systems development of new requirements, which cannot be anticipated in the requirement elicitation and analysis phase. This paper shows how the concept of requirements creep is adopted for the measurement of emergent requirement evolution. We use a case study in the E-commerce domain to illustrate the use of this measure in the prediction of systems development. The findings of this study suggest that requirement evolution can be measured in a practical software project, and the findings demonstrate the strong influence of requirements creep on the systems development effort. The findings of our study encourage us to undertake further studies involving other organizations and projects.",Requirement evolution; Requirements creep; Systems development prediction,Ovaska P.,2004,Conference,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8444235943&partnerID=40&md5=d93d790f644a16117e5428daa56cee67,"Lappeenranta Univ. of Technology, P.O. BOX 20, FIN-53851, Lappeenranta, Finland",,English,,9728865007
Scopus,A COSMIC-FFP based method to estimate web application development effort,"In the paper we address the problem of estimating the effort required to develop dynamic web applications. In particular, we provide an adaptation of the Cosmic Full Function Point method to be applied on design documents for counting data movements. We also describe the empirical analysis carried out to verify the usefulness of the method for predicting web application development effort. © Springer-Verlag Berlin Heidelberg 2004.",,"Costagliola G., Ferrucci F., Gravino C., Tortora G., Vitiello G.",2004,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-27834-4_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944392849&doi=10.1007%2f978-3-540-27834-4_20&partnerID=40&md5=ea8d40812be39176c1cae70557960179,"Dipartimento di Matematica e Informatica, Università degli Studi di Salerno, Via Ponte Don Melillo, Fisciano, SA  84084, Italy",Springer Verlag,English,03029743,3540225110
Scopus,Design Defect trigger for software process improvement,"This research is intended to develop the empirical relationship between defects and their causes to estimate. Also, using defect cause, we understand associated relation between defects and design defect trigger. So when we archive resemblant project, we can forecast defect and prepare to solve defect by using defect trigger. © Springer-Verlag 2004.",COQUALMO; Defect; Defect cause prioritization; Defect reduction; Defect trigger; GQM method,"Lee E., Lee K.W., Lee K.",2004,Journal,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-24675-6_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048857954&doi=10.1007%2f978-3-540-24675-6_16&partnerID=40&md5=8df51f7981eb27613c71e1910a0da704,"School of Computer Science and Engineering, Chung-Ang University, South Korea; Department of Computer Science, School of Engineering, University of Southern California, United States",Springer Verlag,English,03029743,3540219757; 9783540219750
Scopus,Developing analogy cost estimates for space missions,"The analogy approach in cost estimation combines actual cost data from similar existing systems, activities, or items with adjustments for a new project's technical, physical or programmatic differences to derive a cost estimate for the new system. This method is normally used early in a project cycle when there is insufficient design/cost data to use as a basis for (or insufficient time to perform) a detailed engineering cost estimate. The major limitation of this method is that it relies on the judgment and experience of the analyst/estimator. The analyst must ensure that the best analogy or analogies have been selected, and that appropriate adjustments have been made. While analogy costing is common, there is a dearth of advice in the literature on the ""adjustment methodology"", especially for hardware projects. This paper discusses some potential approaches that can improve rigor and repeatability in the analogy costing process. Copyright © 2004 by the American Institute of Aeronautics and Astronautics, Inc.",,Shishko R.,2004,Conference,A Collection of Technical Papers - AIAA Space 2004 Conference and Exposition,10.2514/6.2004-6012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18844390425&doi=10.2514%2f6.2004-6012&partnerID=40&md5=1ae3f8f209210c5599faaa03430696eb,"Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109, United States; Mission and System Architecture Section, Caltech/JPL, M/S 301-180, 4800 Oak Grove Drive, Pasadena, CA 91109, United States",American Institute of Aeronautics and Astronautics Inc.,English,,1563477203; 9781563477201
Scopus,Quantitative metrics for risk assessment in software projects,"This paper introduces a set of metrics for four indicators of the risk in an evolutionary software project. The indicators are requirements volatility, organization efficiency, product complexity, and technology maturity. The proposed metrics can be obtained early in the development process. These metrics are objective and derived. They accommodate the changes in requirements, process, and resources of a project. They will be the basis to build a formal risk assessment model that will make different program managers derive the same projections on the same software project.",Formal Models; Project Management; Risk Assessment; Software Metrics,"Luqi, Zhang L., Berzins V.",2003,Conference,Proceedings of the IASTED International Conference on Software Engineering and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542748781&partnerID=40&md5=6f4809de67f9cd5a3c55744af128464e,"Naval Postgraduate School, Software Eng. Automation Center, 833 Dyer Road, Monterey, CA 93943, United States",,English,,0889863946
Scopus,Application of a revised DIT metric to redesign an OO design,"In this paper, we continue a series of papers that discuss specific design metrics [Alkadi 1999] Alkadi 2000] [Alkadi 2001] [Alkadi 1998]. The design metric discussed in this paper is the Depth of Inheritance [DIT] metric. Design evaluation is a recurring step that should be performed and checked multiple times before committing to the final design implementation. Metrics are utilized to evaluate inheritance and reuse in order to take into account the greater number of abstraction levels inherent in object-oriented systems. Furthermore, they facilitate the designers to address cost estimation and product quality across all life-cycle stages of developing the final product.",,"Alkadi G., Alkadi I.",2003,Journal,Journal of Object Technology,10.5381/jot.2003.2.3.a3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042637506&doi=10.5381%2fjot.2003.2.3.a3&partnerID=40&md5=5c35b10382187ee3c93536bc47363b30,"Southeastern Louisiana University, United States; University of Louisiana, Lafayette, United States",Journal of Object Technology,English,16601769,
Scopus,On prediction of cost and duration for risky software projects based on risk questionnaire,"This paper proposes a new approach that can discriminate risky software development projects from smoothly or satisfactorily going projects and give explanation for the risk. We have already developed a logistic regression model which prediets whether a project becomes risky or not. However, the model returned the decision with the calculated probability only. Additionally, a formula was constructed based on the risk questionnaire which includes 23 questions. We therefore try to-improve the previous method with respect to accountability and feasibility. In new approach, we firstly construct a new risk questionnaire including only 9 questions (or risk factors) Q1, Q2,. ., Q9, each of which concerns with the project management. We then apply multiple regression analysis to actual project data PJ1, PJ2,. . ., PJ32, and clarify a set of factors which contributes essentially to estimate the relative cost error and the relative duration error, respectively. We then apply the constructed formulas to another project data PJ33, PJ34,. . ., PJ40. The analysis results show that both the cost and duration of risky projects are estimated fairly well by the formulas. We can thus confirm that our new approach is applicable to software development projects in order to discriminate risky projects from appropriate projects and give reasonable explanations for the risk. © 2001 IEEE.",Cost and duration; Questionnaire; Regression analysis; Software risk management,"Mizuno O., Adachi T., Kikuno T., Takagi Y.",2001,Conference,"Proceedings - 2nd Asia-Pacific Conference on Quality Software, APAQS 2001",10.1109/APAQS.2001.990010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749078943&doi=10.1109%2fAPAQS.2001.990010&partnerID=40&md5=6c13af381ec3617cb386ca07d80c3118,"Dept. of Informatics and Mathematical Science, Graduate School of Engineering Science, Osaka University, Japan; Social Systems Business Company, OMRON Corporation, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,0769512879; 9780769512877
Scopus,Coherence equals cohesion - Or does it?,"Traditionally, cohesion of a software component is evaluated as the similarity of its constituent parts. It may be more appropriate to measure functional coherence, or coherence of usage of a component, by analyzing external usage patterns of component's clients. We define an appropriate measure, starting from a generic system model and its associated mechanism for calculating object sizes. We show that the new measure satisfies some, but not all, of the properties that a cohesion measure is expected to satisfy. We also provide some intuitive examples to illustrate the concept and its possible uses. © 2000 IEEE.",Coherence; Computer industry; Information management; Object oriented modeling; Size measurement; Software engineering; Software measurement; Software quality; Technology management; Water,Mišić V.B.,2000,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2000.896735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002968528&doi=10.1109%2fAPSEC.2000.896735&partnerID=40&md5=7e86ef8583313937535a293eadbdbc31,"Department of Information and Systems Management, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",IEEE Computer Society,English,15301362,0769509150
Scopus,Undersizing software systems: Third versus fourth generation software development,"A critical component of Information Systems (IS) project management is completing projects on time; however, most software development efforts have completion time overruns. This study examines the prior research in IS project management for assessing the completion time for new software development projects and discusses a project management phenomenon where the expected time for project completion is under- sized. At one firm two groups of software development projects are examined: (1)Cobol projects (third generation) and (2) Natural (fourth generation). It was found that both the Cobol and Natural projects experienced similar overruns. Undersizing is posited as the explanation. © 1998 Operational Research Society Ltd.",,"Lind M.R., Sulek J.M.",1998,Journal,European Journal of Information Systems,10.1057/palgrave.ejis.3000308,https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444456001&doi=10.1057%2fpalgrave.ejis.3000308&partnerID=40&md5=f1d925c29d0e4bdd0ac35e7b80652065,"School of Business and Economics, NCA&T State University, Merrick Hall, Greensboro, NC, 27411, United States",,English,0960085X,
Scopus,A combination of the Mk-II Function Points software estimation method with the ADISSA methodology for systems analysis and design,"We combine the Mk-II Function Points method for software estimation with the ADISSA (architectural design of information systems based on structural analysis) methodology for systems analysis and design. The combined method, which is supported by a software tool, the ADISSA Estimator, permits us to estimate software metrics such as size, effort (man-months) and duration (elapsed time) at the early stages of systems development. This is accomplished by basing the estimations on the products of a thorough system analysis and design process. © 1997 Elsevier Science B.V.",Adissa methodology; Function points analysis; Mk-II FP model; Project management; Software estimation; Software metrics; Systems analysis and design,"Shoval P., Feldman O.",1997,Journal,Information and Software Technology,10.1016/s0950-5849(97)00009-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031373528&doi=10.1016%2fs0950-5849%2897%2900009-8&partnerID=40&md5=54b7d86adfe0b2770bc42a4f3bdd43a4,"Information Systems Program, Dept. of Indust. Eng. and Management, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel; Dept. of Math. and Computer Science, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel",Elsevier,English,09505849,
Scopus,An integrated methodology for knowledge-based system development,"This article details a rigorous development methodology for knowledge-based systems. This rigorous methodology is itself embedded within a multilevel process model for software development. The rigorous methodology is designed to utilize a set of formal or rigorous specifications in a composite style. These specifications detail areas like the knowledge base, the human-computer interface, and the representation, linked together through a process of representation refinement. The rigorous methodology aims at combining the aspects of knowledge engineering, cognitive engineering, and software engineering as they relate to knowledge-based systems. The knowledge-based systems development methodology is embedded withia two-level life-cycle model. The two levels are termed the macro- and microlevels. The macrolevel is used to understand the impact that those factors external to the actual system development have upon the system's creation and life cycle. These external factors include such influences as changes in technology and corporate planning. The microlevel is a process model that utilizes techniques from total quality management, measurement theory, and cost estimation, among others, to assist the software developer in producing software through a process of never-ending quality improvement. All of these techniques are utilized and complimentary to each other. The aim of having two levels is to allow the developer to focus upon each item separately but to understand the factors upon which the factor's development rests and its impact upon the other subprocesses. © 1994.",,"Plant R.T., Tsoumpas P.",1994,Journal,Expert Systems With Applications,10.1016/0957-4174(94)90042-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028417023&doi=10.1016%2f0957-4174%2894%2990042-6&partnerID=40&md5=1b50aa46bd199e1da346eeeddceca60f,"Department of Computer Information Systems, University of Miami, Coral Gables, FL, United States",,English,09574174,
Scopus,"Output measurement metrics in an object-oriented computer aided software engineering (CASE) environment: Critique, evaluation and proposal","Output measurement metrics for the software development process need to be re-examined to determine their performance in the new, radically changed CASE development environment. This paper critiques and empirically evaluates several approaches to the measurement of outputs from the CASE process. The primary metric evaluated is the function points method developed by Albrecht. A second metric tested is a short-form variation of function points that is easier and quicker to calculate. We also propose a new output metric called object points and a related short-form, which are specialized for output measurement in object-oriented CASE environments that include a central object repository. These metrics are proposed as more intuitive and lower cost approaches to measuring the CASE outputs. Our preliminary results show that these metrics have the potential to yield as accurate, if not better, estimates than function points-based measures. © 1991 IEEE.",,"Banker R.D., Kauffman R.J., Kumar R.",1991,Conference,Proceedings of the Annual Hawaii International Conference on System Sciences,10.1109/HICSS.1991.184122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907816772&doi=10.1109%2fHICSS.1991.184122&partnerID=40&md5=d9e88f93acf25c97eacb1bc0c867916c,"Carlson School of Business, University of Minnesota, United States; Stern School of Business, New York University, United States",IEEE Computer Society,English,15301605,
Scopus,Just-in-time customer churn prediction in the telecommunication sector,"Due to the exponential growth in technologies and a greater number of competitors in the telecom sector, the companies are facing a rigorous problem of customer churns. The customer churn is a phenomenon that highlights the customer’s intention who may switch from a certain service or even the service provider company. Many customer churn prediction (CCP) techniques are developed by academics and practitioners to handle the customer churn in order to resolve the problems pertaining to customer retention. However, CCP is not widely studied in the scenario where the company is not having enough historical data due to either been a newly established company or due to the recent start of a new technology or even because of the loss of the historical data. The just-in-time (JIT) approach can be a more practical alternative to address this issue as compared to state-of-the-art CCP techniques. Unfortunately, similar to traditional churn prediction models, JIT also requires enough historical data. To address this gap in the traditional CCP models, this study uses the cross-company data, i.e., data from another company, in the context of JIT for addressing CCP problems in the telecom sector. We empirically evaluated the performance of the proposed model using publicly available datasets of two telecom companies. It is found from the empirical evaluation that in the JIT-CCP context: (i) it is possible to evaluate the performance of the predictive model using cross-company dataset for training purposes and (ii) it is evident that heterogeneous ensemble-based JIT-CCP model is more suitable approach to use as compared to individual classifier or homogeneous ensemble-based technique. © 2017, Springer Science+Business Media, LLC.",Classification; Cross-company; Customer churn prediction; Heterogeneous ensemble; Homogeneous ensemble; Just-in-time,"Amin A., Al-Obeidat F., Shah B., Tae M.A., Khan C., Durrani H.U.R., Anwar S.",2020,Journal,Journal of Supercomputing,10.1007/s11227-017-2149-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031425074&doi=10.1007%2fs11227-017-2149-9&partnerID=40&md5=0faffe168ce25a0b213deca27aa95c47,"Institute of Management Sciences, Peshawar, 25000, Pakistan; College of Technological Innovation, Zayed University, Abu Dhabi, 144534, United Arab Emirates",Springer,English,09208542,
Scopus,Role of Soft Computing Techniques in Software Effort Estimation: An Analytical Study,"In developing software, software effort estimation plays an important role in the success of a software project. Inaccurate, inconsistent, and unreliable estimation of a software leads to failure. Because of various special specifications and changes in the requirements, accurate Software Effort Estimation (SEE) for developing software is a difficult task. This software effort estimation must be calculated effectively to avoid unforeseen results. At early development stages, these inabilities to maintain certainty, inaccurate, unreliability are the limitations of expert judgment and algorithmic effort estimation models. After that, attention was turned to machine learning and soft computing methods. Soft computing is an association with the methodologies centering on fuzzy logic, artificial neural networks, and evolutionary computation. These methods will provide flexible information processing capability for handling real-life situations. The main aim of the study is to provide an in-depth review of software effort estimation from the initial stages that included expert judgment-based SEE to the latest techniques of soft computing. © 2020, Springer Nature Singapore Pte Ltd.",Artificial neural network; Evolution computation; Expert judgment; Fuzzy logic; Particle swarm optimization; Soft computing; Software effort estimation,"Suresh Kumar P., Behera H.S.",2020,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-13-9042-5_70,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077132403&doi=10.1007%2f978-981-13-9042-5_70&partnerID=40&md5=8c2106df92884c6dd95a7c19a90a1169,"Department of Information Technology, Veer Surendra Sai University of Technology, Burla, 768018, India",Springer,English,21945357,9789811390418
Scopus,Decision Trees Based Software Development Effort Estimation: A Systematic Mapping Study,"The decision tree (DT) represents a nonparametric estimation method that has been mostly used for both classification and regression problems. DTs were adopted for software development effort estimation (SDEE) generally for their simplicity of use and interpretation contrary to other learning methods. Nevertheless, to our self-knowledge, no systematic mapping has been devoted especially to decision trees. The aim of this study is to elaborate a systematic mapping study that classifies DTs papers in conformity with the succeeding criteria: research approach, contribution type, techniques employed in combination with DT methods besides identifying publication channels and trends. An automated search of five digital libraries was made to carry out a systematic mapping of DT studies mainly devoted to SDEE that were published in the period 1985-2017. We identify 46 relevant studies. Basically, the results revealed that most researchers focus on technique contribution type. In addition, the majority of papers deal with improving the existing DT models while few studies have proposed novel models to improve the reliability of SDEE. Furthermore, solution proposal and case study are the most frequently used approaches. © 2019 IEEE.",Decision Tree; Regression Tree; Software Development Effort Estimation; Systematic Mapping Study,"Najm A., Zakrani A., Marzak A.",2019,Conference,"Proceedings of 2019 International Conference of Computer Science and Renewable Energies, ICCSRE 2019",10.1109/ICCSRE.2019.8807544,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072027113&doi=10.1109%2fICCSRE.2019.8807544&partnerID=40&md5=4c630bfa2bb9e3bc150c36afc883be2a,"Department of Mathematics Computer Sciences, FSB m'Sik, Hassan II University, Casablanca, Morocco; Department of Industrial Engineering, ENSAM, Casablanca, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781728108278
Scopus,"Understanding Error Rates in Software Engineering: Conceptual, Empirical, and Experimental Approaches","Software-intensive systems are ubiquitous in the industrialized world. The reliability of software has implications for how we understand scientific knowledge produced using software-intensive systems and for our understanding of the ethical and political status of technology. The reliability of a software system is largely determined by the distribution of errors and by the consequences of those errors in the usage of that system. We select a taxonomy of software error types from the literature on empirically observed software errors and compare that taxonomy to Giuseppe Primiero’s Minds and Machines 24: 249–273, (2014) taxonomy of error in information systems. Because Primiero’s taxonomy is articulated in terms of a coherent, explicit model of computation and is more fine-grained than the empirical taxonomy we select, we might expect Primiero’s taxonomy to provide insights into how to reduce the frequency of software error better than the empirical taxonomy. Whether using one software error taxonomy can help to reduce the frequency of software errors better than another taxonomy is ultimately an empirical question. © 2019, Springer Nature B.V.",Computer science education; Error; Philosophy of software engineering; Software error,"Horner J.K., Symons J.",2019,Note,Philosophy and Technology,10.1007/s13347-019-00342-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066491842&doi=10.1007%2fs13347-019-00342-1&partnerID=40&md5=ba39831bb09056b360bac02c5152e229,"University of Kansas, Lawrence, United States",Springer Netherlands,English,22105433,
Scopus,Evaluating filter fuzzy analogy homogenous ensembles for software development effort estimation,"Researchers have developed and evaluated many techniques to deliver accurate estimates of the effort required to complete a new software program. Among these, analogy has emerged as a very promising technique, in particular the fuzzy analogy estimation technique that uses the fuzzy logic concepts in order to deal with both categorical and numerical data. The aim of this paper is twofold: (1) evaluate the impact of 3 filters on the predictive ability of single and ensemble fuzzy analogy techniques and (2) assess whether filters could be a source of diversity for fuzzy analogy ensembles. Moreover, it compares the filter single and ensemble fuzzy analogy techniques with fuzzy analogy ensembles built without using feature selection over 6 datasets. The overall results suggest that (1) more accurate estimates are generated when filters were used with single and ensemble fuzzy analogy techniques, (2) filter single fuzzy analogy techniques outperformed filter fuzzy analogy ensembles, and (3) fuzzy analogy ensembles without feature selection were more accurate than filter single and ensemble techniques. Therefore, though the use of feature selection techniques led single and ensemble fuzzy analogy to generate accurate estimations, they failed to be a source of diversity for fuzzy analogy ensembles. Hence, constructing fuzzy analogy homogenous ensembles that combine single fuzzy analogy techniques with different parameter configurations still generate better accuracy than filter fuzzy analogy ensembles. However, further empirical evaluations of filter/wrappers fuzzy analogy ensembles are required in order to confirm or refute these findings. © 2018 John Wiley & Sons, Ltd.",ensemble; feature selection; filter; fuzzy analogy; software development effort estimation,"Hosni M., Idri A., Abran A.",2019,Journal,Journal of Software: Evolution and Process,10.1002/smr.2117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061585023&doi=10.1002%2fsmr.2117&partnerID=40&md5=72983b70d540ccb14da59885e96cc4ab,"Software Project Management Research Team, ENSIAS, University Mohammed V, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",John Wiley and Sons Ltd,English,20477481,
Scopus,Improving case based software effort estimation using a multi-criteria decision technique,"Producing an accurate effort estimate is essential for effective software project management, and yet remains highly challenging and difficult to achieve, especially at the early stage of software development, because very little detail about the project are known at its beginning. To cope with this challenge, we present a novel framework for software effort estimation, which takes an incremental approach on one hand, using a case-based reasoning (CBR) model, while considering a comprehensive set of different types of requirements models on the other hand, including functional requirements (FRs), non-functional requirements (NFRs), and domain properties (DPs). Concerning the use of CBR, this framework offers a multi-criteria technique for enhancing the accuracy of similarity measures among cases of multiple past projects that are similar to the current software project, towards determining and selecting the most similar one. We have tested our proposed framework on 36 (students’) projects and the results are very encouraging, in the sense that the difference between the estimated effort and the actual effort was lower than 10% in most cases. © 2019, Springer International Publishing AG, part of Springer Nature.",Case based reasoning (CBR); FRs (Functional requirements); Multi-criteria decision analysis (MCDA); NFRs (Non-functional requirements); Software effort estimation,"Fellir F., Nafil K., Touahni R., Chung L.",2019,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-319-91186-1_46,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047956575&doi=10.1007%2f978-3-319-91186-1_46&partnerID=40&md5=90f86a0fec53e593e1b5302314993fd4,"Lastid Laboratory, Faculty of Sciences, Ibn Tofail University, Kenitra, Morocco; Mohamed V University ENSIAS, Rabat, Morocco; Erik Johnson School of Engineering and Computer Science, The University of Texas at Dallas, P.O. Box 830688, Richardson, TX  75083-0688, United States",Springer Verlag,English,21945357,9783319911854
Scopus,Periodic developer metrics in software defect prediction,"Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: File level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction. © 2018 IEEE.",Churn metrics; Code ownership; Periodic developer experience; Software defect prediction,"Ozcan Kini S., Tosun A.",2018,Conference,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",10.1109/SCAM.2018.00016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058291691&doi=10.1109%2fSCAM.2018.00016&partnerID=40&md5=ca9631fabb9e1f2f30795b362d4f07b7,"Faculty of Computer and Informatics Engineering, Istanbul Technical University, Istanbul, Turkey",Institute of Electrical and Electronics Engineers Inc.,English,,9781538682906
Scopus,Systematic literature review on software effort estimation using machine learning approaches,"Accurate effort estimation is amongst the key activities in the software project development. It directly impacts the time and cost of the software projects. This paper presents a systematic literature review of software effort estimation techniques using machine learning. This review presents a discussion about the research trends in machine learning inspired software effort estimation. The results of the systematic review has concluded prominent trends of machine learning approaches, size metrics, benchmark datasets, validation methods etc. used for software effort estimation. © 2017 IEEE.",Effort estimation; Machine learning approaches; Systematic Literature Review,"Sharma P., Singh J.",2018,Conference,"Proceedings - 2017 International Conference on Next Generation Computing and Information Systems, ICNGCIS 2017",10.1109/ICNGCIS.2017.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057722756&doi=10.1109%2fICNGCIS.2017.33&partnerID=40&md5=82fc82cd4e3d3d5324b5d8ce4ac0967c,"Computer Science and Engineering, Chitkara University, Rajpura, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538642054
Scopus,Optimizing time and effort parameters of COCOMO II using fuzzy Multi-objective Particle Swarm Optimization,"Estimating the efforts, costs, and schedules of software projects is a frequent challenge to software development projects. A bad estimation will result in bad management of a project. Various models of estimation have been defined to complete this estimate. The Constructive Cost Model II (COCOMO II) is one of the most famous models as a model for estimating efforts, costs, and schedules. To estimate the effort, cost, and schedule in project of software, the COCOMO II uses inputs: Effort Multiplier (EM), Scale Factor (SF), and Source Line of Code (SLOC). Evidently, this model is still lack in terms of accuracy rates in both efforts estimated and time of development. In this paper, we introduced to use Gaussian Membership Function (GMF) of Fuzzy Logic and Multi-Objective Particle Swarm Optimization (MOPSO) method to calibrate and optimize the parameters of COCOMO II. It is to achieve a new level of accuracy better on COCOMO II. The Nasa93 dataset is used to implement the method proposed. The experimental results of the method proposed have reduced the error downto 11.89% and 8.08% compared to the original COCOMO II. This method proposed has achieved better results than previous studies. © 2018 Universitas Ahmad Dahlan.",COCOMO II; Fuzzy logic; Multi-objective PSO; Software effort estimation,"Langsari K., Sarno R., Sholiq",2018,Journal,Telkomnika (Telecommunication Computing Electronics and Control),10.12928/TELKOMNIKA.v16i5.9698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059349798&doi=10.12928%2fTELKOMNIKA.v16i5.9698&partnerID=40&md5=c00ec654bdfbbc483e510bd5d46c9324,"Fatoni University, Thailand; Department of Informatics, Institut Teknologi Sepuluh Nopember, Indonesia; Department of Information Systems, Institut Teknologi Sepuluh Nopember, Indonesia",Universitas Ahmad Dahlan,English,16936930,
Scopus,Optimizing effort parameter of COCOMO II using Particle Swarm Optimization method,"Estimating the effort and cost of software is an important activity for software project managers. A poor estimate (overestimates or underestimates) will result in poor software project management. To handle this problem, many researchers have proposed various models for estimating software cost. Constructive Cost Model II (COCOMO II) is one of the best known and widely used models for estimating software costs. To estimate the cost of a software project, the COCOMO II model uses software size, cost drivers, scale factors as inputs. However, this model is still lacking in terms of accuracy. To improve the accuracy of COCOMO II model, this study examines the effect of the cost factor and scale factor in improving the accuracy of effort estimation. In this study, we initialized using Particle Swarm Optimization (PSO) to optimize the parameters in a model of COCOMO II. The method proposed is implemented using the Turkish Software Industry dataset which has 12 data items. The method can handle improper and uncertain inputs efficiently, as well as improves the reliability of software effort. The experiment results by MMRE were 34.1939%, indicating better high accuracy and significantly minimizing error 698.9461% and 104.876%. © 2018 Universitas Ahmad Dahlan.",COCOMO II; Estimation of software effort; Particle swarm optimization,"Langsari K., Sarno R., Sholiq",2018,Journal,Telkomnika (Telecommunication Computing Electronics and Control),10.12928/TELKOMNIKA.v16i5.9703,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059344661&doi=10.12928%2fTELKOMNIKA.v16i5.9703&partnerID=40&md5=c85d899ba4207d19cec30a2abfc472ab,"Fatoni University, Thailand; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",Universitas Ahmad Dahlan,English,16936930,
Scopus,Re-estimating software effort using prior phase efforts and data mining techniques,"Software effort estimation has played an important role in software project management. An accurate estimation helps reduce cost overrun and the eventual project failure. Unfortunately, many existing estimation techniques rely on the total project effort which is often determined from the project life cycle. As the project moves on, the course of action deviates from what originally has planned, despite close monitoring and control. This leads to re-estimating software effort so as to improve project operating costs and budgeting. Recent research endeavors attempt to explore phase level estimation that uses known information from prior development phases to predict effort of the next phase by using different learning techniques. This study aims to investigate the influence of preprocessing in prior phases on learning techniques to re-estimate the effort of next phase. The proposed re-estimation approach preprocesses prior phase effort by means of statistical techniques to select a set of input features for learning which in turn are exploited to generate the estimation models. These models are then used to re-estimate next phase effort by using four processing steps, namely data transformation, outlier detection, feature selection, and learning. An empirical study is conducted on 440 estimation models being generated from combinations of techniques on 5 data transformation, 5 outlier detection, 5 feature selection, and 5 learning techniques. The experimental results show that suitable preprocessing is significantly useful for building proper learning techniques to boosting re-estimation accuracy. However, there is no one learning technique that can outperform other techniques over all phases. The proposed re-estimation approach yields more accurate estimation than proportion-based estimation approach. It is envisioned that the proposed re-estimation approach can facilitate researchers and project managers on re-estimating software effort so as to finish the project on time and within the allotted budget. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Data transformation; Feature selection; Learning; Outlier detection; Prior phase effort; Re-estimating software effort,"Jodpimai P., Sophatsathit P., Lursinsap C.",2018,Journal,Innovations in Systems and Software Engineering,10.1007/s11334-018-0311-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051060621&doi=10.1007%2fs11334-018-0311-z&partnerID=40&md5=38b8658435aa607f8ca008e4f0494ddd,"Department of Mathematics and Computer Science, Chulalongkorn University, Patumwan, Bangkok, 10330, Thailand",Springer London,English,16145046,
Scopus,Threats to validity in search-based predictive modelling for software engineering,"A number of studies in the literature have developed effective models to address prediction tasks related to a software product such as estimating its development effort, or its change/defect proneness. These predictions are critical as they help in identifying weak areas of a software product and thus guide software project managers in effective allocation of project resources to these weak parts. Such practices assure good quality software products. Recently, the use of search-based approaches (SBAs) for developing software prediction models (SPMs) has been successfully explored by a number of researchers. However, in order to develop effective and practical SPMs it is imperative to analyse various sources of threats. This study extensively reviews 93 primary studies, which use SBAs for developing SPMs of four commonly used software attributes (effort, defect-proneness, maintainability and change-proneness) in order to discuss and identify the various sources of threats while using these approaches for SPMs. The study also lists various actions that may be taken in order to minimise these threats. Furthermore, best practice examples in literature and the year-wise trends of threats indicating the most common threats missed by researchers are provided to help academicians and practitioners in designing effective studies for developing SPMs using SBAs. © 2018 The Institution of Engineering and Technology.",,"Malhotra R., Khanna M.",2018,Journal,IET Software,10.1049/iet-sen.2018.5143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051730067&doi=10.1049%2fiet-sen.2018.5143&partnerID=40&md5=12e59fe6fa38bc2bd252c5d9fe11f721,"Discipline of Software Engineering, Department of Computer Science and Engineering, Delhi Technological University, Delhi, India; Sri Guru Gobind Singh College of Commerce, University of Delhi, Delhi, India",Institution of Engineering and Technology,English,17518806,
Scopus,A proposal for new software testing technique for component based software system,"A Component-Based Software (CBS) system consists of integrated components that work together to perform specific tasks. Different components are selected and integrated to form a new software system. The components may have been developed by other third party, thus it is expected that the development time and effort can be reduced significantly. However just like any traditional development, the testing activities requires a specific evaluation to assess the software. Most of the components do not come with the source code, but only some information of the components. Thus, a specific testing technique is required. In this paper, we propose a new testing technique for Component-Based Software system. Older techniques had been developed based on traditional metrics, while other CBS system had different strategy from what we propose in this research. The technique help determining test adequacy criteria based on a set of complexity and criticality metrics of a CBS system. Based on this test adequacy criteria, our experimental studies have shown that it has assisted in reducing the number of test suite and test cases. For software testers, this technique would significantly reduce the testing time and effort in CBS development. © 2018, School of Electrical Engineering and Informatics. All rights reserved.",Component based metrics; Component based software (CBS); Component based software engineering; Component integration metrics; Software testing,Hendradjaya B.,2018,Journal,International Journal on Electrical Engineering and Informatics,10.15676/ijeei.2018.10.1.5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046336643&doi=10.15676%2fijeei.2018.10.1.5&partnerID=40&md5=64443a46dac15eb4b4381c213f1edcf0,"School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Jl. Ganesha 10, Bandung, 40132, Indonesia",School of Electrical Engineering and Informatics,English,20856830,
Scopus,Analysis of global warming in India over maximum temperature using Pearson and machine learning,"This paper is to predict the real-time data of aggregated maximum temperature (1.2 m above sea level) data from more than 350 stations spread over the country resulting in global warming. The time series we have taken from the year 1970 to 2011 shows a global warming over India during the recent years, with 5 attributes. © 2017 IEEE.",Linear Regression; Machine Learning; Pearson Correlation,"Thirumalai C., Saikrishna G., Raju C.S., Senthilkumar M.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046696850&doi=10.1109%2fICOEI.2017.8300977&partnerID=40&md5=2d785ba28c375e941d12fd09bd02bf83,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Analyzing complexity nature inspired optimization algorithms using halstead metrics,Nowadays the research is focusing on new software development and analyzing or comparing the existing technologies. One of the rising zones of Software Engineering is Software Metrics. Software Metrics is used to test the quality of code without its execution. The primary Objective of the study is to analyze different software metrics for most commonly used nature-inspired optimization techniques like Genetic optimization and Ant-Colony Optimization. We analyze both the optimization algorithm in terms of solving traveling salesman problem which is developed in Java. © 2017 IEEE.,Ant-Colony optimization; genetic algorithm; Halstead metrics; traveling salesman problem,"Madhan M., Dhivakar I., Anbuarasan T., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300875,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046675821&doi=10.1109%2fICOEI.2017.8300875&partnerID=40&md5=82d9a60c60fdd50eaa59bd878d8c2e42,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Size and Effort Estimation Based on Problem Domain Measures for Object-Oriented Software,"This paper analyzes the correlations between the problem domain measures such as the number of distinct nouns and distinct verbs in the requirements artifacts and the solution domain measures such as the number of software classes and methods in the corresponding object-oriented software. For this purpose, 14 completed software development projects of a CMMI Level-3 certified defense industry company have been analyzed. The observed strong correlation is taken as the indication of linear relationship between the measures and a size estimation model based on linear regression analysis is proposed. Prediction performance of the method is analyzed on the 14 software projects. Moreover, it has been observed that there is a high correlation between the problem domain measures and the development effort. Therefore, the linear regression analysis is also used to estimate the effort in terms of the problem domain measures. The effort estimations are also evaluated and compared with the efforts predicted using the size measured by the COSMIC Function Point (CFP) method. The results show that the proposed method provides more accurate effort estimates compared to the effort estimated by using CFP size measurement. © 2018 World Scientific Publishing Company.",effort estimation; size measurement; Software engineering,"Ayyildiz T.E., Koçyiǧit A.",2018,Conference,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194018500079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043234853&doi=10.1142%2fS0218194018500079&partnerID=40&md5=7cce3a6b828be8eb1b9795e05bd190d3,"Computer Engineering Department, Başkent University, Ankara, 06810, Turkey; Information Systems Department, Middle East Technical University, Ankara, 06800, Turkey",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Software development effort estimation using random forests: An empirical study and evaluation,"There is evidence that Software Development Effort Estimation (SDEE) plays a crucial role in managing the software project and controlling its whole lifecycle; an accurate effort estimate allows an effective monitoring and efficient scheduling of tasks and resources. Although extensive research has been carried out on SDEE techniques, no single technique has been shown to be superior to other in all situation. Recently, there has been an increasing amount of literature on predicting software effort using Machine Learning (ML) methods. Among these ML techniques, regression tree-based models have gained a considerable attention due to their generalization ability and understandability. So far, very few studies have investigated the potential of Random Forests (RF) in software effort estimation. In this paper, a RF model is designed and adjusted empirically by varying the values of its key parameters. Prior to the parameters adjustment, we analysed their impact on RF model accuracy which allows an efficient tuning of the model during the training stage. The performance of the RF is then evaluated and compared with that of classical Regression Trees (RT). The evaluation was performed through the 30% hold-out validation method using five datasets: ISBSG R8, Tukutuku, COCOMO, Desharnais and Albrecht. To identify the most accurate technique, we employed three widely known accuracy measures: Pred(0.25), MMRE and MdMRE. The results obtained show that the adjusted random forest outperforms the regression trees model on all evaluation criteria. Moreover, the proposed model performs better than some recent techniques reported in the literature for software effort estimation. © 2018 International Journal of Intelligent Engineering and Systems.",Accuracy evaluation; Random forest; Regression trees; Software development effort estimation,"Zakrani A., Hain M., Namir A.",2018,Journal,International Journal of Intelligent Engineering and Systems,10.22266/IJIES2018.1231.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062080316&doi=10.22266%2fIJIES2018.1231.30&partnerID=40&md5=4f4eb40a77931e5796ccd281bd1cd57a,"ENSAM, Hassan II University, Casablanca, Morocco; Faculte des Sciences Ben M'sik, Casablanca, Morocco",Intelligent Network and Systems Society,English,2185310X,
Scopus,Influence factors in software productivity a tertiary literature review,"Software organizations need to increase their productivity to stay competitive. Although there is a lot of research on productivity in software development, software organizations still do not know what are the most significant productivity factors in which they should invest. This paper presents a Tertiary Literature Review (TLR) that aimed to identify and analyze Systematic Literature Reviews (SLR) on the influence factors of software productivity reported in the scientific literature. We extracted and classified the influence factors into organizational factors (organizational dependent factors) and human factors (people dependent factors). Using this information, software organizations can improve the productivity of their projects by evaluating the influence factors that best fit their context. © 2018 Universitat zu Koln. All rights reserved.",Productivity Influence Factors; Software Productivity.; Tertiary Literature Review,"Oliveira E., Conte T., Cristo M., Valentim N.M.C.",2018,Conference,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",10.18293/SEKE2018-149,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056865555&doi=10.18293%2fSEKE2018-149&partnerID=40&md5=4ed929451e610288d23a110c071044a1,"Institute of Computing, Federal University of Amazonas, UFAM, Manaus, Brazil; Department of Informatics, Federal University of Paraná, UFPR, Curitiba, Brazil",Knowledge Systems Institute Graduate School,English,23259000,1891706446
Scopus,Methods and metrics for estimating and planning agile software projects,"The nature of agile software projects is different from software projects that use traditional methodologies. Therefore, using traditional techniques of estimates of effort, time and cost can produce imprecise estimates. Several estimation techniques have been proposed by several authors and developers in recent years. This work performs a Systematic Literature Review (SLR) of current estimates practices in the development of agile software and the most used size metrics as inputs for these estimates, collecting and documenting them for a future comparison of their accuracy. With the realization of SLR, it was identified that Story Point and Point of Function are the most used metrics in agile projects as the basis for estimating size, time, effort, productivity and cost. Based on these two-size metrics, it was performed a case study with the estimation of effort, time and cost for an agile project of a software factory, where the actual development values were compared with the estimates made to analyze which provided the estimates that most closely approximated the actual values that were estimated. © 2018 Association for Information Systems. All rights reserved.",Agile software development; Scrum; Software estimates; Software metrics,"Canedo E.D., Da Costa R.P.",2018,Conference,"Americas Conference on Information Systems 2018: Digital Disruption, AMCIS 2018",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054229940&partnerID=40&md5=7e03f1f2e5ec1415fc1db7628dea64b5,"Computer Science Department, University of Brasília - (UnB), Brazil",Association for Information Systems,English,,9780996683166
Scopus,Analysis of the gap between initial estimated size and final (True) size of implemented software,"In this paper, we investigate the gap between the final true functional size of a piece of software at project closure and the functional size estimated much earlier in the development life cycle when the initial set of requirements is incomplete and often ambiguous. The different components and dimensions of this gap are identified and an approach is proposed to address them. The purpose of this approach is to improve early functional size estimation and, in turn, improve effort estimation. For the purposes of this paper, the ISO 19761 COSMIC standard on functional size measurement is taken as reference for discussion, while the majority of concepts presented are generic to most other similar ISO standards. © 2018 CEUR-WS. All rights reserved.",COSMIC; Functional size measurement; ISO 19761; Requirements quality; Size accuracy; Size estimation; Software estimation,"Ungan E., Trudel S., Abran A.",2018,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053791276&partnerID=40&md5=84b1f42adf2111477f5719a21222b5be,"Université du Québec à Montréal – UQAM, Montréal, Canada; École de Technologie Supérieure – ETS, University of Quebec, Montréal, Canada",CEUR-WS,English,16130073,
Scopus,Feature level complexity and coupling analysis in 4GL systems,"Product metrics are widely used in the maintenance and evolution phase of software development to advise the development team about software quality. Although most of these metrics are defined for mainstream languages, several of them were adapted to fourth generation languages (4GL) as well. Usual concepts like size, complexity and coupling need to be re-interpreted and adapted to program elements defined by these languages. In this paper we take a further step in this process to address product line development in 4GL. Adopting product line architecture is a necessary step to handle challenges of a growing number of similar product variants. The product line adoption process itself is a tedious task where features of the product variants play crucial role. Features represent a higher level of abstraction that are cross-cutting to program elements of 4GL applications. We propose a set of metrics related to features by linking existing program elements to metrics and by relating features with each other. The focus of this study is on complexity and coupling metrics. We provide a metrics based analysis of several variants of a large scale industrial product line written in the Magic XPA 4GL language. © Springer International Publishing AG, part of Springer Nature 2018.",4GL; Complexity; Coupling; Feature analysis; Metrics; Product lines; Quality; SPL,"Kicsi A., Csuvik V., Vidács L., Beszédes Á., Gyimóthy T.",2018,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-95174-4_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049963597&doi=10.1007%2f978-3-319-95174-4_35&partnerID=40&md5=3180dc108a9f2f8c193291a71e958776,"Department of Software Engineering, University of Szeged, Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, University of Szeged, Szeged, Hungary",Springer Verlag,English,03029743,9783319951737
Scopus,A review of literature about models and factors of productivity in the software factory,"Software factories seek to develop quality software in a manner comparable to the production of other industrial products, establishing improvements in their production process so as to be more competitive. Productivity, one of the competitiveness pillars, is related to the effort required to fulfill assigned tasks. However, there is no standard way of measuring productivity, making it difficult to establish policies and strategies to improve the factory. In this article, the authors perform a systematic review of the literature on factors affecting productivity of software factories, and models for measuring it. For the period 2005-2017, 74 factors and 10 models are identified. Most of the factors are related to Programming, and a few to Analysis and Design and Testing. Also, most models for measuring productivity only consider activities concerning programming. © 2018, IGI Global.",Analysis; Development; Effectiveness; Efficiency; Performance; Productivity; Software Factory; Testing,"Vargas P.S.C., Mauricio D.",2018,Review,International Journal of Information Technologies and Systems Approach,10.4018/IJITSA.2018010103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038438297&doi=10.4018%2fIJITSA.2018010103&partnerID=40&md5=b2274434294b12ae492986c4f2c6d66c,"National University of San Marcos, Lima, Peru",IGI Global,English,1935570X,
Scopus,Optimizing effort and time parameters of COCOMO II estimation using fuzzy multi-objective PSO,"The estimation of software effort is an essential and crucial activity for the software development life cycle. Software effort estimation is a challenge that often appears on the project of making a software. A poor estimate will produce result in a worse project management. Various software cost estimation model has been introduced to resolve this problem. Constructive Cost Model II (COCOMO II Model) create large extent most considerable and broadly used as model for cost estimation. To estimate the effort and the development time of a software project, COCOMO II model uses cost drivers, scale factors and line of code. However, the model is still lacking in terms of accuracy both in effort and development time estimation. In this study, we do investigate the influence of components and attributes to achieve new better accuracy improvement on COCOMO II model. And we introduced the use of Gaussian Membership Function (GMF) Fuzzy Logic and Multi-Objective Particle Swarm Optimization method (MOPSO) algorithms in calibrating and optimizing the COCOMO II model parameters. The proposed method is applied on Nasa93 dataset. The experiment result of proposed method able to reduce error down to 11.891% and 8.082% from the perspective of COCOMO II model. The method has achieved better results than those of previous researches and deals proficient with inexplicit data input and further improve reliability of the estimation method. © 2017 IEEE.",COCOMO II Model; Effort Estimation; Fuzzy; Multi-Objective PSO; Optimizaton; Time Development Estimation,"Langsari K., Sarno R.",2017,Conference,"International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",10.1109/EECSI.2017.8239157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046430949&doi=10.1109%2fEECSI.2017.8239157&partnerID=40&md5=6f547d73faf0f4334cf0d5aeb864b615,"Department of Informatics Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",Institute of Advanced Engineering and Science,English,2407439X,9781538605486
Scopus,Queueing-theory-based models for software reliability analysis and management,"Software reliability is one of the most important internal attributes of software systems. Over the past three decades, many software reliability growth models have been proposed and discussed. Some research has also shown that the fault detection and removal processes of software can be described and modeled using an infinite-server queueing system. But, there is practically no company that can afford unlimited resources to test and correct detected faults in the real world. Consequently, the number of debuggers should be limited, not infinite. In this paper, we propose an extended finite-server-queueing (EFSQ) model to analyze the fault removal process of the software system. Numerical examples based on real project data are illustrated. Evaluation results show that our proposed EFSQ model has a fairly accurate prediction capability of software reliability and also depict the real-life situation of software development activities more faithfully. Finally, the applications of our proposed model to project management are also presented. Our proposed model can provide a theoretically effective technique for managing the necessary activities of testing and debugging in software project management. © 2017 IEEE.",Non-homogeneous Poisson process (NHPP); Project management; Queueing theory; Software reliability growth model (SRGM); Software testing and debugging,"Huang C.-Y., Kuo T.-Y.",2017,Journal,IEEE Transactions on Emerging Topics in Computing,10.1109/TETC.2014.2388454,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044427603&doi=10.1109%2fTETC.2014.2388454&partnerID=40&md5=3616e1c19af2a6f58c8f1a46e21eb972,"Department of Computer Science, National Tsing Hua University, Hsinchu, 300, Taiwan; Department of Engineering, Garmin Corporation, New Taipei City, Taiwan",IEEE Computer Society,English,21686750,
Scopus,"Yazilim geliştirme süreçlerinin analizi: Zorluklar, tasarim prensipleri ve tekniksel yaklaşimlar","The main purpose of this study is to present solution approaches for the challenges during developing software projects. Besides, process models, design principles and technical approaches are submitted through personal experiences. © 2017 IEEE.",Challenges; Design principles; Software development process; Standardization activities,"Alakuş T.B., Daş R., Türkoǧlu I.",2017,Conference,IDAP 2017 - International Artificial Intelligence and Data Processing Symposium,10.1109/IDAP.2017.8090263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039901126&doi=10.1109%2fIDAP.2017.8090263&partnerID=40&md5=3e9688366e0c765fde34a5512d28d7f8,"Yazilim Mühendisliǧi Bölümü, Firat Üniversitesi, Teknoloji Fakültesi, Elaziǧ, Turkey",Institute of Electrical and Electronics Engineers Inc.,Turkish,,9781538618806
Scopus,A shortcut to estimating non-functional requirements?,Non-Functional Requirements determine a significant amount of the cost and effort that are needed to realize or maintain a software engineering solution. Yet the effect of Non-Functional Requirements on cost and effort estimates is largely underexposed in Software Engineering research. A few estimating solutions have been proposed but yield unsatisfactory predictive power or lack a theoretical foundation of their mechanisms. From our earlier research on packaged software estimation we have derived that the basic mechanisms that drive the estimation of cost and effort from both Functional and NonFunctional Requirements are more complex than the currently proposed methods. In this paper we present why in most cases only Architecture Driven Estimation mechanisms can lead to good cost predictions and we explain why current estimating solutions are unsuccessful. © 2017 Copyright is held by the owner/author(s).,Architecture; Architecture Driven Estimation; Cost driver; Cost Estimation; Effort Estimation; Non-Functional Requirements; Productivity driver; Project Constraints; Project Requirements; Size driver; Size-independent cost.,"Vogelezang F.W., Van Der Vliet E., Nijland R., Poort E.R., Mols H.R.J., De Vries J.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3143434.3143440,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038362064&doi=10.1145%2f3143434.3143440&partnerID=40&md5=8293e432e92791bd1901e508fc951705,"METRI IT Benchmarking, Netherlands; CGI, Netherlands; Capgemini, Netherlands; CGI, Netherlands; Capgemini, Netherlands; Ordina, Netherlands",Association for Computing Machinery,English,,9781450348539
Scopus,Source code comprehension analysis in software maintenance,"Source code comprehension is considered as an essential part of the software maintenance process. It is considered as one of the most critical and time-consuming task during software maintenance process. The difficulties of source code comprehension is analyzed. A static Bottom-up code comprehension model is used. The code is partitioned into functional-based blocks and their data and control dependencies that preserve the functionality of the program are analyzed. The data-flow and control-flow graphs reflects the dependencies and assist in refactoring process. The proposed strategy helps in improving the readability of the program code, increase maintainer productivity, and reducing the time and effort of code comprehension. It helps maintainers to locate the required lines of code that constitute the functional area that the maintainers are searching for in their maintenance work. © 2017 IEEE.",code comprehension; effort estimation; models; refactoring; source code comprehension; source line of code (SLOC),Al-Saiyd N.A.,2017,Conference,"2nd International Conference on Computer and Communication Systems, ICCCS 2017",10.1109/CCOMS.2017.8075175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036455348&doi=10.1109%2fCCOMS.2017.8075175&partnerID=40&md5=38bde1d38236c489968372f9b666cdc5,"Computer Science Department, Faculty of Information Technology, Applied Science Private University, Amman, Jordan",Institute of Electrical and Electronics Engineers Inc.,English,,9781538605394
Scopus,Survey of performance modeling of big data applications,"Enormous amount of data is being generated at a tremendous rate by multiple sources, often this data exists in different formats thus making it quite difficult to process the data using traditional methods. The platforms used for processing this type of data rely on distributed architecture like Cloud computing, Hadoop etc. The processing of big data can be efficiently carried out by exploring the characteristics of underlying platforms. With the advent of efficient algorithms, software metrics and by identifying the relationship amongst these measures, system characteristics can be evaluated in order to improve the overall performance of the computing system. By focusing on these measures which play important role in determining the overall performance, service level agreements can also be revised. This paper presents a survey of different performance modeling techniques of big data applications. One of the key concepts in performance modeling is finding relevant parameters which accurately represent performance of big data platforms. These extracted relevant performances measures are mapped onto software qualify concepts which are then used for defining service level agreements. © 2017 IEEE.",Big Data; Big Data platforms; Cloud Computing; MapReduce framework; Performance Measures; Software Metrics,"Pattanshetti T., Attar V.",2017,Conference,"Proceedings of the 7th International Conference Confluence 2017 on Cloud Computing, Data Science and Engineering",10.1109/CONFLUENCE.2017.7943145,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021774621&doi=10.1109%2fCONFLUENCE.2017.7943145&partnerID=40&md5=4cf3c7ee894b68a42ac7a1c72801d202,"Dept. of Computer Engineering and Information Technology, COEP, Savitribai Phule Pune University, Pune, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509035182
Scopus,Investigating the accuracy of test code size prediction using use case metrics and machine learning algorithms: An empirical study,"Software testing plays a crucial role in software quality assurance. It is, however, a time and resource consuming process. It is, therefore, important to predict as soon as possible the effort required to test software, so that activities can be planned and resources can be optimally allocated. Test code size, in terms of Test Lines Of Code (TLOC), is an important testing effort indicator used in many empirical studies. In this paper, we investigate empirically the early prediction of TLOC for objectoriented software using use case metrics. We used different machine learning algorithms (linear regression, k-NN, Naïve Bayes, C4.5, Random Forest, and Multilayer Perceptron) to build the prediction models. We performed an empirical study using data collected from five Java projects. The use case metrics have been compared to the well-known Use Case Points (UCP) method. Results show that the use case metrics-based approach gives a more accurate prediction of TLOC than the UCP method. © 2017 ACM.",Software testing; Test code size; Testing effort; Use cases,"Badri M., Badri L., Flageol W., Toure F.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3036290.3036323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018686838&doi=10.1145%2f3036290.3036323&partnerID=40&md5=2497e550ec9022caac1630050ec93495,"Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-Rivières, QC, Canada",Association for Computing Machinery,English,,9781450348287
Scopus,How clear is your code? An empirical study with programming challenges,"To maintain a software system, developers have to read and properly understand the source code. Previous work confirms that developers spend too much time reading code before maintaining it. Clear code is the one that is easy to read, to understand and, consequently, to maintain. Since readability and understandability are subjective properties of code, to understand how developers characterize clear code may support maintainability. Software metrics, such as Source Lines of Code (SLOC) and McCabe's Complexity (McCabe), are the basic means to quantify clear code properties. However, we still lack empirical knowledge on the correlation between such metrics and the subjective view of developers on clear code. In this paper, we investigate how developers characterize clear code by analyzing 6,775 alternative code solutions to 131 programming challenges from CheckIO, an online coding challenge platform. In CheckIO, developers submit supposedly clear code solutions to a challenge and other developers provide positive votes to the clearest ones. We aim to identify correlations between the number of positive votes for a clear solution and six metrics: SLOC, McCabe, Number of Commented Lines, Comments Rate, Developer Experience, and Days After 1st Solution. Our results suggest a high correlation between clear code and the two last metrics. We also observed a higher correlation of clear code with SLOC and McCabe when the programming challenges have few (less than 20) solutions. However, we could not find correlation of clear code with Number of Commented Lines and Comments Rate.",Clear code; Software maintainability; Software metrics,"Fernandes E., Ferreira L.P., Figueiredo E., Valente M.T.",2017,Conference,CIbSE 2017 - XX Ibero-American Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026647587&partnerID=40&md5=28a50e7c66e80a6536e4b92816c34edf,"Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil",Ibero-American Conference on Software Engineering,English,,9789873806988
Scopus,A survey of empirical studies in software product maintainability prediction models,"Software product maintainability is critical to the achievement of the software product quality. In order to keep the software useful as long as possible, software product maintainability prediction (SPMP) has become an important endeavor. The objective of this paper is to identify and present the current research on SPMP. The search was conducted using digital libraries to And as much research papers as possible. Selected papers are classified according to the following survey classification criteria (SCs): research type, empirical type, publication year and channel. Based on the results of the survey, we provide a discussion of the current state of the art in software maintainability prediction models or techniques. We believe that this study will be a reliable basis for further research in software maintainability studies. © 2016 IEEE.",Prediction Models; Software Product Maintainability; Survey,"Elmidaoui S., Cheikhi L., Idri A.",2016,Conference,SITA 2016 - 11th International Conference on Intelligent Systems: Theories and Applications,10.1109/SITA.2016.7772267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010468522&doi=10.1109%2fSITA.2016.7772267&partnerID=40&md5=8184d51d9fbbba990544f8dc158d2375,"Software Project Management Team (SPM) ENSIAS, University Mohamed v of Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781509057818
Scopus,A specialized global software engineering taxonomy for effort estimation,"To facilitate the sharing and combination of knowledge by Global Software Engineering (GSE) researchers and practitioners, the need for a common terminology and knowledge classification scheme has been identified, and as a consequence, a taxonomy and an extension were proposed. In addition, one systematic literature review and a survey on respectively the state of the art and practice of effort estimation in GSE were conducted, showing that despite its importance in practice, the GSE effort estimation literature is rare and reported in an ad-hoc way. Therefore, this paper proposes a specialized GSE taxonomy for effort estimation, which was built on the recently proposed general GSE taxonomy (including the extension) and was also based on the findings from two empirical studies and expert knowledge. The specialized taxonomy was validated using data from eight finished GSE projects. Our effort estimation taxonomy for GSE can help both researchers and practitioners by supporting the reporting of new GSE effort estimation studies, i.e. new studies are to be easier to identify, compare, aggregate and synthesize. Further, it can also help practitioners by providing them with an initial set of factors that can be considered when estimating effort for GSE projects. © 2016 IEEE.",Effort Estimation; Global Software Development; Global Software Engineering; Taxonomy,"Britto R., Mendes E., Wohlin C.",2016,Conference,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",10.1109/ICGSE.2016.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994579892&doi=10.1109%2fICGSE.2016.11&partnerID=40&md5=ea00c3eefbda33c57fa98f6b8eec7d5b,"Department of Software Engineering, Blekinge Institute of Technology, Sweden; Department of Computer Science and Engineering, Blekinge Institute of Technology, Sweden",Institute of Electrical and Electronics Engineers Inc.,English,,9781509026807
Scopus,Software cost estimation based on modified K-Modes clustering Algorithm,"Unsupervised technique like clustering may be used for software cost estimation in situations where parametric models are difficult to develop. This paper presents a software cost estimation model based on a modified K-Modes clustering algorithm. The aims of this paper are: first, the modified K-Modes clustering which is an enhancement over the simple K-Modes algorithm using a proper dissimilarity measure for mixed data types, is presented and second, the proposed K-Modes algorithm is applied for software cost estimation. We have compared our modified K-Modes algorithm with existing algorithms on different software cost estimation datasets, and results showed the effectiveness of our proposed algorithm. © 2015, Springer Science+Business Media Dordrecht.",Clustering; Data mining; K-Modes clustering; Software cost estimation,"Bishnu P.S., Bhattacherjee V.",2016,Journal,Natural Computing,10.1007/s11047-015-9492-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922567132&doi=10.1007%2fs11047-015-9492-7&partnerID=40&md5=2819c4e5856f138a5b439829a6fb82dc,"Birla Institute of Technology, Ranchi, 834001, India",Springer Netherlands,English,15677818,
Scopus,Influence of outliers on analogy based software development effort estimation,"In a software development project, project management is indispensable, and effort estimation is one of the important factors on the management. To improve estimation accuracy, outliers are often removed from dataset used for estimation. However, the influence of the outliers to the estimation accuracy is not clear. In this study, we added outliers to dataset experimentally, to analyze the influence. In the analysis, we changed the percentage of outliers, the extent of outliers, variable including outliers, and location of outliers on the dataset. After that, effort was estimated using the dataset. In the experiment, the influence of outliers was not very large, when they were included in the software size metric, the percentage of outliers was 10%, and the extent of outliers was 100%. © 2016 IEEE.",abnormal value; case based reasoning; effort prediction,"Ono K., Tsunoda M., Monden A., Matsumoto K.",2016,Conference,"2016 IEEE/ACIS 15th International Conference on Computer and Information Science, ICIS 2016 - Proceedings",10.1109/ICIS.2016.7550865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987967929&doi=10.1109%2fICIS.2016.7550865&partnerID=40&md5=b5874036a32595164caab38190a85f64,"Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Informatics, Kindai University, Osaka, Japan; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,9781509008063
Scopus,Analyzing the effect of variables in the software development effort estimation,"Due to the variability of variables, the estimation of effort in software development is considered as a very difficult task. Maybe due to small databases, uncertain and very subjective information. For these reason, in this paper, we introduce a study of the effect of variables in the software development effort estimation process. The estimation employing lines of codes, function points and engineering task, were taken as reference. With the use of datamining and machine learning technics, were demonstrated that the effort estimation using engineering task is more accurate. And, by the other hand, was established the influence of each variable, in the software development effort estimation process. © 2003-2012 IEEE.",Datamining; Decision Trees; Effort estimation; Regression Trees; Software metrics; Software projects; Variables selection,"Velarde H., Santiesteban C., Garcia A., Casillas J.",2016,Journal,IEEE Latin America Transactions,10.1109/TLA.2016.7786366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007449376&doi=10.1109%2fTLA.2016.7786366&partnerID=40&md5=7161663901bbe4659c1b27c5f86bbae7,"Facultad de Ciencias e Ingenierías. Físicas y Formales, Universidad Católica de Santa María, Arequipa, Peru; Centro de Bioplantas, Universidad de Ciego de Ávila, Cuba; División Territorial Villa Clara, DESOFT, Cuba; Departamento de Ciencias de la Computación e Inteligencia Artificial, Universidad de Granada, Spain",IEEE Computer Society,Spanish,15480992,
Scopus,Creating a historical database for estimation using the EPCU approximation approach for COSMIC (ISO 19761),"In the actual competitive software industry, software development organizations need a better and formal estimation approaches in order to increase the success rate of software projects. However, currently the estimation approach typically employed in industry is the expert judgment ('experience-based'). Using measures and estimations that continue to be based on researchers' intuition does not contribute to obtaining successful projects nor to mature software engineering. Any organization that aims to have or develop estimation approaches needs historical data, and the majority of the organizations do not have this information. Additionally acquisition of this information has expensive cost and time/effort consuming. This paper proposed the use of the EPCU approximation approach-that has demonstrated a several benefits-aiming to create a formal database with low cost and effort, that can be employed as a starting point to improve estimations in organizations without historical databases. © 2016 IEEE.",Approximate Sizing; COSMIC ISO 19761; EPCU Model; Estimation database; FSM; Functional Size,Valdes-Souto F.,2016,Conference,"Proceedings - 2016 4th International Conference in Software Engineering Research and Innovation, CONISOFT 2016",10.1109/CONISOFT.2016.32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978253145&doi=10.1109%2fCONISOFT.2016.32&partnerID=40&md5=9c587b32f19a52e7ccd7408017779db0,"Science Faculty, National Autonomous University of Mexico (UNAM), CDMX Mexico City, Mexico",Institute of Electrical and Electronics Engineers Inc.,English,,9781509010745
Scopus,Software evolution in web-service ecosystems: A game-theoretic model,"Service orientation is the prvalent paradigm for modular distributed systems, giving rise to service ecosystems defined by software dependencies, which, at the same time, carry business and economic implications. And as software evolves, so do the business relationships among the ecosystem participants, with corresponding economic impact. Therefore, a more comprehensive model of software evolution is necessary in this context, to support the decision-making processes of the ecosystem participants. In this work, we view the ecosystem as a market environment, with providers offering competing services and developing these services to attract more clients by better satisfying their requirements. Based on an economic model for calculating the costs and values associated with service evolution, we develop a game-theoretic model to capture the interactions between providers and clients and support the providers' decision-making process. We demonstrate the use of our model with a realistic example of a cloud-services ecosystem. © 2016 INFORMS.",Game theory; Service-oriented architectures; Software cost; Software evolution; Web-service ecosystems,"Fokaefs M., Stroulia E.",2016,Journal,Service Science,10.1287/serv.2015.0114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045690251&doi=10.1287%2fserv.2015.0114&partnerID=40&md5=8f1bcf4625f584f9eea921c29d681eef,"Department of Computing Science, University of Alberta, Edmonton, AB  T6G 2E8, Canada",INFORMS Inst.for Operations Res.and the Management Sciences,English,21643962,
Scopus,Standardized cost estimation in Thai government's software development projects,"Software features and costs are often unquantifiable due to the abstract nature of software. In many cases, this results in the estimated costs of software development projects to be potentially highly biased, highly inaccurate, or highly unjustified. Hence, current software estimation methodologies can open up areas for corruption as estimated budgets and costs are difficult to verify and validate. The Thai COCOMO Framework and cost estimation model were developed in order to overcome this problem in software development projects for Thai government by providing standard and transparency to the software estimation process with justification to the associated costs. This paper discusses the areas in software project estimation that are prone to corruption and ways that the COCOMO model and framework can be used to address them. © 2015 IEEE.",COCOMO II; cost estimation; software engineering,"Phongpaibul M., Aroonvatanaporn P.",2016,Conference,ICSEC 2015 - 19th International Computer Science and Engineering Conference: Hybrid Cloud Computing: A New Approach for Big Data Era,10.1109/ICSEC.2015.7401449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964317425&doi=10.1109%2fICSEC.2015.7401449&partnerID=40&md5=76d3b7f70aeed5863a53e05b884c30fb,"Department of Computer Science, Faculty of Science and Technology, Thammasat University (Rangsit Campus, Pathumthani, Thailand; MandP Consulting, Bangkok, Thailand",Institute of Electrical and Electronics Engineers Inc.,English,,9781467378253
Scopus,Size estimation of open source board-based software games,"Software effort, schedule, and cost estimation has the highest utility at the time of inception. Since software size is one of the most important determinant of software effort (and, hence, cost), it is extremely beneficial to estimate size early. This early estimation of size is likely to result in better planning for projects dealing with the development of software games. Yet, despite its utility, a survey of the existing literature does not reveal a technique developed for this purpose. In particular, this paper focuses on the size estimation of open source board-based software games. A new software size estimation model is proposed for this sub-domain. In this research, 52 open source board-based software games are thoroughly examined and analyzed. After shortlisting potential predictors of software size for this sub-domain, forward stepwise Multiple Linear Regression (MLR) is used for model fitting. The results show that our model has reasonable estimation accuracy as indicated by the value of the coefficient of determination (R2 = 0.699). By providing a reasonably accurate estimate of software size early in the life cycle, our model makes it easier and simpler to plan and manage the development of open source board-based software games. © 2015 IEEE.",Game Sizing; MMRE; Multiple Linear Regression (MLR); Open source; PRED; Project Management; Project Planning; Simple Linear Regression (SLR); Software Size Estimation,"Sabahat N., Malik A.A., Azam F.",2016,Conference,"ICOSST 2015 - 2015 International Conference on Open Source Systems and Technologies, Proceedings",10.1109/ICOSST.2015.7396414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964865216&doi=10.1109%2fICOSST.2015.7396414&partnerID=40&md5=b400263df0d7c5a662fbcc1b5e244e70,"National University of Science and Technology (NUST), Islamabad, Pakistan; National University of Computer and Emerging Sciences (NUCES), Lahore, Pakistan",Institute of Electrical and Electronics Engineers Inc.,English,,9781479978120
Scopus,Effort estimation of ETL projects using Forward Stepwise Regression,"Effort estimation is a key component of planning a software development project. In the past, there has been a lot of research on estimation methods for traditional applications but, unfortunately, these methods do not apply to Extract Transform Load (ETL) projects. Coming up with a systematic effort estimate for ETL projects is a challenging task since ETL development does not follow the traditional Software Development Life Cycle (SDLC). Traditional application development is requirements-driven whereas ETL application development is data-driven. This research paper describes the development of an effort estimation model for ETL projects and compares this model with the most widely used algorithmic effort estimation model i.e. COCOMO II. A dataset comprising 220 industrial projects from five different software houses is used to build this effort estimation model using Forward Stepwise Regression. After eliminating 20 outliers from this dataset, the adjusted R2 (i.e. goodness of fit) of our model is 0.87. The prediction and training accuracy of this model is measured using the de-facto standard accuracy measures such as MMRE and PRED(25). On a training dataset of 200 projects, the training accuracy value of PRED(25) is 81.16% and MMRE is 0.16. Results show that our proposed estimation model provides considerably better estimation accuracy as compared to COCOMO II. On a validation dataset of 58 projects, the value of PRED(25) was 49% for our model as compared to 21% for COCOMO II. Furthermore, the MMRE of our model is 0.31 as compared to 0.99 for COCOMO II. © 2015 IEEE.",COCOMO II; Data Warehousing; Effort Estimation; Estimation Accuracy; ETL; Forward Stepwise Regression; Project Management; Software Cost Estimation,"Rasool R., Malik A.A.",2016,Conference,"Proceedings of 2015 International Conference on Emerging Technologies, ICET 2015",10.1109/ICET.2015.7389209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963547188&doi=10.1109%2fICET.2015.7389209&partnerID=40&md5=cb912de0121ccb683e73b6aaa13c890b,"Department of Computer Science, National University of Computer and Emerging Sciences, Lahore, Pakistan",Institute of Electrical and Electronics Engineers Inc.,English,,9781509004362
Scopus,Green measurement metrics towards a sustainable software: A systematic literature review,With the advancement in computing hardware's the complexity and energy consumption of software has increased largely. There are various primary studies which uses different software measurement metrics in green and sustainable software development. In this paper we systematically reviewed and analyzed different green measurement metrics in sustainable software. Our objective is to find the recent studies in green metrics and find the different state-of-the-art measures that is taking place in green software development and engineering. We reviewed 14 studies with different aspects of green metrics and sustainable software. This review summarizes different green metrics and methods proposed in the recent years. © 2016 IEEE.,Green metrics; Measurement; Sustainable software,"Debbarma T., Chandrasekaran K.",2016,Conference,"2016 International Conference on Recent Advances and Innovations in Engineering, ICRAIE 2016",10.1109/ICRAIE.2016.7939521,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021656217&doi=10.1109%2fICRAIE.2016.7939521&partnerID=40&md5=04aaed06393fd68edde57832b16463ad,"Computer Science and Engineering Department, National Institute of Technology Agartala, Barjala, Tripura, India; Computer Science and Engineering Department, National Institute of Technology Surathkal, Mangalore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509028078
Scopus,The use of cross-company fault data for the software fault prediction problem,We investigated how to use cross-company (CC) data in software fault prediction and in predicting the fault labels of software modules when there are not enough fault data. This paper involves case studies of NASA projects that can be accessed from the PROMISE repository. Case studies show that CC data help build high-performance fault predictors in the absence of fault labels and remarkable results are achieved. We suggest that companies use CC data if they do not have any historical fault data when they decide to build their fault prediction models. © 2016 TÜBİTAK.,Cross-company data; Defect prediction; Metrics values,Çatal C.,2016,Journal,Turkish Journal of Electrical Engineering and Computer Sciences,10.3906/elk-1409-137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978238268&doi=10.3906%2felk-1409-137&partnerID=40&md5=b07d903ba8ecd103f08d70b307793e51,"Department of Computer Engineering, Istanbul Kültür University, Ataköy Campus, Istanbul, Turkey",Turkiye Klinikleri Journal of Medical Sciences,English,13000632,
Scopus,A comprehensive survey of software development cost estimation studies,"Context: Cost estimation of software projects is critical activity which requires the use of proper techniques and methods in order to have a good estimation results. This task is facing many obstacles which make it somehow challenging. The software size and how accurately it has been measured greatly affects the accuracy of the estimation. Also project management plays vital role in guiding such estimation processes. A lot of research effort has been accomplished which reflects the increasing demands of high quality software through effective cost estimation. Purpose: To summarize and differentiate existing literature related to the cost estimation methods and techniques as away to identify estimation model fundamentals based on the analysis of the already available cost estimation models, also trying to identify the possible gaps in this research area. Scheme: The methodology is based on the survey research method with well defined steps. Outcomes: The survey search process found as preliminary results 274 papers associated to our topic from strong online databases, around 20 papers have been included in this survey and only 13 studies were selected for in-depth analysis. Data were extracted from these studies and synthesized in respect to the formulated Research questions.The survey results has shown that there are many models for cost estimation and the challenge is which one to use in specific software context, but still there are some commonly used models in both industry and academia. Conclusion: The result of our work confirm that in order to have an accurate estimation process many software metrics need to be analyzed and used in estimation model or in a combination of models, and the estimation of software needs a lot of data and inputs to be accurate to meets expectation, this can be achieved using benchmarking of datasets. © 2015 ACM.",Algorithmic and non algorithmic models; Cost estimation; Software metrics; Software process,"Al-Qudah S., Meridji K., Al-Sarayreh K.T.",2015,Conference,ACM International Conference Proceeding Series,10.1145/2816839.2816913,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959933754&doi=10.1145%2f2816839.2816913&partnerID=40&md5=5313baf1eac6da6f771bc605ef2c27f6,"Hashemite University, Software Engineering Department, P.O.Box 33127, Zarqa, 13133, Jordan; University of Petra, Software Engineering Department, P.O.Box 33127, Amman, 11196, Jordan",Association for Computing Machinery,English,,9781450334587
Scopus,An Empirical Study of Dynamic Incomplete-Case Nearest Neighbor Imputation in Software Quality Data,"Software quality prediction is an important yet difficult problem in software project development and management. Historical datasets can be used to build models for software quality prediction. However, the missing data significantly affects the prediction ability of models in knowledge discovery. Instead of ignoring missing observations, we investigate and improve incomplete-case k-nearest neighbor based imputation. K-nearest neighbor imputation is widely applied but has rarely been improved to have the most appropriate parameter settings for each imputation. This work conducts imputation on four well-known software quality datasets to discover the impact of the new imputation method we proposed. We compare it with mean imputation and other commonly used versions of k-nearest neighbor imputation. The empirical results show that the proposed dynamic incomplete-case nearest neighbor imputation performs better when the missingness is completely at random or non-ignorable, regardless of the percentage of missing values. © 2015 IEEE.",imputation; incomplete-case; k-nearest neighbor; missing data treatments; software quality data,"Huang J., Sun H., Li Y.-F., Xie M.",2015,Conference,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",10.1109/QRS.2015.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962044494&doi=10.1109%2fQRS.2015.16&partnerID=40&md5=79b7115a7ec5be4e241762a9854baa0f,"Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong SAR, Hong Kong; Department of Industrial Engineering, CentraleSupelec, Paris, France",Institute of Electrical and Electronics Engineers Inc.,English,,9781467379892
Scopus,Software engineering for image processing systems,"Software Engineering for Image Processing Systems creates a modern engineering framework for the specification, design, coding, testing, and maintenance of image processing software and systems. The text is designed to benefit not only software engineers, but also workers with backgrounds in mathematics, the physical sciences, and other engineering disciplines, who find themselves working on a software project team. The author classifies imaging software as its own distinct caste, thereby providing a common language and framework for imaging engineers of all backgrounds. This common framework could, in turn, lead to more robust, reliable and economical software throughout the imaging industry. © 2003 by Taylor & Francis Group, LLC.",,Laplante P.A.,2015,Book,Software Engineering for Image Processing Systems,10.1201/9780203496107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053092196&doi=10.1201%2f9780203496107&partnerID=40&md5=8cfd1727ab2dc602f599cf62d5c86a9e,,CRC Press,English,,9780203496107; 9780849313769
Scopus,Change effort estimation based on uml diagrams application in ucp and cocomoii,"Change impact must be accounted for during effort estimation to provide for adequate decision making at the appropriate moment in the software lifecycle. Existing effort estimation approaches, like the Use Case Point method and the COnstructive COst MOdeL, estimate the effort only if the change occurs at one level, for example when a new functionality is added (at functional level). However, they do not treat elementary changes at the design level such as the addition of a class or a sequence diagram; because they incur several modifications at different modelling levels, such changes are important to account for in effort estimation during the software development. In this paper, we take advantage of intra and inter UML diagrams dependencies, first, to assist developers in identifying the necessary changes that UML diagrams must undergo after a potential change, and secondly to estimate the necessary effort to handle any elementary change e.g. adding a class, an attribute, etc. We use our traceability technique in order to adapt the UCP and COCOMO methods to estimate the effort whenever a change occurs during the requirement or design phases.",Change; COCOMO; Effort estimation; UCP,"Kchaou D., Bouassida N., Ben-Abdallah H.",2015,Conference,"ICSOFT-EA 2015 - 10th International Conference on Software Engineering and Applications, Proceedings; Part of 10th International Joint Conference on Software Technologies, ICSOFT 2015",10.5220/0005510503010308,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965074454&doi=10.5220%2f0005510503010308&partnerID=40&md5=9af1b4e3c0bec40f0da68e8ede383377,"University of Sfax, Sfax, Tunisia; King Abdulaziz University, Jeddah, Saudi Arabia",SciTePress,English,,9789897581144
Scopus,RBFN networks-based models for estimating software development effort: A cross-validation study,"Software effort estimation is very crucial and there is always a need to improve its accuracy as much as possible. Several estimation techniques have been developed in this regard and it is difficult to determine which model gives more accurate estimation on which dataset. Among all proposed methods, the Radial Basis Function Neural (RBFN) networks models have presented promising results in software effort estimation. The main objective of this research is to evaluate the RBFN networks construction based on both hard and fuzzy C-means clustering algorithms using cross-validation approach. The objective of this replication study is to investigate if the RBFN-based models learned from the training data are able to estimate accurately the efforts of yet unseen data. This evaluation uses two historical datasets, namely COCOMO81 and ISBSG R8. © 2015 IEEE.",,"Idri A., Hassani A., Abran A.",2015,Conference,"Proceedings - 2015 IEEE Symposium Series on Computational Intelligence, SSCI 2015",10.1109/SSCI.2015.136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964998599&doi=10.1109%2fSSCI.2015.136&partnerID=40&md5=ff08a732a997a0619f3c654c70abf5eb,"Department of Software Engineering, ENSIAS, Mohammed v University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, Montréal, H3C IK3, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781479975600
Scopus,A functional software measurement approach to bridge the gap between problem and solution domains,"There are various software size measurement methods that are used in various stages of a software project lifecycle. Although functional size measurement methods and lines of code measurements are widely practiced, none of these methods explicitly position themselves in problem or solution domain. This results in unreliable measurement results as abstraction levels of the measured artifacts vary greatly. Unreliable measurement results hinder usage of size data in effort estimation and benchmarking studies. Furthermore, there exists no widely accepted measurement method for solution domain concepts other than lines of code, such as software design. In this study, an approach is defined to distinguish problem and solution domains for a software project and a software size measurement methodology for solution domain is proposed based on software design sizes. © Springer International Publishing Switzerland 2015.",,"Ungan E., Demirörs O.",2015,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-319-24285-9_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951320412&doi=10.1007%2f978-3-319-24285-9_12&partnerID=40&md5=4a23d9151cf85da3064b4f89ab06dc3f,"Graduate School of Informatics, Middle East Technical University, Ankara, Turkey",Springer Verlag,English,18651348,9783319242842
Scopus,Relative analysis of sizing methods in the sense of e-commerce system,"The estimation of size of software is great solution for determining the amount of time and effort needed for developing software systems. The success of any software project mainly depends on best estimation of project effort, time and cost. Estimation widely used in setting rational objectives for completing the project. The basic element for estimating all is size. The software industry uses various sizing techniques they are Lines of code, Function points, Feature points, Use Case points, Object points, Internet points, etc are not effectively supported for determining the size of E-Commerce systems. The wrong estimates lead imperfectness, loss and customer dissatisfaction. This paper explains the leading sizing techniques and their inabilities in sizing and also the necessities of new sizing approach for E-Commerce systems. © Research India Publications.",E-commerce-electronic commerce; FPA-function point analysis; LOC-line of code; OP-object points; UCP-use case points,"Mesia Dhas J.T., Bharathi C.R.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944527722&partnerID=40&md5=fedf3b8b4cb0c7b175f7b6099977c037,"Vel Tech University, Department of C S E, RVS Padhmavathy College of Engineering and Technology, Chennai, TN, India; Department of Electronics and Communication Engineering, Vel Tech University, Chennai, TN, India",Research India Publications,English,09734562,
Scopus,A comparative study of different software fault prediction and classification techniques,"The main aim of this study is to survey about various techniques of fault prediction, clustering and classification to identify the defects in software modules. A software system consists of various modules and any of these modules can contain the fault that harmfully affects the reliability of the system. But early predictions of faulty modules can help in producing fault free software. So, it is better to classify modules as faulty or non-faulty after completing the coding. Then, more efforts can be put on the faulty modules to produce a reliable software. A fault is a defect or error in a source code that causes failures when executed. A faulty software module is the one containing number of faults, which causes software failure in an executable product. A software module is a set of functionally related source code files based on the system's architecture. Fault data can be collected from problem reporting system based on the module level. Defect prediction is particularly important in the field of software quality and reliability. Accurate prediction of faulty modules enables the verification and validation activities focused on the critical software components. A software quality classification model predicts the risk factor for software modules, which is an effective tool for targeting timely quality improvement actions. A desired classification technique provides better classification accuracy and robustness. This study surveys various fault prediction, clustering and classification techniques in order to identify the defects in software modules. © Maxwell Scientific Organization, 2015.",Bayesian classification; Expectation maximization (EM); Fuzzy c-means (FCM) clustering; Hyper quad tree (HQT); K-means clustering; Similarity-based software clustering (SISC); Spiral life cycle model; Support vector classification (SVM),"Rajaganapathy C.D., Subramani A.",2015,Journal,"Research Journal of Applied Sciences, Engineering and Technology",10.19026/rjaset.10.2437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938894185&doi=10.19026%2frjaset.10.2437&partnerID=40&md5=f6293d13dfa99fbac343e2df421a0639,"Department of Computer Science, PERI Institute of Technology, Chennai, Tamilnadu, India; Department of MCA, KSR College of Engineering, Tiruchengode, Tamilnadu, India",Maxwell Science Publications,English,20407459,
Scopus,A method to effort estimation for XP projects in clients perspective,"All engineering fields have definite methods to calculate the cost of the product whereas in software engineering although many methods are exists, but no two methods will give same value neither for the size of the product nor for the effort and cost of the product. Also, most of the existing methods and models to find the cost of a project are based on previous experience or using the history of the project. But using past experience cannot predict the future works due to the complexity as well as the new projects may not match with the earlier projects. So it is a research problem to get an optimal solution satisfying the objective function. The COCOMO model can help to find the effort of the project based on the previously finished project data for the traditional projects by multiplying the effort multiplier with the number of lines as size of the project. Nowadays the project development methodology has been changing to overcome the problems of project failures, to improve the efficiency, and makes the development methodology much easier. Agile software development methods have various forms like eXtreme Programming (XP) (Kent Beck -1999), SCRUM (Ken Schwaber and Mike Beedle – 2001) etc is introduced since 1999. The aim is to find the optimal size, effort and cost on both client and stack holder perspective for XP projects. The transparency is required for the client in estimation of the project. Based on the previous experience a method is proposed which is easy and accurate in sizing, effort estimation of the project. The sizing is done by a modified COSMIC full function points concepts. The effort estimation is the product of size and effort multipliers of the project, where effort multiplier are found using the finished XP projects of a leading software company from Chennai, India who were practicing XP methodology since 1999. The effort multiplier values are tuned by training the twenty finished projects data. The results are validated by using MMRE and different regression methods. These effort multipliers are applied to five different new projects (these projects are already finished and reserved for validation). These results of the projects are evaluated using different regression methods. Finally a tool is developed which takes the finished project as input and produce the output of the size, effort and cost of the project, which is more transparent and trust worthy to the client. © Research India Publications.",COSMIC full function; Effort estimation; Estimation; Extreme programming; EXtreme software size unit; Sizing the software; Software measurement,"Karunakaran E., Sreenath N.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929340671&partnerID=40&md5=e0d57a2fe746a7a030e8ac9d49d8c390,"Department of CSE, Pondicherry Engineering College, Pillaichavady, Puducherry  605014, India",Research India Publications,English,09734562,
Scopus,Software reliability analysis considering the variation of testing-effort and change-point,"It is commonly recognized that software development is highly unpredictable and software quality may not be easily enhanced after software product is finished. During the software development life cycle (SDLC), project managers have to solve many technical and management issues, such as high failure rate, cost over-run, low quality, late delivery, etc. Consequently, in order to produce robust and reliable software product(s) on time and within budget, project managers and developers have to appropriately allocate limited development- And testing-effort and time. In the past, the distribution of testing-effort or manpower can typically be described by the Weibull or Rayleigh model. Practically, it should be noticed that development environments or methods could change due to some reasons. Thus when we plan to perform software reliability modeling and prediction, these changes or variations occurring in the development process have to be taken into consideration. In this paper, we will study how to use the Parr-curve model with multiple change-points to depict the consumption of testing-effort and how to perform further software reliability analysis. The applicability and performance of our proposed model will be demonstrated and assessed through real software failure data. Experimental results are analyzed and compared with other existing models to show that our proposed model gives better predictions. Copyright © 2014 ACM.",Change points; Non-homogeneous poisson process (nhpp); Parr curve; Software reliability growth model (srgm); Testing-effort,"Ke S.-Z., Huang C.-Y., Peng K.-L.",2014,Conference,"International Workshop on Innovative Software Development Methodologies and Practices, InnoSWDev 2014 - Proceedings",10.1145/2666581.2666588,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942799160&doi=10.1145%2f2666581.2666588&partnerID=40&md5=fc5c0f9af81306984f158b257d04ab6d,"Firmware Department Ability Enterprise Co. Ltd., New Taipei City, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan","Association for Computing Machinery, Inc",English,,9781450332262
Scopus,Quantitatively exploring non-code software artifacts,"Most software engineering research focuses its analyses on source code, because correct, well designed, and efficient program code is the desired end output of software development. Nevertheless, source code is not the only constituent of software systems: Programs also comprise other types of artifacts, such as documentation, build system and configuration files, and graphics. These non-code artifacts only recently got the attention of researchers and are not yet investigated as a whole, but separately and with very specific aims. By taking a quantitative perspective, we look into non-code software artifacts to measure their role in software systems. We analyze 35 mature open-source software systems and we address exploratory questions such as: How many non-code software artifacts do software systems contain? How do they relate to source code? How much effort is put into producing and maintaining them? Our results show that a significant portion of systems is made of non-code artifacts, and that programmers spend a relevant part of their effort on non-code artifacts during the development process. Our analysis opens questions for future investigations. © 2014 IEEE.",empirical study; mining software repositories; non-code artifacts,"Bigliardi L., Lanza M., Bacchelli A., Dambros M., Mocci A.",2014,Conference,Proceedings - International Conference on Quality Software,10.1109/QSIC.2014.31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912120623&doi=10.1109%2fQSIC.2014.31&partnerID=40&md5=12bac05e639da1497148e69b561008c3,"Faculty of Informatics, University of Lugano, Switzerland, Switzerland; Delft University of Technology, Netherlands",IEEE Computer Society,English,15506002,9781479971978
Scopus,Applying a data quality model to experiments in software engineering,"Data collection and analysis are key artifacts in any software engineering experiment. However, these data might contain errors. We propose a Data Quality model specific to data obtained from software engineering experiments, which provides a framework for analyzing and improving these data. We apply the model to two controlled experiments, which results in the discovery of data quality problems that need to be addressed. We conclude that data quality issues have to be considered before obtaining the experimental results. © Springer International Publishing Switzerland 2014.",Controlled experiments; Data quality; Software engineering,"Valverde M.C., Vallespir D., Marotta A., Panach J.I.",2014,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-12256-4_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910685388&doi=10.1007%2f978-3-319-12256-4_18&partnerID=40&md5=45b1fcedc8d2c3b64b541e521c32be28,"Universidad de la República, Montevideo, Uruguay; Departament d’Informàtica, Universitat de València, Valencia, Spain",Springer Verlag,English,03029743,9783319122557
Scopus,Development effort and performance trade-off in high-level parallel programming,Research on high-level parallel programming approaches systematically evaluate the performance of applications written using these approaches and informally argue that high-level parallel programming languages or libraries increase the productivity of programmers. In this paper we present a methodology that allows to evaluate the trade-off between programming effort and performance of applications developed using different programming models. We apply this methodology on some implementations of a function solving the all nearest smaller values problem. The high-level implementation is based on a new version of the BSP homomorphism algorithmic skeleton. © 2014 IEEE.,algorithmic skeletons; C++; Parallel programming; software metrics,"Legaux J., Loulergue F., Jubertie S.",2014,Conference,"Proceedings of the 2014 International Conference on High Performance Computing and Simulation, HPCS 2014",10.1109/HPCSim.2014.6903682,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908654353&doi=10.1109%2fHPCSim.2014.6903682&partnerID=40&md5=6076a2bc2da5753081b61f3689a340fe,"Univ Orléans, INSA Centre Val de Loire, LIFO EA, Orléans, 4022, France",Institute of Electrical and Electronics Engineers Inc.,English,,9781479953127
Scopus,An evaluation of the statistical convertibility of Function Points into COSMIC Function Points,"Since the introduction of COSMIC Function Points, the problem of converting historical data measured using traditional Function Points into COSMIC measures have arisen. To this end, several researchers have investigated the possibility of identifying the relationship between the two measures by means of statistical methods. This paper aims at improving statistical convertibility of Function Points into COSMIC Function Points by improving previous work with respect to aspects - like outlier identification and exclusion, model non-linearity, applicability conditions, etc. - which up to now were not adequately considered, with the purpose of confirming, correcting or enhancing current models. Available datasets including software sizes measured both in Function Points and COSMIC Function Points were analyzed. The role of outliers was studied; non linear models and piecewise linear models were derived, in addition to linear models. Models based on transactions only were also derived. Confidence intervals were used throughout the paper to assess the values of the models' parameters. The dependence of the ratio between Function Points and COSMIC Function Points on size was studied. The union of all the available datasets was also studied, to overcome problems due to the relatively small size of datasets. It is shown that outliers do affect the linear models, typically increasing the slope of the regression lines; however, this happens mostly in small datasets: in the union of the available datasets there is no outlier that can influence the model. Conditions for the applicability of the statistical conversion are identified, in terms of relationships that must hold among the basic functional components of Function Point measures. Non-linear models are shown to represent well the relationships between the two measures, since the ratio between COSMIC Function Points and Function Points appears to increase with size. In general, it is confirmed that convertibility can be modeled by different types of models. This is a problem for practitioners, who have to choose one of these models. Anyway, a few practical suggestions can be derived from the results reported here. The model assuming that one FP is equal to one CFP causes the biggest conversion errors observed and is not generally supported. All the considered datasets are characterized by a ratio of transaction to data functions that is fairly constant throughout each dataset: this can be regarded as a condition for the applicability of current models; under this condition non-linear (log-log) models perform reasonably well. The fact that in Function Point Analysis the size of a process is limited, while it is not so in the COSMIC method, seems to be the cause of non linearity of the relationship between the two measures. In general, it appears that the conversion can be successfully based on transaction functions alone, without losing in precision. © 2013 Springer Science+Business Media New York.",COSMIC function points; Function point analysis; Functional size measure convertibility; Functional size measurement; Statistical confidence,Lavazza L.,2014,Journal,Empirical Software Engineering,10.1007/s10664-013-9246-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901837528&doi=10.1007%2fs10664-013-9246-z&partnerID=40&md5=6c8f12ff811fe290e0d03ccb1e1497b3,"Dipartimento di Scienze Teoriche e Applicate, Università Degli Studi dell'Insubria, Via Mazzini, 5, 21100 Varese, Italy",Kluwer Academic Publishers,English,13823256,
Scopus,Effective development of automation systems through domain-specific modeling in a small enterprise context,"High development and maintenance costs and a high error rate are the major problems in the development of automation systems, which are mainly caused by bad communication and inefficient reuse methods. To overcome these problems, we propose a more systematic reuse approach. Though systematic reuse approaches such as software product lines are appealing, they tend to involve rather burdensome development and management processes. This paper focuses on small enterprises. Since such companies are often unable to perform a ""big bang"" adoption of the software product line, we suggest an incremental, more lightweight process to transition from single-system development to software product line development. Besides the components of the transition process, this paper discusses tool selection, DSL technology, stakeholder communication support, and business considerations. Although based on problems from the automation system domain, we believe the approach may be general enough to be applicable in other domains as well. The approach has proven successful in two case studies. First, we applied it to a research project for the automation of a logistics lab model, and in the second case (a real-life industry case), we investigated the approaches suitability for fish farm automation systems. Several metrics were collected throughout the evolution of each case, and this paper presents the data for single system development, clone&own and software product line development. The results and observable effects are compared, discussed, and finally summarized in a list of lessons learned. © 2012 Springer-Verlag Berlin Heidelberg.",Automation system; Domain-specific modeling; Small enterprise cost model; Software product line; System development process,"Leitner A., Preschern C., Kreiner C.",2014,Journal,Software and Systems Modeling,10.1007/s10270-012-0289-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893788486&doi=10.1007%2fs10270-012-0289-1&partnerID=40&md5=80e26600a4dc9adcd60e9fb70a9ebd5a,"Institute for Technical Informatics, Graz University of Technology, Graz, Austria; HOFERNET IT-Solutions, 9811 Lendorf, Austria",,English,16191366,
Scopus,Predicting the size of test suites from use cases: An empirical exploration,"Software testing plays a crucial role in software quality assurance. Software testing is, however, a time and resource consuming process. It is, therefore, important to estimate as soon as possible the effort required to test software. Unfortunately, little is known about the prediction of the testing effort. The study presented in this paper aims at exploring empirically the prediction of the testing effort from use cases. We address the testing effort from the perspective of test suites size. We used four metrics to characterize the size and complexity of use cases, and three metrics to quantify different perspectives of the size of corresponding test suites. We used the univariate logistic regression analysis to evaluate the individual effect of each use case metric on the size of test suites. The multivariate logistic regression analysis was used to explore the combined effect of the use case metrics. The performance of the prediction models was evaluated using receiver operating characteristic analysis. An experimental study, using data collected from five Java case studies, is reported providing evidence that some of the use case metrics are significant predictors of the size of test suites. © IFIP International Federation for Information Processing 2013.",Logistic Regression Analysis; Metrics; Prediction; Relationship; Software Testing; Test Suite Size; Testing Effort; Use Cases,"Badri M., Badri L., Flageol W.",2013,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-41707-8_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893409178&doi=10.1007%2f978-3-642-41707-8_8&partnerID=40&md5=754403befe1a21feae0ec408e151c3ae,"Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-Rivières, QC, Canada",,English,03029743,9783642417061
Scopus,Software effort estimation using functional link neural networks optimized by improved particle swarm optimization,This paper puts forward a new learning model based on improved particle swarm optimization (ISO) for functional link artificial neural networks (FLANN) to estimate software effort. The improved PSO uses the adaptive inertia to balance the tradeoff between exploration and exploitation of the search space while training FLANN. The Chebyshev polynomial has been used for mapping the original feature space from lower to higher dimensional functional space. The method has been evaluated exhaustively on different test suits of PROMISE repository to study the performance. The simulation results show that the ISO learning algorithm greatly improves the performance of FLANN and its variants for software development effort estimation. © 2013 Springer International Publishing.,Back propagation; FLANN; ISO; Software effort estimation,"Benala T.R., Mall R., Dehuri S.",2013,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-03756-1_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893310578&doi=10.1007%2f978-3-319-03756-1_18&partnerID=40&md5=ca966e731ff1fdbd5922c2061572eca3,"Department of Information Technology, Jawaharlal Nehru Technological University Kakinada, University College of Engineering, Vizianagaram-535003, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of System Engineering, Ajou University, San 5, Woncheon-dong, Yeongtong-gu, Suwon 443-749, South Korea",,English,03029743,9783319037554
Scopus,Software development productivity prediction of individual projects applying a neural network,"Machine learning techniques have been applied in the software engineering field and their models could be applied for predicting the development productivity of software developers. In this paper, a neural network model was trained from a data set of 140 individual projects developed from between years 2005 and 2008 with practices based on a process specificaly designed to laboratory learning environments: Personal Software Process (PSP). Then, this model was applied for predicting the productivity of a new projects consisting of 156 projects developed from between years 2009 and 2010. The code in all projects was developed by 74 graduated students, using object oriented programming languages C++ and Java. Prediction accuracy obtained from neural network was compared to those obtained from a fuzzy logic model as well as from a statistical regression model. Results suggest that a neural network model could be used for predicting development productivity of individual projects, when they are developed in a disciplined way in a laboratory learning environment.",Feedforward neural network; Fuzzy logic; Software development productivity prediction; Statistical regression.,"López-MARTÍN C., Chavoya A., Meda-CAMPAÑA M.E.",2013,Conference,"IMETI 2013 - 6th International Multi-Conference on Engineering and Technological Innovation, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893032450&partnerID=40&md5=3fcabf1558f89b8f5d4b6e9f16e0b08e,"Information Systems Department, Universidad de Guadalajara, Jalisco, Mexico",,English,,9781936338924
Scopus,A mapping model for assessing project effort from requirements,"Since the effort required to develop a system depends on its requirements, it is important to consider the resulting effort when deciding on the requirements. Miscalculating the effort may lead to requirements that cannot be implemented within given budget constraints. In order to support requirements engineers in calculating the effort resulting from the requirements they elaborate correctly, we develop a mapping model for assessing project effort from requirements (MMAPER) in this paper. MMAPER incorporates effort estimation into requirements engineering and thereby enables engineers to proactively assess project effort without demanding that they spend significant additional time on this task. MMAPER measures system size using function point analysis and assesses the resulting effort using the Constructive Cost Model 2. The theoretical underpinning of the methods stems from theoretical perspectives from which we derive theories of how requirements determine the resulting project effort. We also take into consideration that it is important to distinguish requirements of different size but also implemented in different contexts for estimating the resulting effort. We empirically evaluate the model using data from five case studies which we conducted with a financial services organization. The developed model provides very accurate effort estimations, across both controlled experiments and field applications. © 2012 Springer-Verlag.",Design research; Mapping model; Project effort; Requirements,"Zickert F., Beck R.",2013,Journal,Information Systems and e-Business Management,10.1007/s10257-012-0195-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880825187&doi=10.1007%2fs10257-012-0195-7&partnerID=40&md5=00ef7756fa56e2cd8dabd149abd69eb8,"Chair of Business Administration, esp. E-Finance and Services Science, Goethe University Frankfurt, Grüneburgplatz 1, 60323 Frankfurt, Germany",,English,16179846,
Scopus,Multi-step Generation of Bayesian Networks Models for Software Projects Estimations,"Software projects estimations are a crucial component of successful software development. There have been many approaches that deal with this problem by using different kinds of techniques. Most of the successful techniques rely on one shot prediction of some variables, as cost, quality or risk, taking into account some metrics. However, these techniques usually are not able to deal with uncertainty on the data, the relationships among metrics or the temporal aspect of projects. During the last decade, some researchers have proposed the use of Bayesian Belief Networks (BBNs) to perform better estimations, by explicitly taking into account the previous shortcomings. But, these approaches were based on manually defining those BBNs and handling only one of the estimation variables (cost, quality or risk). In this paper, we present an approach for semi-automatically building BBNs by using machine learning techniques. We describe two algorithms to generate such BBNs. The first one generates one-shot BBNs, while the second one generates BBNs that take into account the temporal aspect of project development. We performed experiments on real data coming from two software companies, obtaining a 63% of accuracy on multiclass classification. Our main interest was to find a semantically correct model that can be trained with future projects to increase its accuracy. In this sense, we introduce a well-balanced approach to make good predictions with strong explanatory power. © 2013 Copyright the authors.",Bayesian Belief Networks; Software estimation,"Fuentetaja R., Borrajo D., López C.L., Ocón J.",2013,Journal,International Journal of Computational Intelligence Systems,10.1080/18756891.2013.805583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879633878&doi=10.1080%2f18756891.2013.805583&partnerID=40&md5=3a37489576d3def6bddb3c7f0b119983,"Universidad Carlos III de Madrid, Spain; GMV, S.A., Spain",,English,18756891,
Scopus,The influence of leadership and work culture on software cost effort,"Globalization is adding more dimensions to software effort estimation process. The notions of leadership and culture carry with them highly variable assumptions, and thus, must be explicitly modeled. A new model that incorporates leadership and culture is proposed, elaborated and validated. A survey was undertaken to determine the impact of culture and its effect on the software development process in the areas of project team timeliness, collaboration and team work, leadership characteristics, cultural intelligence, motivation and communication. The use of the Bootstrap method for estimating the effort involved in a given project, along with analogies using real historical data, demonstrates the effectiveness of this approach in surmounting difficulties in describing abstract quantitative variables. Our approach is tested on a cluster sample dataset of 41 cases (projects) collected in 2007 from more than 20 organizations. The results show that the inclusion of leadership and culture in the cost estimation model improves the accuracy of software cost estimation. © 2013 ACADEMY PUBLISHER.",Bootstrapping; CBR; Effort estimation; Leadership; Ontology; Team culture,"Hamdan K., Belkhouche B., Smith P.",2013,Journal,Journal of Software,10.4304/jsw.8.6.1353-1367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878676761&doi=10.4304%2fjsw.8.6.1353-1367&partnerID=40&md5=11c050a04557f47167285c60024485a9,"UGRU, UAE University, Al Ain, United Arab Emirates; College of Information Technology, UAE University, Al Ain, United Arab Emirates; University of Sunderland, Sunderland, United Kingdom",,English,1796217X,
Scopus,Lehman's Laws and the productivity of increments: Implications for productivity,"Which are the consequences of Lehman's Laws of Software Evolution for the productivity of incrementally developed projects? The concept of Incremental Development Productivity Decline (IDPD), which deals with how the productivity of incrementally developed software develops over its increments, is introduced. It is explained how Lehman's Laws of Software Evolution apply to it and how maintenance and reuse are relevant to both. Every Law of Software Evolution is discussed individually from a qualitative standpoint with regard to whether it could be a cause of IDPD. After that discussion, the overall situation is examined in light of how different courses of action cause which laws to apply different degrees of effects. © 2013 IEEE.",Development productivity; Incremental development; Software evolution,"Moazeni R., Link D., Boehm B.",2013,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2013.84,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936873607&doi=10.1109%2fAPSEC.2013.84&partnerID=40&md5=995f77ae02dc2cfd1414e2a1b751c92a,"Computer Science Department, University of Southern California, Los Angeles, United States",IEEE Computer Society,English,15301362,9781479921430; 9780769549224
Scopus,Model-based simplified functional size measurement - An experimental evaluation with COSMIC function points,"Functional Size Measurement methods -like the COSMIC method- are widely used but have two major shortcomings: they require a complete and detailed knowledge of user requirements and they are carried out via relatively expensive and lengthy processes. To tackle these issues, simplified measurement processes have been proposed that can be applied to requirements specifications even if they are incomplete or not very detailed. Since software requirements can be effectively modeled using languages like UML and the models increase their level of detail and completeness through the development lifecycle, our goal is to define the characteristics of progressively refined requirements models that support progressively more sophisticated and accurate measurement processes for functional software size. We consider the COSMIC method and three simplified measurement processes, and we show how they can be carried out, based on UML diagrams. Then, the accuracy of the measurement supported by each type of UML model is empirically tested, by analyzing the results obtained on a set of projects. Our analysis shows that it is possible to write progressively more detailed and complete user requirements UML models that provide the data required by simplified methods, which provide progressively more accurate values for functional size measures of the modeled software. Conclusions. Developers that use UML for requirements model can obtain an estimation of the application's functional size early on in the development process, when only a very simple UML model has been built for the application, and can get increasingly more accurate size estimates while the knowledge of the product increases and UML models are refined accordingly. Copyright © 2013 for the individual papers by the papers' authors.",COSMIC function points; Function points; Functional size measurement; Model-based measurement; Simplified measurement processes; UML,"Del Bianco V., Lavazza L., Liu G., Morasca S., Abualkishik A.Z.",2013,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924961827&partnerID=40&md5=4d18e72a2379f35474b92b66689b2d2b,"Dipartimento di Scienze Teoriche e Applicate, Università degli Studi dell'Insubria, Varese, Italy; IT department, Al-Khawarizmi, International College, Abu Dhabi, United Arab Emirates",CEUR-WS,English,16130073,
Scopus,A systematic mapping study of empirical studies on the use of pair programming in industry,"Previous systematic literature reviews on pair programming (PP) lack in their coverage of industrial PP data as well as certain factors of PP such as infrastructure. Therefore, we conducted a systematic mapping study on empirical, industrial PP research. Based on 154 research papers, we built a new PP framework containing 18 factors. We analyzed the previous research on each factor through several research properties. The most thoroughly studied factors in industry are communication, knowledge of work, productivity and quality. Many other factors largely lack comparative data, let alone data from reliable data collection methods such as measurement. Based on these gaps in research further studies would be most valuable for development process, targets of PP, developers' characteristics, and feelings of work. We propose how they could be studied better. If the gaps had been commonly known, they could have been covered rather easily in the previous empirical studies. Our results help to focus further studies on the most relevant gaps in research and design them based on the previous studies. The results also help to identify the factors for which systematic reviews that synthesize the findings of the primary studies would already be feasible. © 2013 World Scientific Publishing Company.",empirical studies; gaps in research; industrial studies; Pair programming; systematic mapping study,"Vanhanen J., Mäntylä M.V.",2013,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194013500381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899055480&doi=10.1142%2fS0218194013500381&partnerID=40&md5=68aef53147ee3e7e6e304b6084727673,"Department of Computer Science and Engineering, Aalto University, PO Box 15400, FI-00076, Aalto, Finland",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Model of system configuration requirements and measurement of their functional size,"Currently, software development organisations are expected to deliver projects on time and on budget in a context of fixed-price contracts. The main goal of this research paper is to design a new configuration model using, as a basis, the European International standards ECSS-E-40 and ECSS-Q-80B together with the ISO-19761, for measuring in particular the embedded software. Measurement of such non-functional requirements (NFRs) should be feasible and as early as possible in software engineering projects. The motivation for this research project is to contribute to improve some of the inputs required for cost estimation techniques for embedded software, more specifically the measurement of NFRs. Copyright © 2013 Inderscience Enterprises Ltd.",Configuration requirements; ECSS international standards; Functional size; ISO 19759; ISO 19761.,"Al-Khasawneh A., Al-Sarayreh K.T., Meridji K., Khasawneh M.",2013,Journal,International Journal of Networking and Virtual Organisations,10.1504/IJNVO.2013.056992,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885917807&doi=10.1504%2fIJNVO.2013.056992&partnerID=40&md5=e9c4c03c92244f21dd2118fd14a02bb2,"Departments of Computer Information System, Hashemite University, Zarqa 13115, Jordan; Departments of Software Engineering, Hashemite University, Zarqa 13115, Jordan; Software Engineering Department, Petra University, Amman 11196, Jordan; Management Information System Department, World Islamic Sciences and Education University, Amman 11947, Jordan",Inderscience Publishers,English,14709503,
Scopus,Early stage software effort estimation using function point analysis: Empirical evidence,"Software effort and cost estimation are necessary at the early stage of the software development life cycle for the project manager to be able to successfully plan for the software project. Unfortunately, most of the estimation models depend on details that will be available at the later stage of the development process. This paper proposes to use Function Point Analysis in application with dataflow diagram to solve this timing critical problem. The proposed methodology was validated through the graduate student software projects at the Chulalongkorn University Business School. Although the results were disappointed but some interesting insights are worth looking into.",Early stage function point analysis; Early stage software effort estimation; Software effort empirical evidence; Software effort estimations,Arnuphaptrairong T.,2013,Conference,Lecture Notes in Engineering and Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880079456&partnerID=40&md5=020530757b9d6faedd73b4dea56acec9,"Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok 10250, Thailand",Newswood Limited,English,20780958,9789881925268
Scopus,Repository analysis tools in teaching software engineering,"Software Engineering is an important part of modern Communications Engineering education. In this paper a number of open-source tools for analysis of Subversion source code repositories are evaluated for purpose of determining individual developer contribution to a project. These tools are compared in the context of university bachelor level course in Software Engineering. Finally, they are used to measure actual projects developed by students during the length of course and compare their results with student self-reported time spent working on the project. © 2012 IEEE.",measuring developer contribution; repository analysis; software engineering education; software metrics,"Ljubovic V., Nosovic N.",2012,Conference,"2012 9th International Symposium on Telecommunications, BIHTEL 2012 - Proceedings",10.1109/BIHTEL.2012.6412077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874040568&doi=10.1109%2fBIHTEL.2012.6412077&partnerID=40&md5=187ea788923792436610e22252600237,"Elektrotehnicki Fakultet Sarajevo, Zmaja od Bosne bb, 71000, Sarajevo, Bosnia and Herzegovina",,English,,
Scopus,Experiments in parallel matrix multiplication on multi-core systems,"Matrix multiplication is an example of application that is both easy to specify and to provide a simple implementation. There exist numerous sophisticated algorithms or very efficient complex implementations. In this study we are rather interested in the design/programming overhead with respect to performance benefits. Starting from the naive sequential implementation, the implementation is first optimised by improving data accesses, then by using vector units of modern processors, and we finally propose a parallel version for multi-core architectures. The various proposed optimisations are experimented on several architectures and the trade-off software complexity versus efficiency is evaluated using Halstead metrics. © 2012 Springer-Verlag.",matrix multiplication; memory accesses; shared-memory parallelism; SIMD unit; software metrics,"Legaux J., Jubertie S., Loulergue F.",2012,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-33078-0_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866662470&doi=10.1007%2f978-3-642-33078-0_26&partnerID=40&md5=61e42a1944a9aef203ec553a0001c9e2,"LIFO, University of Orléans, France",,English,03029743,9783642330773
Scopus,Measuring metadata-based aspect-oriented code in model-driven engineering,"Metrics measurement for cost estimation in model-driven engineering (MDE) is complex because of number of different artifacts that can potentially be generated. The complexity arises as auto-generated code, manually added code, and non-code artifacts must be sized separately for their contribution to overall effort. In this paper, we address measurement of a special kind of code artifacts called metadata-based aspect-oriented code. Our MDE toolset delivers large database-centric business-critical enterprise applications. We cater to special needs of enterprises by providing support for customization along three concerns, namely design strategies, architecture, and technology platforms (〈d, a, t〉) in customer-specific applications. Code that is generated for these customizations is conditional in nature, in the sense that model-to-text transformation takes place differently based on choices along these concerns. In our recent efforts to apply Constructive Cost Model (COCOMO) II to our MDE practices, we discovered that while the measurement of the rest of code and non-code artifacts can be easily automated, product-line-like nature of code generation for specifics of 〈d, a, t〉 requires special treatment. Our contribution is the use of feature models to capture variations in these dimensions and their mapping to code size estimates. Our initial implementation suggests that this approach scales well considering the size of our applications and takes a step forward in providing complete cost estimation for MDE applications using COCOMO II. © 2012 IEEE.",,"Sunkle S., Kulkarni V., Roychoudhury S.",2012,Conference,"2012 3rd International Workshop on Emerging Trends in Software Metrics, WETSoM 2012 - Proceedings",10.1109/WETSoM.2012.6226990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864152347&doi=10.1109%2fWETSoM.2012.6226990&partnerID=40&md5=6339e417006696241fb247ce61d31693,"Tata Research Development and Design Center, Tata Consultancy Services, 54B, Industrial Estate, Hadapsar, Pune, 411013, India",,English,,9781467317627
Scopus,Improving estimation accuracy by using case based reasoning and a combined estimation approach,"Several models of software estimation are available in the industry today. Models are available for estimating project cost, schedule, defects, personnel required etc. Available estimation models can be classified as being analogy based, parameter based, expert based and size based. Experimental results show that different models yield different results of estimates for the same project. In this paper, we demonstrate a technique that combines two available techniques. Our approach permits adding parameters for estimation dynamically based on the context of the project using case based reasoning. These parameters are used to compute a similarity index, which in turn, is used for estimation. Estimates can also be revised based on delay causing incidents that occur during the execution of the project. Data from about 10 projects executed in the past shows that estimates developed using our approach can converge to actual values over a period of time. © 2012 ACM.",case based reasoning (CBR); cost; estimation; schedule; software size; WBS,"Gopal S., D'Souza M.",2012,Conference,"Proceedings of the 5th India Software Engineering Conference, ISEC'12",10.1145/2134254.2134267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858419115&doi=10.1145%2f2134254.2134267&partnerID=40&md5=d43c155697fffa4861cef97cb07df453,"International Institute of Information Technology, Bangalore, India",,English,,9781450311427
Scopus,EFES: An effort estimation methodology,"The estimation of effort is at the heart of project tasks, since it is used for many purposes such as cost estimation, budgeting, monitoring, project planning, control and software investments. Researchers analyze problems of the estimation, propose new models and use new techniques to improve estimation accuracy. However, effort estimation problem is not only computational but also a managerial problem and we need a defined estimation methodology to guide companies in their effort estimation tasks. Management of such methodology requires estimation goals, execution steps, applied measurement methods and updating mechanisms to be properly specified. In this study we presented the development steps for developing such an Effort Estimation Methodology, called EFES. The methodology is based on the reported best practices, empirical results obtained from a medium scale company and solutions to problems &conflicts described in literature. The validation and applicability of EFES methodology is performed in a medium scale company. © 2012 IEEE.",Development Effort; Effort Estimation Methodology; Estimation Processes; Functional Size Measurement,"Tunalilar S., Demirors O.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900859406&doi=10.1109%2fIWSM-MENSURA.2012.37&partnerID=40&md5=1d1dff9eaab16c8a7e1cacfd544361d8,"Informatics Institute, Middle East Technical University, Ankara, Turkey",IEEE Computer Society,English,,
Scopus,Measuring and evaluating a DotNet application system to better predict maintenance effort,"The ISO standard 9126 defines the basic quality criteria for evaluating a software product and suggests a suite of metrics for measuring them, however it remains for the user of the standard to apply those metrics to his particular situation. This paper describes how the metrics were extended to assess the static quality criteria as well as the complexity of a large Dot Net application. In addition, the size of the software was measured to be able to compare it with similar systems of the same type. The result was a comparative evaluation to aide the owners of that system in planning further maintenance and evolution activities. Besides that cost estimations were made for maintenance and further development. The measurement project described here is a practical example of how metrics can be applied to assess existing software systems. © 2012 IEEE.",ISO-9126; Quality Assessment; Software Measurement,"Sneed H.M., Erdos K.",2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900858018&doi=10.1109%2fIWSM-MENSURA.2012.14&partnerID=40&md5=d5a0b84e4e38a346baeaf8e059ef7529,"ANECON GmbH, Vienna, Austria; ANECON GmbH, Budapest, Hungary",IEEE Computer Society,English,,
Scopus,Empirical effort and schedule estimation for enterprise resource planning projects,"Enterprise Resource Planning (ERP) projects are considered commercial-off-the-shelf (COTS) solutions where one substantial COTS product (e.g. Oracle, SAP) is tailored to provide automated business functions. This paper presents effort and schedule estimating models for predicting ERP implementation. The first two models use product size to predict software development effort. Product size is measured in terms of the number of report, interface, conversion, and extension (RICE) objects that are tailored or modified in the COTS product suite. The third model predicts the duration of software development phase as a function of RICE objects and full time equivalent staff. These statistical models are based on empirical data collected from 22 programs implemented within the federal government over the course of ten years beginning in 2001. Result shows that number of RICE objects is a good predictor of ERP software development effort and duration. © 2012 IEEE.",Effort estimation; ERP; RICE objects; Schedule estimation; Software development; Software estimation,Rosa W.,2012,Conference,"Proceedings of the 2012 Joint Conf. of the 22nd Int. Workshop on Software Measurement and the 2012 7th Int. Conf. on Software Process and Product Measurement, IWSM-MENSURA 2012",10.1109/IWSM-MENSURA.2012.35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900855821&doi=10.1109%2fIWSM-MENSURA.2012.35&partnerID=40&md5=cbe61f988610f459d298f11141db2e4d,"Air Force Cost Analysis Agency, Joint Base Andrews NAF, 1500 West perimeter Rd, Washington, MD 20762-9998, United States",IEEE Computer Society,English,,
Scopus,A rule-based approach for estimating software development cost using function point and goal and scenario based requirements,"Function point is a method used to measure software size and estimate the development cost. However, for large complex systems, cost estimation is difficult because of the large number of requirements expressed in natural language. In this paper we propose a rule-based approach for estimating software development cost in the requirements analysis phase. It combines goal and scenario based requirements analysis with function point based cost estimation. In our proposed approach, Context Analysis Guiding rules, Data Function Extraction Guiding rules, and Transaction Function Extraction Guiding rules have been developed to identify function points from text based goal and scenario descriptions. These rules are established based on a linguistic approach. The contribution of the proposed approach is to help project managers decide which requirements should be realized. © 2011 Elsevier Ltd. All rights reserved.",Cost estimation; Function point; Goal; Project management; Requirements triage; Scenario,"Choi S., Park S., Sugumaran V.",2012,Journal,Expert Systems with Applications,10.1016/j.eswa.2011.07.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855175933&doi=10.1016%2fj.eswa.2011.07.029&partnerID=40&md5=930ebca62bb1e36b6b1bc097e66486d4,"DMC R and D Center, Samsung Electronics, Suwon, Gyeonggi-do 443-742, South Korea; Department of Computer Science, Sogang University, Seoul 121-742, South Korea; Department of Decision and Information Sciences, School of Business Administration, Oakland University, Rochester, MI 48309, United States; Department of Service Systems Management and Engineering, Sogang Business School, Sogang University, Seoul 121-742, South Korea",,English,09574174,
Scopus,Effective monitoring and control of outsourced software development projects,"In our study of four outsourcing projects we discover mechanisms to support managerial decision making during software development processes. We report on Customer Office, a framework used in practice that facilitates reasoning about projects by highlighting information paths and making co-ordination issues explicit. The results suggest a key role of modularisation and standardisation to assist in value creation, by facilitating information flow and keeping the overview of the project. The practical implications of our findings are guidelines for managing outsourcing projects such as to have a modularised view of the project based on knowledge domains and to standardise co-ordination operations. © Springer Science+Business Media, LLC 2011.",Outsourcing; Project management; Software development,"Ponisio L., Vruggink P.",2011,Conference,Information Systems Development: Asian Experiences,10.1007/978-1-4419-7355-9_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881461653&doi=10.1007%2f978-1-4419-7355-9_12&partnerID=40&md5=c9aaddda3b0d4854f33feb4e1a2d7b34,"University of Twente, Twente, Netherlands",,English,,9781441972057
Scopus,Adjusting effort estimation using micro-productivity profiles,"We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model and the team. Micro-productivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show the calculated micro-productivity profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change oriented estimation.",,"Tóth G., Végh A.Z., Beszédes A., Schrettner L., Gergely T., Gyimo ́thy T.",2011,Conference,SPLST'11 - Proceedings 12th Symposium on Programming Languages and Software Tools,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869840947&partnerID=40&md5=ed0ba85998358a1fb4a0ddd50f71d6f8,"Department of Software Engineering, University of Szeged, Hungary",,English,,9789949231782
Scopus,An exploration of functional size based effort estimation models,"A number of methods have been proposed to build a relationship between effort and size. These models are generally based on regression analysis and a widely accepted model is not yet available. Although in some sizing methods, such as MKII and IFPUG, different multipliers for the base functional components (BFC) exist, their origin and the purpose of their usage are undefined. The COSMIC method does not treat components separately and assigns the same measurement unit to each of them. In this study we used the Artificial Neural Network and regression based methods to create effort estimation models that take the four components of the COSMIC method into consideration. In the research we compared several functional size based effort models in terms of accuracy using a reliable company dataset. These models comprised not only the generic models proposed in the literature or currently in use, but also specific models that we generated using our dataset with a single and multi-variate regression analysis and the ANN method. We also explored the effect of functional similarity (FS) using our specific models. We found that using BFC instead of total size improved effort estimation models and the ANN method is a useful approach to calibrate these components according to the company characteristics. © 2011 World Scientific Publishing Company.",artificial neural network; base functional components; Effort estimation model; functional similarity,"Tunalilar S., Demirors O.",2011,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194011005347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960802985&doi=10.1142%2fS0218194011005347&partnerID=40&md5=224564c4645a804f67aeada91a24238b,"Informatics Institute, Middle East Technical University, Ankara, Turkey",,English,02181940,
Scopus,Cluster analysis & Pso for software cost estimation,The modern day software industry has seen an increase in the number of software projects .With the increase in the size and the scale of such projects it has become necessary to perform an accurate requirement analysis early in the project development phase in order to perform a cost benefit analysis. Software cost estimation is the process of gauging the amount of effort required to build a software project. In this paper we have proposed a Particle Swarm Optimization (PSO) technique which operates on data sets which are clustered using the K-means clustering algorithm. The PSO generates the parameter values of the COCOMO model for each of the clusters of data values. As clustering encompasses similar objects under each group PSO tuning is more efficient and hence it generates better results and can be used for large data sets to give accurate results. Here we have tested the model on the COCOMO81 dataset and also compared the obtained values with standard COCOMO model. It is found that the developed model provides better estimation of the effort. © 2011 Springer-Verlag.,Constructive Cost Model (COCOMO); K-Means; Particle Swarm Optimization (PSO); Software Cost Estimation,"Sethi T.S., Hari Ch.V.M.K., Kaushal B.S.S., Sharma A.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-20573-6_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955086949&doi=10.1007%2f978-3-642-20573-6_47&partnerID=40&md5=13dab144ea646fa921525edfcefe47db,"Dept. of CSE, Gitam University, Visakhapatnam, India; Dept. of IT, Gitam University, Visakhapatnam, India",,English,18650929,9783642205729
Scopus,Modeling and scenario simulation for decision support in management of requirements activities in software projects,"There are many tools and techniques readily available to support the work in requirements activities of software development processes. As a consequence, the high frequency of errors still occurring in requirements activities suggests that the misunderstanding of the relationships among key decisions is the probable reason for this. The present work presents a system dynamics model constructed to make it possible for users to better understand the relations among key decision variables in requirements activities. The model was parameterized with data taken from previous studies and from a software development company so as to run two sets of simulations with three scenarios each. Optimistic, baseline and pessimistic scenarios are created on the basis of different assumptions regarding risk factors related to requirements volatility and people turnover. We used our simulation results to foresee the effects of these risk factors on the quality and cost of work in requirements activities. Up-to-date results from the software engineering literature strongly support the simulation outcomes obtained in our research. © 2010 John Wiley & Sons, Ltd.",requirements; risk evaluation; software engineering; software process,"Ambrósio B.G., Braga J.L., Resende-Filho M.A.",2011,Journal,Journal of Software Maintenance and Evolution,10.1002/smr.469,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551492329&doi=10.1002%2fsmr.469&partnerID=40&md5=b02cb2d8312e35433bd9879506980258,"Universidade Federal de Ouro Preto, Rua 37, No. 115, Bairro, Loanda 35931-026, João Monlevade-MG, Brazil; Universidade Federal de Viçosa, Viçosa, MG, Brazil; Universidade Federal de Juiz de Fora, Juiz de Fora, MG, Brazil",John Wiley and Sons Ltd,English,1532060X,
Scopus,Timesheet assistant: Mining and reporting developer effort,"Timesheets are an important instrument used to track time spent by team members in a software project on the tasks assigned to them. In a typical project, developers fill timesheets manually on a periodic basis. This is often tedious, time consuming and error prone. Over or under reporting of time spent on tasks causes errors in billing development costs to customers and wrong estimation baselines for future work, which can have serious business consequences. In order to assist developers in filling their timesheets accurately, we present a tool called Timesheet Assistant (TA) that non-intrusively mines developer activities and uses statistical analysis on historical data to estimate the actual effort the developer may have spent on individual assigned tasks. TA further helps the developer or project manager by presenting the details of the activities along with effort data so that the effort may be seen in the context of the actual work performed. We report on an empirical study of TA in a software maintenance project at IBM that provides preliminary validation of its feasibility and usefulness. Some of the limitations of the TA approach and possible ways to address those are also discussed. © 2010 ACM.",Development activity; Estimation; Mining; Timesheet,"Sindhgatta R., Narendra N.C., Sengupta B., Visweswariah K., Ryman A.G.",2010,Conference,ASE'10 - Proceedings of the IEEE/ACM International Conference on Automated Software Engineering,10.1145/1858996.1859049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649788881&doi=10.1145%2f1858996.1859049&partnerID=40&md5=ee356d50e0a3cea655150c97105a62d6,"IBM Research India, Bangalore, India; IBM Rational, Toronto, Canada",,English,,9781450301169
Scopus,On the problem of attribute selection for software cost estimation: Input backward elimination using artificial neural networks,"Many parameters affect the cost evolution of software projects. In the area of software cost estimation and project management the main challenge is to understand and quantify the effect of these parameters, or 'cost drivers', on the effort expended to develop software systems. This paper aims at investigating the effect of cost attributes on software development effort using empirical databases of completed projects and building Artificial Neural Network (ANN) models to predict effort. Prediction performance of various ANN models with different combinations of inputs is assessed in an attempt to reduce the models' input dimensions. The latter is performed by using one of the most popular saliency measures of network weights, namely Garson's Algorithm. The proposed methodology provides an insight on the interpretation of ANN which may be used for capturing nonlinear interactions between variables in complex software engineering environments. © 2010 IFIP.",Artificial Neural Networks; Connection Weights; Garson's Algorithm; Software Cost Estimation,"Papatheocharous E., Andreou A.S.",2010,Conference,IFIP Advances in Information and Communication Technology,10.1007/978-3-642-16239-8_38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549253481&doi=10.1007%2f978-3-642-16239-8_38&partnerID=40&md5=745ed1a29401a2b56a6fe33a83024a1f,"University of Cyprus, Department of Computer Science, 75 Kallipoleos Street, CY1678 Nicosia, Cyprus; Cyprus University of Technology, Department of Electrical Engineering and Information Technology, 31 Archbishop Kyprianos Street, 3036 Lemesos, Cyprus",,English,18684238,364216238X; 9783642162381
Scopus,Wrapper-based feature ranking techniques for determining relevance of software engineering metrics,"Classification, an important data mining function that assigns class label to items in a collection, is of practical applications in various domains. In software engineering, for instance, a common classification problem is to determine the quality of a software item. In such a problem, software metrics represent the independent features while the fault proneness represents the class label. With many classification problems, one must often deal with the presence of irrelevant features in the feature space. That, coupled with class imbalance, renders the task of discriminating one class from another rather difficult. In this study, we empirically evaluate our proposed wrapper-based feature ranking where nine performance metrics aided by a particular learner and a methodology are considered. We examine five learners and take three different approaches, each in conjunction with one of three different methodologies: 3-fold Cross-Validation, 3-fold Cross-Validation Risk Impact, and a combination of the two. In this study, we consider two sets of software engineering datasets. To evaluate the classifier performance after feature selection has been applied, we use Area Under Receiver Operating Characteristic curve as the performance evaluator. We investigate the performance of feature selection as we vary the three factors that form the foundation of the wrapper-based feature ranking. We show that the performance is conditioned by not only the choice of methodology but also the learner. We also evaluate the effect of sampling on wrapper-based feature ranking. Finally, we provide guidance as to which software metrics are relevant in software defect prediction problems and how the number of software metrics can be selected when using wrapper-based feature ranking. © 2010 World Scientific Publishing Company.",Feature selection; performance measures; ranker aid; sampling techniques; software metrics; wrapper-based feature ranking,"Altidor W., Khoshgoftaar T.M., Gao K.",2010,Journal,"International Journal of Reliability, Quality and Safety Engineering",10.1142/S0218539310003883,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78951489336&doi=10.1142%2fS0218539310003883&partnerID=40&md5=efb2505b6467fe09d758ea74ba6a99a7,"Department of Computer Science and Engineering, Florida Atlantic University, 777 Glades Road, Boca Raton, FL 33431, United States; Department of Mathematics and Computer Science, Eastern Connecticut State University, 83 Windham Street, Willimantic, CT 06226, United States",,English,02185393,
Scopus,Practical software project total cost estimation methods,"Software project management has become a crucial field of research due to the increasing role of software in today's world. When a project is insufficiently supported, the quality and speed of the project will suffer. Improving the functions of project management is a main concern in software development organizations. The essence of this paper is to estimate software project cost by Analogy estimation model, incorporating organizational and intercultural factors was developed and evaluated. Analysis was done to show how such added factors can improve the overall accuracy of estimating the cost of a project. ©2010 IEEE.",,"Hamdan K., El Khatib H., Shuaib K.",2010,Conference,MCIT'2010: International Conference on Multimedia Computing and Information Technology,10.1109/MCIT.2010.5444853,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952772006&doi=10.1109%2fMCIT.2010.5444853&partnerID=40&md5=e881dfdd70f3bc08c7897a230f7645af,"UAE University, United Arab Emirates",,English,,9781424470037
Scopus,GUI of esrcTool: A tool to estimate the software risk and cost,"Function Point Analysis was developed first by Allan J. Albrecht in the mid 1970s. It was an attempt to overcome difficulties associated with lines of code as a measure of software size, and to assist in developing a mechanism to predict effort associated with software development. Function Point is a well known established method to estimate the size of software projects. There are several areas of the software engineering in which we can use the function point analysis (FPA) like project planning, project construction, software implementation etc. In this paper we have implemented the architecture of the esrcTool in C Language. This tool is used for two different purposes, firstly, to estimate the risk in the software and secondly to estimate the cost of the software. The esrcTool is based on SRAEM i.e. Software Risk Assessment and Estimation Model, because in this model FP is used as an input variable, and in order to determine the cost of the software we have used the International Software Benchmarking Standards Group Release Report (ISBSG). ©2010 IEEE.",Cost; esrcTool; Function point; Software risk; SRAEM,"Sadiq M., Sunil, Zafar S., Asim M., Suman R.",2010,Conference,"2010 The 2nd International Conference on Computer and Automation Engineering, ICCAE 2010",10.1109/ICCAE.2010.5451301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952660588&doi=10.1109%2fICCAE.2010.5451301&partnerID=40&md5=8aed693a4fb07b2f95ea91e4e19bb3f8,"Department of University Polytechnic, Faculty of Engineering and Technology, Jamia Millia Islamia (A Central University), New Delhi-110025, India; Department of Computer Science and Engineering, Al-Falah School of Engineering and Technology, Faridabad, India; Maharshi Dayanand University, Haryana, India",,English,,9781424455850
Scopus,Estimating the implementation time for discrete-event simulation model building,"There are several techniques for estimating cost and time for software development. These are known in software engineering as ""software metrics."" LOC (lines of code), COCOMO (COnstructive COst Model), and FPA (Function Point Analysis) are examples of such techniques. Although Discrete Event Simulation Modeling (DESM) has some differences from classical software development, it is possible to draw a parallel between these techniques and DESM. This article reviews some of the metrics from software engineering, and, based on those, proposes a metric for estimating time for the implementation of a simulation model using one specific simulation software. The results obtained for 22 real simulation projects showed that the proposed technique can estimate the time for software development with acceptable accuracy (average error of 6% and maximum absolute error of 38%) for models that have less that 200 simulation objects. ©2010 IEEE.",,"Chwif L., Banks J., Barretto M.R.P.",2010,Conference,Proceedings - Winter Simulation Conference,10.1109/WSC.2010.5678891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951612800&doi=10.1109%2fWSC.2010.5678891&partnerID=40&md5=d5c285985117a51c993aa76f73417dab,"Escola de Engenharia Mauá, Praça Mauá 1, São Caetano do Sul, 09580-900, Brazil; Tecnológico de Monterrey, Av. Eugenio Garza Sada 2501, Monterrey, 64849, NL, Mexico; Universidade de São Paulo, Av. Prof. Mello Moraes 2231, São Paulo, 05508-900, Brazil",Institute of Electrical and Electronics Engineers Inc.,English,08917736,9781424498666
Scopus,Approaching software cost estimation using an entropy-based fuzzy k-modes clustering algorithm,"A new software cost estimation approach is proposed in this paper, which attempts to cluster empirical, non-homogenous project data samples via an entropy- based fuzzy k-modes clustering algorithm. The target is to identify groups of projects sharing similar characteristics in terms of cost attributes or descriptors, and utilise this grouping information to provide estimations of the effort needed for a new project that is classified in a certain group. The effort estimates produced address the uncertainty and fuzziness of the clustering process by yielding interval predictions based on the mean and standard deviation of the samples having strong membership within a cluster. Empirical validation of the proposed methodology was conducted using a filtered version of the ISBSG dataset and yielded encouraging results both in terms of practical usage of the clustered groups and of approximating effectively project costs.",,"Papatheocharous E., Andreou A.S.",2009,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887219145&partnerID=40&md5=aa4f88f4d2712ccf459bd3a5b94050fe,"Department of Computer Science, University of Cyprus, 75 Kallipoleos str., CY1678 Nicosia, Cyprus",,English,16130073,
Scopus,Software productivity measurement: Past analysis and future trends,"The search for productivity improvement has been a growing concern of organizations in recent time. To achieve this goal is necessary, first, to know how to measure the productivity. The wide literature about this subject confirms the interest about this topic. A lot of questions already reside and there are many myths about the productivity. This paper conducted an investigation about the productivity metrics and measurement studies published in literature, categorize the productivity metrics indentified according some criteria and identified the main lessons learned about this matter. We encountered that the most common metric used to this end is the SLOC (Source Line Of Code) despite the heavy critics and paradox published about it. In another way we detected that there are some indications of maturation on this topic, the new studies point to the use of more labored models, which try to evaluate the various perspectives of productivity. Based on the literature review and on analyses of proposed metrics encountered in them, we propose some directions in order to evolve the state of the art in software productivity measurement and, consequently, on the software productivity theme.",,"Soares De Aquino Jr. G., De Lemos Meira S.R.",2009,Conference,"International Conference on Software Engineering Theory and Practice 2009, SETP 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878221710&partnerID=40&md5=897aed3aae6032ca8bc2581c50a3904f,"Centro de Informática, Universidade Federal de Pernambuco (UFPE), P.O. Box 7851, Recife, Brazil; Recife Center for Advanced Studies and Systems (C.E.S.A.R.), Recife, Brazil",,English,,9781615676590
Scopus,Formalization studies in functional size measurement: How do they help?,"Functional size has been favored as a software characteristic that can be measured early and independent of language, tools, techniques and technology; hence has many uses in software project management. It has been about three decades since Albrecht introduced the concept of functional size. However, Functional Size Measurement (FSM) has not been a common practice in the software community. The problems with FSM method structures and practices have been discussed to be the major factors to explain this situation. In this paper, we make a review of formalization proposals to the problems in Functional Size Measurement (FSM). We analyze the works included in the papers and we explore the extent of their contributions. ©Springer-Verlag Berlin Heidelberg 2009.",Formalization; Functional size measurement methods; Software functional size measurement; Software models,"Ozkan B., Demirors O.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650667582&doi=10.1007%2f978-3-642-05415-0_15&partnerID=40&md5=015a70f7886b68e89cec950a8c244173,"Informatics Institute, Middle East Technical University, Ankara 06531, Turkey",,English,03029743,3642054145; 9783642054143
Scopus,A comparison of neural network model and regression model approaches based on sub-functional components,"A number of models have been proposed to build a relationship between effort and software size, however we still do have difficulties for effort estimation. ANN and Regression models are two modeling approaches for effort estimation. In this study we investigated whether considering subcomponents of sizing methods increase the accuracy of prediction of effort on ANN and Regression models. Our effort models were built by utilizing ""subcomponents of Cosmic Functional Size"". Besides these subcomponents, ""application type"" is also considered as input for these models to analyze its effect on effort estimation. We also studied the functional similarity concept by examining its effect on improving the accuracy of these models. The dataset consist of 18 completed projects of the same organization. © Springer-Verlag Berlin Heidelberg 2009.",Artificial neural networks; Functional components; Regression models; Size estimation,"Tunalilar S., Demirors O.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650649435&doi=10.1007%2f978-3-642-05415-0_20&partnerID=40&md5=941a3ef12db68df7771585a17bf8186a,"ASELSAN MGEO Division, Turkey; Informatics Institute, Middle East Technical University, Turkey",,English,03029743,3642054145; 9783642054143
Scopus,Personal software process capability assessment method,"Personal Software Process (PSP) was introduced by Watts Humphrey in CMU/SEI. It is a measured software process aiming at individual software engineers. With the increasing industrial demand for software process improvement, PSP has become a hot topic for software organizations to achieve the goal of total (from macro to micro) quantitative process management. Since higher process capability is recognized as a determinant of better project performance, it is a critical step to assess the personal software process. However, the assessment of PSP capability exhibits Variable Return to Scale (VRS), Multi-Input-Multi-Output (MIMO) and Decision-Making preference problems, which makes existing traditional assessment methods ineffective. In this paper, a novel Personal Software Process Assessment method by synthesizing Data Envelopment Analysis (DEA) and Analytical Hierarchy Process (AHP)-PSPADA is proposed. PSPADA's hybrid model and fundamental assessment algorithms (incorporating decision-making preferences and estimating return to scale) are introduced. Experimental results show that the proposed PSPADA model would be particularly helpful in assessing the capability of personal software processes under the MIMO and VRS constraint, by incorporating Decision-Making preferences. © by Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Analytic hierarchy process (AHP); Data envelopment analysis (DEA); Multi-input-multi-output (MIMO); Personal software process (PSP); Variable return to scale (VRS),"Zhang S., Wang Y.-J., Ruan L.",2009,Journal,Ruan Jian Xue Bao/Journal of Software,10.3724/SP.J.1001.2009.00582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72749124476&doi=10.3724%2fSP.J.1001.2009.00582&partnerID=40&md5=90be52eaeb8f3b530699569c84ae23f3,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Acad. of Sci., Beijing 100190, China; Graduate University, Chinese Acad. of Sci., Beijing 100049, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Acad. of Sci., Beijing 100190, China; School of Computer Science and Engineering, Beihang University, Beijing 100191, China",,English,10009825,
Scopus,Timeline prediction framework for iterative software engineering projects with changes,"Even today, software projects still suffer from delays and budget overspending. The causes for this problem are compounded when the project team is distributed across different locations and generally attributed to the decreasing ability to communicate well (due to cultural, linguistic, and physical distance). Many projects, especially those with off-shoring component, consist of small iterations with changes, deletions and additions, yet there is no formal model of the flow of iterations available. A number of commercially available project prediction tools for projects as a whole exist, but the model adaptation process by iteration, if it exists, is unclear. Furthermore, no project data is available publicly to train on and understand the iterative process. In this work, we discuss parameters and formulas that are well founded in the literature and demonstrate their use within a simulation tool. Project timeline prediction capability is demonstrated on various scenarios of change requests. On a real-world example, we show that iteration-based data collection is necessary to train both the parameters and formulas to accurately model the software engineering process to gain a full understanding of complexities in software engineering process. © 2009 Springer-Verlag Berlin Heidelberg.",,"Berkling K., Kiragiannis G., Zundel A., Datta S.",2009,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-642-01856-5_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876245945&doi=10.1007%2f978-3-642-01856-5_2&partnerID=40&md5=2bda551db0985378e648c74d4aa1ffd4,"Polytechnic University of Puerto Rico, Department of Computer Science and Engineering, 377 Ponce de León Ave., San Juan, 00918, Puerto Rico; Department of Computer Science, School of Computational Science, Florida State University, Tallahassee, FL 32306, United States",Springer Verlag,English,18651348,3642018556; 9783642018558
Scopus,Hybrid computational models for software cost prediction: An approach using artificial neural networks and genetic algorithms,"Over the years, software cost estimation through sizing has led to the development of various estimating practices. Despite the uniqueness and unpredictability of the software processes, people involved in project resource management have always been striving for acquiring reliable and accurate software cost estimations. The difficulty of finding a concise set of factors affecting productivity is amplified due to the dependence on the nature of products, the people working on the project and the cultural environment in which software is built and thus effort estimations are still considered a challenge. This paper aims to provide size- and effort-based cost estimations required for the development of new software projects utilising data obtained from previously completed projects. The modelling approach employs different Artificial Neural Network (ANN) topologies and input/output schemes selected heuristically, which target at capturing the dynamics of cost behavior as this is expressed by the available data attributes. The ANNs are enhanced by a Genetic Algorithm (GA) whose role is to evolve the network architectures (both input and internal hidden layers) by reducing the Mean Relative Error (MRE) produced by the output results of each network. © 2009 Springer Berlin Heidelberg.",Artificial neural networks; Genetic algorithms; Software cost estimation,"Papatheocharous E., Andreou A.S.",2009,Journal,Lecture Notes in Business Information Processing,10.1007/978-3-642-00670-8_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64549085386&doi=10.1007%2f978-3-642-00670-8_7&partnerID=40&md5=32c6f9991bdd8718c799fcb7a55bd91c,"University of Cyprus, Department of Computer Science, 75 Kallipoleos str., CY1678 Nicosia, Cyprus",Springer Verlag,English,18651348,9783642006692
Scopus,A framework for understanding and addressing the semiotic quality of use case models,"As software systems become ever more interactive, there is a need to model the services they provide to users, and use cases are one abstract way of doing that. As use cases models become pervasive, the question of their communicability to stakeholders arises. In this chapter, we propose a semiotic framework for understanding and systematically addressing the quality of use case models. The quality concerns at each semiotic level are discussed and process- and product-oriented means to address them in a feasible manner are presented. The scope and limitations of the framework, including that of the means, are given. The need for more emphasis on prevention over cure in improving the quality of use case models is emphasized. The ideas explored are illustrated by examples. © 2009, IGI Global.",,Kamthan P.,2008,Book Chapter,Model-Driven Software Development: Integrating Quality Assurance,10.4018/978-1-60566-006-6.ch013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901562378&doi=10.4018%2f978-1-60566-006-6.ch013&partnerID=40&md5=416c26605e07e930231db00d8587ed5d,"Concordia University, Canada",IGI Global,English,,9781605660066
Scopus,MaST: A tool for aiding the staff training management by using influence diagrams,Large software development projects involve several risk factors that adversely influence their performance and completion. The risk factors can be avoided by properly training the staff members involved with the project. A procedure is required to help the project managers make decisions about the management of staff training. This paper presents the use of influence diagrams to implement a risk model to formalize combining several risk factors to aid decision making about the management of staff training. The proposed model presents the possible risk factors that have an impact on staff training. It also considers the cost of conducting the training which is based on these factors. The model can provide accurate requirement details for staff training at the beginning of a typical software development project. A prototype tool is developed based on this model.,Bayesian network; Influence diagram; Risk exposure; Risk management; Staff training,"Jeet K., Mago V.K., Prasad B., Minhas R.S.",2008,Conference,"International Conference on Software Engineering Theory and Practice 2008, SETP 2008",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249090175&partnerID=40&md5=102d7fab1869e6de031b2e4e27314969,"Department of Computer Science, DAV College, Jalandhar, India; Department of Computer and Information Sciences, Florida A and M University, Tallahassee, FL 32307, United States; Department of Computer Science, MLUDAV College, Phagwara, India",,English,,9781615677191
Scopus,Early estimate the size of test suites from use cases,"Software quality becomes an increasingly important factor in software marketing. It is well known that software testing is an important activity to ensure software quality. Despite the important role that software testing plays, little is known about the prediction of test suites size. Estimation of testing size is a crucial activity among the tasks of testing management. Work plan and subsequent estimations of the effort required are made based on the estimation of test suites size. The earlier test suites size estimation we do, the more benefit we will get in the process of testing. This paper presents an experience-based approach for the test suites size estimation. The main findings are: (1) Model of use case verification points. (2) Linear relationship between use case verification points and test case number. The test case number prediction model deduced from the data of real projects in a financial software company. © 2008 IEEE.",,"Qu Y., Zhou B., Zhu X.",2008,Conference,"Neonatal, Paediatric and Child Health Nursing",10.1109/APSEC.2008.62,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60849100802&doi=10.1109%2fAPSEC.2008.62&partnerID=40&md5=fd19455c3850f5ff00526c72b24d3a0d,"College of Computer Science, Zhejiang University, Hangzhou, China",,English,14416638,
Scopus,Managing uncertainty in erp project estimation practice: An industrial case study,Uncertainty is a crucial element in managing projects. This paper's aim is to shed some light into the issue of uncertain context factors when estimating the effort needed for implementing enterprise resource planning (ERP) projects. We outline a solution approach to this issue. It complementarily deploys three techniques to allow a tradeoff between ERP projects requiring more effort than expected and those requiring less. We present the results of a case study carried out in a telecommunication company site. © 2008 Springer-Verlag Berlin Heidelberg.,,Daneva M.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249117858&doi=10.1007%2f978-3-540-69566-0_10&partnerID=40&md5=8db94ccae56972e48d6688ff197c60df,University of Twente,,English,03029743,3540695648; 9783540695646
Scopus,NBC-Universal uses a novel qualitative forecasting technique to predict advertising demand,"NBC-Universal (NBCU), a subsidiary of the General Electric Company (GE), implemented a novel demand prediction and analysis system to support its annual upfront market. The upfront market is a brief period in late May when the television networks sell a majority of their on-air advertising inventory. The system uses an innovative combination of the Delphi method and the Grass Roots forecasting methodology to estimate demand for television commercial time. We embedded this forecasting methodology within a workflow system that automates the demand estimates gathering process and seamlessly integrates into NBCU's existing sales systems. Since 2004, over 200 sales and finance personnel at NBCU have been using the system to support sales decisions during the upfront market when NBCU signs advertising deals worth over $4.5 billion. The system enables NBCU to sell and analyze pricing scenarios across all of NBCU's television properties with ease and sophistication, while predicting demand with a high accuracy. NBCU's sales leaders credit the system with having given them a unique competitive advantage. © 2008 INFORMS.",Advertising and media; Decision analysis; Delphi technology; Forecasting,"Bollapragada S., Gupta S., Hurwitz B., Miles P., Tyagi R.",2008,Journal,Interfaces,10.1287/inte.1080.0346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349110829&doi=10.1287%2finte.1080.0346&partnerID=40&md5=cfa03edc4cc03deeff81eeb0fe104283,"GE Global Research Center, Niskayuna, NY 12309, United States; ESPN Media Networks, New York, NY 10023, United States",,English,00922102,
Scopus,A statistical methodology to simplify software metric models constructed using incomplete data samples,"Software metric models predict the target software metric(s), e.g., the development work effort or defect rates, for any future software project based on the project's predictor software metric(s), e.g., the project team size. Obviously, the construction of such a software metric model makes use of a data sample of such metrics from analogous past projects. However, incomplete data often appear in such data samples. Moreover, the decision on whether a particular predictor metric should be included is most likely based on an intuitive or experience-based assumption that the predictor metric has an impact on the target metric with a statistical significance. However, this assumption is usually not verifiable ""retrospectively"" after the model is constructed, leading to redundant predictor metric(s) and/or unnecessary predictor metric complexity. To solve all these problems, we derived a methodology consisting of the k-nearest neighbors (k-NN) imputation method, statistical hypothesis testing, and a ""goodness-of-fit"" criterion. This methodology was tested on software effort metric models and software quality metric models, the latter usually suffers from far more serious incomplete data. This paper documents this methodology and the tests on these two types of software metric models. © 2007 World Scientific Publishing Company.",Imputation method; Missing data; Model simplification; Models; Software metrics; Software quality,"Chan V.K.Y., Wong W.E., Xie T.F.",2007,Conference,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194007003495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849129507&doi=10.1142%2fS0218194007003495&partnerID=40&md5=534a91ad8899525e3919d39588bad96c,"School of Business, Macao Polytechnic Institute, Rua de Luis Gonzaga Gomes, Macau; Department of Computer Science, University of Texas at Dallas, Richardson, TX 75083, United States; Department of Mathematics, School of Science, Beijing Institute of Technology, Beijing 100081, China",,English,02181940,
Scopus,Cost estimation and analysis for government contract pricing in China,"Software cost estimation methods and their applications in government contract pricing have been developed and practiced for years. However, in China, the government contract process has been questioned in some aspects. It is largely based on analogy to past experience and/or expert judgment, with a lack of informed decision making supported by mature estimation methods. Moreover, two primary stages of the contract review process for technical and finance contents are disjointed, which greatly limits the accuracy and efficiency of government investment decision. To improve cost estimation and assessment practices in Chinese government contract pricing, we propose the Constructive Government cost MOdel (COGOMO), which provides guidance and insights for formal cost estimation. This model emphasizes the importance of accumulating knowledge from both government and industry data repositories, and leverages to establish an industry benchmarking reference model for local government contract pricing. It integrates multiple classical research results in addition to COCOMO II, and establishes the first formal model on software cost estimation and analysis for Chinese government context. A list of suggestions is also discussed for government's further improvement on estimating practices. © Springer-Verlag Berlin Heidelberg 2007.",Cost analysis; Cost estimation; Government contract pricing,"He M., Yang Y., Wang Q., Li M.",2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-72426-1_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149040169&doi=10.1007%2f978-3-540-72426-1_12&partnerID=40&md5=66f13666beea04bb99fcde9e2d1a1a5f,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China; Center for Systems and Software Engineering, University of Southern California, 941 W. 37th Place, SAL 330, Los Angeles, CA 90089, United States; Graduate University, Chinese Academy of Sciences, Beijing 100039, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China",Springer Verlag,English,03029743,3540724257; 9783540724254
Scopus,Mining open source web repositories to measure the cost of evolutionary reuse,"This paper proposes evolutionary reuse as a metric to measure the effect of maintenance and replacement decisions made by open source developers and relate them to cost efficiency. Evolutionary reuse is defined as the similarity of code between two versions of the same application. Maintenance can be seen as the creation of a new version of an application with an high degree of evolutionary reuse. Conversely, replacement takes places when a significant part of code is re-implemented from scratch and evolutionary reuse is low. The paper proposes an empirical model to measure evolutionary reuse and development costs by mining open-source software repository. 26 projects for a total of 171 application versions were analyzed. Results show that maintenance choices in an open-source context are not always cost efficient. Developers tend to maximize the reuse of code from the most recent versions of applications, even if their requirements are far from current needs. Consequently, the development costs per new line of code are found to grow with evolutionary reuse. © 2006 IEEE.",,Capra E.,2006,Conference,"2006 1st International Conference on Digital Information Management, ICDIM",10.1109/ICDIM.2007.369242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849146205&doi=10.1109%2fICDIM.2007.369242&partnerID=40&md5=7fb8ce737b97ed7171d1ec02848c9151,"Politecnico di Milano, Department of Electronics and Information, Italy",,English,,142440682X; 9781424406821
Scopus,Industrial case study of software maintenance evaluations,"This paper describes an empirical case study focusing on maintenance evaluation of a large industrial legacy system. Aspects relevant to these kinds of studies are exemplified. The first part of the study focused on characterizing performed maintenance tasks and required effort at general level. The second part was larger and included gathering and analysis of versatile information relevant for improving maintenance and evolution of the system. The information concerned: maintainability aspects, maintenance problems, and maintenance processes. The information was gathered from maintenance experts via interviews. This part provided insights on the nature of relevant maintenance issues in improving practical industrial software engineering. The case study approach has been a practical and good way to focus to those legacy systems, which are problematic regarding their further evolution. Recommendations for improving the evolvability of the target system were provided. The case study also supports initiation of long-term software maintenance process improvement.",Empirical software engineering; Software maintainability; Software maintenance; Software process improvement,"Koskinen J., Lintinen H., Tilus T., Sivula H., Kankaanpää I., Ahonen J.J., Juutilainen P.",2006,Conference,"Proceedings of the IASTED International Conference on Software Engineering, as part of the 24th IASTED International Multi-Conference on APPLIED INFORMATICS",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047115874&partnerID=40&md5=b5881f9ab06dea28b1520060523b59f9,"Information Technology Research Institute, University of Jyväskylä, P.O. Box 35, 40014 Jyväskylä, Finland; Department of Computer Science, University of Kuopio, P.O. Box 1627, 70211 Kuopio, Finland",,English,,0889865744; 9780889865747
Scopus,An empirical study to investigate software estimation trend in organizations targeting CMMISM,"Paper discusses software estimation practices existing in industry and literature, its strengths and weaknesses. Main focus is the gap analysis of organization with respect to CMMI Level 3 for SE/SW/IPPD/SS. Data collection reveals that company makes use of heuristic approaches in which expert judgment supplemented with wideband Delphi, was mainly used for software estimation. In the light of CMMI Level 3 for SE/SW, it was suggested that formal methods for estimating size, effort and cost for the project should be implemented apart from heuristics used for estimation. Different estimation methodologies are applicable in different categories of projects. None of them gives 100% accuracy but proper use of them makes estimation process smoother. Future work is calibration of parametric software estimation approaches for the organization under study by making use of organizational process database to plan, estimate and tailor project variables that best suits organization's processes and procedures, © 2006 IEEE.",,"Nasir M., Ahmad H.F.",2006,Conference,"Proceedings - 5th IEEE/ACIS Int. Conf. on Comput. and Info. Sci., ICIS 2006. In conjunction with 1st IEEE/ACIS, Int. Workshop Component-Based Software Eng., Softw. Archi. and Reuse, COMSAR 2006",10.1109/ICIS-COMSAR.2006.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947684885&doi=10.1109%2fICIS-COMSAR.2006.19&partnerID=40&md5=b9174140fff158b7b49aba1663a44e8b,"NUST Institute of Information Technology, National University of Sciences and Technology, Pakistan; Communication Technologies, Omachi, Aoba-ku, Sendai, Japan",,English,,0769526136; 9780769526133
Scopus,Cost implications of software commonality and reuse,"Commonality of requirements and reuse of code are two properties of software systems which can potentially reduce development and maintenance costs. This paper provides a possible definition for commonality and reuse and analyzes their impact on maintenance and replacement cost estimations based on the software engineering literature. Eventually, research questions are formulated to guide the empirical analysis through software evolution trees, a tool which is proposed to formalize the different vertical and horizontal changes that a software system can undergo. © 2006 IEEE.",Commonality; Maintenance and replacement policies; Software cost; Software evolution trees; Software reuse,"Capra E., Francalanci C.",2006,Conference,"Proceedings - Third International Conference onInformation Technology: New Generations, ITNG 2006",10.1109/ITNG.2006.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750823682&doi=10.1109%2fITNG.2006.45&partnerID=40&md5=7cacb146472c617bb490aa656988b8b4,"Politecnico di Milano, Department of Electronics and Information, Italy",,English,,0769524974; 9780769524979
Scopus,Resource decisions in software development using risk assessment model,"The resource decisions in software project using cost models do not satisfy managerial decision, as it does not support trade-off analysis among resources. A Bayesian net approach enables this analysis; however, it requires discretization of parameter values and thus sacrifices accuracy. Although narrow intervals can alleviate this problem, the number of states grows quickly with the demanded accuracy. The Bayesian approach also requires pre-setting of the measurement scale, which may not be applicable to all users. In this paper, we propose using a risk assessment model to aid software resource decisions. The methodology employs a continuous function that captures key parameters of software development, such as development time, staff productivity, requirement volatility and software complexity. Using the model, users can perform trade-off analysis among various resource allocations and outcomes. We also propose integrating this model with optimization to solve complicated problems, which can be accomplished straightforwardly with the proposed methodology. © 2006 IEEE.",,"Jiamthubthugsin W., Sutivong D.",2006,Conference,Proceedings of the Annual Hawaii International Conference on System Sciences,10.1109/HICSS.2006.419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749624198&doi=10.1109%2fHICSS.2006.419&partnerID=40&md5=228ab7c8303a6573a7b188d7cd106d92,"Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Thailand",,English,15301605,0769525075; 9780769525075
Scopus,Support Vector Machines for Data Modeling with Software Engineering Applications,"support vector machine (SVM)data modelingsoftwareengineering applications This chapter presents the basic principles of support vector machines (SVM) and their construction algorithms from an applications perspective. The chapter is organized into three parts. The first part consists of Sects. 53.2 and 53.3. In Sect. 53.2 we describe the data modeling issues in classification and prediction problems. In Sect. 53.3 we give an overview of a support vector machine (SVM) with an emphasis on its conceptual underpinnings. In the second part, consisting of Sects. 53.4–53.9, we present a detailed discussion of the support vector machine for constructing classification and prediction models. Sections 53.4 and 53.5 describe the basic ideas behind a SVM and are the key sections. Section 53.4 discusses the construction of optimal hyperplane for the simple case of linearly separable patterns and its relationship to the Vapnik–Chervonenkis dimension. A detailed example is used for illustration. The relatively more difficult case of nonseparable patterns is discussed in Sect. 53.5. The use of inner product kernels for nonlinear classifiers is described in Sect. 53.6 and is illustrated via an example. Nonlinear regression is described in Sect. 53.7. The issue of specifying SVM hyperparameters is addressed in Sect. 53.8, and a generic SVM construction flowchart is presented in Sect. 53.9. The third part details two case studies. In Sect. 53.10 we present the results of a detailed analysis of module-level NASA data for developing classification models. In Sect. 53.11, effort data from 75 projects is used to obtain nonlinear prediction models and analyzetheir performance. Section 53.12 presents some concluding remarks, current activities in support vector machines, and some guidelines for further reading. © 2006, Springer-Verlag.",,"Lim H., Goel A.",2006,Book Chapter,Springer Handbooks,10.1007/978-1-84628-288-1_53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71249109938&doi=10.1007%2f978-1-84628-288-1_53&partnerID=40&md5=be481267bde8257dad4a6e4b6967739a,"Ubiquitous Computing Research Center, Korea Electronics Technology Institute (KETI), 68 Yatap-dong, Bundang-Gu, Seongnam-Si, Gyeonggi-Do  463-816, South Korea; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY  13244, United States",Springer,English,25228692,
Scopus,Efficiency analysis of model-based review in actual software design,"In this paper, we quantitatively analyze the efficiency of the Model-Based Review (MBR) method in an actual software design from the two points of view; cost and reviewability. The MBR method is a modeling procedure for the purpose of reviewing preliminary design specifications of web-based applications. We have collected process data in applying both of the MBR, method and an ordinary review to a preliminary design of a developing web-based library system. Analyzing the collected process data, we quantitatively compare the efficiency of the MBR method and that of the ordinary review. As a, result of this comparative analysis, we show that the MBR, method is superior to the ordinary review in terms of not only reviewability but also cost through the experimental design process.",Cost estimation; Design specification; Model based review; Reviewability; Web-based application,"Furusawa H., Choi E.-H., Watanabe H.",2006,Conference,Proceedings - International Conference on Software Engineering,10.1145/1134285.1134372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247133113&doi=10.1145%2f1134285.1134372&partnerID=40&md5=a1e087dee7f2304ac144818c54c3c19c,"Research Center for Verification and Semantics, National Institute of Advanced Industrial Science and Technology (AIST), 1-2-14 Shin-Senri, Nishi, Toyonaka, Osaka, 560-0083, Japan",IEEE Computer Society,English,02705257,1595933751; 9781595933751
Scopus,A fuzzy logic model for software development effort estimation at personal level,"No single software development estimation technique is best tor all situations, A careful comparison of the results of several approaches is most likely to produce realistic estimates, On the other hand, unless engineers have the capabilities provided by personal training, they cannot properly support their teams or consistently and reliably produce quality products. In this paper, an investigation aimed to compare a personal Fuzzy Logic System (FLS) with linear regression is presented. The evaluation criteria are based upon ANOVA of MRE and MER, as well as MMRE, MMER and pred(25). One hundred five programs were developed by thirty programmers. From these programs, a FLS is generated for estimating the effort of twenty programs developed by seven programmers. The adequacy checking as well as a validation of the FLS are made. Results show that a FLS can be used as an alternative for estimating the development effort at personal level. © Springer-Verlag Berlin Heidelberg 2006.",,"Lopez-Martin C., Yáñez-Márquez C., Gutierrez-Tornes A.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11925231_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845920339&doi=10.1007%2f11925231_12&partnerID=40&md5=8a1a23e3553bf1cde0e551c8bdbc9ad9,"Center for Computing Research, Edificio CIC, National Polytechnic Ins0074itute, Av. Juan Dios Batiz s/n e.M.O.M., P.O. 07738, Mexico D.F., Mexico; Banamex, Mexico, D.F., Mexico; ITESM, Mexico",Springer Verlag,English,03029743,3540490264; 9783540490265
Scopus,Integrating reuse measurement practices into the ERP requirements engineering process,"The management and deployment of reuse-driven and architecturecentric requirements engineering processes have become common in many organizations adopting Enterprise Resource Planning solutions. Yet, little is known about the variety of reusability aspects in ERP projects at the level of requirements. Neither, we know enough how exactly ERP adopters benefit from reuse as part of the requirements engineering process. This paper sheds some light into these questions and suggests a practical approach to applied ERP requirements reuse measurement by incorporating reuse metrics planning as part of the implementation of metrics on an ERP project. Relevant process integration challenges are resolved in the context of SAP R/3 implementation projects in which the author participated while being employed at the second largest telecommunication company in Canada. © Springer-Verlag Berlin Heidelberg 2006.",,Daneva M.,2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11767718_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746238139&doi=10.1007%2f11767718_12&partnerID=40&md5=ec538e583d1431eb778e41524e9379aa,"Department of Computer Science, University of Twente, P.O. Box 217, 7500 AE Enschede, Netherlands",Springer Verlag,English,03029743,3540346821; 9783540346821
Scopus,Do arbitrary function approximators make sense as software prediction models?,"Predicting software development effort with high precision is still a major challenge for the industry and a major research area in software engineering. A significant share of research on software prediction is devoted to research on arbitrary function approximators, such as estimation by analogy, regression trees and artificial neural networks. This paper questions the use of arbitrary function approximators (AFA's) for software prediction by invoking theory of science and appealing to common sense. We argue that arbitrary function approximators may be useful in exploratory data analysis but we question their value for predictive purposes, and especially for software effort prediction. © 2004 IEEE.",,"Myrtveit I., Stensrud E.",2005,Conference,STEP 2004 Proceedings - The 12th International Workshop on Software Technology and Engineering Practice,10.1109/STEP.2004.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249125847&doi=10.1109%2fSTEP.2004.9&partnerID=40&md5=1b44c173e28bed94b937566b0742de09,Norwegian School of Management; Buskerud University College,,English,,0769522939; 9780769522937
Scopus,Metrics for software system families,"Due to the fact that the complexity and development effort of software systems is constantly increasing, software components have to be reused. Software system families are a promising solution to gain a cost reduction by reusing common software assets in different variants of similar products. To support the economic management of this development approach we need software metrics to estimate the effort of building software system families. In general techniques of size measurement and cost estimation for software system families are highly insufficient. Furthermore measurement and estimation approaches do not support a process orientation which characterizes the software of many domains. Therefore this paper describes the Process-Family-Points approach to measure the size and estimate the effort of process focused software system families in multiple domains. Every single step of this new metrics approach will be illustrated from a high level perspective to communicate a conceptional view of this innovative sizing and estimating method.",Automotive; eBusiness; Effort estimation; Process; Process family points; Size measurement; Software system families,"Kiebusch S., Franczyk B., Speck A.",2005,Conference,"7th International Workshop on Economics-Driven Software Engineering Research, EDSER 2005 - International Conference on Software Engineering 2005",10.1145/1083091.1083098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745204822&doi=10.1145%2f1083091.1083098&partnerID=40&md5=3dd3da205895cf7d84073d06d632d457,"Information Systems Institute, Faculty of Economics and Management, University of Leipzig, Germany; Commercial Information Systems, Faculty of Economics and Business, University of Jena, Germany","Association for Computing Machinery, Inc",English,,159593118X; 9781595931184
Scopus,Does the linear size adjustment to estimated effort improve web applications effort estimation accuracy?,"Over the last 16 years, and particularly over the last 8 years, Analogy-based effort estimation has been used to estimate effort for software projects and in several studies has presented comparable estimation accuracy to, or better than, algorithmic methods. The Analogy technique is also potentially easier to understand and apply by both researchers and practitioners. These two factors suggest that this technique has great potential as an effort estimation technique to be used within Companies. However, there are still several challenges, in particular regarding the type of effort adaptation to use in order to obtain the highest prediction accuracy, that need further investigation. Therefore this paper compares several methods of Analogy-based effort estimation and investigates the use of adaptation rules as a contributing factor to better estimation accuracy. Two datasets are used in the analysis; results show that the best predictions are obtained for the dataset that first, presents a continuous ""cost"" function, translated as a strong linear relationship between size and effort, and second, is more ""intact"" in terms of outliers and collinearity. Only one of the two types of adaptation rules employed generated good predictions. © 2005 IOS Press and the authors.",case-based reasoning; prediction models; Web effort prediction; Web hypermedia; Web hypermedia metrics,"Mendes E., Mosley N.",2005,Journal,Journal of Computational Methods in Sciences and Engineering,10.3233/jcm-2005-5s113,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651066762&doi=10.3233%2fjcm-2005-5s113&partnerID=40&md5=d9a1903b25d9db02551afb533ba7ef37,"Computer Science Department, University of Auckland, New Zealand; Metriq Limited, New Zealand",IOS Press,English,14727978,
Scopus,F-effort: A fuzzified model of software effort estimation,"Estimation of effort and resources for a software project is required for planning purposes in very early stages of development. The traditionally used models to estimate effort need very accurate inputs and result in a very precise value, but it leads to a number of problems such as over commitment, which have not been overcome by these models. The reason for these problems lies with the imprecision and vagueness present in various stages of this estimation. In this paper, we have proposed a new model named as F-Effort model, which is a generalization of COCOMO model. The size of the software is estimated as a fuzzy set and the imprecision of complexity is managed with help of fuzzy logic. The F-Effort model has got two phases. First phase uses the complexity of the software to estimate two input parameters, and second phase uses these input parameters along with size to estimate the effort. The model is able to generate the effort estimation either as a crisp value along with degree of possible variation or as a fuzzy set. © 2004 Taylor & Francis Group, LLC.",,"Aggarwal K.K., Singh Y., Chhabra J.K.",2004,Journal,Journal of Discrete Mathematical Sciences and Cryptography,10.1080/09720529.2004.10698016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021728383&doi=10.1080%2f09720529.2004.10698016&partnerID=40&md5=de8831d56acf497eb6f4464da9239673,"G.G.S. Indraprastha University, Delhi, India; School of Information Technology, G.G.S. Indraprastha University, Delhi, India; Department of Computer Engineering, National Institute of Technology, Kurukshetra, India",,English,09720529,
Scopus,Software metrics for real-time systems using fuzzy sets,The recent development of COSMIC-FFP (Common Software Measurement International Consortium) as a functional size measurement metric has greatly improved the estimation of real-time systems. This research attempts to develop effort estimation methods to complement the COSMIC-FFP by fuzzy set based generalization of effort models. The two methods used to achieve this are the fuzzy set model and the fuzzy linear regression model. © 2003 IEEE.,Costs; Data engineering; Design for manufacture; Equations; Fuzzy sets; Lab-on-a-chip; Linear regression; Real time systems; Software engineering; Software metrics,"Raman A., Noore A.",2003,Conference,Proceedings of the Annual Southeastern Symposium on System Theory,10.1109/SSST.2003.1194533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944189509&doi=10.1109%2fSSST.2003.1194533&partnerID=40&md5=490b9997dcd31237c71e6e1216c59030,"Lane Department of Computer Science and Electrical Engg., West Virginia University, Morgantown, WV  26506-6109, United States",Institute of Electrical and Electronics Engineers Inc.,English,,0780376978
Scopus,The personal software process in practice: Experience in two cases over five years,"The Personal Software Process (PSP) started in 1995 and promised to improve individual software engineering practice. PSP addresses in particular the software quality in terms of defect densities and the process quality in terms of defect prevention (yield) and predictability of development time and size of the software products. This experience report of two software developers applying the PSP over five years first in an academic setting and later over three years in industrial software development shows that (1) PSP is an appropriate method to understand your software development process and its capabilities, (2) PSP makes it easy to identify areas of improvements in the process, and (3) PSP allows to do impressively accurate time, size, and defect estimates. The main drawback of PSP is its restricted applicapability in todays software development community. The challenging task to use PSP in your daily work requires a huge amount of discipline. © Springer-Verlag Berlin Heidelberg 2002.",,"Grütter G., Ferber S.",2002,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-47984-8_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958651220&doi=10.1007%2f3-540-47984-8_20&partnerID=40&md5=cfba6e7e6678be52aa3ac998366faab9,"Line Information GmbH, Garbershoff 4, Helmstorf, 21218, Germany; Robert Bosch GmbH, Corporate Research and Development, Eschborner Landstraße 130–132, Frankfurt, D-60489, Germany",Springer Verlag,English,03029743,3540437495; 9783540479840
Scopus,Web hypermedia cost estimation: Further assessment and comparison off cost estimation modelling techniques,"Research into Web cost estimation is relatively new, where few studies have compared cost estimation modelling techniques for Web development, with an emphasis placed on techniques such as Case-based Reasoning (CBR), linear and stepwise regression. Although in a large subset of these studies CBR has given the best predictions, results were based on a simple type of CBR, where no adaptation rules were used to adjust the estimated effort obtained. In addition, when comparing the prediction accuracy of estimation models, analysis has been limited to a maximum of three training/validation sets, which according to recent studies, may lead to untrustworthy results. Since CBR is potentially easier to understand and apply (two important factors to the successful adoption of estimation methods within Web development companies), it should be examined further. This paper has therefore two objectives: i) to further investigate the use of CBR for Web development effort prediction by comparing effort prediction accuracy of several CBR techniques; ii) to compare the effort prediction accuracy of the best CBR technique against stepwise and multiple linear regression, using twenty combinations of training/validation sets. Various measures of effort prediction accuracy were applied. One dataset was used in the estimation process. Stepwise and multiple linear regression showed the best prediction accuracy for the dataset employed. © 2002 Taylor & Francis Group, LLC.",,"Mendes E., Counsell S., Mosley N.",2002,Journal,New Review of Hypermedia and Multimedia,10.1080/13614560208914741,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038617730&doi=10.1080%2f13614560208914741&partnerID=40&md5=ed76d66bce3f517b97f806d58fa7feab,"Computer Science Department, The University of Auckland, Private Bag 92019, Auckland, New Zealand; Computer Science and Information Systems, Birkbeck College, Malet Street, London, WC1E 7HX, United Kingdom; MxM Technology, P.O. Box 3139, Shortland Street, Auckland, New Zealand",,English,13614568,
Scopus,A metric framework for the assessment of object-oriented systems,"In this paper an abstract of the Ph.D. thesis discussed at the University of Florence by the author is presented. The main focus of the thesis was in the identification and validation, both theoretical and empirical, of Object-Oriented metrics for complexity, and then, effort estimation. Complexity/effort metrics have been validated against test cases for verifying effectiveness of the estimation for evaluating development and maintenance effort.",Effort estimation; Maintenance; OO metrics,Fioravanti F.,2001,Conference,"IEEE International Conference on Software Maintenance, ICSM",10.1109/ICSM.2001.972771,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956620162&doi=10.1109%2fICSM.2001.972771&partnerID=40&md5=6f7066d32ec7469e4001ceb816836ee9,,IEEE Computer Society,English,,
Scopus,Metrics-based decision support tool for software module interfacing technique selection to lower maintenance cost,"The Interfacing Techniques Comparison Graph visually compares applications in terms of attributes that relate to maintenance cost. Applications that have both lower coupling and lower complexity lie closer to the origin of the graph and exhibit lower maintenance cost than those that do not. This study supports the idea that compositional techniques are important for achieving these improved metrics. The graph can be used in three ways. First it serves as a decision support tool for managers to determine whether expected maintenance savings compensate for the additional training, effort and time needed to support compositional development. Second, it functions as a decision support tool for designers and coders as they determine, for each module interface, whether to use coupled techniques or composition. The graph can help identify those situations in which the long term cost gain justifies the extra time needed for compositional design. Third, it can serve as a maintenance cost estimation tool. This study found a close correlation between predicted and actual maintenance effort.",,Bitman William R.,1999,Conference,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033316836&partnerID=40&md5=6de0988c4b18d971a2226b9590fdd49b,"Johns Hopkins Univ Applied Physics, Lab, Baltimore, United States","IEEE, Los Alamitos, CA, United States",English,,
Scopus,Measurement and prediction of the verification cost of the design in a formalized methodology,"This article presents a new way to enhance the prediction of software attributes in the first stages of software life cycle through the study of the relationships among the elements of models used in different development phases. A system to predict the cost of verifying the design derived from data flow diagrams (DFD) was defined using this approach in structured analysis and merise (SAM) a formalized methodology. The construction and validation of measures for featuring design attributes is based on a solid theoretical analysis of the properties that should be found in the measured object. Prediction validity is achieved as a logical consequence of development relationships between and among the involved models. Moreover, some empirical data is included to enable observation of the application of the system. The article concludes that this approach can also lead to prediction enhancement through process improvement in other methodologies like SSADM or the object-oriented (OO) ones.",,"Fernández L., Dolado J.J.",1999,Journal,Information and Software Technology,10.1016/S0950-5849(99)00010-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344718535&doi=10.1016%2fS0950-5849%2899%2900010-5&partnerID=40&md5=45522bc8de23abb2ae150c90f6fac761,"Escla. Sup. de Informática, Univ. Europea Madrid, V., Madrid, Spain; Facultad de Informática, Univ. of the Basque Country, 20.009, San Sebastián, Spain","Elsevier Sci B.V., Amsterdam",English,09505849,
Scopus,A straightforward approach to effort estimation for updating programs in object-oriented prototyping development,"Discusses the estimation of the effort needed to update programs according to a given requirement change. In object-oriented prototyping development (OO prototyping), the requirement changes occur frequently and regularly. Thus, a simple and fast estimation of effort is strongly required by both developers and managers. However, existing estimation methods cannot be applied to OO prototyping. Therefore, we propose a straightforward approach to effort estimation, which reflects the specific properties of OO prototyping. First, we analyze the following characteristics of OO prototyping: (1) updating activities consist of creation, deletion and modification; (2) the target to be updated has four kinds of types (void type, basic type, library type and custom type); and (3) the degree of information hiding is classified into private, protected and public. Then, we present a new formula E(P,σ) to calculate the effort needed to update a program P according to a set of requirement changes σ. The formula E(P,σ) includes the weighting parameters wupd, wtype and w/sub inf-h/ according to the characteristics (1), (2) and (3), respectively. Finally, we conduct experimental evaluations by applying the formula E(P,σ) to actual project data in a certain company. The evaluation results prove statistically (to some extent) the validity of the proposed approach. © 1999 IEEE.",,"Uehara S., Mizuno O., Kikuno T.",1999,Conference,"Proceedings - 6th Asia Pacific Software Engineering Conference, APSEC 1999",10.1109/APSEC.1999.809595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978195266&doi=10.1109%2fAPSEC.1999.809595&partnerID=40&md5=103ab44ebe30f330e9d1e93be1e3f568,"Department of Informatics and Mathematical Science, Graduate School of Engineering Science, Osaka University, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,0769505090; 9780769505091
Scopus,A probabilistic model for software projects,A probabilistic model for software development projects is constructed. The model can be applied to compute an estimate for the development time of a project. The chances of succeeding with a given amount of time and the risk of deviating from the estimate can be computed as well. Examples show that the model behaves as expected when the input data are changed. © Springer-Verlag Berlin Heidelberg 1999.,,Padberg F.,1999,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-48166-4_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887415105&doi=10.1007%2f3-540-48166-4_8&partnerID=40&md5=674d83c0fa937e24dfbc0dd853560d64,"Fachbereich Informatik, Universität Saarbrücken, Germany",Springer Verlag,English,03029743,3540665382; 9783540665380
Scopus,A model of the problem or a problem with the model?,"The word 'model' must be one of the most overused in the English language. It can mean an ideal, an abstraction or a cardboard replica. It may be presented as a diagram, printed text, or a set of mathematical formulae. It may represent a physical structure, a logical structure, a sequence of events, or a set of causal relationships. This article examines the various meanings of 'model' and asks if the usages of the term in software engineering jargon are always meaningful. It reviews several types of software model, including life-cycle models, control-flow and data-flow models, design paradigms such as object-orientation and functional decomposition, quality models, and models of system behaviour, and outlines their deficiencies.",,Mellor P.,1998,Review,Computing and Control Engineering Journal,10.1049/cce:19980103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11544340080&doi=10.1049%2fcce%3a19980103&partnerID=40&md5=13399a38c6ba3cffa69ab64c939ca3d8,"Centre for Software Reliability, City University, Northampton Square, London EC1V 0HB, United Kingdom",Institution of Engineering and Technology,English,09563385,
Scopus,Metrics in the software engineering curriculum,"As a recognized discipline, software engineering traces its roots back to the 1968 NATO conference where the term was first used extensively to highlight the need for an engineering approach to the development of software. In the 30 years since that first ""software engineering"" conference, significant attempts have been made to improve the overall effectiveness of the software development process, and thus reduce the frequency and severity of software project failures. A major part of this improvement effort has been the attempt to develop quantitative measures which can be used to more accurately describe and better understand and manage the software development life cycle. Thus, many software metrics and models have been introduced during this period. In this article, we briefly trace the history of the development of software metrics and models, and then summarize the current state of the field. For discussion purposes, this entire development period is then arbitrarily divided into an Introductory Period (1971-1985), Growth Period (1985-1997) and the Current Period (1997-?). The development of metrics during each of these periods is then related to the treatment of software metrics and models in software engineering curricula during that same period. Our conclusion is that software engineering curricula have indeed reflected the state of software engineering as the work in software metrics and models has progressed. Furthermore, software engineering curricula of the future should reflect the relatively mature state that software metrics have attained, by covering the basic concepts of metrics in appropriate core courses, and more advanced metrics topics in a specialized, elective metrics course.",,Mills E.E.,1998,Journal,Annals of Software Engineering,10.1023/a:1018909531948,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043230855&doi=10.1023%2fa%3a1018909531948&partnerID=40&md5=303f1848f8fbb8f8cb1d94e06cdfb0dc,"Comp. Sci./Software Eng. Department, Seattle University, 900 Broadway, Seattle, WA 98122-4340, United States",Springer Netherlands,English,10227091,
Scopus,Statistical analysis of nonstationary software metrics,"Prediction, estimation, and assessment of software process attributes form an integral part of process management. Process modeling is a quantitative and systematic approach to gauging such critical project parameters. However, process modeling relies heavily on idealizations. Mathematical compromises necessitate continuous calibration of such models to maintain some level of accuracy. This is because of the nature of software measurement data, most of which are nonstationary, and thus requires special treatment. A methodology for analyzing such data is presented in this paper. It is shown that measures based on time averages are insufficient in representing data of this nature. The improved representativeness of ensemble based measures over those based on time is demonstrated. A model for LOC generation, based on ensemble averaging, is presented in this paper. However, the process of validating such a model involves testing the model to a range of unique inputs. But, exhaustive testing of process models is generally severely constrained by the lack of sufficient amounts of input data sets, generated under the conditions imposed by the modeling approach. A method employing the random walk, by which an ensemble can be generated from a single time record, on the basis of certain invariants, is provided. This method of generating an ensemble is shown to be useful, especially in situations where certain properties of the proprietary data are known, but sufficient quantities of the data are inaccessible to the analyst. The ensemble computed in this manner is then used to derive a time dependent model that is more representative of the real life entity being modeled. © 1997 Elsevier Science B.V.",Ensemble; Metrics; Nonstationary,"Pillai K., Nair V.S.S.",1997,Journal,Information and Software Technology,10.1016/s0950-5849(96)00002-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031143945&doi=10.1016%2fs0950-5849%2896%2900002-x&partnerID=40&md5=456a75fdf045aa39640332d7ecf77f75,"Dept. of Comp. Sci. and Engineering, SIC, Southern Methodist University, 6425 Airline Road, Dallas, TX 75275, United States",Elsevier,English,09505849,
Scopus,Combining function points software estimation model with ADISSA methodology for systems analysis and design,"The study proposes a combination of the function points model for software estimation with the ADISSA methodology for systems analysis and design. This combined approach, which is supported by a software tool, enables one to estimate various software metrics, such as size, effort, and duration, in the early stages of systems development, by basing them on the products of a thorough system analysis and design process. © 1996 IEEE.",Function points model; Project management; Software estimation; Software metrics; Systems analysis and design,"Shoval P., Feldman O.",1996,Conference,"Proceedings of the 7th Israeli Conference on Computer Systems and Software Engineering, ICCSSE 1996",10.1109/ICCSSE.1996.554842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939771878&doi=10.1109%2fICCSSE.1996.554842&partnerID=40&md5=375beb5c24375f4108ef2a62f6e4a62f,"Information Systems Program, Dept.of Industrial Engineering and Management, Israel; Dept.of Mathematics and Computer Science, Ben-Gurion University of Negev, Beer-Sheva, 84105, Israel",Institute of Electrical and Electronics Engineers Inc.,English,,0818675365; 9780818675362
Scopus,Comparing the top-down and bottom-up approaches of function point analysis: a case study,"Function point analysis is a widely cited method for estimating software project size, which is an important activity of project management. At the beginning stage of planning, the top-down approach can be applied. Having obtained more systems specifications at later stages, the bottom-up approach might also be used to improve the accuracy of the estimation. However, the bottom-up approach is not a conventional way of function point analysis. There was no empirical evidence showing the difference between the fully informed top-down approach and the bottom-up approach. Through the implementation of a function point analysis system in an in-house software development department, this paper compares the results of the two approaches. This comparison study shows that the bottom-up approach does not contribute a significant added value to a fully-informed top-down approach. Therefore, the fully-informed top down approach has been chosen as a method for building a software metric database in the organization. More important, the observations and experience gained from this project may help in-house development organizations to establish their own function point analysis systems. © 1995 Chapman & Hall.",function point analysis; software metric; software size estimation,"Yau C., Gan L.-Y.",1995,Journal,Software Quality Journal,10.1007/BF01351922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345817959&doi=10.1007%2fBF01351922&partnerID=40&md5=effb218f9b6cc37928ea94c173aa330e,"School of Computing and Information Technology, Griffith University, Nathan, 4111, Queensland, Australia; Harnischfeger of Australia, Australia",Kluwer Academic Publishers,English,09639314,
Scopus,A survey of current practice in aerospace software development,"This paper integrates the premise that current software level practices within the aerospace industry are weak and that there is a lack of rigour in both technical and managerial areas. Results from a survey of practitioners are presented that indicate a lack of information interchange exists and that the use of formal techniques is limited. The paper proposes that this is indicative of poor life-cycle practices and that more rigorous methodologies, ones that integrate formal methods with quality practices, are required. A two-level model is proposed to address the issue. © 1995.",aerospace software; software practice; software process modelling,"Plant R.T., Tsoumpas P.",1995,Journal,Information and Software Technology,10.1016/0950-5849(95)98299-U,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029406971&doi=10.1016%2f0950-5849%2895%2998299-U&partnerID=40&md5=9b669586deecbc5c3e25204fbd323beb,"Department of Computer Information Systems, University of Miami, Coral Gables, FL 33124, United States",,English,09505849,
Scopus,MONSET - a prototype software development estimating tool,"The development of large-scale computer software has traditionally been a difficult cost estimation problem. Software has been developed for more than thirty years and it is reasonable to expect that the experience gained in this time would make software development effort predictions more reliable. One way by which an organization can benefit from past projects is to measure, track and control each project and use the collected results to assist future project estimation. This paper describes a hybrid model for software effort prediction and its validation against available data on some large software projects. A prototype software development estimation system (MONSET - Monash Software Estimating Tool) based on the proposed model is described. The system aims to provide guidance for project managers during the software development process.",,"Srinivasan Bala, Martin Geoff",1994,Conference,Proceedings of the Symposium on Assessment of Quality Software Development Tools,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028571627&partnerID=40&md5=ffa531181d818fd727c45b6fe6227647,"Monash Univ, Monash, Australia","IEEE, Los Alamitos",English,,
Scopus,An information-hiding metric,"The most critical problem facing developers and maintainers of large software systems is overwhelming complexity. Abstraction, a powerful tool for dealing with complexity, limits details that must be considered to those that are most relevant. Programming languages have evolved to provide a high-level construct, called a module or a class, that supports abstraction by allowing the encapsulation of details and enforcing information hiding. Programs written in these modular programming languages should be designed to produce programs that are easier to understand, modify, and test. This article describes the creation of metrics for information hiding at the module level. The relationship between the information-hiding metrics and the maintainability of programs is examined by use of subjective validation and a case study of a large Ada program. © 1994.",,"Rising L.S., Calliss F.W.",1994,Journal,The Journal of Systems and Software,10.1016/0164-1212(94)90012-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028497812&doi=10.1016%2f0164-1212%2894%2990012-4&partnerID=40&md5=9c0716c41dc170972b9cfd76ee77fe10,"Air Transport Systems Division, Honeywell Inc., Phoenix, ArizonaUSA; Department of Computer Science and Engineering, Arizona State University, Tempe, ArizonaUSA",,English,01641212,
Scopus,Function points in SSADM,"A set of explicit counting rules for the use of function point analysis in SSADM v4 is presented, motivated by the need for an unambiguous, objective, and inexpensive measure of system size during the development process. Unadjusted MKII function points can be derived from the requirements specification (RS) phase of SSADM; it is argued that the 'complexity adjustment' factor of the standard function point method can be safely omitted. A prescriptive approach is taken, resulting in a repeatable and objective counting technique that may be automated. The application of the counting rules to other SSADM phases is considered. It is concluded that application to requirements analysis (RA) is possible but reduces the objectivity of the proposed method; application to logical system specification (LS) requires only monor extensions to the counting rules; and that the project-specific nature of physical design (PD) makes it unsuitable for application of such a general, prescriptive technique. There are no results of application or validation of the technique. The method can operate satisfactorily from existing project documentation and may be embedded in tools which generate such documentation. It satisfies the need for an xplicit set of objective counting rules for MkII function points for the widely used SSADM environment. © 1993 Chapman & Hall.",function points; metrics; SSADM; structured methods,"O'Brien S.J., Jones D.A.",1993,Journal,Software Quality Journal,10.1007/BF00417423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250079175&doi=10.1007%2fBF00417423&partnerID=40&md5=5c5c8645b301485771a0f5fcc711b2ea,"School of Computing and Mathematics, University of Teesside, Cleveland, TS1 3BA, United Kingdom",Kluwer Academic Publishers,English,09639314,
Scopus,Industrial applications of software measurements,"The development in the field of software measurements is outlined to identify the state of the art available to practitioners. Progress in theoretical, experimental and observational approaches is described together with factors driving the evolution. These factors include development of standards, large-scale research projects and success stories from industry. Recent examples are described of how software measures have been applied to support the management of the software development process. The examples are concerned with the establishment of company norms for, and use of anomalies among, measurement values, and the successful implementation of a large-scale measurement programme. © 1992.",data analysis; practical application; software metrics,Andersen O.,1992,Journal,Information and Software Technology,10.1016/0950-5849(92)90073-X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249013023&doi=10.1016%2f0950-5849%2892%2990073-X&partnerID=40&md5=33356dcef4c8549dec112e7033c5f03c,"ElektronikCentralen, Venlighedsvej 4, DK-2970 Hørsholm, Denmark",,English,09505849,
Scopus,CAN STATISTICAL METHODS HELP SOLVE PROBLEMS IN SOFTWARE MEASUREMENT?,[No abstract available],,Sayward Frederick G.,1981,Conference,,10.1007/978-1-4613-9464-8_28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019691399&doi=10.1007%2f978-1-4613-9464-8_28&partnerID=40&md5=d0c2db4421dfbde5a662340e4216d63b,,"Springer-Verlag, New York, NY, USA",English,,0387906339; 9780387906331
Scopus,Fuzzy Cognitive Mapping Analysis to Recommend Machine Learning-Based Effort Estimation Technique for Web Applications,"Effort estimation is a fairly researched field in the area of software engineering. Algorithmic and non-algorithmic methods are the two popular ways of estimating software development efforts. Various machine learning techniques are also being used to determine project efforts based on the historical project-related dataset. These techniques consume an array of project characteristics to estimate the project cost. The selection of the right technique to correctly determine the project cost is a significant challenge that the software industry is facing. This paper presents a fuzzy cognitive mapping (FCM) approach to recommend the best machine learning-based software estimation technique for Web applications. FCM shows synergistic interactions between system variables, and this property is used in the context of Web application estimation for suggesting an estimation technique based on the Web project configuration. To counter the ambiguity in defining abstract relationships between system variables, this article also proposes to incorporate fuzzy numbers. The current analysis involves using five different estimation techniques on 125 student project records. The mean square error (MSE) was taken as a performance metric to declare the supremacy of one estimation technique over others. The experimental results show that the selection of an effort estimation technique should not ignore the presence of project characteristics in the input vector. The achievement of this work is that the proposed technique is capable of recommending the suitable most Web estimation model based on project credentials for a specific Web project; it refrains from suggesting an estimation model optimum for the most project configurations. The FCM approach on software estimation technique recommendation results in a probability of success equals to 70%. © 2020, Taiwan Fuzzy Systems Association.",Effort estimation; Fuzzy cognitive maps; Fuzzy numbers; Machine learning; MSE,"Pandey P., Litoriya R.",2020,Journal,International Journal of Fuzzy Systems,10.1007/s40815-020-00815-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082846052&doi=10.1007%2fs40815-020-00815-y&partnerID=40&md5=a1b59d896fecb8e08038682444cb7b00,"Department of Computer Science & Engineering, Jaypee University of Engineering & Technology, Raghogarh, Guna, India",Springer,English,15622479,
Scopus,SLDeep: Statement-level software defect prediction using deep-learning model on static code features,"Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable. © 2019",Defect; Fault prediction model; Machine learning; Software fault proneness; Software metric,"Majd A., Vahidi-Asl M., Khalilian A., Poorsarvi-Tehrani P., Haghighi H.",2020,Journal,Expert Systems with Applications,10.1016/j.eswa.2019.113156,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077495784&doi=10.1016%2fj.eswa.2019.113156&partnerID=40&md5=4f7426cf00845674d73b0d9d3be3e13d,"Faculty of Computer Science and Engineering, Shahid Beheshti University G. C.Tehran, Iran; Department of Software Engineering, Faculty of Computer Engineering, University of Isfahan, Isfahan, Iran",Elsevier Ltd,English,09574174,
Scopus,The moving target of visualization software for an increasingly complex world,"Visualization has evolved into a mature scientific field and it has also become widely accepted as a standard approach in diverse fields, including physics, life sciences, and business intelligence. However, despite its successful development, there are still many open research questions that require customized implementations in order to explore and establish concepts, and to perform experiments and take measurements. Many methods and tools have been developed and published but most are stand-alone prototypes and have not reached a mature state that can be used in a reliable manner by collaborating domain scientists or a wider audience. In this study, we discuss the challenges, solutions, and open research questions that affect the development of sophisticated, relevant, and novel scientific visualization solutions with minimum overheads. We summarize and discuss the results of a recent National Institute of Informatics Shonan seminar on these topics. © 2020 Elsevier Ltd",Software engineering; Visualization; Visualization community; Visualization research; Visualization software,"Reina G., Childs H., Matković K., Bühler K., Waldner M., Pugmire D., Kozlíková B., Ropinski T., Ljung P., Itoh T., Gröller E., Krone M.",2020,Journal,Computers and Graphics (Pergamon),10.1016/j.cag.2020.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079246962&doi=10.1016%2fj.cag.2020.01.005&partnerID=40&md5=69c209b2a482b6fd34406e4d08a80362,"University of Stuttgart, Germany; University of Oregon, Eugene, OR, United States; VRVis Research Center, Vienna, Austria; TU Wien, Vienna, Austria; Oak Ridge National Laboratory, Oak Ridge, TN, United States; Masaryk University, Brno, Czech Republic; Ulm University, Ulm, Germany; Linköping University, Norrköping, Sweden; Ochanomizu University, Tokyo, Japan; University of Tübingen, Germany",Elsevier Ltd,English,00978493,
Scopus,Nonparametric regression estimates based on imputation techniques for right-censored data,"Censored data is a kind of data type where the exact value of a response variable is not completely known. Therefore, this case is a problem that should be solved in order to obtain an accurate and efficient data analysis. Recently, imputation methods have been used in order to overcome censored data problems, especially in medical research and microarray data sets. In this study, we compared two imputation methods, k-nearest neighbors (kNN) and a prediction model (PM), for the evaluation of right-censored data. In order to see the effects of the imputation methods on the nonparametric regression estimates, the imputed right-censored data modelled by the penalized splines for two methods. We also supported the study with a Monte Carlo simulation experiment and a real data study. © Springer Nature Switzerland AG 2020.",Imputation methods; Penalized spline; Right-censored data,"Ahmed S.E., Aydin D., Yılmaz E.",2020,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-030-21248-3_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068211895&doi=10.1007%2f978-3-030-21248-3_8&partnerID=40&md5=44363ebbb53aa75b33df88e911baa312,"Department of Mathematics and Statistics, Brock University, St. Catharines, Canada; Department of Statistics, Mugla Sitki Kocman University, Mugla, Turkey",Springer Verlag,English,21945357,9783030212476
Scopus,Simsax: A measure of project similarity based on symbolic approximation method and software defect inflow,"Background: Profiling software development projects, in order to compare them, find similar sub-projects or sets of activities, helps to monitor changes in software processes. Since we lack objective measures for profiling or hashing, researchers often fall back on manual assessments. Objective: The goal of our study is to define an objective and intuitive measure of similarity between software development projects based on software defect-inflow profiles. Method: We defined a measure of project similarity called SimSAX which is based on segmentation of defect-inflow profiles, coding them into strings (sequences of symbols) and comparing these strings to find so-called motifs. We use simulations to find and calibrate the parameters of the measure. The objects in the simulations are two different large industry projects for which we know the similarity a priori, based on the input from industry experts. Finally, we apply the measure to find similarities between five industrial and six open source projects. Results: Our results show that the measure provides the most accurate simulated results when the compared motifs are long (32 or more weeks) and we use an alphabet of 5 or more symbols. The measure provides the possibility to calibrate for each industrial case, thus allowing to optimize the method for finding specific patterns in project similarity. Conclusions: We conclude that our proposed measure provides a good approximation for project similarity. The industrial evaluation showed that it can provide a good starting point for finding similar periods in software development projects. © 2019 Elsevier B.V.",Defect inflow; Metrics; Project comparison,"Ochodek M., Staron M., Meding W.",2019,Journal,Information and Software Technology,10.1016/j.infsof.2019.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067440195&doi=10.1016%2fj.infsof.2019.06.003&partnerID=40&md5=d5852981c26c594aeb0cd5a13af974de,"Poznan University of Technology Faculty of Computing, Institute of Computing Science, ul. Piotrowo 2, Poznań, 60-965, Poland; Chalmers University of Gothenburg, Sweden; Ericsson AB, Sweden",Elsevier B.V.,English,09505849,
Scopus,Software effort estimation using FAHP and weighted kernel LSSVM machine,"In the life cycle of software product development, the software effort estimation (SEE) has always been a critical activity. The researchers have proposed numerous estimation methods since the inception of software engineering as a research area. The diversity of estimation approaches is very high and increasing, but it has been interpreted that no single technique performs consistently for each project and environment. Multi-criteria decision-making (MCDM) approach generates more credible estimates, which is subjected to expert’s experience. In this paper, a hybrid model has been developed to combine MCDM (for handling uncertainty) and machine learning algorithm (for handling imprecision) approach to predict the effort more accurately. Fuzzy analytic hierarchy process (FAHP) has been used effectively for feature ranking. Ranks generated from FAHP have been integrated into weighted kernel least square support vector machine for effort estimation. The model developed has been empirically validated on data repositories available for SEE. The combination of weights generated by FAHP and the radial basis function (RBF) kernel has resulted in more accurate effort estimates in comparison with bee colony optimisation and basic RBF kernel-based model. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Fuzzy analytic hierarchy process; Least square support vector machine; Software effort estimation,"Sehra S.K., Brar Y.S., Kaur N., Sehra S.S.",2019,Journal,Soft Computing,10.1007/s00500-018-3639-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058026439&doi=10.1007%2fs00500-018-3639-2&partnerID=40&md5=9c01937e1165c4d38548c25875817cbc,"I.K.G. Punjab Technical University, Jalandhar, Punjab, India; Guru Nanak Dev Engineering College, Ludhiana, Punjab, India; Sri Guru Granth Sahib World University, Fatehgarh Sahib, Punjab, India; Elocity Technology Inc., Toronto, Canada",Springer Verlag,English,14327643,
Scopus,A systematic literature review of software effort prediction using machine learning methods,"Machine learning (ML) techniques have been widely investigated for building prediction models, able to estimate software development effort as well as to improve the accuracy of other estimation techniques. The objective of this paper is to systematically review the recent studies which used and discussed the software effort estimation models built using ML techniques. The performed literature review is based on the empirical studies published in the time period of January 1991 to December 2017, by employing widely used guidelines. The review has selected a total of 75 primary studies after the careful filtering of inclusion/exclusion and quality assessment criteria. The performed analysis reveals that artificial neural network (ANN) as ML model, NASA as dataset, and mean magnitude of relative error (MMRE) as accuracy measure are widely used in the selected studies. ANN and support vector machine (SVM) are the two techniques which have outperformed other ML techniques in more studies. Regression techniques are the mostly used among the non-ML techniques, which outperformed other ML techniques in about 19 studies. Moreover, SVM and regression techniques in combination are characterized by better predictions when compared with other ML and non-ML techniques. © 2019 John Wiley & Sons, Ltd.",accuracy measure; effort estimation; machine learning; SLR; software engineering,"Ali A., Gravino C.",2019,Review,Journal of Software: Evolution and Process,10.1002/smr.2211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074100616&doi=10.1002%2fsmr.2211&partnerID=40&md5=a362cc6d8c977a0719a10226e38563c8,"Department of Computer Science, University of Salerno, Fisciano, Italy",John Wiley and Sons Ltd,English,20477481,
Scopus,Feature engineering for enhanced model performance in software effort estimation,"Many new methodologies have been defined in the last two decades in the domain of Software Effort Estimation. They include manual methods based on expert judgment, analogy-based models, parametric models, regression models, machine learning models, and more recently, deep learning models. Except for manual methods, all other models depend heavily on data. Lack of quality data in this domain is a motivation to explore means to optimize the sparse data available. Machine learning algorithms depend on domain features, and their ability to represent and model the domain, to solve the problems irrespective of whether it is classification or regression, image, or voice synthesis. There is continued research for the best representation of the issue through the right feature space. While most of the traditional research rely on the original dataset and concentrate more on feature selection, modern-day approaches explore creating additional features that have the potential to extend the models representational space.This research builds on our last research exploring the potential to improve Software Effort Estimation accuracy by employing engineered features in addition to the original ones. The features are created manually based on the literature. Through the engineered features, we captured additional representational features such as missingness and proportion of categorical data available in the dataset. We present the rationale for the features generated and compare the prediction accuracy between a model using the original dataset and the engineered data set.Our experiments in Feature Engineering is innovative in the Software Estimation domain and the results conclusive establishing its use in predicting Software Effort. We report an improved accuracy of 38% with engineered features at PRED(15), and 11% improvement at PRED(20). The quantitative growth that we have been able to achieve in terms of accuracy is promising enough for this to be adopted as a standard in future research on the subject and practical applications. © BEIESP.",Artificial Neural Network; Effort Prediction; Feature Engineering; Generalized Linear Model; Random Forests; Software Cost Estimation; Software Effort Estimation,"Pillai S.P., Radharamanan T., Madhukumar S.D.",2019,Journal,International Journal of Recent Technology and Engineering,10.35940/ijrte.C5602.098319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073541447&doi=10.35940%2fijrte.C5602.098319&partnerID=40&md5=44942857ded37c49d75c899cd2a07895,"School of Management Studies, National Institute of Technology Calicut, Kozhikode, Kerala, India; Dept. of Computer Science, NITC, Kozhikode, Kerala, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22773878,
Scopus,Experience: Quality benchmarking of datasets used in software effort estimation,"Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the “fitness for purpose” of these commonly used datasets and (2) an assessment of the utility of the taxonomy in terms of dataset benchmarking. We also propose a template that could be used to both improve the ESE data collection/submission process and to evaluate other such datasets, contributing to enhanced awareness of data quality issues in the ESE community and, in time, the availability and use of higher-quality datasets. © 2019 Association for Computing Machinery.",Benchmarking; Data quality; Empirical software engineering; Missing data; Noise; Software effort estimation,"Bosu M.F., Macdonell S.G.",2019,Journal,Journal of Data and Information Quality,10.1145/3328746,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071250156&doi=10.1145%2f3328746&partnerID=40&md5=9688ef242a08b11815afd6a1c1759e65,"Department of Information Science, University of Otago, New Zealand; Centre for Information Technology, Wintec, Private Bag 3036, Waikato Mail Centre, Hamilton, 3240, New Zealand; School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Private Bag 92006, Auckland, 1142, New Zealand",Association for Computing Machinery,English,19361955,
Scopus,Crystal Balls and Black Boxes: What Makes a Good Forecast?,"As a discipline that concerns itself with the future, planning relies on forecasts to inform and guide action. With this reliance comes a concern that the best possible forecasts be produced. This review identifies three distinct ways in which forecasts may be evaluated (methodology, accuracy, and usefulness) and describes challenges associated with evaluating forecasts along any of these three dimensions. By way of example, this general discussion of forecasting is applied to the specific case of demand forecasts for transportation infrastructure, with an emphasis on transit infrastructure. There is a continuing need for planners to engage with interdisciplinary forecasting literature. © The Author(s) 2019.",demographic analysis; econometric methods; ethics; transportation,Voulgaris C.T.,2019,Journal,Journal of Planning Literature,10.1177/0885412219838495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063350692&doi=10.1177%2f0885412219838495&partnerID=40&md5=f210b05864e14b9d85265b579849ce23,"Department of Civil and Environmental Engineering, California Polytechnic State University, San Luis Obispo, CA, United States",SAGE Publications Inc.,English,08854122,
Scopus,Performance of Maintainability Index prediction models: a feature selection based study,"Numerous design metrics have been studied in the literature to assess software performance and future maintainability issues. Maintainability Index (MI) is an indicator of software maintenance efforts, but that can be computed only after the complete code is developed. MI is complex to calculate and hence has been estimated by several researchers using design metrics. Various prediction models are built using different sets of design metrics as independent variables. Further, researchers have used different prediction models for predicting software maintainability. To identify most prominent design metrics and the prediction models that show consistent performance irrespective of the choice of design metrics, the present empirical study is performed on datasets of 26 open source Java projects with more than one thousand and six hundred class hierarchies. Seven feature selection methods are applied to find out the most prominent metrics that are selected by majority of methods. Then four most frequently used prediction models namely, multi linear regression, multilayer perceptron, support vector regression and M5P regression tree are analyzed for their performance with respect to initial selection of design metrics. The study concludes that MIF, MaxDIT, LCC, TCC are the most significant predictors for Maintainability Index of class hierarchies. Among the all the prediction models, support vector regression model exhibits the best performance both with respect to the choice of metrics and irrespective of initial metric selection. © 2017, Springer-Verlag GmbH Germany.",Design metrics; Feature selection; Maintainability Index; Object oriented software; Regression models,"Reddy B.R., Ojha A.",2019,Journal,Evolving Systems,10.1007/s12530-017-9201-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065915687&doi=10.1007%2fs12530-017-9201-0&partnerID=40&md5=dca52e9fc6a7fc45c3cf69119aacf592,"PDPM IIITDMJ, Dumna Airport Road, Jabalpur, India",Springer Verlag,English,18686478,
Scopus,Towards concept based software engineering for intelligent agents,"The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of Concept Based Software Engineering, which is focused on supporting productivity and quality goals during the development of such systems. © 2019 IEEE.",Artificial Intelligence; Components; Machine Teaching; Reinforcement Learning; Software Engineering,"Ole M., Volker G.",2019,Conference,"Proceedings - 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2019",10.1109/RAISE.2019.00015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072922952&doi=10.1109%2fRAISE.2019.00015&partnerID=40&md5=24eb7e556641e89fb5099828551d3737,"Paluno - the Ruhr Institute for Software Technology, University of Duisburg-Essen, Essen, Germany",Institute of Electrical and Electronics Engineers Inc.,English,,9781728122724
Scopus,Optimizing software effort estimation models based on metaheuristic methods: A proposed framework,"Software effort estimation are part of the field of project management in software that is very important for development efforts. Software development planning is something very complex and serious, which determines the success of a software project. Because of the lack of good requirements and information, it causes software project failures. Although there are many studies that aim to solve the problem of noisy, irrelevant and excessive data to achieve accuracy. The purpose of this study is to combine metaheuristic optimization techniques as a framework for using Machine Learning models. By proposing a hybrid estimation model based on a combination of the Satin Bowerbird Optimizer (SBO) algorithm and Support Vector Regression (SVR) to improve the accuracy of software estimation efforts. This study is to determine the effort estimation and duration estimation. The proposed framework is based on theoretical concepts. the proposed model will be tested using a heterogeneous dataset, namely the ISBSG dataset. the results of the study are expected to be used as decision making as the initial planning of software project development. © 2019, World Academy of Research in Science and Engineering. All rights reserved.",Metaheuristic; Parameter optimization; Satin Bowerbird Optimizer (SBO) algorithm; Software effort estimation; Support Vector Regression (SVR),"Marco R., Herman N.S., Ahmad S.S.S.",2019,Journal,International Journal of Advanced Trends in Computer Science and Engineering,10.30534/ijatcse/2019/5181.52019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075554237&doi=10.30534%2fijatcse%2f2019%2f5181.52019&partnerID=40&md5=8a70db680192ab5b4ed0457b93d91393,"Department of Computer Science, Universitas Amikom Yogyakarta, Indonesia; Department of Information & Communication Technology, Universiti Teknikal Malaysia Melaka (UTeM), Melaka, Malaysia",World Academy of Research in Science and Engineering,English,22783091,
Scopus,Adjustment factor for use case point software effort estimation (study case: Student Desk Portal),"With the growth of technology, the requirement for customized software to support business increases and the experts on software development also increases. The more Software Developers means the more competition in Software Development. Software metric for effort estimation is a strategy to use metrics related to effort estimation in software development becomes necessary to determine the effort required to develop the system and hence determine pricing (in the case of software house). Several methods exist to estimate effort. This research discusses the Use Case Point Method using a Student Desk Portal as the study case. The effort calculated based on Use Case method may differ from the actual case. In this paper, the differences are analyzed, and an adjustment factor is constructed to resolve the differences between the effort estimated by use case point and the actual effort placed in the actual software development. A new criterion is introduced by using data from the actual software development, be used as further adjustment to the effort obtained by using Use Case Point method. This new criteria, the simplicity of project is added to be a part of the environment factor. The result is positive, the effort calculated by Use Case Point for three different application is in accordance with the actual result. © 2019 The Authors. Published by Elsevier B.V.",Adjustment Factor; Effort Estimation; Function Point; Software Development; Software Metric; Use Case Point,"Effendi A., Setiawan R., Rasjid Z.E.",2019,Conference,Procedia Computer Science,10.1016/j.procs.2019.08.215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073116739&doi=10.1016%2fj.procs.2019.08.215&partnerID=40&md5=760e07d67947534860ddb2aa40e37c48,"Information Systems Department, School of Information Systems, Bina Nusantara University, Jl. K.H. Syahdan No. 9, Jakarta, 11480, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jl. K.H. Syahdan No. 9, Jakarta, 11480, Indonesia",Elsevier B.V.,English,18770509,
Scopus,"Complexity, size and internal quality in the evolution of mobile applications: An exploratory study [Mobil uygulamalarin evriminde karmasiklik, boyut ve iç kalite gelisimi: Kesifsel bir çalisma]","Mobile applications are becoming complex software systems as they rapidly evolve and grow constantly to meet user requirements. However, satisfying these requirements may lead to poor design choices known as 'antipatterns' that can degrade software quality and performance. Therefore, perception and monitoring of the characteristics of mobile applications are important activities to facilitate maintenance and development, so that developers are directed to restructure their practices and upgrade their qualifications. This study aims to better understand the development of complexity, size and internal quality in the evolution of mobile applications and, in particular, to investigate the validity of three of Lehman's laws (increasing complexity, continuous growth, decreasing quality) in mobile applications. In this context, an exploratory study was carried out by analyzing the evolution of application quality, according to hypotheses established and using object- oriented design metrics in 61 versions of three Android-based mobile applications. As a result of the analyses, Lehman's 'continuous growth' law was validated for all apps, while the 'increased complexity' and 'declining quality' laws have not been validated. In addition, the results of the experimental study have been verified with Spearman correlation analysis and it was observed that there is a significant relation between the design metrics and the quality attributes. © 2019 Gazi Universitesi Muhendislik-Mimarlik. All rights reserved.",C&K metric set; Lehman laws; Mobile software; Open source; Software evolution; Software quality,"Gezici B., Tarhan A., Chouseinoglou O.",2019,Journal,Journal of the Faculty of Engineering and Architecture of Gazi University,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069717602&partnerID=40&md5=816327c1e44790723c2976e2f7260935,"Department of Computer Engineering, Hacettepe University, Ankara, 06800, Turkey; Department of Industrial Engineering, Hacettepe University, Ankara, 06800, Turkey",Gazi Universitesi Muhendislik-Mimarlik,Turkish,13001884,
Scopus,Software cost estimation: A state-of-the-art statistical and visualization approach for missing data,"Software cost estimation (SCE) is a critical phase in software development projects. A common problem in building software cost models is that the available datasets contain projects with lots of missing categorical data. There are several techniques for handling missing data in the context of SCE. The purpose of this article is to show a state-of-art statistical and visualization approach of evaluating and comparing the effect of missing data on the accuracy of cost estimation models. Five missing data techniques were used: multinomial logistic regression, listwise deletion, mean imputation, expectation maximization and regression imputation; and compared with respect to their effect on the prediction accuracy of a least squares regression cost model. The evaluation is based on various expressions of the prediction error. The comparisons are conducted using statistical tests, resampling techniques and visualization tools like the regression error characteristic curves. Copyright © 2019, IGI Global.",Imputation; Missing Data; Regression Error Characteristic (REC) Curves; Regression Receiver Operating Curves (RROC); Software Cost Estimation,Chatzipetrou P.,2019,Journal,"International Journal of Service Science, Management, Engineering, and Technology",10.4018/IJSSMET.2019070102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065722445&doi=10.4018%2fIJSSMET.2019070102&partnerID=40&md5=eda85bea31c0b0b4669a33878be45eb8,"Department of Informatics, CERIS, Örebro University School of Business, Örebro, Sweden",IGI Global,English,1947959X,
Scopus,MINN: A missing data imputation technique for Analogy-Based Effort Estimation,"Success and failure of a complex software project are strongly associated with the accurate estimation of development effort. There are numerous estimation models developed but the most widely used among those is Analogy- Based Estimation (ABE). ABE model follows human nature as it estimates the future project's effort by making analogies with the past project's data. Since ABE relies on the historical datasets, the quality of the datasets affects the accuracy of estimation. Most of the software engineering datasets have missing values. The researchers either delete the projects containing missing values or avoid treating the missing values which reduce the ABE performance. In this study, Numeric Cleansing (NC), K-Nearest Neighbor Imputation (KNNI) and Median Imputation of the Nearest Neighbor (MINN) methods are used to impute the missing values in Desharnais and DesMiss datasets for ABE. MINN technique is introduced in this study. A comparison among these imputation methods is performed to identify the suitable missing data imputation method for ABE. The results suggested that MINN imputes more realistic values in the missing datasets as compared to values imputed through NC and KNNI. It was also found that the imputation treatment method helped in better prediction of the software development effort on ABE model. © 2013 The Science and Information (SAI) Organization.",Analogy-based estimation; Effort estimation; Missing data imputation; Software development,"Shah M.A., Jawawi D.N.A., Isa M.A., Wakil K., Younas M., Mustafa A.",2019,Journal,International Journal of Advanced Computer Science and Applications,10.14569/ijacsa.2019.0100230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063571513&doi=10.14569%2fijacsa.2019.0100230&partnerID=40&md5=4201ec8bf39eea9d136fed73c0bb74db,"Department of Software Engineering, School of Computing, Faculty of Engineering Universiti Teknologi Malaysia, Johor Bahru, Malaysia; City University of Science and Information Technology, Peshawar, Pakistan; Research Center, Sulaimani Polytechnic University, Sulaimani Kurdistan Region, 46001, Iraq; Department of Computer Science, Government College University, Faisalabad, Pakistan",Science and Information Organization,English,2158107X,
Scopus,Software effort interval prediction via Bayesian inference and synthetic bootstrap resampling,"Software effort estimation (SEE) usually suffers from inherent uncertainty arising from predictive model limitations and data noise. Relying on point estimation only may ignore the uncertain factors and lead project managers (PMs) to wrong decision making. Prediction intervals (PIs) with confidence levels (CLs) present a more reasonable representation of reality, potentially helping PMs to make better-informed decisions and enable more flexibility in these decisions. However, existing methods for PIs either have strong limitations or are unable to provide informative PIs. To develop a “better” effort predictor, we propose a novel PI estimator called Synthetic Bootstrap ensemble of Relevance Vector Machines (SynB-RVM) that adopts Bootstrap resampling to produce multiple RVM models based on modified training bags whose replicated data projects are replaced by their synthetic counterparts. We then provide three ways to assemble those RVM models into a final probabilistic effort predictor, from which PIs with CLs can be generated. When used as a point estimator, SynB-RVM can either significantly outperform or have similar performance compared with other investigated methods. When used as an uncertain predictor, SynB-RVM can achieve significantly narrower PIs compared to its base learner RVM. Its hit rates and relative widths are no worse than the other compared methods that can provide uncertain estimation. © 2019 Association for Computing Machinery.",Bootstrap resampling; Ensemble learning; Prediction intervals with confidence levels; Relevance vector machine; Software effort estimation; Software risk management; Synthetic replacement; Uncertain effort estimation,"Song L., Minku L.L., Xin Y.A.O.",2019,Journal,ACM Transactions on Software Engineering and Methodology,10.1145/3295700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060873241&doi=10.1145%2f3295700&partnerID=40&md5=4c48c686e0bdb1efcfff5a6220d6d2ec,"Southern University of Science and Technology, China; University of Birmingham, United Kingdom",Association for Computing Machinery,English,1049331X,
Scopus,Performance measure of the proposed cost estimation model: Advance use case point method,"Estimating size and cost of a software system is one of the primary challenges in software project management. An estimate is of critical significance to a project’s success, hence the estimate should go through a rigorous assessment process. The estimate should be evaluated for its quality or accuracy, and also to ensure that it contains all of the required information and is presented in a way that is easily understandable to all project stakeholders. Software cost model research results depend on model accuracy measures such as MRE, MMRE and PRED. Advance Use Case Point Method (AUCP) is an enhancement of UCP. AUCP is our previously proposed and published model (Srivastava et al in Int. J. Control Theor. Appl. Eval. Softw. Project Estimation Methodol. AUCP 9(41):1373–1381, 2017) [1]. In this paper, performance evaluation of AUCP is carried out using the three widely accepted metrics including MRE, MMRE and percentage of the PRED. © Springer Nature Singapore Pte Ltd. 2019.",Advanced use case point method (AUCP); End user development (EUD); EUD_environmental factors; EUD_technical factors; MMRE (Mean magnitude of relative error); MRE (Magnitude of relative error); Use case point method (UCP),"Srivastava A., Singh S.K., Abbas S.Q.",2019,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-13-0589-4_21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053936951&doi=10.1007%2f978-981-13-0589-4_21&partnerID=40&md5=4521b7ded9f5ae44243b8f3f1d308d70,"Amity University, Lucknow, India; Ambalika Institute of Management & Technology, Lucknow, India",Springer Verlag,English,21945357,9789811305887
Scopus,ESPRET: A tool for execution time estimation of manual test cases,"Manual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test steps is already available or not. Since executing test cases on the several machines may take different time, we predict the actual execution time for test cases by a set of regression models. Finally, an empirical evaluation of the approach and tool has been performed on a railway use case at Bombardier Transportation (BT) in Sweden. © 2018 Elsevier Inc.",Execution time; Manual testing; Optimization; Regression analysis; Software testing; Test specification,"Tahvili S., Afzal W., Saadatmand M., Bohlin M., Ameerjan S.H.",2018,Journal,Journal of Systems and Software,10.1016/j.jss.2018.09.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053193472&doi=10.1016%2fj.jss.2018.09.003&partnerID=40&md5=bb19b632d239f3c2396a411b664c9e3a,"RISE SICS Västerås AB, Sweden; Mälardalen University, Västerås, Sweden",Elsevier Inc.,English,01641212,
Scopus,An Empirical Analysis of Cost Estimation Models on Undergraduate Projects Using COCOMO II,Now a day's software engineering has inclination towards formality. Many formal and mathematical mergers have taken place in theoretical software engineering. Giving mathematical foundation to various aspects of software engineering enhance their practicability and predictability. Cost estimation models like Constructive Cost Model (COCOMO) is one major proven example. COCOMO effort and time calculation is based on mathematical foundations. Several cost drivers having mathematical values also contributed in these calculations. This paper presents a sensitivity analysis using on Constructive Cost Model II (COCOMO II) and their relationships with the projects taken on undergraduate level using web based estimation tool Estimator. The analysis studies the behavior of cost drivers having higher impact on these projects and needs detail understanding and some has low impact. This categorization further used in functional and nonfunctional requirements level. As which nonfunctional requirement need more and detailed understanding at undergraduate study level. © 2018 IEEE.,COCOMO I; COCOMO II; functional requirements verification; software cost estimation; undergraduate project,"Tahir F., Adil M.",2018,Conference,"2018 International Conference on Smart Computing and Electronic Enterprise, ICSCEE 2018",10.1109/ICSCEE.2018.8538361,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059406143&doi=10.1109%2fICSCEE.2018.8538361&partnerID=40&md5=333ad70a0cae0c193be86c8f6f33b194,"Dept. of Software Engineering, University of Gujrat Sialkot Sub Campus, Sialkot, Pakistan",Institute of Electrical and Electronics Engineers Inc.,English,,9781538648360
Scopus,Utilization of function point method for measuring software project complexity,"The purpose of this study is to know the complexity of a project so that the time and cost of project work in accordance with the needs and the company can complete the project on time. The complexity of a project can be defined as something consisting of so many interrelated sections that can be operated in the context of difference and interdependence. The method used to achieve research objectives using the Function Point Method. Function point method is a method used to estimate the complexity of a software project, providing project volume estimates in the form of development resources required before the project is undertaken. This estimate provides an important basis for providing estimates of the resources required by software companies to prepare tender proposals and project plans. One of the problems encountered in software project development is that the project experiences delays in its completion due to errors in estimating the complexity of the project undertaken and impacting the time and cost of the project. Function point method can prevent or reduce the error of project cost plan. By using the Function Point method, the complexity of software projects can be known so that the time and cost of project work in accordance with the needs and the company can complete the project on time. © Published under licence by IOP Publishing Ltd.",,"Atin S., Harihayati T., Widianti U.D.",2018,Conference,IOP Conference Series: Materials Science and Engineering,10.1088/1757-899X/407/1/012086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054790140&doi=10.1088%2f1757-899X%2f407%2f1%2f012086&partnerID=40&md5=0fbbd57014800e8ef5a5a7f42a6cfe26,"Informatics Engineering, Faculty of Engineering and Computer Science, Universitas Komputer Indonesia, Jl. Dipatiukur No.112-116, Bandung, Indonesia",Institute of Physics Publishing,English,17578981,
Scopus,A Comparison Study between Soft Computing and Statistical Regression Techniques for Software Effort Estimation,"In this paper, we conduct a comparison between soft computing and statistical regression techniques in terms of a software development estimation regression problem. Our study includes both support vector regression (SVR) and artificial neural network (ANN) as soft computing methodologies on one side, and stepwise multiple linear regression and log linear regression as statistical regression methods on the other side. The experiments are conducted using NASA93 dataset from the well-known PROMISE software repository. Multiple dataset preprocessing steps are performed in order to guarantee confident results including outliers study. We rely on the holdout technique associated with 25 random repetitions with confidence interval calculation within 95% statistical confidence level. Pred(30) evaluation criteria, from literature, is employed to compare between different models. Also, examining different feature combinations, in the case of SVR, shows significant impact on the model precision. © 2018 IEEE.",Linear regression; Machine learning; Neural network; Oft computing; Regression; Software effort estimation; Support vector regression,Mohamed Abdellatif T.,2018,Conference,Canadian Conference on Electrical and Computer Engineering,10.1109/CCECE.2018.8447687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053619551&doi=10.1109%2fCCECE.2018.8447687&partnerID=40&md5=639b58f03218fa68453cd2b8b91d44a9,"Department of Electrical and Computer Engineering, Western University, London, ON, Canada",Institute of Electrical and Electronics Engineers Inc.,English,08407789,9781538624104
Scopus,Detailed use case points (DUCPs): A size metric automatically countable from sequence and class diagrams,"Sequence and class diagrams are widely used to model the behavioral and structural aspects of a software system. A size metric that is defined automatically countable from sequence and class diagrams boosts both the efficiency and the accuracy of size estimation by producing reproducible software size measurements. To fulfill the purposes, a size metric called Detailed Use Case Points (DUCPs) is proposed based on the information automatically derived from sequence and class diagrams. The automation is largely supported by our proposed user-system interaction model (USIM) that fills the gap between the system abstraction by the sizing model and the metamodels of the UML diagrams. The effectiveness of our proposed size metric in project effort estimation is validated by an empirical study of 22 historical projects. © 2018 ACM.",automated analysis; automated model transformation; effort estimation; function point analysis; model calibration; model-based analysis; object-oriented modeling; software size metric; unified modeling language (UML); use case analysis; use case points,"Qi K., Boehm B.W.",2018,Conference,Proceedings - International Conference on Software Engineering,10.1145/3193954.3193955,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054818167&doi=10.1145%2f3193954.3193955&partnerID=40&md5=040830024c9c12e96901ede615e8be86,"University of Southern California, United States",IEEE Computer Society,English,02705257,9781450357357
Scopus,Web complexity factors! A novel approach for predicting size measures for web application development,"Effort estimation is critical phase in web application development. Effort estimates are directly influenced by web application size. Accuracy in size creates accuracy in estimate efforts. Web application size is determined by eliciting various web sizing measures and are calculated as web size metrics like LOC, FP, WO, etc. Web size metrics are actually obtained by aggregating quantitate and qualitative behaviour of functional and dimensional size measurements. There is no standard approach that can guide through proper elicitation, elaboration and selection of size measures for web application development. This study attempts to propose new size measurement approach, the Web Complexity Factor (WCF) to obtain size measures for web application. WCF has five measuring factors. WCF is first and primary component to develop effort estimation model for web effort estimation. © 2017 IEEE.",Effort estimation; Size measures; Web application; Web complexity factors,"Saif S.M., Wahid A.",2018,Conference,"Proceedings of the International Conference on Inventive Computing and Informatics, ICICI 2017",10.1109/ICICI.2017.8365266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048346609&doi=10.1109%2fICICI.2017.8365266&partnerID=40&md5=573c2f4e2250ab34ce47803d35b4749b,"Dept. of CSandIT, MANUU, Hyderabad, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538640319
Scopus,The prediction of software complexity based on complexity requirement using artificial neural network,"In the recent years, the productivity of software has grown in size, complexity, and also cost. As that software productivity growth, several problems has been appeared in software project management especially that correlated to complexity. One of complexity factors is requirement. A unit of requirement used as an option to the design phase of product development. The requirement is also a main option in verification process. So the the requirement complexity in this research is used as parameter to predict the software complexity. Because of the data pattern to connect between the requirement and the complexity is complex. So that this paper attempt to make a connectivity model between requirement complexity and prediction complexity of software using artificial neural network method with Levenberg Marquadt and Bayesian Regulation algorithm. So it can be seen comparison of experimental results by using the two algorithms. © 2017 IEEE.",artificial neural network; complexity; prediction; requirement; software,"Purawinata W.M., Gaol F.L., Nugroho A., Abbas B.S.",2018,Conference,"2017 IEEE International Conference on Cybernetics and Computational Intelligence, CyberneticsCOM 2017 - Proceedings",10.1109/CYBERNETICSCOM.2017.8311687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050754185&doi=10.1109%2fCYBERNETICSCOM.2017.8311687&partnerID=40&md5=1705c92f7bae16de059dd206271698e6,"Faculty of Engineering and Computer Science, Indonesian Computer University, Bandung, 40132, Indonesia; Computer Science Department, BINUS Graduate, Program - Doctor of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Industrial Engineering Department, Faculty of Engineering, Bina Nusantara University, Jakarta, 11480, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781538607831
Scopus,Analyzing correlation coefficient using software metrics,"In this paper, we investigate three distinct strategies for processing similitudes for acquiring suggestion for them. There are various diverse scientific details that can be utilized to compute the connection coefficient between two things. On the premise of different parameters, we are going to find the preferable quality over others mainly between spearman and Pearson's correlation coefficient. © 2017 IEEE.",Correlation coefficient; Cryptographic size,"Ujera, Sudha R., Ragavi V., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300894,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046689499&doi=10.1109%2fICOEI.2017.8300894&partnerID=40&md5=2b1c22306e6bddda6d1cdd96c1a145ea,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Calculating the heart disease in Switzerland using Pearson's correlation,"In this paper, we look techniques for computing similarities for obtaining the factors that causes heart disease with respect to age. There are a number of different mathematical formulations that can be used to calculate the correlation coefficient between age and several factors. On the basis of various parameters we conclude Pearson's Correlation Coefficient provides better quality than the others. © 2017 IEEE.",,"Kalyanasundaram R., Prasanth A., Tamizhselvan B.R., Kumaran U.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046672136&doi=10.1109%2fICOEI.2017.8300885&partnerID=40&md5=7afa3f752bac9fb2b62ad172c73dd81f,"VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,"Analysis on diabetes patients using Pearson, cost optimization, control chart","In this paper, we have taken some important factors for health parameters of diabetes patients especially in children by birth (pediatric congenital). We use three metrics methods we are going to assess the importance of each attributes in the dataset and thereby determining the most highly responsible and co-related attribute causing diabetics among young patients. Cost optimization, Spearmen methodologies, and control chart for the real-time application to find the data efficiency in these dataset related to diabetes. The Spearmen methodology is the correlation methodologies used in Software development process to identify the complexity between the various modules of the software. Identifying the complexity is important because if the complexity is higher then there is a higher chance of occurrence of the risk in the software. With the use of a control, chart means, variance and standard deviation of data are calculated. With the use of Cost optimization model, we find to optimize the variables. Hence we choose the Spearmen, control chart and cost optimization methods to assess the data efficiency in diabetes datasets. © 2017 IEEE.",Congenital Diabetics; Correlation; Linear Relationship; Monotonic function; Pediatric; Ranking samples,"Poovarasan R., Keerthi S., Yuvashree K., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046651365&doi=10.1109%2fICOEI.2017.8300891&partnerID=40&md5=0f562ca815f4900aa706039c23f9d8fa,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Factors that influence software project cost and schedule estimation,"Software Project Management is a core topic in software engineering courses because it teaches how software projects planned, implemented, controlled, monitored, and evaluated. The development of theories in software metrics and prediction models builds on the broader project management field but also attempt to overcome the difficulties inherent in measuring an intangible object like software. This paper is situated within research into the factors that influence cost and time estimation for software projects that continue to challenge software development organizations. The study described in this paper explored technical and non-Technical factors seen by Sudanese software practitioners as critical in estimation, and if not managed, can result in cost and time overrun or in some cases lead to project failure. Using a mixed-method approach, the research project was first informed through a qualitative study that explored the kinds of problems that face the estimation process from the perspectives of different staff levels. This part of the study revealed a number of factors that can be broadly categorized as technical factors, e.g. The skills of those involved in the estimation process, and non-Technical factors such as the high level of uncertainty in the local business environment. The second part of the study focused on one of the leading factors, software project staff training and experience, using the survey method to examine how well the software engineering curriculum is aligned with skills required in the software market, especially those related to estimation. The recommendations this study produced on reducing estimation errors, whether geared towards companies or academia, are preliminary and may only reflect the local setting. However, they also drew upon the vast literature on cost estimation techniques and case studies in similar and more advanced settings. The problem of software effort prediction and estimation models has been a thorny issue in the software engineering field since the concept of 'software crisis' and the field itself, as a response to the crisis, emerged in the late 1960s. It still seems to some that 'After forty years of currency the phrase 'software engineering' still denotes no more than a vague and largely unfulfilled aspiration' [2]. This study develops our understanding of problems facing one of the young professions in the country, as well as contributes to the global body of research on developing techniques to manage the intricacy of software engineering compared to more established engineering disciplines. © 2017 IEEE.",Software Effort Prediction; Software Project Cost Estimation techniques; Software Project Failure; Software Project Management,"Suliman S.M.A., Kadoda G.",2018,Conference,"Proceedings of: 2017 Sudan Conference on Computer Science and Information Technology, SCCSIT 2017",10.1109/SCCSIT.2017.8293053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050500885&doi=10.1109%2fSCCSIT.2017.8293053&partnerID=40&md5=d86ee9ea55e29d7a28b7b41c444a902c,"Faculty of Mathematical Sciences, University of Khartoum, Khartoum, Sudan; Independent Researcher, Khartoum, Sudan",Institute of Electrical and Electronics Engineers Inc.,English,,9781538606674
Scopus,Preliminary causal discovery results with software effort estimation data,"Correlation does not imply causation. Though this is a well-known fact, most analyses depend on correlation as proof of relationships that are often treated as causal. Causal discovery, also referred to as causal model search, involves the application of statistical methods to identify causal relationships from conditional independences (and/or other statistical relationships) in the data. Though software cost estimation models use both domain knowledge and statistics, to date, there has yet to be a published report describing the evaluation of a software dataset using causal discovery. Two of the authors have previously used regression analysis to evaluate the effectiveness of the International Function Points User Group (IFPUG)'s and the Common Software Measurement International Consortium (COSMIC)'s functional size measurement methods for analyzing the Unified Code Count (UCC)1's dataset of maintenance tasks. Using the same dataset, the authors will report in this paper on what types of information causal discovery provides, and how they differ from correlation tests. This paper will introduce causal discovery to software engineering research, and its use in the future may impact how software effort models are built. © 2018 Association for Computing Machinery.",Causal discovery; Causal discovery; Causal inference; CFPs; COCOMO; COSMIC function points; Cost estimation; Effort estimation; Function point analysis; IFPUG; SLOC; Software non-functional assessment process; Source lines of code,"Hira A., Boehm B., Stoddard R., Konrad M.",2018,Conference,ACM International Conference Proceeding Series,10.1145/3172871.3172876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044396927&doi=10.1145%2f3172871.3172876&partnerID=40&md5=df81e8f1111feb6fad647dfbaf3bc39e,"University of Southern California, Los Angeles, CA, United States; Software Engineering Institute, Pittsburgh, PA, United States",Association for Computing Machinery,English,,9781450363983
Scopus,COSMIC function points evaluation for software maintenance,"The Common Software Measurement International Consortium (COSMIC) group reviewed the existing functional size methods, such as the International Function Points User Group (IFPUG)'s Function Points (FPs), to develop a functional size metric based on ""the basic principles"" that applies to a wide range of application domains. Though several empirical studies on the COSMIC method verify that COSMIC Function Points (CFPs) successfully accomplished the goal of being applicable to a wide range of application domains and that its size correlate well with effort over a very wide range of sizes, one study of telecom switching software noticed that the correlation between CFPs and cost is very low for small projects (5 CFPs or less). The COSMIC method does not explicitly size data manipulations (such as, mathematical algorithms), which causes it to be less effective for mathematically-intensive software. IFPUG's FPs method has the same drawback of not explicitly measuring mathematical operations, but IFPUG developed the Software NonFunctional Assessment Process (SNAP) to complement a project's functional size. This empirical analysis will determine whether CFPs can be an effective size metric for small, maintenance tasks (between 2 and 12 CFPs) using a dataset consisting of Unified Code Count (UCC)1's maintenance tasks. Additionally, this analysis will consider whether using IFPUG's SNAP with COSMIC's FPs can lead to better effort estimates, as the former provides a method to measure data manipulation. The authors found that tasks adding new features require a different effort estimate model from those that modify existing features. © 2018 Association for Computing Machinery.",CFPs; COCOMO; COSMIC function points; Cost estimation; Effort estimation; Function point analysis; IFPUG; Local calibration; Project management; Software maintenance; Software non-functional assessment process,"Hira A., Boehm B.",2018,Conference,ACM International Conference Proceeding Series,10.1145/3172871.3172874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044358229&doi=10.1145%2f3172871.3172874&partnerID=40&md5=a87c6ff153c3bd69823509914f6bf3cf,"University of Southern California, Los Angeles, CA, United States",Association for Computing Machinery,English,,9781450363983
Scopus,Optimizing COCOMO II parameters using artificial bee colony method,"Cost estimation is a crucial and essential process in software industry. The more accurate cost estimated, the more efficient the project became. This cost estimation become a challenge for software industry to bring accurate result. There are many methods to solve this problem. Constructive Cost Model is usual method that is used to estimate software cost. This model was proposed in 1981 by using regression analysis with 63 types of project data. In 2000, COCOMO II was introduced. This new model of COCOMO use cost drivers, scale factors, and project size that measured by line of code. COCOMO II has 4 parameters A, B, C and D. However, using this parameters are not guarantee accurate result. This paper proposed Bee Colony Optimization to calibrate the COCOMO II model parameter to be more accurate for effort estimation. This Bee Colony Optimization is applied on Nasa93 dataset that consisted of 93 projects which each project has 22 cost drivers, project's size, effort, and development time. This proposed method gives MMRE result 50.584% on effort and 14.192% on development time. © 2017 IEEE.",Bee Algorithm; Bee Colony Optimization; COCOMO; MRE; Software Cost Estimation,"Pratama R.Y., Sarno R., Sholiq",2018,Conference,"Proceedings of the 11th International Conference on Information and Communication Technology and System, ICTS 2017",10.1109/ICTS.2017.8265657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050526195&doi=10.1109%2fICTS.2017.8265657&partnerID=40&md5=5b3b5fe2d11ca177b6a3ffa9eba6e83a,"Department of Information Technology Management, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics Systems, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781538628256
Scopus,A comparison of the planning poker and team estimation game: A case study in software development capstone project course,"Effort estimation is a crucial part of software development projects. Despite the availability of several assessment techniques, accurate assessment still remains an extremely difficult task. Team Estimation Game is a relatively new estimation technique for agile software development methods that has not received significant attention from the scientific community despite its growing popularity between practitioners. In this paper, we attempt to bridge this gap by presenting the results of an empirical study with undergraduate students in which we compare Team Estimation Game with the more established Planning Poker technique. We mainly focus our analysis of the two techniques on the time needed for user story estimation and estimation accuracy. The results of the empirical study reveal that Team Estimation Game produces more accurate story estimates than Planning Poker. Additionally, we found that for the Team Estimation Game, estimation and planning skills of the development teams improve from Sprint to Sprint. Team Estimation Game proved to be a useful estimation method for agile projects within the capstone course. Furthermore, we have shown that the study can be successfully incorporated into a software engineering capstone course without hindering the teaching goals while retaining the validity of research goals. © 2019 TEMPUS Publications.",Agile software development; Capstone course; Empirical study; Planning poker; Scrum; Team estimation game,"Poženel M., Hovelja T.",2018,Journal,International Journal of Engineering Education,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060981448&partnerID=40&md5=032ac6baf4a74d7872a3d0afd6909f35,"Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia",Tempus Publications,English,0949149X,
Scopus,Software cost estimation using environmental adaptation method,"Environmental adaptation method (EAM) is one of the evolutionary algorithms for solving single objective optimization problems. After the first proposal of EAM, other variants have been suggested to speed up the convergence and to maintain the population diversity. Among them, IEAM-RP works with real numbers and was able to achieve the desired goal during the optimization process. In this paper, IEAM-RP is used to predict the effort required to develop the software product. The experiments are carried out on NASA software project dataset to check the effectiveness of IEAM-RP. The experimental results demonstrated that the overall performance of IEAM-RP is quite satisfactory in predicting the effort required to develop a software. © 2018 The Authors. Published by Elsevier B.V.",Environmental Adaptation Method; Evolutionary Algorithms; Optimization Problems; Software Cost Estimation,"Singh T., Singh R., Mishra K.K.",2018,Conference,Procedia Computer Science,10.1016/j.procs.2018.10.403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058263231&doi=10.1016%2fj.procs.2018.10.403&partnerID=40&md5=f2257762661f18f90607d14862132153,"Motilal Nehru National Institute of Technology Allahabad, Allahabad, 211004, India",Elsevier B.V.,English,18770509,
Scopus,Applicability of the software cost model COCOMO II to HPC projects,"The complexity of parallel computer architectures continuously increases with the pursuit of exaflop computing, which makes accurate development effort estimation and modelling more important than ever. While sophisticated cost models are widely used in traditional software engineering, they have rarely been investigated for the performance-oriented HPC domain. Therefore, we evaluate the fit and accuracy of the popular COCOMO II model to HPC setups. We lay out a general methodology to evaluate HPC projects with COCOMO II and analyse its cost parameters for the investigated parallelisation projects with OpenACC on NVIDIA GPUs. Further, we evaluate the accuracy of the model in comparison to the reported efforts of the projects and investigate the impact of inaccuracies in the cost parameter ratings by means of a global sensitivity analysis. © 2018 Inderscience Enterprises Ltd.",COCOMO; Development effort; Effort estimation; GPU; OpenACC; Sensitivity analysis,"Miller J., Wienke S., Schlottke-Lakemper M., Meinke M., Müller M.S.",2018,Journal,International Journal of Computational Science and Engineering,10.1504/IJCSE.2018.095849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055869726&doi=10.1504%2fIJCSE.2018.095849&partnerID=40&md5=776512b8e6ea3da2d82785814707121d,"IT Center, RWTH Aachen University, Aachen, Germany; JARA-High-Performance Computing, Aachen, Germany; Institute of Aerodynamics, RWTH Aachen University, Aachen, Germany",Inderscience Publishers,English,17427185,
Scopus,Causal-effect analysis using Bayesian LiNGAM comparing with correlation analysis in Function Point metrics and effort,"Software effort estimation is a critical task for successful software development, which is necessary for appropriately managing software task assignment and schedule and consequently producing high quality software. Function Point (FP) metrics are commonly used for software effort estimation. To build a good effort estimation model, independent explanatory variables corresponding to FP metrics are required to avoid a multicollinearity problem. For this reason, previous studies have tackled analyzing correlation relationships between FP metrics. However, previous results on the relationships have some inconsistencies. To obtain evidences for such inconsistent results and achieve more effective effort estimation, we propose a novel analysis, which investigates causal-effect relationships between FP metrics and effort. We use an advanced linear non-Gaussian acyclic model called BayesLiNGAM for our causal-effect analysis, and compare the correlation relationships with the causal-effect relationships between FP metrics. In this paper, we report several new findings including the most effective FP metric for effort estimation investigated by our analysis using two datasets. © 2018 International Journal of Mathematical, Engineering and Management Sciences.",BayesLiNGAM; Causal-effect analysis; Correlation analysis; Function point (FP) metrics; Linear non-Gaussian acyclic model (LiNGAM); Software effort estimation,"Kondo M., Mizuno O., Choi E.-H.",2018,Journal,"International Journal of Mathematical, Engineering and Management Sciences",10.33889/ijmems.2018.3.2-008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050264386&doi=10.33889%2fijmems.2018.3.2-008&partnerID=40&md5=40531c781a9fa7727920cccedaaae146,"Kyoto Institute of Technology, Kyoto, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Ikeda, Osaka, Japan","International Journal of Mathematical, Engineering and Management Sciences",English,24557749,
Scopus,Using nonlinear quantile regression for the estimation of software cost,"Estimation of effort costs is an important task for the management of software development projects. Researchers have followed two approaches –namely, statistical/machine-learning and theory-based– which explicitly rely on mean/median regression lines in order to model the relationship between software size and effort. Those approaches share a common drawback deriving from their inability to properly incorporate risk attitudes in the presence of heteroskedasticity. We propose a more flexible quantile regression approach that enables risk aversion to be incorporated in a systematic way, with the higher order conditional quantiles of the relationship between project size and effort being used to represent more risk adverse decision makers. A cubic quantile regression model allows consideration of economies/diseconomies of scale. The method is illustrated with an empirical application to a database of real projects. Results suggest that the shapes of higher order regression quantiles may sharply differ from that of the conditional median, revealing that the naive expedient of translating or multiplying some average norm (adding a safety margin to median estimates or including a multiplicative correction factor) is a potentially biased way to consider risk aversion. The proposed approach enables a more realistic analysis, adapted to the specificities of software development databases. © Springer International Publishing AG, part of Springer Nature 2018.",Effort; Nonlinear quantile regression; Project size; Risk aversion; Software cost estimation,"De Andrés J., Landajo M., Lorca P.",2018,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-92639-1_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048854065&doi=10.1007%2f978-3-319-92639-1_35&partnerID=40&md5=044ff59ee069fcb77c6ed68740e08de5,"University of Oviedo, Oviedo, Spain",Springer Verlag,English,03029743,9783319926384
Scopus,Software project management: Resources prediction and estimation utilizing unsupervised machine learning algorithm,"Software project effort estimation is a major process in software development cycle. This process helps in decision making in resource allocation and distribution. In this work, a new effort estimation clustering method based on estimation maximization soft-clustering unsupervised machine learning algorithm is proposed. This model classifies any software project into one of four categories. An enterprise will accept to develop a software project if this project is clustered into a class that requires resources equal or less than the enterprises resources. The new model helps in decision making process in one hand and helps consumers in assigning projects to a developing enterprise in the other hand. COCOMO dataset has been used to implement, deploy and test the model. The propose model has been compared with K-means algorithm to show the differences between soft and hard clustering. The paper results show that soft-clustering has the ability to estimate efforts like any supervised machine learning algorithms. © 2018, Springer International Publishing AG, part of Springer Nature.",Effort estimation; Estimation maximization; K-means clustering; Maximum likelihood; Soft-clustering,"Masoud M., Abu-Elhaija W., Jaradat Y., Jannoud I., Dabbour L.",2018,Book Chapter,Lecture Notes in Mechanical Engineering,10.1007/978-3-319-74123-9_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044248490&doi=10.1007%2f978-3-319-74123-9_16&partnerID=40&md5=1bca67d4e18a862aafc62ee1445624a2,"Electrical Engineering Department, Al-Zaytoonah University of Jordan, 130, Amman, 11733, Jordan; Architecture Engineering Department, Al-Zaytoonah University of Jordan, 130, Amman, 11733, Jordan",Pleiades Publishing,English,21954356,9783319666969; 9783319686189; 9789811053283; 9789811322723
Scopus,An empirical study on the factors affecting software development productivity,"Background - Software development productivity is widely investigated in the Software Engineering literature. However, continuously updated evidence on productivity is constantly needed, due to the rapid evolution of software development techniques and methods, and also the regular improvement in the use of the existing ones. Objectives -The main goal of this paper is to investigate which factors affect productivity. It was also investigated whether economies or diseconomies of scale exist and whether they may be influenced by productivity factors. Method -An empirical investigation was carried out using a dataset available at the software project repository ISBSG. The major focus was on factors that may affect productivity from a functional point of view. The the conducted analysis was compared with the productivity data provided by Capers Jones in 1996 and 2013 and with an investigation on open-source software by Delorey et al. Results -This empirical study led to the discovery of interesting models that show how the different factors do (or do not) affect productivity. It was also found out that some factors appear to allow for economies of scale, while others appear to cause diseconomies of scale. Conclusions -This paper provides some more evidence about how four factors, i.e., programming languages, business areas, architectural types, and the usage of CASE tools, influence productivity and highlights some interesting divergences in comparison with the results reported by Capers Jones and Delorey et al.",Development; Effort; Empirical study; Factors; Function point; ISBSG dataset; Productivity,"Lavazza L., Morasca S., Tosi D.",2018,Journal,E-Informatica Software Engineering Journal,10.5277/e-Inf180102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039864730&doi=10.5277%2fe-Inf180102&partnerID=40&md5=530101f9936ef62e629d51092349e321,"Dipartimento di Scienze Teoriche E Applicate, Università degli Studi dell'Insubria, Italy",Politechnika Wroclawska,English,18977979,
Scopus,Software cost estimation for user-centered mobile app development in large enterprises,"Since development processes for mobile applications (apps) are becoming more user centered and agile, effort and cost estimation for app development projects in large enterprises faces new challenges. In this paper, we propose a new experience-driven approach for effort and cost estimation in software development projects. A Delphi study is conducted that takes into account different perspectives in mobile app development. Recurring app features are identified and associated with effort estimates and their variations based on different roles, perspectives, and complexity levels. In order to utilize our findings, a prototypical tool is introduced that allows effort and cost estimation based on incomplete information at an early stage in a project. © Springer International Publishing AG 2018.",Human Factors; Mobile application development; Software cost estimation; Software effort estimation; Systems Engineering; User centered design,"Lusky M., Powilat C., Böhm S.",2018,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-319-60011-6_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026248909&doi=10.1007%2f978-3-319-60011-6_6&partnerID=40&md5=bd6a5f10308d6cbb17f120f6b481c0ce,"Center of Advanced E-Business Studies (CAEBUS), RheinMain University of Applied Sciences, Unter den Eichen 5, Wiesbaden, 65195, Germany",Springer Verlag,English,21945357,9783319600109
Scopus,Should duration and team size be used for effort estimation?,"Project management activities such as scheduling and project progress management are important to avoid project failure. As a basis of project management, effort estimation plays a fundamental role. To estimate software development effort by mathematical models, variables which are fixed before the estimation are used as independent variables. Some studies used team size and project duration as independent variables. Although they are sometimes fixed because of the limitation of human resources or business schedule, they may change by the end of the project. For instance, when delivery is delayed, actual duration and estimated duration is different. So, although using team size and project duration may enhance estimation accuracy, the error may also lower the accuracy. To help practitioners to select independent variables, we analyzed whether team size and duration should be used or not, when we consider the error included in the team size and the duration. In the experiment, we assumed that duration and team size include errors when effort is estimated. To analyze influence of the errors, we add n% errors to duration and team size. As a result, using duration as an independent variable was not very effective in many cases. In contrast, using maximum team size as an independent variable was effective when the error rate is equal or less than 50%. © Springer International Publishing AG 2018.",Estimation error; Productivity; Project management; Software effort prediction,"Kakimoto T., Tsunoda M., Monden A.",2018,Conference,Studies in Computational Intelligence,10.1007/978-3-319-62048-0_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021947579&doi=10.1007%2f978-3-319-62048-0_7&partnerID=40&md5=934965faed6ee434c147ce6ade587c7b,"Department of Electrical and Computer Engineering, National Institute of Technology, Kagawa College, Takamatsu, Japan; Department of Informatics, Kindai University, Higashiosaka, Japan; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan",Springer Verlag,English,1860949X,
Scopus,Energy consumption patterns of mobile applications in android platform: A systematic literature review,"Studies related to resource consumption of mobile devices and mobile applications have been brought to the fore lately as mobile applications depend largely on their resource consumption. The study aims to identify the key factors and holistic understanding of how a factor influences Consumption Pattern (CP) effectiveness for an android platform mobile application. The study presents a Systematic Literature Review (SLR) on existing studies that examined factors influencing the effectiveness of CP for android mobile application and measured the effectiveness of CP. Therefore, the current SLR is conducted to answer the following questions: (1) What is the evidence of CP factors that drain the battery of a mobile device? (2) What are the energy conservation techniques to overcome all the factors that drain battery life? and (3) How can developers measure the effectiveness of an energy conservation technique?. The SLR investigated factors affecting the effectiveness of CP for android platform mobile application. The analyses of forty papers were used in our synthesis of the evidence related to the research questions above. Therefore, the analyses showed 22 studies that investigated how to measure the energy conservation technique effectiveness while 18 studies focused on better understanding of how the resources of mobile devices are actually spent. In this sense, 2 studies show the effectiveness of early analysis of software application design. Additionally, five factors i.e., architecture, interface, behavior of the application, resources, and network technologies that affect CP effectiveness were identified. This study investigated a SLR targeting at studies of CP effectiveness in android platform. The total of 40 studies were identified and selected for result synthesis purpose in this work (SLR). The evidences show there are five factors affecting the CP’s effectiveness. Three of them have received a little attention among developers regarding choosing the most suitable: software architecture, application interface and behavior of the application in terms of resource consumption. © 2005 – ongoing JATIT & LLS.",Android mobile application; Energy conservation technique; Energy consumption patterns; Systematic literature review,"Al Nidawi H.S.A., Wei K.T., Dawood K.A., Khaleel A.",2017,Review,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039921555&partnerID=40&md5=edb660ede4fb29c26c7468329da70694,"Department of Software Engineering and Information System, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Malaysia; Department of Computer, Faculty of Education for Girls, Universiti of Kufa, Iraq",Asian Research Publishing Network,English,19928645,
Scopus,Consolidating evidence based studies in software cost/effort estimation - A tertiary study,"Software Effort Estimation is key to the success of any project since all downstream activities such as planning, budgeting, developing and Monitoring cannot be executed without clarity on the scope of the activity that needs to be performed. This is a tertiary study that follows the Systematic Literature Review (SLR) process as put forth by Kitchenham in her seminal paper, based on five criteria: estimation technique, estimation accuracy, type of dataset and independent variables used in empirical research on effort estimation. Our study covering 820 Primary Studies through 14 SLRs, shows that Software Effort Estimation studies focus more on statistical techniques and Machine Learning is taking precedence in comparison to the others; whereas Expert Judgement is preferred by the industry due to its intuitiveness. There is a need for models that are simple to understand and global, due to the distributed nature of software development. The studies are inconclusive about the accuracy benefits of using a within company dataset vs.external datasets. Machine learning techniques such FL and GA in combination with Analogy methods generate more accurate estimates. There is increasing consensus on the use of Mean Magnitude of Relative Error (MMRE), Median Magnitude of Relative Error (MdMRE) and Prediction Pred (25%) as the accuracy metric. 78% of the Primary Studies reported accuracy using MMRE. The best MMRE reported is in the range of 7 to 75. ISBSG (International Software Benchmarking Standards Group) and Desharnais datasets with 27% and 17% usage respectively are the most widely used datasets in empirical studies on effort estimation. Fewer than 20 independent variables account for more than 90% impact of variables in empirical analysis on Software effort estimation. © 2017 IEEE.",Effort Prediction; Software Cost Estimation; Software Effort Estimation; Systematic Literature Review; Tertiary Survey,"Pillai S.P., Madhukumar S.D., Radharamanan T.",2017,Conference,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",10.1109/TENCON.2017.8227974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044191749&doi=10.1109%2fTENCON.2017.8227974&partnerID=40&md5=b9292c7cb065c5b10568332f62f5f822,"School of Management Studies NIT Calicut, Kozhikode Kerala, India; Dept. of Computer Science, NIT Calicut, Kozhikode, Kerala, India; Dept. of Mechanical Engg., NIT Calicut, Kozhikode, Kerala, India",Institute of Electrical and Electronics Engineers Inc.,English,21593442,9781509011339
Scopus,Early Phase Cost Models for Agile Software Processes in the US DoD,"Background: Software effort estimates are necessary and critical at an early phase for decision makers to establish initial budgets, and in a government context to select the most competitive bidder for a contract. The challenge is that estimated software requirements is the only size information available at this stage, compounded with the newly increasing adoption of agile processes in the US DoD. Aims: The objectives are to improve cost estimation by investigating available sizing measures, and providing practical effort estimation models for agile software development projects during the contract bidding phase or earlier. Method: The analysis explores the effects of independent variables for product size, peak staff, and domain on effort. The empirical data for model calibration is from 20 industrial projects completed recently for the US DoD, among a larger dataset of recent projects using other lifecycle processes. Results: Statistical results showed that initial software requirements is a valid size metric for estimating agile software development effort. Prediction accuracy improves when peak staff and domain are added as inputs to the cost models. Conclusion: These models may be used for estimates of agile projects, and evaluating software development contract cost proposals with inputs available during the bidding phase or earlier. © 2017 IEEE.",Agile software processes; domain; interfaces; peak staff; productivity; requirements volatility; software cost estimation; software effort; software requirements; software size,"Rosa W., Madachy R., Clark B., Boehm B.",2017,Conference,International Symposium on Empirical Software Engineering and Measurement,10.1109/ESEM.2017.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042374547&doi=10.1109%2fESEM.2017.10&partnerID=40&md5=1cb6f585498a64e9bb7636ff1f133454,"IT Estimating Division, Naval Center for Cost Analysis, Washington, DC, United States; Department of Systems Engineering, Naval Postgraduate School, Monterey, CA, United States; Software Metrics, Inc., Haymarket, VA, United States; USC Center for Systems and Software Engineering, University of Southern California, Los Angeles, CA, United States",IEEE Computer Society,English,19493770,9781509040391
Scopus,Estimated measurement quality software on structural model academic system with function point analysis,"In the software development indispensable is the suitability and accuracy in determining the size or value of the software to fit the operation to be performed. A wide variety of calculation methods have been widely used to estimate the size of the software, one of which is by using Function Point Analysis (FPA). Volume calculation software based on a scale of complexity. Since the point of measurement is highly subjective, in order to maintain consistency and validity of the results, the method should be run by an experienced professional. This method is then applied by the authors to measure the complexity of academic information system STIKOM Dinamika Bangsa Jambi using structured modeling approach. Measurements were performed in this study consisted of depictions information system is built into the structure. Which is then analyzed by counting models Crude Function Points (CRP), the relative complexity of Adjustment Factor (RCAF), and then calculate the point function. From the results of calculations using the FPA to software quality measurement academic system STIKOM Dinamika Bangsa Jambi obtained value FP 166.32 is good. Function point value produced will be used by developers in determining the price and the cost of software systems to be built or developed. © 2017 IEEE.",Function point analysis; Software quality measurement; Structural model; System,"Rohayani H., Gaol F.L., Soewito B., Warnars H.L.H.S.",2017,Conference,"Proceedings - 2017 International Conference on Applied Computer and Communication Technologies, ComCom 2017",10.1109/COMCOM.2017.8167085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043450872&doi=10.1109%2fCOMCOM.2017.8167085&partnerID=40&md5=c61c95f0de061aa782ced059030d0f55,"Computer Science Program, Bina Nusantara University, n. Kebon Jeruk No27, Jakarta, Indonesia; Computer Science, Bina Nusantara University, n. Kebon Jeruk No27, Jakarta, Indonesia; Binus Graduate Program, Bina Nusantara University, n. Kebon Jeruk No27, Jakarta, Indonesia; Computer Science, Bina Nusantara University, JI. Kebon Jeruk No27, Jakarta, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781509040483
Scopus,Survey of Software Reliability Growth Model,"SRGM (software reliability and growth model), as an important mathematical tool of modeling reliability and improving reliability process, plays significant role in measuring, predicting and ensuring reliability, managing testing resources and releasing optimal software. Research on SRGM is elaborated and analyzed in this paper. First, main research content and modeling process of SRGM are analyzed, and the basic function is sketched. In the mean time, research evolution is summarized, the state of art is illustrated and the current research characteristics are formulated. Second, from the three aspects including the total number of faults in software, FDR (fault detection rate) and TE (testing-effort), the key factors influencing SRGM are analyzed. Based on the unified framework model proposed in author's previous research, the classical numerical models are classified, compared and analyzed. In addition, the SRGMs based on finite and infinite queue model are discussed and simulation technique emphasizing on RDEP (rate-driven event processes) is elaborated. Furthermore, to evaluate the differences in the models, 26 models are compared by 16 published failure data sets. Experimental results reveal that the differences depend on the objectivity of failure data set collected and the subjectivity of establishing mathematical model by researchers under the different assumptions. Finally, the challenges, the trend of development and the problems to be solved are pointed out. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Framework model; Imperfect debugging; Queue theory; Simulation; Software reliability growth model; Testing effort,"Zhang C., Meng F.-C., Kao Y.-G., Lü W.-G., Liu H.-W., Wan K., Jiang J.-N., Cui G., Liu Z.-H.",2017,Review,Ruan Jian Xue Bao/Journal of Software,10.13328/j.cnki.jos.005306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033383978&doi=10.13328%2fj.cnki.jos.005306&partnerID=40&md5=5177cc2b6ae748b0676cc6a78f7dfdd3,"School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; School of Computer Science and Technology, Harbin Institute of Technology at Weihai, Weihai, 264209, China; School of Science, Harbin Institute of Technology at Weihai, Weihai, 264209, China",Chinese Academy of Sciences,Chinese,10009825,
Scopus,Source code size prediction using use case metrics: an empirical comparison with use case points,"Software source code size, in terms of source lines of code (SLOC), is an important parameter of many parametric software development effort estimation methods. In this paper, we investigate empirically the early prediction of SLOC for object-oriented software using use case metrics. We used different modeling techniques to build the prediction models. We used the univariate logistic regression and the simple linear regression methods to evaluate the individual effect of each use case metric on SLOC, and the multivariate logistic regression and the multiple linear regression methods to explore the combined effect of the use case metrics on SLOC. We also used in the study different machine learning methods (k-NN, naïve Bayes, C4.5, random forest, and multilayer perceptron neural network). The prediction models were evaluated using the receiver operating characteristic analysis, particularly the area under the curve measure, and leave-one-out cross validation. An empirical study, using data collected from five open source Java projects, is reported in the paper. The use case metrics have been compared to the well-known use case points method. Results provide evidence that the use case metrics-based approach gives a more accurate prediction of SLOC than the use case points-based approach. © 2016, Springer-Verlag London.",C4.5; k-NN; Linear regression; Logistic regression; LOO cross validation; Multilayer perceptron neural network; Naïve Bayes; Prediction models; Random forest; ROC and AUC analysis; Source code size; Use case metrics; Use case points; Use cases,"Badri M., Badri L., Flageol W., Toure F.",2017,Journal,Innovations in Systems and Software Engineering,10.1007/s11334-016-0285-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986260110&doi=10.1007%2fs11334-016-0285-7&partnerID=40&md5=f5d08487c14a51568f373e9fb732cf05,"Software Engineering Research Laboratory, Department of Mathematics and Computer Science, University of Quebec, Trois-Rivières, QC, Canada",Springer London,English,16145046,
Scopus,A Hybrid eBusiness Software Metrics Framework for Decision Making in Cloud Computing Environment,"Developing high-quality software is essential for eBusiness organizations to cope with drastic market competition. With the development of cloud computing technologies, eBusiness systems and applications pay more attention to open endedness. In a cloud computing environment, eBusiness systems have the ability to provide information technology resources on demand. Traditional software metric methods in distributed systems and applications are technical and project driven, making the market demand and internal practical operation not perfectly balanced within a cloud-computing-based eBusiness corporation. To address this issue, this paper presents a hybrid framework based on the goal/question/metric paradigm to evaluate the quality and efficiency of previous software products, projects, and development organizations in a cloud computing environment. In our approach, to support decision making at the project and organization levels, three angular metrics are used, i.e., project metrics, product metrics, and organization metrics. Furthermore, an improved radial-basis-function-based model is also provided to manage existing projects and design new projects. Experimental results on a well-known eBusiness organization show that the proposed framework is effective, efficient, and operational. Moreover, using the described decision-making algorithm, the predicted data are very close to actual results on the software cost, the fault rate, the development workload, etc., which are greatly helpful in achieving high-quality software. © 2016 IEEE.",Cloud computing; decision making; eBusiness; prediction; radial basis function (RBF); software metrics,"Zhao F., Nian G., Jin H., Yang L.T., Zhu Y.",2017,Journal,IEEE Systems Journal,10.1109/JSYST.2015.2443049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027465450&doi=10.1109%2fJSYST.2015.2443049&partnerID=40&md5=8ad453f6dd188c6ed37d6f23c24fcac7,"Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Department of Computer Science, Faculty of Science, St. Francis Xavier University, Antigonish, NS  B2G 2W5, Canada",Institute of Electrical and Electronics Engineers Inc.,English,19328184,
Scopus,Empirical evaluation of fuzzy analogy for Software Development Effort Estimation,"Software Development Effort Estimation (SDEE) plays a primary role in software project management. Among several techniques suggested for estimating software development effort, analogybased software effort estimation approaches stand out as promising techniques. In this paper, the performance of Fuzzy Analogy is compared with that of six other SDEE techniques (Linear Regression, Support Vector Regression, Multi-Layer Perceptron, M5P and Classical Analogy). The first step of the evaluation aimed to ensure that the SDEE techniques outperformed random guessing by using the Standardized Accuracy (SA). Then, we used a set of reliable performance measures and Borda count to rank them and identify which techniques are the most accurate. The results suggest that Fuzzy Analogy statistically outperformed the other SDEE techniques regardless of the dataset used. © 2017 ACM.",Fuzzy analogy; Software Development Effort Estimation,"Abnane I., Idri A., Abran A.",2017,Conference,Proceedings of the ACM Symposium on Applied Computing,10.1145/3019612.3019905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020898807&doi=10.1145%2f3019612.3019905&partnerID=40&md5=df809b8bee59594505af4961af714d5e,"Software Project Management Research Team, ENSIAS, Mohammed v University, Rabat, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",Association for Computing Machinery,English,,9781450344869
Scopus,Software Size Estimation Using Activity Point,"Software size is widely recognized as an important parameter for effort and cost estimation. Currently there are many methods for measuring software size including Source Line of Code (SLOC), Function Points (FP), Netherlands Software Metrics Users Association (NESMA), Common Software Measurement International Consortium (COSMIC), and Use Case Points (UCP). SLOC is physically counted after the software is developed. Other methods compute size from functional, technical, and/or environment aspects at early phase of software development. In this research, activity point approach is proposed to be another software size estimation method. Activity point is computed using activity diagram and adjusted with technical complexity factors (TCF), environment complexity factors (ECF), and people risk factors (PRF). An evaluation of the approach is present. © Published under licence by IOP Publishing Ltd.",,"Densumite S., Muenchaisri P.",2017,Conference,IOP Conference Series: Materials Science and Engineering,10.1088/1757-899X/185/1/012013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017443047&doi=10.1088%2f1757-899X%2f185%2f1%2f012013&partnerID=40&md5=5ce2c65d2c76718f025c7d5c2964d30d,"Chulalongkorn University, Bangkok, 10330, Thailand",Institute of Physics Publishing,English,17578981,
Scopus,An assessment framework of routing complexities using LOC metrics,"Size plays a vital role in the software product or project. Lines of Code (LOC) or Source Lines of Code (SLOC) is the most important activities in the software metrics. LOC is used to measure the size of the program or size of the project or size of the text in the coding. Based on the effort we can maintain the system and it associated to the technical quality of that system. Calculated the effort in the system is difficult and challenging task for developer. We have discussed about the new trends of the Lines of Code (LOC). And further, this paper surveys on Impact of LOC on Complexity and we have also discussed about the need of LOC. Herewith we included the tools to find LOC of Routing Algorithm and compared LOC of Routing Algorithm between the tools. © 2017 IEEE.",Complexity; Lines of Code; Routing,"Aswini S., Yazhini M.",2017,Conference,"2017 Innovations in Power and Advanced Computing Technologies, i-PACT 2017",10.1109/IPACT.2017.8245022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045837477&doi=10.1109%2fIPACT.2017.8245022&partnerID=40&md5=91f2fc5a67f7b205639fac021314ca25,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509056828
Scopus,A reusability assessment of UCP-based effort estimation framework using object-oriented approach,"Software effort estimation has become one of the most important concerns of software industries and Use Case Points (UCP) is seen as one of the most popular estimation models for object-oriented software development. Since year 2005, more than 10 UCP-based effort estimation techniques have been proposed either to give more options or to enhance the capability of UCP. However, there is no guidance for software practitioners to develop a quality UCP-based effort estimation applications. Therefore, we have proposed a new design framework for UCP-based technique to promote reusability in developing software applications. This paper will experiment and provide evidence showing that the framework achieved a good quality design using Quality Model for Object-oriented Design (QMOOD). The results showed that the framework has met five quality attributes and good to be reused as a guideline at the early stages of software development.",QMOOD.; Reusability; Software Effort Use Case Points; —Estimation,"Ani Z.C., Basri S., Sarlan A.",2017,Journal,"Journal of Telecommunication, Electronic and Computer Engineering",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041740267&partnerID=40&md5=e0a9b139fdd1e5b17b002f10e09469e2,"School of Computing, Universiti Utara Malaysia, UUM Sintok, Kedah  06010, Malaysia; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Tronoh, Perak  31750, Malaysia",Universiti Teknikal Malaysia Melaka,English,21801843,
Scopus,An examination of determinants of software testing and project management effort,"Software estimation research has primarily focused on software effort involved in direct software development. As more and more organizations buy instead of building software, more effort is spent on software testing and project management. In this empirical study, the effect of program duration, computer platform, and software development tool (SDT) on program testing effort and project management effort is studied. The study results point to program duration and software tool as significant determinants of testing and management effort. Computer platform, however, does not have an effect on testing and management effort. Furthermore, the mean testing effort for third generation (3G) development environment was significantly higher than the mean testing effort for fourth generation (4G) environments that used IDE. In addition, the management effort for 4G environment projects without the use of IDE was lower than nonprogramming report generation projects. © 2017 International Association for Computer Information Systems.",Effort; Platform; Project management; Software estimation; Testing,"Subramanian G.H., Pendharkar P.C., Pai D.R.",2017,Journal,Journal of Computer Information Systems,10.1080/08874417.2016.1183428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034850103&doi=10.1080%2f08874417.2016.1183428&partnerID=40&md5=46e022f9c3e6391edef318474e5c062d,"Pennsylvania State University at Harrisburg, Middletown, PA, United States",Taylor and Francis Inc.,English,08874417,
Scopus,Cost overruns on the Norwegian continental shelf: The element of surprise,"We examine drivers of cost overruns in Norwegian development projects in the oil and gas sector. The multivariate longitudinal econometric analysis employs a unique and detailed dataset consisting of 79 different projects between 2000 and 2013. Among the significant results, we find that the unexpected change in economic activity has a positive effect on the overruns, there is a considerable positive momentum in the transitional cost overruns, more experienced operators tend to incur less overruns and finally that the size of the investment of the projects has a positive impact on the overruns. Furthermore, we find evidence that current economic activity matters to an extent, but that the pivotal factor is the unexpected change in activity. © 2017 Elsevier Ltd",Oil projects; Project metrics; Project valuation,"Lorentzen S., Oglend A., Osmundsen P.",2017,Journal,Energy,10.1016/j.energy.2017.05.106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021061427&doi=10.1016%2fj.energy.2017.05.106&partnerID=40&md5=80ee7ce4558df8100ebe0cb506953c27,"University of Stavanger, NO-4036, Stavanger, Norway",Elsevier Ltd,English,03605442,
Scopus,Analysis on Causal-Effect Relationship in Effort Metrics Using Bayesian LiNGAM,"In the effort estimation studies, we can obtain open datasets from the past research. Those datasets are either within-company or cross-company dataset. On effort estimation, it was long discussed which dataset is appropriate for building accurate model. To find a new viewpoint in this discussion, we introduce the causal-effect relationship estimation technique. We use a simple Bayesian approach that is defined by the data generation model in a Linear Non-Gaussian Acyclic Model (LiNGAM). This model is applied to the function point and effort metrics in both within-company and cross-company datasets. We assume that if a dataset is appropriate for effort estimation, causal-effect relationships between metrics and effort will be extracted more. The result of case study shows that we can extract more causal-effect relationships from the cross-company dataset than that of from the within-company dataset. © 2016 IEEE.",,"Kondo M., Mizuno O.",2016,Conference,"Proceedings - 2016 IEEE 27th International Symposium on Software Reliability Engineering Workshops, ISSREW 2016",10.1109/ISSREW.2016.18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009823670&doi=10.1109%2fISSREW.2016.18&partnerID=40&md5=f2d945e8d46f7e78ac627e8ae15304eb,"School of Science and Technology, Kyoto Institute of Technology, Kyoto, Japan; Faculty of Information and Human Sciences, Kyoto Institute of Technology, Kyoto, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,9781509036011
Scopus,Quantitative estimation of cost drivers for intermediate COCOMO towards traditional and cloud based software development,"Software project estimation is the process of analyzing the resource requirements for the given time duration of product development. Cost estimation models are used for calculating the associated amount required for developing the stakeholder's requirement within the defined time boundaries. Among several models available for the cost estimation of software projects, COCOMO is one of the well-known models which serve the field most. Resources applied for the given time will generate the rough estimates, but for more accurate values, various factors are analyzed. These factors are termed as cost drivers. Software estimation using COCOMO is performed by selecting values of cost drivers on a predefined scale. This approach solely depends on experience of a software analyst. However, there is a lack of a systematic approach available for the selection of values of these cost drivers. Our work suggests the quantification of cost drivers for intermediate COCOMO. Quantification will implicitly fetch the values from the system and its environment which reduces the manual selection of ranges of scaling factors. Hence the systems cost will be generated directly without analyst and selector logic. Finally, if the selection of correct scaling is performed, then the calculation of cost will definitely get improved. An experimental analysis is performed between the above suggested model and the Intermediate COCOMO. The results show that the ""COCOMOUP"" is performing well under the known conditions and in uncertain requirements conditions, the system is getting better predictions. © 2016 ACM.",Cloud based software cost estimation; Cost drivers; Cost estimation; Intermediate COCOMO; KLOC (Kilo Line of Code); Quantification,"Agrawal A., Jain V., Sheikh M.",2016,Conference,ACM International Conference Proceeding Series,10.1145/2998476.2998488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996564572&doi=10.1145%2f2998476.2998488&partnerID=40&md5=921a83b7f683dc73b25baaed9ee13654,"Dept. of CSE, Medi-Caps University, Indore, M.P., India; Institute of Engineering and Technology, Devi Ahilya Vishwavidyala, Indore, M.P., India",Association for Computing Machinery,English,,9781450348089
Scopus,Combatting use case points' limitations with COCOMO(R) II and relative difficulty,"Software cost estimates become more accurate as more information becomes available, but are needed early for business case analyses, bids, and resource management. Use Case Points satisfy the ability to make software size estimates early in the lifecycle because they only require understanding how an actor will use the system. Though Use Case Points are easy to calculate, they might over-simplify a project's size and lead to inaccurate estimates. The Use Case Points method has also been heavily criticized for its technical and environmental factors, since they were not calibrated and verified with data. COCOMO(R) II possesses a rich knowledge base of factors that were calibrated and verified with data and expert judgment. Calculating and calibrating an effort model on Unified Code Count (UCC)'s project set using COCOMO(R) II's parameters and Use Case Points needed an additional relative difficulty factor for greatly improved effort estimates. © 2016 IEEE.",COCOMO; Cost Estimation; Effort Estimation; Local Calibration; Project Management; Use Case Points,"Hira A., Boehm B.",2016,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2016.058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018462964&doi=10.1109%2fAPSEC.2016.058&partnerID=40&md5=1dbdbf716516181daff5406d2a0204eb,"Computer Science Department, University of Southern California, Los Angeles, United States",IEEE Computer Society,English,15301362,9781509055753
Scopus,Do estimators learn? on the effect of a positively skewed distribution of effort data on software portfolio productivity,"We study whether an assumed positively skewed distribution of effort data prevents software estimators to learn over time; leading to increasing differences between planned and actual effort and a deteriorating (worsening) trend on productivity. We analyze data of 25 software releases of one application, collected over a period of six years in a public sector institution in The Netherlands. We statistically test for distribution, trend on differences between planned versus actual effort over time, and productivity of software portfolios. The key contributions of this paper are that we show that a proposed assumption that assumes any relation between a positively skewed distribution of effort data and a deteriorating productivity is not applicable to the subject dataset. We find that the effort data is to be characterized as positively skewed distributed, and we do see a shift over time from under-estimation to over-estimation. We do not find evidence for a deteriorating productivity; on the contrary productivity improves over time, indicating that estimators in the subject organization did learn. © 2016 ACM.",Function Point Analysis; Software Economics; Software Estimation,"Huijgens H., Vogelezang F.",2016,Conference,"Proceedings - 7th International Workshop on Emerging Trends in Software Metrics, WETSoM 2016",10.1145/2897695.2897698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974575554&doi=10.1145%2f2897695.2897698&partnerID=40&md5=60a66ed315ac86dfa800bc3b20426c4d,"Delft University of Technology and Goverdson, Delft, Netherlands; Ordina, Nieuwegein, Netherlands","Association for Computing Machinery, Inc",English,,9781450341776
Scopus,An empirical validation of an automated genetic software effort prediction framework using the ISBSG dataset,"Background: The complexity of providing accurate software effort prediction models is well known in the software industry. Several prediction models have been proposed in the literature using different techniques, with different results, in different contexts. Objectives: This paper reports a benchmarking study using a genetic approach that automatically generates and compares different learning schemes (preprocessing+ attribute selection+learning algorithms). The effectiveness of the software development effort prediction models (using function points) were validated using the ISBSG R12 dataset. Methods: Eight subsets of projects were analyzed running a M×N-fold cross-validation. We used a genetic approach to automatically select the components of the learning schemes, to evaluate, and to report the learning scheme with the best performance. Results: In total, 150 learning schemes were studied (2 data preprocessors, 5 attribute selectors, and 15 modeling techniques). The most common learning schemes were: Log+ForwardSelection+M5-Rules, Log+BestFirst+M5-Rules, Log+LinearForwardSelection+SMOreg, ForwardSelection+ SMOreg and ForwardSelection+ SMOreg, BackwardElimination+ SMOreg, LinearForwardSelection+SMOreg, and Log+Best First+SMOreg. Conclusions: The results show that we should select a different learning schemes for each datasets. Our results support previous findings regarding that the setup applied in evaluations can completely reverse findings. A genetic approach that automatically selects best combination based on a specific dataset could improve the performance of software effort prediction models.",Effort prediction model; Experiment; Function points; Genetic approach; ISBSG dataset; Learning schemes,"Murillo-Morera J., Quesada-López C., Castro-Herrera C., Jenkins M.",2016,Conference,CIBSE 2016 - XIX Ibero-American Conference on Software Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988350386&partnerID=40&md5=09eb6bbb921cb2cc3c422cc8de6c0115,"Center for ICT Research, University of Costa Rica, Costa Rica; Intel Corporation, Costa Rica",Universidad de las Fuerzas Armadas ESPE,English,,9789978301814
Scopus,Workflow Development Effort Estimation as Applied to Web Human Resource Management,"Over the past decade most of engineers, researchers, institutions, standard organizations and businessmen had been faced with challenges on how to represent productivity functions for effort estimation in information system workflow development. Despite the fact that workflows are everywhere, only few reports about its development effort are available. Accurate workflow sizing is a very difficult task at conceptual state. This paper proposes the proximity scoring measurement method to determine workflow size and to estimate workflow development effort with its knowledge retention effort. The approach is applied to the web publishing industry as a human resources case study. The approach of this paper is scalable and easy to apply to evaluate Web human resource workflow development effort. The implication of this work is that workflows can be estimated more precisely, helping to estimate the benefit of workflow reuse. © 2016, Global Institute of Flexible Systems Management.",Activity proximity scoring; Business rules; Costing; Effort estimation; Flexibility; Knowledge retention; Process measurement; Workflow development; Workflow size,Raheem R.O.,2016,Journal,Global Journal of Flexible Systems Management,10.1007/s40171-015-0121-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960355244&doi=10.1007%2fs40171-015-0121-1&partnerID=40&md5=8a0ef4fad91114c3cbd1a4b2aad4b22e,"Technology Management Economics and Policy Program (TEMEP), College of Engineering, Seoul National University, Seoul, South Korea",Global Institute of Flexible Systems Management,English,09722696,
Scopus,A literature survey on the accuracy of software effort estimation models,"Accurate software effort estimation is crucial for the software development process. Neither over estimated nor underestimated effort is welcome. Academic researchers and practitioners have long been searching for more accurate estimation models or methods. The objective of this paper is to explore the studies on software effort estimation accuracy available in the literature and analyze the findings to obtain the answer for the question: which software effort estimation model is more accurate? There were very limited reports that satisfied the research criteria. Only 8 studies with 10 reports were discovered for the analysis. It is found that Use Case Point Analysis outperforms other models with the least weighted average Mean Magnitude of Relative Error (MMRE) of 39.11%, compare to 90.38% for Function Point Analysis and 284.61% for COCOMO model. It indicates that there is still need to improve the estimation performance but the question is how.",COCOMO models; Function point analysis; Performance of software effort estimation models; Software effort estimation; Use Case point analysis,Arnuphaptrairong T.,2016,Conference,Lecture Notes in Engineering and Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978689312&partnerID=40&md5=cb00b30f4b211bc2aa81cfe24198abca,"Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok, 10250, Thailand",Newswood Limited,English,20780958,9789881404763
Scopus,An approach to software cost estimation by improved - Time variant acceleration coefficient based PSO,"Software Cost estimation plays an important role in its development. A software cost (effort) estimation error can ruin any software. Number of models already exist that defines relationship between software size (input) and effort (output); however still these models fails to estimate the cost of software because of uncertainties, and imprecision associated with it. In this paper the parameters of existing cost estimation model (COCOMO) are tuned using improved Time Varying Acceleration coefficient based PSO and is tested on 10 NASA projects. The experimental results prove that the parameter tuned cost estimation model (COCOMO) has better estimation capabilities as compared to other existing cost estimations models. © 2016 Old City Publishing, Inc.",KLOC-kilo lines of codes; Parameter tuning; Particle swarm optimization; PM-person month; Software cost estimation; Time-variant acceleration coefficient,"Bhatia P., Mishra K.K., Misra A.K.",2016,Journal,Journal of Multiple-Valued Logic and Soft Computing,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973443928&partnerID=40&md5=bffbcadc2001b0868ede1f369f9ae810,"Computer Science and Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India",Old City Publishing,English,15423980,
Scopus,Dealing with missing values in software project datasets: A systematic mapping study,"Missing Values (MV) present a serious problem facing research in software engineering (SE) which is mainly based on statistical and/or data mining analysis of SE data. Therefore, various techniques have been developed to deal adequately with MV. In this paper, a systematic mapping study was carried out to summarize the existing techniques dealing with MV in SE datasets and to classify the selected studies according to six classification criteria: research type, research approach, MV technique, MV type, data types and MV objective. Publication channels and trends were also identified. As results, 35 papers concerning MV treatments of SE data were selected. This study shows an increasing interest in machine learning (ML) techniques especially the K-nearest neighbor algorithm (KNN) to deal with MV in SE datasets and found that most of the MV techniques are used to serve software development effort estimation techniques. © Springer International Publishing Switzerland 2016.",Missing values; Software engineering; Systematic mapping study,"Idri A., Abnane I., Abran A.",2016,Conference,Studies in Computational Intelligence,10.1007/978-3-319-33810-1_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969232970&doi=10.1007%2f978-3-319-33810-1_1&partnerID=40&md5=bcf59f4d24258293351f5636660fd78e,"Software Project Management Research Team, ENSIAS, Mohamed V University, Rabat, Morocco; Department of Software Engineering, Ecole de Technologie Supérieure, Montréal, H3C IK3L, Canada",Springer Verlag,English,1860949X,9783319338095
Scopus,Investigating the relationship between software defect density and cost estimation drivers: An empirical study,"This paper reports the results of an empirical study, where some hypotheses about the relationship between software quality, expressed in terms of defect density, and cost estimation drivers (cost factors) are assessed. The study is performed using three cost factors, these are: work effort, project size, measured in terms of function points, and average assigned work per team member. The study is performed using the ISBSG repository data set release 12. ISBSG dataset which contains 6006 completed project. For statistical analyses, Regression Analysis and stepwise Analysis of Variance (ANOVA) are used. Our results show that there is a significant negative impact of the project size and work effort on the defect density. On the other hand, the average assigned work per team member has a significant positive impact on defect density. These results suggest that while assigning resources to a project, these quality factors should be considered in order to decrease the defect density, and thus, the maintenance cost of the product. © 2005 - 2015 JATIT & LLS.",Cost estimation drivers; Defect density; Empirical study; ISBSG repository; Software quality,"Wedyan F., Bani-Salameh H., Al-Ajlouni W., Al-Manai S.",2015,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952791057&partnerID=40&md5=97dacdaca90262ee091ae9a8bf45cafc,"Department of Software Engineering, Hashemite University, Zarqa, 13315, Jordan",Asian Research Publishing Network,English,19928645,
Scopus,Automated web application functional size estimation based on a conceptual model,"Project managers and developers need methods which would help them to predict the size of the software to be developed, to plan the effort and resources required for development and to estimate a number of people to be employed for the task. Determining the functional size of software is one of the methods that can meet these requirements. The paper builds on the authors' previous work on estimating effort and software size based on the conceptual models in the earlier stage of software development (the requirements phase) based on mapping between the UWE conceptual models of web applications and the COSMIC functional size. In this paper, a tool for automated functional size estimation of web applications on the basis of UWE conceptual models is presented. A case study that was carried out was based on real applications and thus evaluates this tool in a realistic environment. © 2015 University of Split, FESB.",conceptual model; functional size measurement; web application,"Ceke D., Milasinovic B.",2015,Conference,"2015 23rd International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2015",10.1109/SOFTCOM.2015.7314074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958622020&doi=10.1109%2fSOFTCOM.2015.7314074&partnerID=40&md5=01cda7c5e5c9e552adc5cedbad2e41aa,"Quality Assurance Office, University of Tuzla, Bosnia and Herzegovina; Department of Applied Computing, Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia",Institute of Electrical and Electronics Engineers Inc.,English,,9789532900569
Scopus,Soft computing based estimation of software development effort,"Software development cost estimation is an important activity in the early software design phases. The input datasets are primarily taken from the promise repository. Data mining and soft computing techniques are used to assess the software development cost estimation. Each feature in the input dataset is divided, the linguistic terms along with the membership are identified using trapezoidal membership functions, and associative classification is adopted for generating rules. The large number of rules is filtered with respect to the support and confidence. Genetic algorithm is employed as an optimization tool for selecting the best rules. The example presented demonstrates the improvement in accuracy. The crisp efforts are presented after defuzzification of the output. © 2014 IEEE.",cost estimation; fuzzy logic; genetic algorithm; soft computing; software effort,"Saraswathi S., Kannan N.",2015,Conference,"2014 International Conference on Information Communication and Embedded Systems, ICICES 2014",10.1109/ICICES.2014.7033775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925337735&doi=10.1109%2fICICES.2014.7033775&partnerID=40&md5=608241b8fa0b7b9665c5e476bc676aae,"Jayaram College of Engineering and Technology, Tiruchirappalli, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781479938346
Scopus,Validating the design theory for managing project scope during software sourcing and delivery,"Most software project estimation and measurement (PEM) processes have been designed for providers. Customers need to leverage PEM to better direct software sourcing. A design theory for software project-scoping has been developed that supports the PEM processes of both customers and providers. This paper probes the validity of the theory from providers' and customers' viewpoints through three longitudinal case studies. A software provider used a preliminary version of the theory in dozens of software development projects annually, systematically (1) achieving higher customer satisfaction through better estimation accuracy and (2) improving productivity. Two government organizations used the theory to speed up their software sourcing and reduce their sourcing costs to a half without giving up any functional or quality requirements. Similar results have not been obtained before. Future research is needed to generalize the findings.",Design theory; Information systems sourcing; Software project scope management,"Käkölä T., Forselius P.",2015,Conference,"2015 International Conference on Information Systems: Exploring the Information Frontier, ICIS 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964596891&partnerID=40&md5=8b8b67579e75a952488adf2dda4b4761,"University of Jyväskylä, Jyväskylä, 40014, Finland; 4SUM Partners Ltd, Tekniikantie 14, Espoo, 02150, Finland",Association for Information Systems,English,,
Scopus,From function points to COSMIC - A transfer learning approach for effort estimation,"Software companies exploit data about completed projects to estimate the development effort required for new projects. Software size is one of the most important information used to this end. However, different methods for sizing software exist and companies may require to migrate to a new method at a certain point. In this case, in order to exploit historical data they need to resize the past projects with the new method. Besides to be expensive, resizing is also often not possible due to the lack of adequate documentation. To support size measurement migration, we propose a transfer learning approach that allows to avoid resizing and is able to estimate the effort of new projects based on the combined use of data about past projects measured with the previous measurement method and projects measured with the new one. To assess our proposal, an empirical analysis is carried out using an industrial dataset of 25 projects. Function Point Analysis and COSMIC are the measurement methods taken into account in the study. © Springer International Publishing Switzerland 2015.",COSMIC; Effort estimation; Function points; Transfer learning,"Corazza A., Di Martino S., Ferrucci F., Gravino C., Sarro F.",2015,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-26844-6_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952359662&doi=10.1007%2f978-3-319-26844-6_19&partnerID=40&md5=a88207fe6f4d31b54cd009aaed504ca7,"Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione, Università di Napoli ‘Federico II’, Naples, Italy; Dipartimento di Informatica, Università di Salerno, Fisciano, Italy; CREST, Department of Computer Science, University College London, London, United Kingdom",Springer Verlag,English,03029743,9783319268439
Scopus,A new software cost estimation model for small software organizations: An empirical approach,Software cost estimation is a challenging issue for the modern software industry to improve the software quality and to avoid the risk. During the last 20 years there had been much more developments to estimate the software. So many software estimation models are also developed for the betterment of software industry. Using these models we cannot find accurate results to estimate the cost of software in all kind of environments such as component based software development and Business process outsourcing environments. The authors present in this study a new technique for software cost estimation that can be used for software projects developed for Business process outsourcing environment. The model was calibrated using the empirical data collected from 60 projects from different BPO Industry. Efficiency of the model was also compared with an existing model used for such environment. The proposed model achieved better predictive accuracy. © Research India Publications.,BPO; KLOC; MMRE; MRE; PRED; SLIM; SLOC; Software effort estimation,"Patra H.P., Rajniish K., Panda U.S.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941064688&partnerID=40&md5=c47aa5b142b13c905f1fb6bc3934d6a5,"Department of Computer science, BIT Mesra, India; TCS, India",Research India Publications,English,09734562,
Scopus,A Fuzzy Logic Model for Predicting the Development Schedule of Software Projects,"The development schedule of software projects is mainly measured in months and it is a necessary and important phase, since the under prediction or over prediction of it at the planning stage can negatively impact budgets. Unfortunately, only 39 percent of software projects finish on time relative to their original plan. According to its development type, a software project can be classified as new, enhanced, or a re-development. In this study, a Fuzzy Logic Model (FLM) for predicting the schedule of new development software projects is proposed. The hypothesis to be tested is that the accuracy of schedule prediction for an FLM is statistically better than the accuracy obtained from a simple linear regression (SLR) model when adjusted function points, obtained from new development software projects, are used as the independent variable. The FLM and SLR models were trained and tested using a sample of new software projects obtained from the International Software Benchmarking Standards Group (ISBSG) Release 11. The accuracy of the FLM was compared against that of the SLR model. The criteria for evaluating the accuracy of these two models were the Absolute Residuals and a Wilcox on statistical test. Results showed that prediction accuracy of an FLM was statistically better than that of an SLR model at the 95% confidence level. We can conclude that an FLM could be applied for predicting the schedule of new development software projects developed on mainframes and coded in third generation programming languages. © 2015 IEEE.",fuzzy logic; ISBSG; software engineering; software project schedule prediction,"Lopez-Martin C., Chavoya A., Meda-Campana M.E.",2015,Conference,"Proceedings - 12th International Conference on Information Technology: New Generations, ITNG 2015",10.1109/ITNG.2015.73,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936804968&doi=10.1109%2fITNG.2015.73&partnerID=40&md5=a6c34e8686c54db8fad6e42e96e66889,"Information Systems Department, Universidad de Guadalajara, Guadalajara, Jalisco, Mexico",Institute of Electrical and Electronics Engineers Inc.,English,,9781479988273
Scopus,Metrics that matter in software integration testing labs,"Without having in place data-driven metrics that give a holistic business perspective of software integration testing laboratories, leaders of the DoD's weapons programs are unable to optimize the performance of these full-system and subsystem integration labs that test and certify integrated hardware and software during the development, modernization, and sustainment of the U.S. military's integrated and complex systems. Yet these metrics are not available across the DoD lab footprint, even though the labs' significance is growing in parallel with the military's rapid shift from using equipment with capabilities based on advanced hardware to equipment that is dependent on fully integrated, complex systems of both hardware and software. © 2015, U.S. Department of Defense. All rights reserved.",,"Hagen C., Hurt S., Williams A.",2015,Journal,CrossTalk,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930173799&partnerID=40&md5=f33e1549e471d67515a8e042060b8831,"A.T.Kearney, United States",U.S. Department of Defense,English,,
Scopus,Quality changes in high and low oil content canola during storage: Part II - Mathematical models to predict germination,"Novel mathematical models to predict germination of canola (including rapeseed) stored under laboratory and field conditions were developed with certain assumptions. The models were developed based on the data collected under laboratory condition (constant temperatures). The hypothesis of the developed models was that germination of stored canola was influenced by temperature, moisture content, storage time, and airtightness. The effect of these factors on germination of stored canola was the product of effective storage time and interactive effect of temperature, moisture content, storage time, and airtightness. Data collected under both laboratory and filed conditions were used to calibrate and verify the developed models. The prediction accuracy of the models associated with field conditions (except canola seeds with 14% moisture content stored inside silo bags) was higher than that under laboratory conditions. The developed models could visually or mathematically predict germination of canola or rapeseed stored under any condition. The developed models explained more than 96% variation of observed germination of the canola or rapeseed seeds with ≤10% moisture content stored inside a flat bottom bin or commercial silo bags. © 2014 Elsevier Ltd.",Canola; Germination; Mathematical model; Rapeseed; Storage,"Jian F., Sun K., Chelladurai V., Jayas D.S., White N.D.G.",2014,Journal,Journal of Stored Products Research,10.1016/j.jspr.2014.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911449707&doi=10.1016%2fj.jspr.2014.05.007&partnerID=40&md5=e1ec08369c12edcb43c3fc5de93bd8af,"Department of Biosystems Engineering, University of Manitoba, Winnipeg, MB  R3T 5V6, Canada; Agriculture and Agri-Food Canada, Department of Biosystems Engineering, University of Manitoba, Winnipeg, MB  R3T 5V6, Canada",Elsevier Ltd,English,0022474X,
Scopus,A framework of statistical and visualization techniques for missing data analysis in software cost estimation,"Software Cost Estimation (SCE) is a critical phase in software development projects. However, due to the growing complexity of the software itself, a common problem in building software cost models is that the available datasets contain lots of missing categorical data. The purpose of this chapter is to show how a framework of statistical, computational, and visualization techniques can be used to evaluate and compare the effect of missing data techniques on the accuracy of cost estimation models. Hence, the authors use five missing data techniques: Multinomial Logistic Regression, Listwise Deletion, Mean Imputation, Expectation Maximization, and Regression Imputation. The evaluation and the comparisons are conducted using Regression Error Characteristic curves, which provide visual comparison of different prediction models, and Regression Error Operating Curves, which examine predictive power of models with respect to under- or over-estimation. © 2015 by IGI Global. All rights reserved.",,"Angelis L., Mittas N., Chatzipetrou P.",2014,Book Chapter,Handbook of Research on Innovations in Systems and Software Engineering,10.4018/978-1-4666-6359-6.ch003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944384395&doi=10.4018%2f978-1-4666-6359-6.ch003&partnerID=40&md5=b43501d984a5dfcfad643fedaaaf072e,"Department of Informatics, Aristotle University of Thessaloniki, Greece; Computer Science Department, Aristotle University of Thessaloniki, Greece",IGI Global,English,,9781466663626; 9781466663602
Scopus,Building defect prediction models in practice,"The information about which modules of a future version of a software system will be defect-prone is a valuable planning aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. In this chapter, building a defect prediction model from data is characterized as an instance of a data-mining task, and key questions and consequences arising when establishing defect prediction in a large software development project are discussed. Special emphasis is put on discussions on how to choose a learning algorithm, select features from different data sources, deal with noise and data quality issues, as well as model evaluation for evolving systems. These discussions are accompanied by insights and experiences gained by projects on data mining and defect prediction in the context of large software systems conducted by the authors over the last couple of years. One of these projects has been selected to serve as an illustrative use case throughout the chapter. © 2014 by IGI Global. All rights reserved.",,"Ramler R., Himmelbauer J., Natschläger T.",2014,Book Chapter,Handbook of Research on Emerging Advancements and Technologies in Software Engineering,10.4018/978-1-4666-6026-7.ch024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945998993&doi=10.4018%2f978-1-4666-6026-7.ch024&partnerID=40&md5=ac5ff72f10a1ecbffe1453dde25d2308,"Software Competence Center Hagenberg, Austria",IGI Global,English,,9781466660281; 9781466660274
Scopus,Predicting project effort intelligently in early stages by applying genetic algorithms with neural networks,"In the early stages of a software development project, estimating the amount of effort is one of the most important project management concerns. This study has successfully produced global optimal reduced models intelligently predicting software cost estimation by employing neural networks with back-propagation learning algorithms combined with genetic algorithms (GA-NN) to determine the most significant explanatory variables among the 16 COCOMO cost drivers. The performance of the full model of GA-NN is much superior to that of the COCOMO, whilst the predicting performance of its global optimal reduced model is also comparable to that of the COCOMO in terms of MMRE and PRED (25). The optimal reduced models and their found significant factors can offer powerful supports for the project managers to make right decisions in the early stages of the projects. © (2014) Trans Tech Publications, Switzerland.",Computing intelligence; Genetic algorithms; Neural network; Project management; Software engineering,Li Z.Y.,2014,Conference,Applied Mechanics and Materials,10.4028/www.scientific.net/AMM.513-517.2035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897741767&doi=10.4028%2fwww.scientific.net%2fAMM.513-517.2035&partnerID=40&md5=92d8632372cb07c0d9c567fe701adb47,"Business School of Central South University, Changsha, 410083, China",,English,16609336,9783038350125
Scopus,Efficiency of software development projects: A case study on an information technology company in India,"In this chapter, Data Envelopment Analysis (DEA) is applied to evaluate the relative efficiency of software development projects of a leading software company in India. Further, the projects are categorized as per their efficiency scores into highly efficient, moderately efficient and less efficient companies through a process called Tier Analysis. The chapter also includes an Improvement Path for the projects with low efficiencies. Furthermore, through the application of Kruskal Wallis test, the software development project efficiency is compared with team size to determine whether efficiency vary across various team size categories i.e. small, medium, large and extra large. © Springer-Verlag Berlin Heidelberg 2014.",Data Envelopment Analysis; Efficiency of software development projects; Managing service productivity; Team size,"Sharma G., Kataria A.",2014,Book Chapter,International Series in Operations Research and Management Science,10.1007/978-3-662-43437-6_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954372156&doi=10.1007%2f978-3-662-43437-6_15&partnerID=40&md5=c4b3982995021ecbda92d8992a009225,"IIPS, Devi Ahilya UniversityMP, India",Springer New York LLC,English,08848289,
Scopus,Describing the correlations between metamodels and transformations aspects,"Metamodels are a key concept in Model-Driven Engineering. Any artifact in a modeling ecosystem has to be defined in accordance to a metamodel prescribing its main qualities. One of the most important artifact is model transformation that are considered to be the heart and soul of MDE and as such advanced techniques and tools are needed for supporting the development, quality assurance, maintenance, and evolution of model transformations. Several works propose the adoption of metrics to measure quality attributes of transformation without considering any metamodel aspects. In this paper, we present an approach to understand structural characteristics of metamodels and how the model transformations depend on corresponding input and target metamodels.",Metamodel metrics; Metamodeling; Model driven engineering; Transformation metrics,"Di Rocco J., Di Ruscio D., Iovino L., Pierantonio A.",2014,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928814900&partnerID=40&md5=02f627266b46f1f669a60268b5360879,"Department of Information Engineering Computer Science and Mathematics, University of L'Aquila, Italy",CEUR-WS,English,16130073,
Scopus,Language extended lexicon points: Estimating the size of an application using its language,"Estimating the size of a software system is a critical task due to the implications the estimation has in the management of the development project. There are some widely accepted estimation techniques: Function Points, Use Case Points and Cosmic Points, but these techniques can only be applied after the availability of a requirements specification. In this paper, we propose an approach to estimate the size of an application previous to its requirements specification by using the application language itself, captured by the Language Extended Lexicon (LEL). Our approach is based on Use Case Points and on a technique which derives Use Cases from the LEL. The proposed approach provides a measure of the application's size earlier than the usual techniques, thus reducing the effort needed to apply them. An initial experiment was conducted to evaluate the proposal. © 2014 IEEE.",Domain Analysis; Language Extended Lexicon; Requirements specifications; Software Sizing; Use Case Points,"Antonelli L., Rossi G., Do Prado Leite J.C.S., Oliveros A.",2014,Conference,"2014 IEEE 22nd International Requirements Engineering Conference, RE 2014 - Proceedings",10.1109/RE.2014.6912268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909978364&doi=10.1109%2fRE.2014.6912268&partnerID=40&md5=3862ac0d355af180cd9d67b99b130cf9,"Lifia, Fac. de Informática, UNLP, Bs As, Argentina; Dep. Informática, PUC-Rio, Gávea, RJ, Brazil; INTEC, UADE, Bs As, Argentina",Institute of Electrical and Electronics Engineers Inc.,English,,9781479930333
Scopus,Communication of software cost estimates,"The meaning of an effort or cost estimate should be understood and communicated consistently and clearly to avoid planning and budgeting mistakes. Results from two studies, one of 42 software companies and one of 423 individual software developers, suggest that this is far from being the case. In both studies we found a large variety in what was meant by an effort estimate and that the meaning was frequently not communicated. To improve the planning and budgeting of software projects we recommend that the meaning of effort estimates is understood and communicated using a probability-based terminology. Copyright 2014 ACM.",Communication of estimates; Cost estimation; Terminology,Jørgensen M.,2014,Conference,ACM International Conference Proceeding Series,10.1145/2601248.2601262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905501114&doi=10.1145%2f2601248.2601262&partnerID=40&md5=09c976ef35e6110a3ba7b656173c9263,"Simula Research Laboratory, University of Oslo, Lysaker, Norway",Association for Computing Machinery,English,,9781450324762
Scopus,A simplified formulation of predictive object points (POP) sizing metric for OO measurement,Different effort estimation techniques exist for sizing software systems but none is directly applicable to object-oriented software. Although many researchers worked for size and thus effort estimation but still the problem not resolved fully. Different existing estimation techniques works specifically for specific development environment. PRICE systems has developed the predictive object point (POP) metric for predicting effort required for developing an object oriented software system and is based on the counting scheme of function point (FP) method. Though it was an interesting theoretical development but could not gain sufficient recognition from practitioners to be used on a regular basis due to lack of an easy to use support tool and too much complicated formulations. In this paper we tried to simplify the POP calculation. The POP count formula suggested by PRICE system has been modified for estimating the effort. An easy to use support tool to automate the counting method has been prepared. The refined POP count formula and preliminary results of its application in an industrial environment are presented and discussed here for validation of the suggested modification or simplification in formula. © 2014 IEEE.,Automation; Functional size measurement; Object Orientation; Predictive Object Point; Software Measurement; Software Metrics,"Jain S., Yadav V., Singh R.",2014,Conference,"Souvenir of the 2014 IEEE International Advance Computing Conference, IACC 2014",10.1109/IAdCC.2014.6779526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899082235&doi=10.1109%2fIAdCC.2014.6779526&partnerID=40&md5=bf11409174b82ec22a818ebcfcc19f1f,"Department of Computer Science and Engineering, Kanpur Institute of Technology, Kanpur, India; Computer Science and Engineering Department, Harcourt Butler Technological Institute, Kanpur, India",IEEE Computer Society,English,,
Scopus,Factors affecting personal software development productivity: A case study with PSP data,"Understanding the factors that affect the productivity of software developers and may cause productivity variations among individuals and projects is important for anyone interested in improving software engineering performance and estimates, and in particular for users of high-maturity processes, such as the Personal Software Process (PSP) and the Team Software Process (TSP). In order to contribute to the understanding of the personal and non-personal factors that affect productivity, we analyzed the data from more than 3000 developers that concluded successfully the 10 projects of the PSP for Engineers I/II training course. Regarding non-personal factors, by conducting a detailed per-phase analysis, we found significant variations of productivity among projects that can be partially explained by process changes. Regarding personal factors, we found significant variations among individuals that can be partially explained by personal experience.",Factors; Productivity; PSP; Software development,"Raza M., Faria J.P.",2014,Conference,"Proceedings of the IASTED International Conference on Software Engineering, SE 2014",10.2316/P.2014.810-014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898470280&doi=10.2316%2fP.2014.810-014&partnerID=40&md5=e45852646bf7844bf9c7fa1824925a80,"Faculty of Engineering, INESC TEC, University of Porto, Porto, Portugal",Acta Press,English,,
Scopus,A lean approach to estimate the functional size of operating applications,"Functional size measures have been gaining increasing acceptance not only for project estimation and productivity comparisons but also for the evaluation of the company assets concerning with operating applications. However, functional sizing operating applications can represent a complex task (they could be developed in past projects, in some cases by others, with long standing technologies, and without useful documentation supporting them). Thus, companies require cheaper strategies to estimate functional size of these applications. To this end, we proposed some measurement approaches concerning with the number of physical tables and the number of read and read/written tables that can be easily collected from the database schema and the application interfaces. We assessed the effectiveness of the proposals performing an empirical study based on 17 applications of an Italian company. The results highlight that the proposed approaches allowed to obtain accurate functional size estimations, thus turning out to be suitable for the involved company. © 2013 IEEE.",Function Points; Operating applications; Physical tables; Software functional size measurement,"Ferrucci F., Gravino C., Moretto G.",2013,Conference,"Proceedings - 39th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2013",10.1109/SEAA.2013.40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889038149&doi=10.1109%2fSEAA.2013.40&partnerID=40&md5=a3b4ab84d3159cd9a5082ad09a56dfc8,"DISTRA(MIT), University of Salerno, Fisciano (SA), Italy; InfoCamere, Corso Stati Uniti 14, 35127 Padova, Italy",,English,,9780769550916
Scopus,A new method to estimate software size,"Cost estimation is a key content of software project management, its accurate depends on size estimation. FPA is a method used widely to estimate size, but it has many problems in itself. A novel method is proposed to solve these problems which estimate sizes using FPA based on UML for objects having no analogy in historical data and using paired comparisons for objects having analogy in historical data. FPA based on UML is used to solve complexity of FPA; Paired comparisons are used to solve problems which do not reflect the trends of today's software engineer. The processes of the novel method are elaborated on and illuminated using an example. © Springer-Verlag Berlin Heidelberg 2013.",Case-based reasoning; FPA; Paired comparison; UML,"Ren X.-L., Dai Y.-B.",2013,Conference,"International Asia Conference on Industrial Engineering and Management Innovation: Core Areas of Industrial Engineering, IEMI 2012 - Proceedings",10.1007/978-3-642-38445-5-65,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891765563&doi=10.1007%2f978-3-642-38445-5-65&partnerID=40&md5=296cf50ae619aef067189e2c6fb92ff2,"College of Computer Science and Engineering, Qujing Normal University, Qujing, China",,English,,9783642384448
Scopus,Effective approaches for delivering affordable military software,"As the U.S. military shifts its focus from metal and mechanics to developing and integrating control systems-unmanned vehicles, drones, and smart bombs-delivering advanced software-enabled systems affordably will become a vital success factor in weaponry.",,"Hagen C., Hurt S., Sorenson J.",2013,Journal,CrossTalk,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890961079&partnerID=40&md5=0f26ad7dce6f2e1e91981a1114dc0a5d,"A.T. Kearney, India",,English,,
Scopus,Adjusting effort estimation using micro-productivity profiles [Töökoormuse hinnangu täpsustamine mikroproduktiivsuse profiili abil],"We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model, and the team. Micro-productivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show that the calculated Micro-Productivity Profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change-oriented estimation.",Change estimation; Effort estimation; Prediction model; Productivity,"Tóth G., Végh Á.Z., Beszédes Á., Schrettner L., Gergely T., Gyimóthy T.",2013,Journal,Proceedings of the Estonian Academy of Sciences,10.3176/proc.2013.1.08,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874259795&doi=10.3176%2fproc.2013.1.08&partnerID=40&md5=1a818bd93aaa71a53c5ba067c48e27e1,"Department of Software Engineering, University of Szeged, Árpád tér 2, H-6720, Hungary",,English; Estonian,17366046,
Scopus,"Requirements engineering for software and systems, second edition","As requirements engineering continues to be recognized as the key to on-time and on-budget delivery of software and systems projects, many engineering programs have made requirements engineering mandatory in their curriculum. In addition, the wealth of new software tools that have recently emerged is empowering practicing engineers to improve their requirements engineering habits. However, these tools are not easy to use without appropriate training. Filling this need, Requirements Engineering for Software and Systems, Second Edition has been vastly updated and expanded to include about 30 percent new material. In addition to new exercises and updated references in every chapter, this edition updates all chapters with the latest applied research and industry practices. It also presents new material derived from the experiences of professors who have used the text in their classrooms. Improvements to this edition include: • An expanded introductory chapter with extensive discussions on requirements analysis, agreement, and consolidation • An expanded chapter on requirements engineering for Agile methodologies • An expanded chapter on formal methods with new examples • An expanded section on requirements traceability • An updated and expanded section on requirements engineering tools • New exercises including ones suitable for research projects Following in the footsteps of its bestselling predecessor, the text illustrates key ideas associated with requirements engineering using extensive case studies and three common example systems: an airline baggage handling system, a point-of-sale system for a large pet store chain, and a system for a smart home. This edition also includes an example of a wet well pumping system for a wastewater treatment station. With a focus on software-intensive systems, but highly applicable to non-software systems, this text provides a probing and comprehensive review of recent developments in requirements engineering in high integrity systems. © 2014 by Taylor & Francis Group, LLC.",,Laplante P.A.,2013,Book,"Requirements Engineering for Software and Systems, Second Edition",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053951796&partnerID=40&md5=e0d4d2c208087139e290934cd88b9189,,CRC Press,English,,9781466560826; 9781466560819
Scopus,Calibrating productivity drivers for software customisation projects,"Effort estimation for software customisation projects is a challenging task. Similar to the COCOMO approach, an initial estimate can be based on basic project characteristics, and then modified according to various productivity drivers. The aim of this paper is to build on an initial estimation model, generated from a set of contributing factors for software customisation projects, and to improve the accuracy of the model through calibration of effort multipliers derived from customisation project characteristics. Projects from Release 12 of the ISBSG Data Repository were analysed to see which of the productivity drivers were usefully related to variations in software productivity, after other project characteristics had been taken into account. The effect on estimation accuracy of multiplying an initial estimate by each of these productivity drivers was evaluated, in terms of changes in mean and median absolute error, and mean and median magnitude of relative error. Finally, a set of useful productivity drivers is identified for customisation projects, and their effect is quantified. They are found to improve effort estimation accuracy significantly. © 2013 IEEE.",Model calibration; Productivity drivers; Software effort estimation,"Hasan M.M., Lokan C.",2013,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",10.1109/APSEC.2013.110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897425972&doi=10.1109%2fAPSEC.2013.110&partnerID=40&md5=485a102a27fa1d036c1babf782d6f8af,"School of Engineering and Information Technology, University of New South Wales, Canberra, Australia",IEEE Computer Society,English,15301362,9781479921430; 9780769549224
Scopus,"Expert webest tool: A web based application, estimate the cost and risk of software project using function points","There are several area of the software engineering in which we can use the function point analysis (FPA) like project planning, project construction, software implementation etc. In software development, accuracy and efficiency of cost estimation methodology for a web based application is very important. The proposed web based application (i.e. Expert webest tool), is to produce accurate cost estimation and risk estimation throughout the software development cycle to determine feasibility of software project. Cost of the software projects depends on the project size, project type, cost adjustment factor, cost driven factors, nature and characteristics of the project. Software estimation needs to estimates or predict the software costs and software risk early in the software life-cycle. In this paper we proposed the Expert webest tool in Java, this tool is used to two different purpose, first to estimate the cost of the software & secondly, to estimate the risk in the software. Most of the software's fails due to over budget, delay in the delivery of the software & so on. Function point is a well known established method to estimate the size of software projects. Its measure of software size that uses logical functional terms, business owners & user, more readily understand. The management of risks is a central issue in the planning and management of any venture. In the field of software, Risk Management is a critical discipline. The process of risk management embodies the identification, analysis, planning, tracking, controlling, and communication of risk. It gives us a structured mechanism to provide visibility into threats to projects success. Risk management is a discipline for living with the possibility that future events may cause adverse effects. Risk management partly means reducing uncertainty. The propose tool indicates the risk & estimates risk using risk exposure. Management team to estimates the cost & risk within a planned budget and provide a fundamental motivation towards the development of web based application project. Find heuristic risk assessment using cost factors, indicating product & project risk using some risk factors & check some risk management strategies in under estimation development time. © 2013 Springer-Verlag.",Cost estimation; Expert judgment; Expert webest tool; Risk factors; Risk management strategies; Type of risks,"Jaiswal A., Sharma M.",2013,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-642-31552-7_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868313422&doi=10.1007%2f978-3-642-31552-7_9&partnerID=40&md5=fc3482547fb8701cace897cab09f0e64,"Department of Computer Science and Engineering, CDSE, Indore (MP)-452008, India; Department of Computer Engineering, IET, DAVV, Indore (MP)-452001, India",Springer Verlag,English,21945357,9783642315510
Scopus,Object oriented metrics for prototype based languages,"Prototype (classless) based object oriented programming approach has several advantages for representing default knowledge and dynamically modifying concepts over traditional class based languages. Many modern languages like C#, JavaScript and others are in part or completely utilizing astounding features of prototypes. With this growing interest in adoption of prototypes a sheer need is emerging to redesign software metrics for prototype based languages. These paper highlights issues for prototype based software metrics for object oriented programming.",Design metrics; Prototype object modeling; Software complexity; Software life cycle,"Ahsan S., Hayat F., Afzal M., Ahmad T., Asif K.H., Shahzad Asif H.M., Saleem Y.",2012,Journal,Life Science Journal,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877061028&partnerID=40&md5=5f2ac73d4b903442c3a64513e2f257c4,"Department of Computer Science and Engineering, University of Engineering and Technology, Lahore, Pakistan",,English,10978135,
Scopus,Proposed software development model for small organization and its explanation,"Software Development is the process to illustrate the overall mechanism involved in the progress of software during different stages of development. To moderate the computational efficiency of earlier and later phases of development often occurred in small scale software developing organization we have proposed a new software development model for small organization. Through this model we can elicit the software requirement and we can also compute the functionality and risk in each and every phase of the software development. © Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2012.",Elicitation techniques; Function point; Software model; Software risk,"Kumar V., Gupta S.",2012,Conference,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",10.1007/978-3-642-27308-7_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888397493&doi=10.1007%2f978-3-642-27308-7_17&partnerID=40&md5=2c892accfd2fd35e4c5f5886d6dd6452,"Department of Computer Science and Information Technology, Sunder deep College of Engineering and Technology, Ghaziabad, India; U.P. Technical University, Lucknow UP, India; Department of Computer Science and Information Technology, Raj Kumar Goel Institute of Technology, Ghaziabad, India",,English,18678211,9783642273070
Scopus,Lessons learnt in the implementation of CMMI® maturity level 5,"CMMI® has proven benefits in software process improvement. Typically, organisations that achieve a CMMI level rating improve their performance. However, CMMI implementation is not trivial, in particular for high maturity levels, and not all organisations achieve the expected results. Certain CMMI implementation problems may remain undetected by SCAMPISM since only a sample of the organisation is analysed during the appraisal and assessing the quality of implementation of some practices may be difficult. In this paper we present the case of three CMMI level 5 organisations. From the lessons learnt and based on an extensive bibliographic research, we identify a set of problems and difficulties that organisations willing to implement CMMI should be aware of and provide a set of recommendations to help avoid them. As future research we will develop a framework to help to evaluate the quality of implementation of CMMI practices. © 2012 IEEE.",Capability Maturity Model Integration; high maturity; quality of implementation; Standard CMMI Appraisal Method for Process Improvement,"Margarido I.L., Vidal R.M., Vieira M.",2012,Conference,"Proceedings - 2012 8th International Conference on the Quality of Information and Communications Technology, QUATIC 2012",10.1109/QUATIC.2012.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878870736&doi=10.1109%2fQUATIC.2012.37&partnerID=40&md5=3d2be2a0bd324baf8306c362e024703c,"Department of Informatics Engineering, Faculty of Engineering, University of Porto, Porto, Portugal; Department of Informatics Engineering, Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal",,English,,9780769547770
Scopus,Definition of software metrics for software project development by using fuzzy sets and logic,"Software metrics measure certain properties of the software or its specifications. As quantitative measurements are required in all the sciences, there are ongoing efforts among computer science practitioners and academics to apply analogous approaches to software development process. The main aim is to provide objective, reproducible and quantitative measure that can have many useful applications in such important parts of software project management as the schedule and budget planning, cost estimation, quality assurance, software debugging, performance optimization and optimal staff task assignments as well as in the whole software development life cycle. There exist a number of techniques for modeling software metrics including FPA estimation (mean-based, median-based), LS regression, LMS regression, Neural Networks, and Fuzzy Logic. In this paper we are going to talk about singularities of applying Fuzzy Logic approach to the software metrics modeling. © 2012 IEEE.",fuzzy logic and sets; software metrics; software project management,"Mirseidova S., Atymtayeva L.",2012,Conference,"6th International Conference on Soft Computing and Intelligent Systems, and 13th International Symposium on Advanced Intelligence Systems, SCIS/ISIS 2012",10.1109/SCIS-ISIS.2012.6505336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877810899&doi=10.1109%2fSCIS-ISIS.2012.6505336&partnerID=40&md5=ac459aa793dc9155fb32326b5176e4cf,"Kazakh-British Technical University, Tole bi st. 59, Almaty, Kazakhstan",,English,,9781467327428
Scopus,Size measurement in cost estimation,"Size estimation whose accuracy is directly related to success of software project is the key content of software project management. FPA is a method to estimate size and has been used widely, but there are many problems in it. To improve estimation accuracy, a new method is proposed which estimate sizes using FPA based on UML for objects having no similar objects in historical data and paired comparison is used for other objects. FPA based on UML is used to solve complexity of FPA, Paired comparison is used to solve problems which do not reflect the trends of today's software engineer. The analogy objects are chosen based on case-based reasoning, and the relative sizes are calculated using fuzzy comprehensive evaluation for estimation by experts. © 2012 IEEE.",Case-based reasoning; FPA; Fuzzy comprehensive evaluation; Paired comparison; UML,"Dai Y.-B., Ren X.-L.",2012,Conference,"Proceedings of the 2012 4th International Symposium on Information Science and Engineering, ISISE 2012",10.1109/ISISE.2012.95,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877668277&doi=10.1109%2fISISE.2012.95&partnerID=40&md5=89fcbd2664908d40516fbfe246b679c2,"College of Computer Science and Engineering, Qujing Normal University, Qujing, China",,English,,9780769549514
Scopus,An analysis of accuracy and learning in software project estimating,"This paper presents a study into the accuracy of different dimensions of IT project estimating: schedule, budget & effort. The study is based on a dataset of 171 projects that have been collected at the IT-department of a large Dutch multinational company. The paper also analyses whether there is any learning (improvement) effect over time. Our results show that there is no relation between accuracy of budget, schedule and effort in the analyzed organization. Besides, they show that over time there is no change of the accuracy effectiveness and efficiency. The paper reflects and provides recommendations on how to improve the learning from historical estimates. © 2012 IEEE.",accuracy of estimation; budget; management; organizational learning; Project estimation; schedule; time,"Zapata A.H., Chaudron M.R.V.",2012,Conference,"Proceedings - 38th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2012",10.1109/SEAA.2012.46,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869814176&doi=10.1109%2fSEAA.2012.46&partnerID=40&md5=e53035d11f57e4fe2ade4cc8031edbe8,"Leiden Institute of Advanced Computer Science, P.O. Box 9512, 2300 RA Leiden, Netherlands",,English,,9780769547909
Scopus,Similarity measurement of software cost data,"For the characters of high dimension and mixed attributes of software cost data, traditional similarity measurement becomes no longer applicable. A new similarity measurement was proposed by designing a high dimension FCM clustering algorithm. Firstly, an initialization of ordinal-numerical mappings is given; secondly, new ordinal-numerical mappings are learned from the iterative high dimension FCM clustering algorithm and the clustering effect becomes optimized at the same time; finally, a new similarity measurement of software cost data is proposed with the fuzzy partition matrix. The experimental results show that the similarity measurement improves the precision of software cost estimation.",High dimensionality; Ordinal attribute; Similarity measurement; Software engineering,"Liu H.-T., Wei R.-X., Jiang G.-P.",2012,Journal,Shanghai Jiaotong Daxue Xuebao/Journal of Shanghai Jiaotong University,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871978210&partnerID=40&md5=cdc9026ecc0193422ec5b368c408617c,"Department of Equipment Economics and Management, Naval University of Engineering, Wuhan 430033, China; College of Science, Naval University of Engineering, Wuhan 430033, China",,Chinese,10062467,
Scopus,Proposing a novel artificial neural network prediction model to improve the precision of software effort estimation,"Nowadays, software companies have to mange different software development processes based on different time, cost, and number of staff sequentially, which is a very complex task and supports project planning and tracking. Software time, cost and manpower estimation for separate projects is one of the critical and crucial tasks for project managers. Accurate software estimation at an early stage of project planning is counted as a great challenge in software project management, in the last decade, as it allows considering project financial, controlling, and strategic planning. Software effort estimation refers to the estimations of the likely amount of cost, schedule, and manpower required to develop software. This paper proposes a novel artificial neural network prediction model incorporating Constructive Cost Model (COCOMO). The new model uses the desirable features of artificial neural networks such as learning ability, while maintaining the merits of the COCOMO model. This model deals efficiently with uncertainty of software metrics to improve the accuracy of estimates. The experimental results show that using the proposed model improves the accuracy of the estimates, 8.36% improvement, when the obtained result compared to the COCOMO model. © 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",artificial neural networks; COCOMO model; soft computing techniques; software cost estimation models; Software engineering; software project management,"Attarzadeh I., Ow S.H.",2012,Conference,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering",10.1007/978-3-642-32615-8_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869597098&doi=10.1007%2f978-3-642-32615-8_33&partnerID=40&md5=b9aaf906a5b5c0e4bb342b3780374031,"Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, 50603 Kuala Lumpur, Malaysia",,English,18678211,9783642326141
Scopus,Using UML stereotypes to support the requirement engineering: A case study,"In this paper we discuss the transition of an educational process to real-life use. Specifically, a Requirements Engineering (RE) process was tailored and improved to comply with the organization business goals. We discuss challenges faced and proposed solutions, focusing on automation and integration support for RE activities. We use stereotypes to enhance UML diagram clarity, to store additional element properties, and to develop automated RE process support. Stereotypes are one of the core extension mechanisms of the Unified Modeling Language (UML). The benefits founds in their use in a software development organization support the claims that stereotypes play a significant role in model comprehension, reduce errors and increase productivity during the software development cycle. © 2012 Springer-Verlag.",Constraints; Functional Point; Stereotype; Unified Modeling Language; User Interface Prototyping,"Batista V.A., Peixoto D.C.C., Pádua W., Pádua C.I.P.S.",2012,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-31128-4_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863890823&doi=10.1007%2f978-3-642-31128-4_5&partnerID=40&md5=4b173a2bbce391b923379202e8306aca,"Computer Science Dept., Federal University of Minas Gerais, Brazil",,English,03029743,9783642311277
Scopus,The development and achievements of software size measurement,"Software size measurement is crucial for the software development process. It is used for project planning and control purposes during the project execution. It is required for productivity measurement after the project finished. Software size is also an important driver of software effort and cost estimation. This paper analyzed software sizing articles reviewed from the literature and presents the development, and achievements of software size measurement. Comments on the findings and future trends and challenges of the software size estimation models are also given.",Software Size; Software Sizing Software size measurement,Amuphaptrairong T.,2012,Conference,Lecture Notes in Engineering and Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867481292&partnerID=40&md5=3ed0adbc5083478942496654e4c1a958,"Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok 10250, Thailand",Newswood Limited,English,20780958,9789881925114
Scopus,Using data mining techniques for time estimation in software maintenance,"Measuring and estimating are fundamental activities for the success of any project. In the software maintenance realm the lack of maturity, or even a low level of interest in adopting effective maintenance techniques and related metrics, has been pointed out as an important cause for the high costs involved. In this paper, data mining techniques are applied to provide a sound estimation for the time required to accomplish a maintenance task. Based on real-world data regarding maintenance requests, some regression models are built to predict the time required for each maintenance. Data on the team skill and the maintenance characteristics are mapped into values that predict better time estimations in comparison to the one predicted by the human expert. A particular finding from this research is that the time prediction provided by a human expert works as an inductive bias that improves the overall prediction accuracy of the models. Copyright © 2011 Inderscience Enterprises Ltd.",Data mining; Informal reasoning; Metrics; Software maintenance,"Ferneda E., Do Prado H.A., D'Arrochella Teixeira E., Campos F.B.",2011,Journal,International Journal of Reasoning-based Intelligent Systems,10.1504/IJRIS.2011.042265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857273245&doi=10.1504%2fIJRIS.2011.042265&partnerID=40&md5=49f58a04253de79c9872dbe4bb9d7bc8,"Graduate Program on Knowledge and IT Management, Catholic University of Brasilia, SGAN 916, Módulo B, 70.790-160 Brasília, DF, Brazil; Management and Strategy Secretariat, Embrapa - Brazilian Agricultural Research Corporation, Parque Estação Biológica - PqEB, s/n Asa Norte, 70770-901, Brasília, DF, Brazil",Inderscience Publishers,English,17550556,
Scopus,An empirical validation of the suite of metrics for object-relational data modelling,"The important constituent of improvement is the ability to measure, quantify, and authenticate the attributes by measuring several known and unknown, dependent and independent attributes of a product. Designing a software system is an engineering activity, which is not without measurement, and here is where product metrics are applied. In this work, a suite of object-relational metrics is defined, evaluated and formalised. The empirical validation of the metrics is carried with two case studies: knowledge management tool (KMT) and music expert system (MES). Four main, three allied and two derived metrics are empirically validated with KMT and two storage metrics are applied to the MES, which showed better results for the efficient storage and retrieval of such knowledge units. This complete suite of metrics for knowledge modelling will serve as footage for future research and practice, and will contribute to the knowledge of the software metric community. Copyright © 2011 Inderscience Enterprises Ltd.",Empirical validation; Knowledge systems; Metrics; Object-relational data modelling,"Justus S., Iyakutti K.",2011,Journal,International Journal of Intelligent Information and Database Systems,10.1504/IJIIDS.2011.037704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650728011&doi=10.1504%2fIJIIDS.2011.037704&partnerID=40&md5=3bf50c85eaa15d6b794e3fc6df71cf7c,"Department of Computer Applications, Velammal College of Engineering and Technology, Madurai 625009, TN, India; CSIR, School of Physics, Madurai Kamaraj University, Madurai, TN, India",Inderscience Publishers,English,17515858,
Scopus,Comparative analysis of requirements elaboration of an industrial product,"Requirements for a software development project are gradually refined as more information becomes available. This process of requirements elaboration can be quantified using the appropriate set of metrics. This paper reports the results of an empirical study conducted to analyze the requirements elaboration of an industrial software process management tool - SoftPM - being used by more than 300 Chinese commercial software organizations. After adjusting for the effects of overlaps amongst different versions of SoftPM, multi-level requirements data are gathered and elaboration factors for each version are obtained. These elaboration data are compared with the data from a previous empirical study that analyzed requirements elaboration of a set of different small e-services projects. This comparison reveals that the elaboration factors of different SoftPM versions have much less variation confirming the intuition that projects with similar characteristics have comparable elaboration factors. © 2010 IEEE.",Comparative analysis; Elaboration factor; Elaboration profile; Empirical study; Requirements elaboration; Software cost estimation; Software sizing,"Malik A.A., Boehm B., Ku Y., Yang Y.",2010,Conference,"ICSTE 2010 - 2010 2nd International Conference on Software Technology and Engineering, Proceedings",10.1109/ICSTE.2010.5608964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650001394&doi=10.1109%2fICSTE.2010.5608964&partnerID=40&md5=bedb67e0fd364e314171b036ed4f8277,"Center for Systems and Software Engineering, University of Southern California, Los Angeles, United States; Institute of Software, Chinese Academy of Sciences, Beijing, China",,English,,9781424486656
Scopus,Computational intelligence for information technology project management,"With the growing complexity of information technology (IT) projects, the management of these projects is proving to be a daunting task. The magnitude of this problem is underscored by the assertion that approximately 70% of IT projects fail to meet their objectives (Lewis, 2007). Computational intelligence (CI) is an area of research focused on developing intelligent systems to help with complex problems. Specifically, CI seeks to integrate techniques and methodologies to assist in problem domains in which information, data and perhaps even the problem itself are vague, approximate, and uncertain. It would seem that research aimed at leveraging the power of CI against IT project management problems is critical if IT project success rates are to be improved. This work examines the core CI technologies - fuzzy logic, neural networks, and genetic algorithms - and looks at current and potential future applications of these techniques to assist IT project managers. © 2010, IGI Global.",,"Hammell II R.J., Hoksbergen J., Wood J., Christensen M.",2010,Book Chapter,"Intelligent Systems in Operations: Methods, Models and Applications in the Supply Chain",10.4018/978-1-61520-605-6.ch005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899374791&doi=10.4018%2f978-1-61520-605-6.ch005&partnerID=40&md5=7ee726c2bdbaa773da7903c7b0c98e89,"Department of Computer and Information Sciences, Towson University, United States; Towson University, Towson, MD, United States",IGI Global,English,,9781615206056
Scopus,Knowledge transfer aspects of Project Portfolio Management,"Knowledge transfer has a particular relevance for enterprises with large research and development (R&D) departments. Project Portfolio Management (PPM) is a discipline often applied to structure and align R&D activities. Typical functions of such standardized process are project data repository, project assessment, selection, reporting, and portfolio reevaluation. In this work we discuss how PPM can benefit from knowledge transfer activities and suggest some specific knowledge transfer tasks within the PPM process. This enhancement is based on a knowledge and learning strategy and process in the context of PPM. We also evaluate the applicability of these extensions at different stages of the PPM process. © Springer-Verlag Berlin Heidelberg 2010.",Evaluation; Innovation; Knowledge transfer; Project portfolio management,"Stantchev V., Franke M.R., Discher A.",2010,Journal,"Smart Innovation, Systems and Technologies",10.1007/978-3-642-14594-0_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879322110&doi=10.1007%2f978-3-642-14594-0_35&partnerID=40&md5=12541737bff83a74c1cf65fb9323e5e8,"Berlin Institute of Technology, Berlin, Germany; Fachhochschule für Oekonomie und Management (FOM), Berlin, Germany; BearingPoint GmbH, Frankfurt am Main, Germany",,English,21903018,9783642145933
Scopus,Comparison of imputation techniques for efficient prediction of software fault proneness in classes,"Missing data is a persistent problem in almost all areas of empirical research. The missing data must be treated very carefully, as data plays a fundamental role in every analysis. Improper treatment can distort the analysis or generate biased results. In this paper, we compare and contrast various imputation techniques on missing data sets and make an empirical evaluation of these methods so as to construct quality software models. Our empirical study is based on NASA's two public dataset. KC4 and KC1. The actual data sets of 125 cases and 2107 cases respectively, without any missing values were considered. The data set is used to create Missing at Random (MAR) data Listwise Deletion(LD), Mean Substitution(MS), Interpolation, Regression with an error term and Expectation-Maximization (EM) approaches were used to compare the effects of the various techniques.",Imputation; Missing data; Missing data techniques,"Sikka G., Takkar A.K., Uddin M.",2010,Journal,"World Academy of Science, Engineering and Technology",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871239668&partnerID=40&md5=65fee5b52e3acedfa966d1b8f69819ac,"Department of Computer Science and Engineering, Dr. B R Ambedkar National Institute of Technology, Jalandhar, 14401 1, Punjab, India; School of Information Technology, Guru Gobind Singh Indrapratha University, Delhi, India; Dr. B R Ambedkar National Institute of Technology, Jalandhar, 14401 1, Punjab, India",,English,2010376X,
Scopus,Content management system effort estimation using bagging predictors,"This paper presents an effort estimation model for the content management systems which can be used to estimate the effort required for designing and developing. The data from the different content management system projects are studied and the model is finalized by using the bagging predictor on linear regression learning model. These projects are categorized based on their size and total/build effort ratio. The size of the project is estimated by using the modified object point analysis approach. A questionnaire is prepared to help project managers to find out the different objects, their categories and their complexity in the project. Final effort is estimated using the project size and the different adjustment factors. For better calculation of these adjustments factors, these are categorized based on their characteristics viz. Production and General system characteristics, Developer's experience and capability. Another questionnaire is used to refine the model and it has to be filled by the project managers after completing the project. The proposed model is validated by studying twelve completed projects taken from industry and seventy different projects completed by the students. The proposed model shows a great improvement as compared to the earlier models used in effort estimation of content management system projects. © 2010 Springer Science+Business Media B.V.",,"Aggarwal N., Prakash N., Sofat S.",2010,Conference,Technological Developments in Education and Automation,10.1007/978-90-481-3656-8_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871211556&doi=10.1007%2f978-90-481-3656-8_5&partnerID=40&md5=594df126899f89b561ca3fd8445afd08,"CSE Deptt, University Instt. of Engg. and Technology, Panjab University, Chandigarh, India; School of Information Technology, GGSIP University, Delhi, India; CSE Deptt., Punjab Engg. College, Chandigarh, India",,English,,9789048136551
Scopus,Software cost estimation inhibitors-a case study in automotive context,"As cost estimation is an important yet problematic part of project planning there is a need of process improvement to decrease the estimation errors. This paper has two purposes. Based on a qualitative case study conducted at a Swedish automotive company it describes the process of cost estimation in a real-life setting focusing on inhibitors to accurate cost estimates. Second, the results of this study are compared with four other studies with a similar purpose of concluding that a company-specific investigation of cost estimation inhibitors might give more actionable input to the activity of cost estimation process improvement than a more general investigation. Copyright © 2010 John Wiley & Sons, Ltd.",Automotive systems; Case study; Cost estimation; Empirical software engineering,"Magazinius A., Pernsta J., Öhman P.",2010,Conference,Journal of Software Maintenance and Evolution,10.1002/smr.457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955455774&doi=10.1002%2fsmr.457&partnerID=40&md5=2f7de7cf6e3ffd0ab9748190ac10207e,"Chalmers, Computer Science and Engineering, SE-421 96 Gothenburg, Sweden",,English,1532060X,
Scopus,Scalable V & V effort estimation for ultra-large-scale systems,"Project planning requires early-on effort estimation. As such, predictions into the future are made with some degree of unavoidable uncertainty. It is likely that some of those predictions will be proven wrong. Thus, solid techniques and concrete procedures are needed to reduce the inaccuracy of estimates. This paper is motivated by practical industrial experiences. It presents a scalable technique that enables scalable effort estimation and is used within an industrial project. The approach determines the critical factors that have to be considered when estimating the V&V effort for systems. © 2010 IEEE.",Effort estimation; Verification & validation,"Budnik C.J., Subramanyan R., Tanikella R.",2010,Conference,SSIRI 2010 - 4th IEEE International Conference on Secure Software Integration and Reliability Improvement,10.1109/SSIRI.2010.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954826160&doi=10.1109%2fSSIRI.2010.33&partnerID=40&md5=dad8ffe2c69029e667bc409d7ed03d24,"Software Engineering Department, Siemens Corporation, Corporate Research, Princeton, NJ, United States",,English,,9780769540863
Scopus,Valuing software service development in support of business-to-consumer services,"In efforts to innovate, organizations are increasingly offering competitive value-added services, frequently by evolving their software infrastructure. Business decisions are often motivated by economic trade-offs related to the development and management of this software. To develop an economic-modelling framework for the evolutionary development of software-based services, we examine the problem of estimating the return on a software-evolution investment. We present methods for estimating both the cost of developing a new service by incrementally modifying existing software and the value generated by introducing the service to the market in terms of revenue generated by the new service and the value of potential future services it may enable. An example case study illustrates the model. Copyright © 2010 ASA C. Published by John Wiley & Sons, Ltd.",Econometric modelling; Real option theory; Serviceoriented architecture; Software engineering economics; Software value estimation,"Tansey B., Stroulia E.",2010,Journal,Canadian Journal of Administrative Sciences,10.1002/cjas.130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952988959&doi=10.1002%2fcjas.130&partnerID=40&md5=dbcee669bfdbc14d21025ec054431fcf,"Department of Computing Science, University of Alberta, Edmonton, AB T60 2E8, Canada",,English,08250383,
Scopus,An earned-value approach to assess and monitor software project uncertainty: A case study in software test execution,"In this study, we proposed an earned-value approach to assess and monitor software uncertainty. It combines the value-at-risk method in financial field and the earned-value-feedback process of earned value project management and proposes a framework which contains value-at-uncertainty system, consumed-value-feedback system and experienced base to support in-process uncertainty measurement and contingency buffer management. Value-at-uncertainty system is used to assess the uncertainty in progress of a project and consumed-value-feedback system is applied to provide the decision support of how to deal with the buffer at that specific time. A case study in software test execution of 24 projects is conducted to evaluate the approach. The study shows how our approach works to measure the uncertainty and manage the buffer size in different project shapes. With the approach, both the accuracy and the effectiveness of uncertainty assessment can be improved along with the test execution progress. © 2010 Asian Network for Scientific Information.",Contingency buffer; Earned value feedback process; Uncertainty assessment; Value at risk,"Zhu X., Zhou B.",2010,Journal,Information Technology Journal,10.3923/itj.2010.1104.1114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954525668&doi=10.3923%2fitj.2010.1104.1114&partnerID=40&md5=022dd631a954ef3ca6cea62c1e34dab9,"College of Computer Science, Zhejiang University, Hangzhou, 310027, China",Asian Network for Scientific Information,English,18125638,
Scopus,Empirical results from the transformation of a large commercial technical computing environment,"Technical computing has unique requirements which are exemplified by factors such as: an extreme focus on run-time performance, a high degree of responsiveness to the customer base, a continued focus on innovation, concurrent support on multiple computing platforms and most importantly, a very limited set of deep subject matter experts who have the skills to build the solution. Exacerbating the above issues is the fact that the field has traditionally seen a great deal of mergers and acquisition activity which leads quickly to an accumulation of disjoint software development systems. This paper describes a detailed case study built within a leading technical computing company which achieved significant success by focusing relentlessly on enhancing the productivity of the individual developer. The work was driven by the author as the general manager of the organization, and measured results of the transformation will be presented in this paper. © 2009 IEEE.",,"Razdan R., Esposito S., Lawrence J., Conner P.",2009,Conference,"2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",10.1109/ESEM.2009.5316000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449192173&doi=10.1109%2fESEM.2009.5316000&partnerID=40&md5=5dff4e5cdad6f21efb3395730aba649d,"University of Florida, Gainesville, FL 32611, United States; Cadence Design Systems, Chelmsford, MA 01824, United States",,English,,9781424448418
Scopus,Developing dependable systems by maximizing component diversity,"In this chapter, we maximize component diversity as a means to achieve the goal of system dependability. Component diversity is examined from four different perspectives: 1) environmental perspective that emphasizes a component's strengths and weaknesses under diverse operational environments, 2) target perspective that examines different dependability attributes, such as reliability, safety, security, fault tolerance, and resiliency, for a component, 3) internal perspective that focuses on internal characteristics that can be linked logically or empirically to external dependability attributes, and 4) value-based perspective that focuses on a stakeholder's value assessment of different dependability attributes. Based on this examination, we develop an evaluation framework that quanties component diversity into a matrix, and use a mathematical optimization technique called data envelopment analysis (DEA) to select the optimal set of components to ensure system dependability. Illustrative examples are included to demonstrate the viability of our approach. © 2009 Springer-Verlag US.",,"Tian J., Nair S., Huang L., Alaeddine N., Siok M.F.",2009,Book Chapter,High Assurance Services Computing,10.1007/978-0-387-87658-0_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892322504&doi=10.1007%2f978-0-387-87658-0_10&partnerID=40&md5=a2ecedb6fcecf9cb685a01309353ebe2,"Southern Methodist University, Dallas, TX, United States",Springer US,English,,9780387876573
Scopus,What are the significant cost drivers for COSMIC functional size based effort estimation?,"This paper investigates the nature of the relationship between software product size measured by Common Software Measurement International Consortium (COSMIC) Function Points and the development effort. We investigated which of the numerical and categorical cost drivers explain the variation in the development effort by performing step wise Analysis of Variance (ANOVA), Analysis of Co-Variance (ANCOVA) and Linear Regression Analysis. For the statistical analyses, we utilized the International Software Benchmarking Standards Group (ISBSG) Dataset Release 10. In this paper, we discuss the results we obtained and the significance of the results for the software organizations who would like to improve their effort estimation processes. © Springer-Verlag Berlin Heidelberg 2009.",Cosmic function points; Cost drivers; Functional size measurement; Project planning; Software benchmarking; Software effort estimation,"Bajwa S.S., Gencel C.",2009,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-05415-0_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650641326&doi=10.1007%2f978-3-642-05415-0_5&partnerID=40&md5=d53d0f50f1b0f4584d565423d5fa1626,"Department of Computer Science and Engineering, University of Engineering and Technology, Lahore, Pakistan; Department of Systems and Software Engineering, Blekinge Institute of Technology, Ronneby, Sweden",,English,03029743,3642054145; 9783642054143
Scopus,Can we build software faster and better and cheaper?,"""Faster, Better, Cheaper"" (FBC) was a development philosophy adopted by the NASA administration in the mid to late 1990s. that lead to some some dramatic successes such as Mars Pathfinder as well as a number highly publicized mission failures, such as the Mars Climate Orbiter & Polar Lander. The general consensus on FBC was ""Faster, Better, Cheaper? Pick any two"". According to that view, is impossibly to optimize on all three criteria without compromising the third. This paper checks that view using an AI search tool called STAR. We show that FBC is indeed feasible and produces similar or better results when compared to other methods However, for FBC to work, there must be a balanced concern and concentration on the quality aspects of a project. If not, ""FBC"" becomes ""CF"" (cheaper and faster) with the inevitable lose in project quality. © ACM 2009.",COCOMO; faster better cheaper; predictor models; simulated annealing; software engineering; software processes,"Menzies T., El-Rawas O., Hihn J., Boehm B.",2009,Conference,ACM International Conference Proceeding Series,10.1145/1540438.1540442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953755143&doi=10.1145%2f1540438.1540442&partnerID=40&md5=e4d51c9651c4378c2bc178fabd083fa0,"CS and EE, West Virginia University, United States; Jet Propulsion Laboratory, CA, United States; Computer Science Dept., Uni. S. California, United States",,English,,9781605586342
Scopus,Applications of simulation and AI search: Assessing the relative merits of agile vs traditional software development,"This paper augments Boehm-Turner's model of agile and plan-based software development augmented with an AI search algorithm. The AI search finds the key factors that predict for the success of agile or traditional plan-based software developments. According to our simulations and AI search algorithm: (1) in no case did agile methods perform worse than plan-based approaches; (2) in some cases, agile performed best. Hence, we recommend that the default development practice for organizations be an agile method. The simplicity of this style of analysis begs the question: why is so much time wasted on evidence-less debates on software process when a simple combination of simulation plus automatic search can mature the dialog much faster? © 2009 IEEE.",,"Lemon B., Riesbeck A., Menzies T., Price J., D'Alessandro J., Carlsson R., Prifiti T., Peters F., Lu H., Port D.",2009,Conference,ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering,10.1109/ASE.2009.42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952168361&doi=10.1109%2fASE.2009.42&partnerID=40&md5=4e7652d44c798db87c2860ef94568cee,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, United States; Information Technology Management, University of Hawaii, United States",,English,,9780769538914
Scopus,Are we more productive now? Analyzing change tasks to assess productivity trends during software evolution,"Organizations that maintain and evolve software would benefit from being able to measure productivity in an easy and reliable way. This could allow them to determine if new or improved practices are needed, and to evaluate improvement efforts. We propose and evaluate indicators of productivity trends that are based on the premise that productivity during software evolution is closely related to the effort required to complete change tasks. Three indicators use data about change tasks from change management systems, while a fourth compares effort estimates of benchmarking tasks. We evaluated the indicators using data from 18 months of evolution in two commercial software projects. The productivity trend in the two projects had opposite directions according to the indicators. The evaluation showed that productivity trends can be quantified with little measurement overhead. We expect the methodology to be a step towards making quantitative self-assessment practices feasible even in low ceremony projects.",,"Benestad H.C., Anda B., Arisholm E.",2009,Conference,"ENASE 2009 - 4th International Conference on Evaluation of Novel Approaches to Software Engineering, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549178674&partnerID=40&md5=20013c1f9ebe4521a5b8e267a7225b2d,"Simula Research Laboratory, University of Oslo, P.O.Box 134, 1325 Lysaker, Norway",,English,,9789898111982
Scopus,Empirically-based decision support for task allocation in global software development,Task allocation will be supported by developing models for evaluating assignments. The models are extensions of existing project management models and based on empirical studies. Research Area: Project management for global software development; task allocation; empirical software engineering © 2009 IEEE.,,"Lamersdorf A., Rombach D.",2009,Conference,"Proceedings - 2009 4th IEEE International Conference on Global Software Engineering, ICGSE 2009",10.1109/ICGSE.2009.38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049136908&doi=10.1109%2fICGSE.2009.38&partnerID=40&md5=677ce3d5c4ef7e3fb146aad2b17c6320,"University of Kaiserslautern Kaiserslautern, Germany; University of Kaiserslautern and Fraunhofer IESE, Kaiserslautern, Germany",,English,,9780769537108
Scopus,Empirical evaluation of similarity-based missing data imputation for effort estimation,"Multivariate regression models have been commonly used to estimate the software development effort to assist project planning and/or management. Since project data sets for model construction often contain missing values, we need to build a complete data set that has no missing values either by using imputation methods or by removing projects and metrics having missing values (removing method). However, while there are several ways to build the complete data set, it is unclear which method is the most suitable for the project data set. In this paper, using project data of 706 cases (47% missing value rate) collected from several companies, we applied four imputation methods (mean imputation, pair-wise deletion, k-nn method and applied CF method) and the removing method to build regression models. Then, using project data of 143 cases (having no missing values), we evaluated the estimation performance of models after applying each imputation and removing method. The result showed that the similarity-based imputation methods (k-nn method and applied CF method) showed the best performance.",,"Tamura K., Toda K., Tsunoda M., Monden A., Matsumoto K.-I., Kakimoto T., Ohsugi N.",2009,Journal,Computer Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350451005&partnerID=40&md5=ebe9294dc2b8dc42861cd1a2531c2be5,"Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science and Technology, Osaka University, Japan; NTT Data Corporation, Japan",,Japanese,02896540,
Scopus,Project tracking using a metrics binder analysis (MBA) model on software project initiatives (SPI),"Project management and software project management in particular is about managing the implementation effort required towards the completion of a project in time, budget and quality. The management effort is based heavily on project tracking techniques and practices were the project implementation progress is closely monitored and analyzed. This paper presents a project tracking model based primarily on the project requirements and its implementation evolution through the project lifecycle. The tracking process is supported by a set of metrics which cumulatively collaborate, since day one of the project implementation, towards the interpretation of the real project progress via various measurements and results continuously. This live project tracking system is a metric binder analysis for software projects and initiatives, were more than 30 metrics are bind together, creating an accurate, practical and realistic picture of the project progress for the entire project or for any of its components, all the way down to its requirements.",Project management; Project tracking; Software metrics; Software quality engineering,"Markopoulos E., Alexopoulos G., Bouzoukou N., Bilbao J.",2009,Journal,WSEAS Transactions on Business and Economics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71449109157&partnerID=40&md5=6d063bb056b0ed0e2d35bdf244fe3d91,"Department of Informatics, University of Piraeus, 80 Karaoli Dimitriou Str., Piraeus, Greece; Process Engineering Research Unit, EMPROSS Strategic IT Consultants-Germany, Tumblingerstrasse 54/446, 80337 Bavaria, Germany; International Business Development Unit, EMPROSS Strategic IT Consultants - USA, One Broadway, 14th floor, Boston, Cambridge, MA 02142, United States; Applied Mathematics Department, Engineering School, University of the Basque Country, Alda Urkijo s/n., Bilbao, Spain",,English,11099526,
Scopus,An influence diagram-based approach for estimating staff training in software industry,"The successful completion of a software development process depends on the analytical capability and foresightedness of the project manager. For the project manager, the main intriguing task is to manage the risk factors as they adversely influence the completion deadline. One such key risk factor is staff training. The risk of this factor can be avoided by pre-judging the amount of training required by the staff. So, a procedure is required to help the project manager make this decision. This paper presents a system that uses influence diagrams to implement the risk model to aid decision making. The system also considers the cost of conducting the training, based on various risk factors such as, (i) Lack of experience with project software; (ii) Newly appointed staff; (iii) Staff not well versed with the required quality standards; and (iv) Lack of experience with project environment. The system provides estimated requirement details for staff training at the beginning of a software development project.",Influence diagram; Risk management; Staff training,"Jeet K., Mago V.K., Prasad B., Minhas R.S.",2009,Journal,Journal of Intelligent Systems,10.1515/JISYS.2009.18.4.267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249123854&doi=10.1515%2fJISYS.2009.18.4.267&partnerID=40&md5=d8dca57c1aef22bc6786aaf7bbf4b27e,"Department of Computer Science, DAV College, Jalandhar, India; Department of Computer and Information Sciences, Florida A and M University, Tallahassee, FL 32307, United States; Department of Computer Science, MLUDAV College, Phagwara, India",Walter de Gruyter GmbH and Co. KG,English,03341860,
Scopus,Uncertainty in ERP effort estimation: A challenge or an asset?,"Traditionally, software measurement literature considers the uncertainty of cost drivers in project estimation as a challenge and treats it as such. This paper develops the position that uncertainty can be seen as an asset. It draws on results of a case study in which we replicated an approach to balancing uncertainties of project context characteristics in requirements-based effort estimation for ERP implementations. © 2008 Springer Berlin Heidelberg.",,"Daneva M., Wettflower S., De Boer S.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-89403-2-18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049129331&doi=10.1007%2f978-3-540-89403-2-18&partnerID=40&md5=a24a403c7ebf3dd37170faed7da7d940,"University of Twente, Netherlands; ProMetrix, Toronto, Canada",,English,03029743,3540894020; 9783540894025
Scopus,Using PSU for early prediction of COSMIC size of functional and non-functional requirements,"The project effort calculation with a functional size measurement method such as COSMIC can only be properly performed after the ""Requirements Analysis"" phase in a Project Life Cycle. The goal of this research is to investigate an early and project-level tuned prediction of the product size with the intent to reduce the effect of the 'cone of uncertainty' phenomenon. The lack of size measurement methods which take into account the effect of the product non-functional requirements (NFR) on size also contributes to the above phenomenon. We propose to use the Project Size Unit (PSU) technique for predicting the product (FUR and NFR) size measured in COSMIC functional size units. Such early prediction will lower the cost of size counting the project and minimize the estimation error in the requirements phase. Furthermore, the PSU calculation procedure can be automated, which would further reduce the cost of size counting. The expected advantage of jointly using PSU and COSMIC is the ability to get early estimates of the whole project effort. © 2008 Springer Berlin Heidelberg.",COSMIC; Functional User Requirements (FUR); Non-Functional Requirements (NFR); Prediction; Project size; Project Size Unit (PSU),"Buglione L., Ormandjieva O., Daneva M.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-89403-2-29,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049123087&doi=10.1007%2f978-3-540-89403-2-29&partnerID=40&md5=7ec0c2cf2e51a5b5c6514fcf65efe754,"École de Technologie Supérieure (ETS) / Engineering.it; Computer Science and Software Dept., Concordia University; Information Systems Group, University of Twente",,English,03029743,3540894020; 9783540894025
Scopus,Software measurement @ Siemens - A practical approach allows best practice sharing of various organizations,"The Siemens Measurement System for Software-based Systems (SMS) was set up in 2004 (see Keynote of F. Paulisch at the Metrikon 2006) and until now information of more than 440 projects has been collected. The active contribution of almost all software developing organizations within Siemens to the company-wide Measurement System provides a good data base to compare and analyze performance data such as ""Budget Deviation"" and the various success-critical influencing factors such as review practices, ""Team Size"" or ""Unplanned Team Changes"" between various organizations. This way, areas of interest can be objectively identified, where organizations could benefit from Best Practice Sharing. © 2008 Springer Berlin Heidelberg.",Benefits; Measurement system; Organization-wide best practice sharing; Statistical analyses; Success factors,Schunk S.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-89403-2-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049121643&doi=10.1007%2f978-3-540-89403-2-7&partnerID=40&md5=c0cfdaf0713edf78abda5d579afc106e,"Siemens AG, Corporate Technology, CT SE SWI, Otto-Hahn-Ring 6, Munich 81739, Germany",,English,03029743,3540894020; 9783540894025
Scopus,Web development effort estimation: An empirical analysis,"Effort models and effort estimates help project managers allocate resources, control costs and schedule, and improve current practices, leading to projects that are finished on time and within budget. In the context of Web development and maintenance, these issues are also crucial, and very challenging, given that Web projects have short schedules and a highly fluidic scope. Therefore, the objective of this chapter is to introduce the concepts related to Web effort estimation and effort estimation techniques. In addition, this chapter also details and compares, by means of a case study, three effort estimation techniques, chosen for this chapter because they have been to date the ones mostly used for Web effort estimation: Multivariate regression, Case-based reasoning, and Classification and Regression Trees. The case study uses data on industrial Web projects from Spanish Web companies. © 2008, IGI Global.",,"Mendes E., Abrahão S.",2008,Book Chapter,Handbook of Research on Web Information Systems Quality,10.4018/978-1-59904-847-5.ch002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898558178&doi=10.4018%2f978-1-59904-847-5.ch002&partnerID=40&md5=36bcf9c46329337b9ae247bb4029842e,"University of Auckland, New Zealand; Department of Computer Science and Computation, Valencia University of Technology, Spain",IGI Global,English,,9781599048475
Scopus,Component-based project estimation issues for recursive development,"In this paper we investigated the component-based specific issues that might affect project cost estimation. Component-based software development changes the style of software production. With component-based approach the software is developed as the composition of reusable software components. Each component production process must be treated as a stand-alone software project, which needs individual task of management. A typical pure component-based development can be considered as decomposition/integration activities successively applied at different levels and therefore results in recursive style of development. We analyzed and presented our results of studies on the component-based software development estimation issues from recursive point of view. © Springer Science+Business Media B.V. 2008.",,"Altunel Y., Tolun M.R.",2008,Conference,Advances in Computer and Information Sciences and Engineering,10.1007/978-1-4020-8741-7_103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878619157&doi=10.1007%2f978-1-4020-8741-7_103&partnerID=40&md5=844fe304b8fdfa2c582f6405121d625e,"EMU Computer Eng., Istanbul Kültür University, Turkey; Çankaya Üniversity, Turkey",,English,,9781402087400
Scopus,Parametric software cost estimation for the solitary programmer,"Software cost estimation is known to be a challenging task at best, and these challenges are increased for programmers operating outside the support of an established team programming environment. While many software cost estimation models exist with varying levels of support, the nature of metrics collected for these models is seldom applicable to the solitary programmer. In addition, the workload associated with their collection and analysis is quite costly, leading to very limited adoption rates outside large organizations. In this paper, we propose a parametric software cost estimation model suitable for use by the solitary programmer. Our results suggest that such a model is practical to use and can be effective at estimating software development costs, when properly configured.",,"Nobles R., Simmonds D., Tagliarini G.",2008,Conference,"International Conference on Software Engineering Theory and Practice 2008, SETP 2008",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878261490&partnerID=40&md5=580b370c3751fb96112501e5b9dd6a24,"University of North Carolina, Wilmington, United States",,English,,9781615677191
Scopus,An investigation on performance of software enhancement projects in china,"As two major performance measures, software productivity and quality convey critical information in supporting many decision making situations during project planning, management, as well as process/organization benchmarking processes. However, there is a lack of investigation on these performance measures with respect to enhancement projects, this leads to, in many enhancement/ maintenance cases, the inappropriate application of techniques or benchmarking data resulted from studies using development project data. In this paper, through analysis of 264 enhancement projects in China, we seek to develop in-depth and comprehensive understanding about software enhancement projects, by examining the variance of productivity and defect density by several significant influencing factors, such as business area, region, language, programming tool, project size, and team size.",,"He M., Yang Y., Wang Q., Li M.",2008,Conference,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650516848&partnerID=40&md5=fbbf8684d7df5048c221fe7a584c72e4,"Institute of Software, Chinese Academy of Sciences, China; Graduate University of Chinese Academy of Sciences, China",,English,15301362,9780769534466
Scopus,Analysis of software quality cost modeling's industrial applicability with focus on defect estimation,The majority of software quality cost models is by design capable of describing costs retrospectively but relies on defect estimation in order to provide a cost forecast. We identify two major approaches to defect estimation and evaluate them in a large scale industrial software development project with special focus on applicability in quality cost models. Our studies show that neither static models based on code metrics nor dynamic software reliability growth models are suitable for an industrial application. © 2008 IEEE.,Defect prediction; Quality and reliability; Quality cost accounting; Software development,"Karg L.M., Beckhaus A.",2008,Conference,"2008 IEEE International Conference on Industrial Engineering and Engineering Management, IEEM 2008",10.1109/IEEM.2008.4737876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949102325&doi=10.1109%2fIEEM.2008.4737876&partnerID=40&md5=44a05b2c3bd22e298f5876620c90b44e,"SAP Research, Darmstadt, Germany",,English,,9781424426300
Scopus,"Large software projects: Application estimate with ""Use Case Points"" [Große Softwareprojekte]","The estimating of large custom software development projects on the basis of an early rough specification is still a challenge for software companies and no satisfactory solution is available yet. Since competition favors the bid with the lowest price, higher estimation accuracy means a competitive advantage (""survival of the fittest estimator""). This paper points out that especially for large projects the commonly used intuitive estimation by experts has to be complemented for validation reasons by an estimation method based onmetrics.We present the method UCP 2.0 which has been field-proven. The method is a significant enhancement of the Use Case Point method. The major influencing factors of the effort estimation have been standardized and thus are now reproducible. © 2008 Springer-Verlag.",,Frohnhoff S.,2008,Journal,Informatik-Spektrum,10.1007/s00287-008-0288-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049144243&doi=10.1007%2fs00287-008-0288-3&partnerID=40&md5=5e369d55dd72efe3cffc2dbc168964a3,"Capgemini Sd and M AG, Carl-Wery-Str. 42, 81739 München, Germany",,German,01706012,
Scopus,Preliminary results in a multi-site empirical study on cross-organizational ERP size and effort estimation,"This paper reports on initial findings in an empirical study carried out with representatives of two ERP vendors, six ERP adopting organizations, four ERP implementation consulting companies, and two ERP research and advisory services firms. Our study's goal was to gain understanding of the state-of-the practice in size and effort estimation of cross-organizational ERP projects. Based on key size and effort estimation challenges identified in a previously published literature survey, we explored some difficulties, fallacies and pitfalls these organizations face. We focused on collecting empirical evidence from the participating ERP market players to assess specific facts about the state-of-the-art ERP size and effort estimation practices. Our study adopted a qualitative research method based on an asynchronous online focus group. © 2008 Springer-Verlag Berlin Heidelberg.",,Daneva M.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85553-8_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249137380&doi=10.1007%2f978-3-540-85553-8_5&partnerID=40&md5=b1abc48dd491c264577c2c2550331dc9,"Dept. of Computer Science, University of Twente, Enschede, Netherlands",,English,03029743,3540855521; 9783540855521
Scopus,Estimating the development cost for intelligent systems,"In this chapter we suggest several estimation techniques for the prediction of the functionality and productivity required to develop an Intelligent System application. The techniques considered are Analogy Based Estimation, Classification trees, Rule Induction and Bayesian Belief Networks. Estimation results of each technique are discussed and several conclusions are drawn regarding the methods, the platform and the languages used for the development of Intelligent Systems. The data set used in the analysis is the publicly available ISBSG data set. © 2008 Springer-Verlag Berlin Heidelberg.",,"Bibi S., Stamelos I.",2008,Journal,Studies in Computational Intelligence,10.1007/978-3-540-77471-6_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46949111675&doi=10.1007%2f978-3-540-77471-6_3&partnerID=40&md5=f0856bcb93469bf0380ff92cdfec0e18,"Department of Informatics, Aristotle University of Thessaloniki, Aristotle University Campus, Thessaloniki 54 124, Greece",,English,1860949X,9783540774709
Scopus,Integrating portfolio management and simulation concepts in the ERP project estimation practice,"This paper presents a two-site case study on requirements-based effort estimation practices in enterprise resource planning projects. Specifically, the case study investigated the question of how to handle qualitative data and highly volatile values of project context characteristics. We counterpart this challenge and expound upon the integration of portfolio management concepts and simulation concepts into a classic effort estimation model (COCOMO II). © 2008 Springer-Verlag Berlin Heidelberg.",,Daneva M.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69062-7_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849146185&doi=10.1007%2f978-3-540-69062-7_15&partnerID=40&md5=93005f6406192052f44a3b3f16e3bccc,University of Twente,,English,03029743,3540690603; 9783540690603
Scopus,Project management and scheduling for enterprise integration,"We identify important phases in managing enterprise integration projects so as to obtain consensus for integration requirements, select suitable integration technologies and estimate total cost of ownership with an integration project schedule. We propose an Enterprise Integration Project Management (EIPM) process where we identify the requirements of integration by modelling and analysing the organisations’ business process. Here, consensus among the stakeholders is obtained by allowing collaborative planning through the Delphi technique. The total cost of ownership for integration is estimated through software complexity metrics and the resource requirements based on the work breakdown structure. Further, for successful management projects, we demonstrate the application of the Distributed Arrival Time Control (DATC) algorithm for integration project management. The application of DATC allows us to generate adaptive schedules that meet due dates for management under highly uncertain tasks of enterprise integration. Finally, we present a case study to demonstrate the overall capabilities of EIPM. © 2008 Inderscience Enterprises Ltd.",enterprise integration; enterprise modelling; project management; scheduling; software metrics; the Delphi technique,"Salaka V., Prabhu V.",2008,Journal,International Journal of Project Organisation and Management,10.1504/IJPOM.2008.022190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952958113&doi=10.1504%2fIJPOM.2008.022190&partnerID=40&md5=12e8f53564b741d059bb9fa96136a72e,"Department of Industrial and Manufacturing Engineering, Pennsylvania State University, University Park, United States",,English,17402891,
Scopus,Analogy based cost estimation configuration with rules,"Analogy-based estimation is a widely adopted method in software cost estimation that identifies analogous projects to the one under estimation and uses their data to derive an estimate, i.e. it is a Case Based Reasoning approach. The similarity measures between pairs of projects are critical for identifying the most appropriate historical data from which the estimation will be generated. Usually the similarity measures are selected empirically, using jackknife-like procedures. Typically, the measures that identify the most similar projects in most of the cases are considered the most appropriate ones and are applied in every new estimation procedure. However there are situations that the default similarity measures may not be the most appropriate ones. In this study we determine the situations in which the default parameters are not the best and we propose the similarity measures for these cases. In particular we provide rules that point out which projects are not accurately estimated with the default parameters. © 2008 The authors and IOS Press. All rights reserved.",Analogy; Case based reasoning; Rules; Software Cost estimation,"Bibi S., Stamelos I.",2008,Conference,Frontiers in Artificial Intelligence and Applications,10.3233/978-1-58603-900-4-317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875939499&doi=10.3233%2f978-1-58603-900-4-317&partnerID=40&md5=18a9badab76965009f5da1e27ce4efcc,"Department of Informatics, Aristotle University of Thessaloniki, Greece",IOS Press,English,09226389,9781586039004
Scopus,Cost optimising aircraft systems at a conceptual design stage,This paper describes a project undertaken with industry to develop a software tool that automates the configuration of conceptual aircraft systems so that their costs can be estimated. The development of accurate cost estimates at the conceptual design stage helps identify ways to reduce costs throughout a projects life cycle. A case study on the Hydraulics system is used to demonstrate a novel methodology for capturing and representing the design rules. The developed prototype requires Top Level Definitions (TLD's) of the proposed new aircraft design and then uses the captured design rules together with rationalised judgements to calculate the architecture and a pseudo Bill of Materials (BOM) for a conceptual system.. Subsequently the generated system is costed using a commercial cost estimating software. The piping cost and its installation is calculated within the tool and added to the output from the commercial software. This tool allows different configurations to be compared so that a conceptual system can be optimised with regard to cost. Copyright © 2008 Inderscience Enterprises Ltd.,Aerospace; Aircraft systems; Conceptual design; Cost engineering; Cost estimating; Cost optimisation; Systems cost,"Houseman O., Roy R., Wainwright C., Lavdas E.",2008,Journal,International Journal of Manufacturing Technology and Management,10.1504/IJMTM.2008.020174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449098030&doi=10.1504%2fIJMTM.2008.020174&partnerID=40&md5=aedb908f0998eee302a25ba4096822bf,"Decision Engineerinog Centre, Cranfield University, Bedfordshire, United Kingdom; Association for the Advancement of Cost Engineering, Royal Institute of Naval Architects; Silsoe Campus, Cranfield University; Lockheed Martin (UK)",Inderscience Publishers,English,13682148,
Scopus,Software estimation using function point analysis: Difficulties and research challenges,"Function Point Analysis method serves better efficient way of predicting estimation in beginning phase of software development life cycle(SDLC). Size and complexity of the software can be derived by function point analysis method. Difficulties of estimation using LOC(Lines of Code) can be avoided using Function Point Analysis, since it deals directly with functions or requirements and independent of language or technology. This paper explains how to calculate Function point analysis for the case study Defect Tracking System(DTS) by using function point analysis. Defect tracking system(DTS) case study has been taken from ""XYZ"" company. In the intention of maintaining confidentiality, authors are not disclosing the company name. Authors also discusses difficulties and challenges by using Function Point Analysis as part of their Research Work. © 2007 Springer.",,"Basavaraj M.J., Shet K.C.",2007,Conference,Innovations and Advanced Techniques in Computer and Information Sciences and Engineering,10.1007/978-1-4020-6268-1-21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879625229&doi=10.1007%2f978-1-4020-6268-1-21&partnerID=40&md5=9883824d555f5aea68bee3997ecbec04,"Perot Systems, Bangalore, Karnataka-India, India; Computer Department, National Institute of Technology Karnataka, Surathkal, India",,English,,9781402062674
Scopus,Temporal software change prediction using neural networks,"Software change prediction plays a key role in software maintenance and evolution. It is primarily utilized to know ""where"" the most change-prone entities are, and how the change will be propagated through a system. The results of the prediction are used to plan different tasks in maintenance and evolution, such as re-factoring operation. This paper argues that knowing ""when"" the changes may happen can give more insight to managers and developers for planning the maintenance activities. To address this issue, a Neural Network-based Temporal Change Prediction (NNTCP) framework is proposed. Such a novel framework determines ""where"" the changes would be applied (as hot spots), and then adds time dimension to predict ""when"" it may occur. As a proof of concept, the NNTCP framework is applied to Mozilla as a large-scale open source software. We provide a short discussion on obtained prediction results. Copyright © (2007) by Knowledge Systems Institute (KSI).",,"Amoui M., Salehie M., Tahvildari L.",2007,Conference,"19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249093920&partnerID=40&md5=0e533674a61537f97abdf6ddd5b74fa1,"Software Technologies Applied Research Group, Department of Electrical and Computer Engineering, University of Waterloo, ON, Canada",,English,,9781627486613
Scopus,Impacts of architecture and quality investment in software product line development,"Investment in architecture and quality improvement for a software product line can increase reuse, and consequently reduce effort, enhance product reliability, and shorten time-to-market. Such investments should be carefully chosen to be effective, to avoid over-investment, and to return benefits within the desired time. In this paper, we show how a stochastic simulation model can be used to explore the impacts of such investments. The model is validated by comparison to COPLIMO, a COCOMO II based effort estimation model for product line development, and by inspecting effort distributions of the generated unplanned work. For the illustrative model and scenarios in this paper, we show that the degree of architecture reuse has the largest impact. Preventing degraded architectural dependencies itself does not have a meaningful impact, but if such degradation is also associated with adverse effects on defect injection and detection, it can be significant. Process improvement has a meaningful impact, but over-investment is possible. © 2007 IEEE.",,"Nonaka M., Babar M.A., Zhu L., Staples M.",2007,Conference,"Proceedings - 11th International Software Product Line Conference, SPLC 2007",10.1109/SPLINE.2007.4339256,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949121286&doi=10.1109%2fSPLINE.2007.4339256&partnerID=40&md5=0c0407f4360a76d1c71e7bad446cf8a0,"Toyo University, Japan; Lero, University of Limerick, Ireland; National ICT Australia",,English,,0769528880; 9780769528885
Scopus,Software metric estimation: An empirical study using an integrated data analysis approach,"Automatic software effort estimation is important for quality management in the software development industry, but it still remains a challenging issue. In this paper we present an empirical study on the software effort estimation problem using a benchmark dataset. A number of machine learning techniques are employed to construct an integrated data analysis approach that extracts useful information from visualisation, feature selection, model selection and validation. © 2007 IEEE.",Machine learning; Software effort estimation,"Deng D., Purvis M., Purvis M.",2007,Conference,Proceedings - ICSSSM'07: 2007 International Conference on Service Systems and Service Management,10.1109/ICSSSM.2007.4280207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549096053&doi=10.1109%2fICSSSM.2007.4280207&partnerID=40&md5=83455c67e77732330f53227964a2eb4c,"Department of Information Science, University of Otago, PO Box 56, Dunedin, New Zealand",,English,,1424408857; 9781424408856
Scopus,Software project management and planning: the case of the Greek IT sector,"Some of the most important factors that affect the competitiveness of the companies within the IT/IS sector are the use of modern management techniques and state-of-the-art production tools. This research attempts to examine the production process adopted and the management practices used by Greek IT/IS sector. A structured questionnaire was addressed to project managers of almost all Greek IT/IS companies. The results show that, although very experienced and highly educated people are involved in the production process, most companies are not using any formal development methodology and they do not plan or manage their development process in the right way. © 2007 Inderscience Enterprises Ltd.",information system development; IS development methodologies; IS project management,"Chatzoglou P.D., Theriou N.G., Dimitriadis E., Aggelides V.",2007,Journal,International Journal of Applied Systemic Studies,10.1504/IJASS.2007.017713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946442556&doi=10.1504%2fIJASS.2007.017713&partnerID=40&md5=0d61cdf042bad620f55ff555fe460009,"Production and Management Engineering Department, Democritus University of Thrace, Library Building, Kimmeria, 67100 Xanthi, Greece; TEI of Kavala, Department of Business Administration, Agios Loukas, 65404 Kavala, Greece; Production and Management Engineering Department, Democritus University of Thrace, Library Building, Kimmeria, 67100 Xanthi, Greece",,English,17510589,
Scopus,A replicated study comparing web effort estimation techniques,"The objective of this paper is to replicate two previous studies that compared at least three techniques for Web effort estimation in order to identify the one that provides best prediction accuracy. We employed the three effort estimation techniques that were mutual to the two studies being replicated, namely Forward Stepwise Regression (SWR), Case-Based Reasoning (CBR) and Classification & Regression Trees (CART). We used a cross-company data set of 150 Web projects from the Tukutuku data set. This is the first time such large number of Web projects is used to compare effort estimation techniques. Results showed that all techniques presented similar predictions, and these predictions were significantly better than those using the mean effort. Thus, all the techniques can be exploited for effort estimation in the Web domain, also using a cross-company data set that is specially useful when companies do not have their own data on past projects from which to obtain their estimates, or that have data on projects developed in different application domains and/or technologies. © Springer-Verlag Berlin Heidelberg 2007.",Case-based reasoning; Cost estimation; Effort estimation; Regression tree; Stepwise regression; Web applications; Web projects,"Mendes E., Di Martino S., Ferrucci F., Gravino C.",2007,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-76993-4_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349176852&doi=10.1007%2f978-3-540-76993-4_35&partnerID=40&md5=84c941a289685a376348ae473bb0911f,"University of Auckland, Private Bag 92019, Auckland, New Zealand; University of Salerno, Via Ponte Don Melillo, I-84084 Fisciano (SA), Italy",Springer Verlag,English,03029743,9783540769927
Scopus,Applying rule induction in software prediction,"Recently, the use of machine learning (ML) algorithms has proven to be of great practical value in solving a variety of software engineering problems including software prediction, for example, cost and defect processes. An important advantage of machine learning over statistical analysis as a modelling technique lies in the fact that the interpretation of production rules is more straightforward and intelligible to human beings than, say, principal components and patterns with numbers that represent their meaning. The main focus of this chapter is upon rule induction (RI): providing some background and key issues on RI and further examining how RI has been utilised to handle uncertainties in data. Application of RI in prediction and other software engineering tasks is considered. The chapter concludes by identifying future research work when applying rule induction in software prediction. Such future research work might also help solve new problems related to rule induction and prediction. © 2007, Idea Group Inc.",,"Twala B., Cartwright M., Shepperd M.",2006,Book Chapter,Advances in Machine Learning Applications in Software Engineering,10.4018/978-1-59140-941-1.ch011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900226237&doi=10.4018%2f978-1-59140-941-1.ch011&partnerID=40&md5=345674201f68e85a281ab32d959b2d88,"Brunel Software Engineering Research Centre (B-SERC), School of Information Systems, Computing and Mathematics, Brunel University, United Kingdom; Brunel University, London, United Kingdom",IGI Global,English,,9781591409410
Scopus,On the value of code inspections for software project management: An empirical analysis,Code inspections continue to gain significance as a software verification scheme since Fagan introduced the concept. Software engineering researchers examining the value of code inspections have exclusively focused on defect removal benefits of inspections. In this paper we develop and test empirical models of both quality improvement and project management benefits realized because of effort spent on code inspections. We analyze data collected on 40 real world projects from a leading software corporation to provide rigorous empirical evidence for the value of code inspections. We find evidence for hitherto unexplored hypothesis that improved understanding gained during code inspection has project management benefit of better test planning and control that could eventually avoid project overruns. We provide a research frame work that takes into account the sequential characteristics of waterfall software development model and the effects of rework generated by verification schemes to answer an important research question on the value of code inspections for project management.,Code inspections; Estimation; Life cycle; Process metrics; Project management; Software engineering; Software quality; Software testing,"Ramasubbu N., Subramanyam R., Mithas S., Krishnan M.S.",2006,Conference,"Association for Information Systems - 12th Americas Conference On Information Systems, AMCIS 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870310813&partnerID=40&md5=2191def088131e357bc176060a048d04,"Singapore Management University, Singapore; University of Illinois, Urbana-Champaign, United States; University of Maryland, College Park, United States; University of Michigan, United States",,English,,9781604236262
Scopus,Prototype selection of customized development process based on open source software,"Selecting an appropriate prototype for customized development process will increase development efficiency, decrease development costs and shorten development time. Based on the COCOMO (costructive cost model) II model, a new software cost assessment method was proposed to analyze the influence of the key factors, such as the function differences, system performances and scale, stability, in the development process. The influences of the key factors on the equivalent code, which is essential in the cost estimation formula, were quantified. A practical project shows that the assessment error is about 15%, well within the 40% acceptable threshold in COCOMO II model.",Cost assessment; Customized development; Equivalent code; Prototype selection,"Wang Z., Yin B.",2006,Journal,Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846933797&partnerID=40&md5=0f4102b73455cf6bea4717911f4eb7fe,"School of Computer Science and Technology, Beijing University of Aeronautics and Astronautics, Beijing 100083, China",,Chinese,10015965,
Scopus,Milestone markets: Software cost estimation through market trading,"Software cost estimation remains a difficult challenge despite decades of attention by both researchers and practitioners. Predictions are often inaccurate and characterized by very wide confidence intervals. Direct approaches base ""expert"" estimates on detailed requirements, along with the experience and intuition of the estimator. The Delphi method seeks a consensus estimate among a group of expert estimators. Still other approaches use historical project data to fit estimation models, such as COCOMO. While hybrid techniques like COBRA combine aspects of several methods. This paper proposes a very different approach, the use of information markets to continually aggregate the individual estimates of diverse software project stakeholders. Information markets have been applied successfully in several areas with the market consensus often outperforming individual experts. The paper describes a market mechanism for software cost estimation, explores the characteristics that make such an approach possible, and presents initial experiments based on a simple estimation task. © 2006 IEEE.",,"Berndt D.J., Jones J.L., Finch D.",2006,Conference,Proceedings of the Annual Hawaii International Conference on System Sciences,10.1109/HICSS.2006.271,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749622764&doi=10.1109%2fHICSS.2006.271&partnerID=40&md5=872acc36601a579a23b70a6ef9f7b32a,"Information Systems and Decision Sciences, College of Business Administration, University of South Florida, United States",,English,15301605,0769525075; 9780769525075
Scopus,Model-based methods for software cost estimation,"Accurate estimation is the foundation of effective project planning, tracking and controlling. Model-based methods, in contrast to expert-experience-based methods, are more independent of individual's capabilities and more reusable in software development organizations, so they are the focus of research in software cost estimation. They may be classified into algorithm-driven models, data-driven models and composite models. In term of the classification schema, the typical methods are described. Then, their assumptions, application environment, advantages and limitations are analyzed deeply which are viewed from both internal attributes and external evaluation. Finally, the future of software cost estimation research is discussed. The purpose of the paper is to compare the advantages and disadvantages of the representative estimation models and provide support for using the suitable models for software development organization.",Algorithm-driven model; Composite model; Data-driven model; Software cost estimation,"He X., Wang Y.",2006,Journal,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,10.1360/crad20060502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745962270&doi=10.1360%2fcrad20060502&partnerID=40&md5=ee1fc41f7382388db800fb72eeba9195,"Institute of Software Engineering, School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China",,Chinese,10001239,
Scopus,Using linear regression models to analyse the effect of software process improvement,In this paper we publish the results of a thorough empirical evaluation of a CMM-based software process improvement program that took place at the IT department of a large Dutch financial institution. Data of 410 projects collected over a period of four years are analysed and a productivity improvement of about 20% is found. In addition to these results we explain how the use of linear regression models and hierarchical linear models greatly enhances the sensitivity of analysis of empirical data on software improvement programs. © Springer-Verlag Berlin Heidelberg 2006.,,"Schalken J., Brinkkemper S., Van Vliet H.",2006,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/11767718_21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746256461&doi=10.1007%2f11767718_21&partnerID=40&md5=b56154edb844eacb1e548c33f78c4076,"Vrije Universiteit, Department of Computer Science, Amsterdam, Netherlands; Utrecht University, Institute of Information and Computing Sciences, Netherlands",Springer Verlag,English,03029743,3540346821; 9783540346821
Scopus,Knowledge representation in pattern management,[No abstract available],,"Kamthan P., Pai H.-I.",2005,Book Chapter,Encyclopedia of Knowledge Management,10.4018/978-1-59140-573-3.ch062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898392387&doi=10.4018%2f978-1-59140-573-3.ch062&partnerID=40&md5=9fca568b45c7e5276aa075f3cc347df8,"Concordia University, Canada",IGI Global,English,,9781591405733
Scopus,A survey of data imputation methods and tools for software project data sets,"Every researcher who deals with data sets encounters problems caused by missing data. Analysts from numerous disciplines have used different data imputation methods to fill in probable values for these missing data. Software project data sets too suffer from missing data. The use of data imputation techniques so as to enhance the completeness of software project data sets betters the prediction accuracies on effort/cost/time estimates. In this paper, we describe the different kinds of imputation methods outlining their pros and cons. We also present the categorization of these methods, along with the information of different tools available to implement these methods. We also explain the usefulness of each of these techniques taking into account the characteristics of the data set being analyzed.",Data imputation methods; Incomplete data sets; Software project estimates; Tools,"Yenduri S., Iyengar S.S., Vinnakota S.",2005,Conference,"WMSCI 2005 - The 9th World Multi-Conference on Systemics, Cybernetics and Informatics, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867388566&partnerID=40&md5=efc3a17ff23e8adccb522557228625ef,"Louisiana State University, Baton Rouge, LA, 70803, United States",,English,,9806560531; 9789806560536
Scopus,Improving prediction accuracies using data imputation,"Significant amounts of missing or incomplete data are frequently found in data sets utilized by the effort/cost/time prediction models used in the current software industry. The traditional approaches used by the companies ignore all the missing data and provide estimates based on the remaining complete information. Thus, the very estimates are prone to bias. In this survey, we investigate the application of a few well-known data imputation techniques (Listwise Deletion (LD), Mean Imputation (MI), and 2 variants of Hot-Deck (HD) Imputation) to a software project data set. We perform a case study and compare the impacts of these methods for enhancing prediction accuracies. Finally, we discuss the findings and elaborate the appropriateness of data imputation to software project data sets.",,"Yenduri S., Iyengar S.S., Perkins L.A.",2005,Conference,"Proceedings of the 2005 International Conference on Software Engineering Research and Practice, SERP'05",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749095005&partnerID=40&md5=3cc50d58146622935164d2d353a37531,"Dept. of Computer Science, Louisiana State University, Baton Rouge, LA 70803, United States; Dept. of Computer Science, Univ. of Southern Mississippi, Long Beach, MS 39560, United States",,English,,9781932415506
Scopus,Cost-estimating aircraft systems at a conceptual design stage,"The development of accurate cost estimates at the conceptual design stage helps identify ways to reduce costs throughout a project's life cycle. This task, carried out before any substantial investment has occurred, identifies possible cost savings at a point when action can be taken to ensure that these benefits can be realised. The aim of the research project presented within this paper is to provide increased confidence in cost estimates at a conceptual design stage by reducing subjectivity found in the use of expert judgements. Working with a leading aircraft manufacturer this study focuses on costing the systems of aircraft. The paper presents a case study on the hydraulics system, demonstrating a unique format that has been adopted for capturing rules that automate the costing of the system at a conceptual design stage. The developed prototype requires top level definitions of the proposed new aircraft design and then uses captured design rules together with rationalised judgements to calculate the architecture and a pseudo BOM for a conceptual hydraulics system. Subsequently the cost of the generated system is estimated using a commercial cost estimating software to allow comparisons to be made for different system configurations. © 2005 ISPE, Inc.",,"Houseman O.W., Roy R., Wainwright C., Lavdas E.",2005,Conference,"Next Generation Concurrent Engineering: Smart and Concurrent Integration of Product Data, Services, and Control Strategies, CE 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449091889&partnerID=40&md5=508661f262192a3ab1360e46fd718974,"Enterprise Integration, Cranfield University, Cranfield, Bedfordshire, United Kingdom",,English,,
Scopus,A model for performance management and estimation,"Traditional cost estimation models in software engineering are based on the concept of productivity defined as the ratio of output to input; for instance, detailed software estimation models, such as COCOMO, can take multiple factors into account, but their multipliers lead to a single perspective based on the productivity concept. A less explored relationship in software engineering is the one between productivity and performance. This paper presents some classic concepts on the multidimensionality of performance, and proposes some suggestions to implement multidimensional performance models in software engineering based on certain fundamental concepts from geometry, that is, the QEST/LIME family of models. © 2005 IEEE.",CMMI; COCOMO; Estimation; Performance Management; QEST/LIME,"Buglione L., Abran A.",2005,Conference,Proceedings - International Software Metrics Symposium,10.1109/METRICS.2005.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749057897&doi=10.1109%2fMETRICS.2005.3&partnerID=40&md5=84c455e73a296445046047fb902fdbdc,"École de Technologie Supérieure (ETS); École de Technologie Supérieure (ETS), Université du Québec, Canada; Atos Origial; Italian Function Point Users Group, Italian Software Metrics Association (GUFPI-ISMA), Italy; Software Measurement Committee; IIEEE Computer Society; Software Engineering Research Laboratory, École de Technologie Supérieure (ETS), Université du Québec, Canada; Common Software Metrics International Consortium (COSMIC), Canada",,English,15301435,0769523714; 9780769523712
Scopus,Assessing empirical software data with MLP neural networks,"Software measurements provide developers and software managers with information on various aspects of software systems, such as effectiveness, functionality, maintainability, or the effort and cost needed to develop a software system. Based on collected data, models capturing some aspects of software development process can be constructed. A good model should allow software professionals to not only evaluate current or completed projects but also predict future projects with an acceptable degree of accuracy. Artificial neural networks employ a parallel distributed processing paradigm for learning of system and data behavior. Some network models, such as multilayer perceptrons, can be used to build models with universal approximation capabilities. This paper describes an application in which neural networks are used to capture the behavior of several sets of software development related data. The goal of the experiment is to gain an insight into the modeling of software data, and to evaluate the quality of available data sets and some existing conventional models. ©ICS AS CR 2005.",Accuracy; Effort estimation; Evaluation; Model size; Neural networks; Software development,"Musilek P., Meltzer J.",2005,Journal,Neural Network World,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27544465384&partnerID=40&md5=5a52aadb98708e0bc5ff6d3f5f19b15c,"Department of Electrical and Computer Engineering, University of Alberta, Edmonton T6G 2V4, Canada",,English,12100552,
Scopus,Hypermedia maintenance support applications: Benefits and development costs,"The most common questions asked by industrialists regarding any electronic based systems to support maintenance activities are, what are the benefits and how much will it cost. In this paper we will consider these questions by examining two hypermedia based maintenance support systems. In order to access the tangible and intangible benefits of hypermedia, the identification of benefits and cost are required at an early stage of the development process. This paper discusses some of the benefits to be gained by industry from using hypermedia applications to support their information provision. The cost is directly related to the effort required to produce a hypermedia application, with the greatest effort in authoring. Two methods of costing are presented: a detailed engineering approach and an approach using heuristics. In addition we will comment on how the presented approaches can be applied to Web and Semantic Web applications. © 2005 Elsevier B.V. All rights reserved.",Effort Analysis; Manufacturing; Open Hypermedia,"Crowder R., Wills G., Hall W.",2005,Journal,Computers in Industry,10.1016/j.compind.2005.03.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24144450284&doi=10.1016%2fj.compind.2005.03.004&partnerID=40&md5=948b50991dde77a57f4a9e1ab0f63895,"Department of Electronics and Computer Science, School of Electronics and Computer Science, University of Southampton, United Kingdom; Robotics and Control, School of Electronics and Computer Science, University of Southampton, United Kingdom",,English,01663615,
Scopus,Effort estimation of use cases for incremental large-scale software development,"This paper describes an industrial study of an effort estimation method based on use cases, the Use Case Points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates. Copyright 2005 ACM.",Estimation; Incremental development; Use cases,"Mohagheghi P., Anda B., Conradi R.",2005,Conference,"Proceedings - 27th International Conference on Software Engineering, ICSE05",10.1145/1062455.1062516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085787054&doi=10.1145%2f1062455.1062516&partnerID=40&md5=1407fd239cea64a60315a402a1996025,"Department of Computer and Information Science, Norwegian University of Science and Technology, NO-7491 Trondheim, Norway; Faculty of Engineering, Agder University College, NO-4876 Grimstad, Norway; Simula Research Laboratory, P.O.Box 134, NO-1325 Lysaker, Norway",Association for Computing Machinery (ACM),English,,
Scopus,SW cost estimation: Measuring model performance of arbitrary function approximators,"Estimating software development cost with high accuracy is still a largely unsolved problem. Consequently, there is ongoing, high activity in this research field; a large number of different estimation models ranging from mathematical functions to arbitrary function approximators (AFA's) have been proposed over the last 20+ years. Unfortunately, the studies do not converge with respect to the question ""which model is best?"" when functions and AFA's are compared. So far, it has not been understood why this is so. In this empirical study, we show that this is due to inappropriate validation methods as far as the validation of AFA's is concerned. In fact, the de facto validation method, cross-validation combined with MMRE, will give completely arbitrary results for AFA's. Obviously, other criteria are called for in order to appropriately assess the performance of AFA's. This should be a topic of future research.",Arbitrary function approximators; Artificial intelligence; Case-based reasoning; Classification and regression trees; Estimation by analogy; Software cost estimation,"Myrtveit I., Stensrud E.",2004,Conference,Proceedings of the Eigtht IASTED International Conference on Software Engineering and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144243139&partnerID=40&md5=1cbc08e7098148ee77c89712e428967c,"Norwegian School of Management, Elias Smiths vei 15, 1301 Sandvika, Norway; Buskerud University College, P.O.B. 251, 3603 Kongsberg, Norway",,English,,0889864276; 9780889864276
Scopus,'fuzzy Project Manager' - Framework for software project management using fuzzy logic,"Project Management is the application of knowledge, skills, tools and techniques to project activities in order to meet project requirements. The success of any project relies heavily on the initial estimation of all project parameters. The absence of reliable estimations leads to ineffective project planning, over- or under-commitment of resources and therefore an increased likelihood of a software project failure. Fuzzy Logic is a soft-computing technique used to effectively solve uncertainties due to imprecise inputs to generate linguistic or quantitative outputs. This paper presents a novel framework for project management incorporating fuzzy logic known as 'fuzzy ProjectManager'. Furthermore, this paper demonstrates the application of fuzzy logic as a feasible technique for improved estimation accuracy of all software project estimations to ensure higher software project success rates. © 2004 World Scientific Publishing Company.",estimations; fuzzy logic; Project Management; project management framework; project variables,"Siwani I., Capretz M.",2004,Journal,International Journal of Innovation and Technology Management,10.1142/S0219877004000301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899887717&doi=10.1142%2fS0219877004000301&partnerID=40&md5=72f44b250b7686ff81bae7ebdd56c15b,"University of Western Ontario, Department of Electrical and Computer Engineering, London, ON, N6A 5B9, Canada",World Scientific Publishing Co. Pte Ltd,English,02198770,
Scopus,Flight software trends and patterns in the aerospace industry: JPL lessons learned,"As in industry, JPL has experienced numerous changes in how we develop both flight and ground software. Software languages, design methods, and development processes have all changed from what they were in the early to mid 80's. It used to be that our spacecraft were flying hardware with a computer on board. Today our spacecraft are more and more becoming complex flying computers especially with the advent of sophisticated fault protection and autonavigation software. Because software is playing a more critical role in our deep space missions, there has been a significant increase in interest in software at JPL. As part of this new focus a number of activities have been initiated to improve how we manage and how we measure our software activities. Currently we are validating and calibrating commercial parametric tools, as well as developing our own models. As a result of integrating our cost databases and engaging in an extensive software metrics activity, it has become possible to analyze JPL's historical datasets for trends in metrics. In this paper, we will summarize the software trends and their impact on the cost of developing flight and ground software. © 2003 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.",,"Hihn J.M., Lum K., Powell J.",2003,Conference,AIAA Space 2003 Conference and Exposition,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897779112&partnerID=40&md5=84302b64348ad335a9677de962e74bf2,"Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109, United States",,English,,9781624101038
Scopus,Impact Analysis in Software Evolution,"Project planning relies on accurate estimates of the work at hand. In software development, the work at hand is represented by the requirements. In software evolution, when new requirements are added to an existing system in order to produce a new software release, it is important to base the project plan on how much the requirements will cause change in the software. Requirements-Driven Impact Analysis (rdia) is a critical tool in the planning process as it identifies the set of software entities that need to be changed to implement a new requirement in an existing system. rdia thus involves a transition from requirements to software entities or to a representative model of the implemented system. rdia is performed during the release-planning phase. Input is a set of requirements and the existing system. Output is, for each requirement, a set of software entities that have to be changed. The output is used as input to many project-planning activities, for example cost estimation based on change volume. The overall goal of this work has been to gather knowledge about rdia and how to improve this crucial activity. The overall means has been an empirical study of rdia in the industrial object-oriented pmr-project. rdia has been carried out as a normal part of project developers' work. This in-depth case-study has been carried out over four years and in close contact with project developers. Problems with underprediction have been identified and many more entities than predicted are changed. We have also found that project developers are unaware of their own positive and negative capabilities in predicting change. We have found patterns that indicate that certain characteristics among software entities, such as size, relations and inheritance, may be used together with complementary strategies for finding candidates for change. Techniques and methods for data collection and data analysis are provided as well as a thorough description of the context under which this research project was conducted. Simple and robust methods and tools such as sccs, Cohen's kappa, median tests and graphical techniques facilitate future replications in other projects than pmr. © 2003 Elsevier Science (USA). All rights reserved.",,Lindvall M.,2003,Review,Advances in Computers,10.1016/S0065-2458(03)59004-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149173274&doi=10.1016%2fS0065-2458%2803%2959004-0&partnerID=40&md5=3cf79763cfb77522104f0c6be32fe64c,"Fraunhofer Center for Experimental Software Engineering Maryland, 4321 Hartwick Rd., Suite 500, College Park, MD 20742-3290, United States",,English,00652458,
Scopus,Survey of artificial intelligence methods on software development effort estimation,"This paper present, using a classification proposed by the authors, a survey of the methods used in the field of software development effort estimation. Principally is focused in the machine learning or artificial intelligence methods that are been applied. Methods as neural networks, case based, regression trees, fuzzy logic models or dynamical agents are classified and described.",,"Crespo J., Cuadrado J.J., García L., Marbán O., Sánchez-Segura M.I.",2003,Conference,Proceedings of the 10th ISPE International Conference on Concurrent Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442309962&partnerID=40&md5=dab4c091d8d608402bcd2d5d403916db,"Universidad Carlos III de Madrid, Leganés, Madrid, Spain",Swets en Zeitlinger B.V.,English,,905809524X
Scopus,Cost Estimating Rationale Capture: Beyond the Aerospace Industry,The design concept and use of cost estimating rationale capture (CERC) tool in the aerospace industry are discussed. The main objective of CERC tool development is to formalize the capture and reuse of cost engineering knowledge. The benefits of using the CERC tool are also presented.,,"Roy R., Rush C.S., Gorroño V.",2003,Conference,AACE International. Transactions of the Annual Meeting,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18544395273&partnerID=40&md5=133a35a63bf80350759ed55264dfc813,"Department of Enterprise Integration, Building 53, Cranfield University, Cranfield, Bedford, MK43 0AL, United Kingdom",,English,00657158,
Scopus,Genetically optimized rule-based fuzzy polynomial neural networks: Synthesis of computational intelligence technologies,"In this study, we introduce a concept of Rule-based fuzzy polynomial neural networks(RFPNN), a hybrid modeling architecture combining rule-based fuzzy neural networks(RFNN) and polynomial neural networks(PNN). We discuss their comprehensive design methodology. The development of the RFPNN dwells on the technologies of Computational Intelligence(CI), namely fuzzy sets, neural networks, and genetic algorithms. The architecture of the RFPNN results from a synergistic usage of RFNN and PNN. RFNN contribute to the formation of the premise part of the rule-based structure of the RFPNN. The consequence part of the RFPNN is designed using PNN. We discuss two kinds of RFPNN architectures and propose a comprehensive learning algorithm. In particular, it is shown that this network exhibits a dynamic structure. The experimental results include well-known software data such as the Medical Imaging System(MIS) dataset. © Springer-Verlag Berlin Heidelberg 2003.",Computational intelligence(CI); Design methodology; Genetic algorithms(GAs); Polynomial neural networks(PNN); Rule-base fuzzy polynomial neural networks(RFPNN); Rule-based fuzzy neural networks(RFNN),"Oh S.-K., Peters J.F., Pedrycz W., Ahn T.-C.",2003,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-39205-x_73,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344221963&doi=10.1007%2f3-540-39205-x_73&partnerID=40&md5=1b4a2551f756a4e626638cf58b715110,"Department of Electrical Electronic and Information Engineering, Wonkwang University, 344-2, Shinyong-Dong, Iksan, Chon-Buk, 570-749, South Korea; Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB  R3T 5V6, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB  T6G 2G6, Canada; Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland",Springer Verlag,English,03029743,3540140409; 9783540140405
Scopus,Software development and quality assurance for the healthcare manufacturing industries,"Completely revised and updated, this book is a practical guide for anyone involved in all levels of the development and quality assurance of software programs for healthcare products - particularly in the medical device and equipment manufacturing industries. From high-level strategies and mechanics to detailed flow charts, tables, and sample forms, the text helps assure that all requisite phases of development and testing have been considered and implemented as readers develop a complete, integrated, cohesive, and interrelated software quality assurance program. Readers will learn how to design, implement, and manage the software development effort, as well as how to audit the program. © 2002 by Taylor & Francis Group, LLC CRC Press is an imprint of Taylor & Francis Group, an Informa business.",,Mallory S.R.,2002,Book,"Software Development and Quality Assurance for the Healthcare Manufacturing Industries, Third edition",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056971954&partnerID=40&md5=07eeba48b7624a602280fbf5b1422b8c,,CRC Press,English,,9781420025897; 9781574911367
Scopus,An experiment to improve cost estimation and project tracking for software and systems integration projects,"It is becoming increasingly difficult to predict the resources, costs, and time scales for the integration of software and systems built from components supplied by third parties. Many cost models use the concept of product size as the prime driver for cost estimation but our experience has shown that the supplied quality of components and the required quality of the integrated systems are becoming the dominant factors affecting the costs and time scales of many projects today. ICL has undertaken an experiment using an alternative life cycle model, known as the Cellular Manufacturing Process Model (CMPM), which describes how a product is integrated from its constituent components. The experiment has helped to develop a method and sets of metrics to improve cost estimation and project tracking. © 1999 IEEE.",,"Chatters B., Henderson P., Rostron C.",1999,Conference,Conference Proceedings of the EUROMICRO,10.1109/EURMIC.1999.794779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889015615&doi=10.1109%2fEURMIC.1999.794779&partnerID=40&md5=161a058aa6e47947c5ede2c88609e617,"ICL, Manchester, United Kingdom; University of Southampton, United Kingdom",,English,10896503,0769503217; 9780769503219
Scopus,Fuzzy dynamics in software project simulation and support,"Established simulation techniques require quantification of relevant aspects of the entity whose behaviour is being investigated. The data expressing the quantification provides a disciplined link between the simulation model and the domain being modelled. In this connection, domains such as software projects or processes that. are, for example, fuzzy, uncertain or lack precise data quantification, may present difficulties. Alternative paradigms must be established to capture, in some sense, the behaviour to be simulated. One such paradigm, complementary to existing techniques, is provided by fuzzy dynamics. Though appearing promising, this suggestion awaits validation in real-world situations. © 1998, Springer-Verlag. All rights reserved.",,"Ramil J.F., Lehman M.M.",1998,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-64956-5_12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957640302&doi=10.1007%2f3-540-64956-5_12&partnerID=40&md5=791d70a94aa229361fc03cc89af5b2dc,"Department of Computing, Imperial College of Science, Technology and Medicine, London, SW7 2BZ, United Kingdom",Springer Verlag,English,03029743,3540649565; 9783540649564
Scopus,Quantitative analytic approaches in software engineering,"This paper describes the current state of quantitative analysis in software engineering along with the overview on software engineering papers published in Japan in the 1990s and the history of implementation and application of the GINGER system, which is a measurement-based programming support system with real-time feedback. The topics of papers concerning quantitative analysis in software engineering can be classified into four major groups: 'Quality', 'Sizing', 'Human', and 'Code-level diagnosis'. Most of them take a model-based approach and include experimental projects. At this stage, 'Specification-level' and 'Design-level' analysis is not so common, but we believe GINGER can make it easy to quantitatively analyse products and processes at these levels with the advent of modern CASE tools.",Measurement environment; Quantitative analysis; Software metrics,"Torii K., Matsumoto K.-I.",1996,Journal,Information and Software Technology,10.1016/0950-5849(95)01082-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029710843&doi=10.1016%2f0950-5849%2895%2901082-3&partnerID=40&md5=0b2d36aa822adfce91e7f649cd1c1c07,"Grad. School of Information Science, Nara Inst. of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-01, Japan",Elsevier,English,09505849,
Scopus,An automatically created novel bug dataset and its validation in bug prediction,"Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning. We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected release versions of the code. Our approach, on the other hand, captures the buggy and the fixed states of the same source code elements from the narrowest timeframe we can identify for a bug's presence, regardless of release versions. To show the usefulness of the new dataset, we built and evaluated bug prediction models and achieved F-measure values over 0.74. © 2020 The Authors",Bug dataset; Bug prediction; Code metrics; GitHub; Machine learning; Static code analysis,"Ferenc R., Gyimesi P., Gyimesi G., Tóth Z., Gyimóthy T.",2020,Journal,Journal of Systems and Software,10.1016/j.jss.2020.110691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086642369&doi=10.1016%2fj.jss.2020.110691&partnerID=40&md5=6e52c4da63beda2bf2629ecc658adb6c,"Department of Software Engineering, University of Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary",Elsevier Inc.,English,01641212,
Scopus,Guidelines for the search strategy to update systematic literature reviews in software engineering,"Context: Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE. Objective: The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines. Method: To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication. Results: The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results. Conclusions: Our proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE. © 2020",Searching for evidence; Snowballing; Software engineering; Systematic literature review update; Systematic literature reviews,"Wohlin C., Mendes E., Felizardo K.R., Kalinowski M.",2020,Journal,Information and Software Technology,10.1016/j.infsof.2020.106366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086562618&doi=10.1016%2fj.infsof.2020.106366&partnerID=40&md5=a4d1aab8c5c2dee49934d9c1a475b4bc,"Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computing, Federal Technological University of Paraná, Cornélio Procópio, Brazil; Department of Computer Science, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Informatics, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil",Elsevier B.V.,English,09505849,
Scopus,Efficient development of high performance data analytics in Python,"Our society is generating an increasing amount of data at an unprecedented scale, variety, and speed. This also applies to numerous research areas, such as genomics, high energy physics, and astronomy, for which large-scale data processing has become crucial. However, there is still a gap between the traditional scientific computing ecosystem and big data analytics tools and frameworks. On the one hand, high performance computing (HPC) programming models lack productivity, and do not provide means for processing large amounts of data in a simple manner. On the other hand, existing big data processing tools have performance issues in HPC environments, and are not general-purpose. In this paper, we propose and evaluate PyCOMPSs, a task-based programming model for Python, as an excellent solution for distributed big data processing in HPC infrastructures. Among other useful features, PyCOMPSs offers a highly productive general-purpose programming model, is infrastructure-agnostic, and provides transparent data management with support for distributed storage systems. We show how two machine learning algorithms (Cascade SVM and K-means) can be developed with PyCOMPSs, and evaluate PyCOMPSs’ productivity based on these algorithms. Additionally, we evaluate PyCOMPSs performance on an HPC cluster using up to 1,536 cores and 320 million input vectors. Our results show that PyCOMPSs achieves similar performance and scalability to MPI in HPC infrastructures, while providing a much more productive interface that allows the easy development of data analytics algorithms. © 2019 The Authors",,"Álvarez Cid-Fuentes J., Álvarez P., Amela R., Ishii K., Morizawa R.K., Badia R.M.",2020,Journal,Future Generation Computer Systems,10.1016/j.future.2019.09.051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074532448&doi=10.1016%2fj.future.2019.09.051&partnerID=40&md5=1cae7dd63728770695e192eb808bb677,"Barcelona Supercomputing Center (BSC), Spain; Fujitsu, Ltd., Japan; Artificial Intelligence Research Institute (IIIA), Spanish National Research Council (CSIC), Spain",Elsevier B.V.,English,0167739X,
Scopus,On an optimal analogy-based software effort estimation,"Context: An analogy-based software effort estimation technique estimates the required effort for a new software project based on the total effort used in completing past similar projects. In practice, offering high accuracy can be difficult for the technique when the new software project is not similar to any completed projects. In this case, the accuracy will rely heavily on a process called effort adaptation, where the level of difference between the new project and its most similar past projects is quantified and transformed to the difference in the effort. In the past, attempts to adapt to the effort used machine learning algorithms; however, no algorithm was able to offer a significantly higher performance. On the contrary, only a simple heuristic such as scaling the effort by consulting the difference in software size was adopted. Objective:More recently, million-dollar prize data-science competitions have fostered the rapid development of more powerful machine learning algorithms, such as the Gradient boosting machine and Deep learning algorithm. Therefore, this study revisits the comparison of software effort adaptors that are based on heuristics and machine learning algorithms. Method:A systematic comparison of software effort estimators, which they all were fully optimized by Bayesian optimization technique, was carried out on 13 standard benchmark datasets. The comparison was supported by robust performance metrics and robust statistical test methods. Conclusion:The results suggest a novel strategy to construct a more accurate analogy-based estimator by adopting a combined effort adaptor. In particular, the analogy-based model that adapts to the effort by integrating the Gradient boosting machine algorithm and a traditional adaptation technique based on productivity adjustment has performed the best in the study. Particularly, this model significantly outperformed various state-of-the-art effort estimation techniques, including a current standard benchmark algorithmic-based technique, analogy-based techniques, and machine learning-based techniques. © 2020 Elsevier B.V.",Analogy; Effort adaptation; Empirical experiments; Ensemble; Hyperparameter optimization; Software effort estimation,Phannachitta P.,2020,Journal,Information and Software Technology,10.1016/j.infsof.2020.106330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084797206&doi=10.1016%2fj.infsof.2020.106330&partnerID=40&md5=2555806db7f886a828d95a7172fb8e2c,"College of Arts, Media and Technology University, 239 Suthep, MuangChiang Mai  50200, Thailand",Elsevier B.V.,English,09505849,
Scopus,Code and commit metrics of developer productivity: a study on team leaders perceptions,"Context: Developer productivity is essential to the success of software development organizations. Team leaders use developer productivity information for managing tasks in a software project. Developer productivity metrics can be computed from software repositories data to support leaders’ decisions. We can classify these metrics in code-based metrics, which rely on the amount of produced code, and commit-based metrics, which rely on commit activity. Although metrics can assist a leader, organizations usually neglect their usage and end up sticking to the leaders’ subjective perceptions only. Objective: We aim to understand whether productivity metrics can complement the leaders’ perceptions. We also aim to capture leaders’ impressions about relevance and adoption of productivity metrics in practice. Method: This paper presents a multi-case empirical study performed in two organizations active for more than 18 years. Eight leaders of nine projects have ranked the developers of their teams by productivity. We quantitatively assessed the correlation of leaders’ rankings versus metric-based rankings. As a complement, we interviewed leaders for qualitatively understanding the leaders’ impressions about relevance and adoption of productivity metrics given the computed correlations. Results: Our quantitative data suggest a greater correlation of the leaders’ perceptions with code-based metrics when compared to commit-based metrics. Our qualitative data reveal that leaders have positive impressions of code-based metrics and potentially would adopt them. Conclusions: Data triangulation of productivity metrics and leaders’ perceptions can strengthen the organization conviction about productive developers and can reveal productive developers not yet perceived by team leaders and probably underestimated in the organization. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Developer productivity; Mixed method; Repository mining; Software metrics; Team leaders perceptions,"Oliveira E., Fernandes E., Steinmacher I., Cristo M., Conte T., Garcia A.",2020,Journal,Empirical Software Engineering,10.1007/s10664-020-09820-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083805789&doi=10.1007%2fs10664-020-09820-z&partnerID=40&md5=e067cfb191c10bd03fa26cab519c540d,"Federal University of Amazonas (UFAM), Manaus, Brazil; Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil; Federal University of Technology – Paraná (UTFPR), Curitiba, Brazil; Northern Arizona University, Flagstaff, AZ, United States",Springer,English,13823256,
Scopus,Assessing the effectiveness of approximate functional sizing approaches for effort estimation,"Context: Functional Size Measurement (FSM) methods, like Function Points Analysis (FPA) or COSMIC, are well-established approaches to estimate software size. Several approximations of these methods have been recently proposed as they require less time/information to be applied, however their effectiveness for effort prediction is not known. Objective: The effectiveness of approximated functional size measures for estimating the development effort is a key open question, since an approximate sizing approach may miss to capture factors affecting the effort. Therefore, we empirically investigated the use of approximate FPA and COSMIC sizing approaches, also compared with their standard versions, for effort estimation. Method: We measured 25 industrial software projects realised by a single company by using FPA, COSMIC, two approximate sizing approaches proposed by IFPUG for FPA (i.e. High Level and Indicative FPA), and three approximate sizing approaches proposed by the COSMIC organisation for COSMIC (i.e. Average Functional Process, Fixed Size Classification, and Equal Size Band). Then we investigated the quality of the regression models built using the obtained measures to estimate the development effort. Results: Models based on High Level FPA are effective, providing a prediction accuracy comparable to the one of the original FPA, while those based on the Indicative FPA method show poor estimation accuracy. Models based on COSMIC approximate sizing methods are also quite effective, in particular those based on the Equal Size Band approximation provided an accuracy similar to the one of standard COSMIC. Conclusion: Project managers should be aware that predictions based on High Level FPA and standard FPA can be similar, making this approximation very interesting and effective, while Indicative FPA should be avoided. COSMIC approximations can also provide accurate effort estimates, nevertheless, the Fixed Size Classification and Equal Size Band approaches introduce subjectivity in the measurement. © 2020",Approximate sizing approaches; COSMIC; FPA; Functional size measures; Software effort estimation,"Di Martino S., Ferrucci F., Gravino C., Sarro F.",2020,Journal,Information and Software Technology,10.1016/j.infsof.2020.106308,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083290763&doi=10.1016%2fj.infsof.2020.106308&partnerID=40&md5=f273f2653b851f553ad7a17d786a2141,"Department of DIETI, University of Naples Federico II, Italy; Department of Computer Science, University of Salerno, Italy; Department of Computer Science, University College London, United Kingdom",Elsevier B.V.,English,09505849,
Scopus,Software reliability prediction and management: A multiple change-point model approach,"It is commonly recognized that software development is highly unpredictable and software quality may not be easily enhanced after software product is finished. During the software development life cycle (SDLC), project managers have to solve many technical and management issues, such as high failure rate, cost over-run, low quality, and late delivery. Consequently, in order to produce robust and reliable software product(s) on time and within budget, project managers and developers have to appropriately allocate limited time, manpower, development, and testing effort. In the past, the distribution of testing effort or manpower can typically be described by the Weibull or Rayleigh model. Practically, it should be noticed that development environments or methods could be changed due to some reasons. Thus, when we plan to perform software reliability modeling and prediction, these changes or variations occurring in the development process have to be taken into consideration. In this paper, we will study how to use the Parr-curve model with multiple change-points to depict the consumption of testing effort and how to perform further software reliability analysis. Some mathematical properties of proposed model will be given and discussed. The applicability and performance of our proposed model will be demonstrated and assessed through real software failure data. Experimental results are analyzed and compared with other existing models to show that our proposed model gives better predictions. Finally, an optimal software release policy based on cost-reliability criteria is proposed and studied. The main purpose is aimed at minimizing the total cost of software development when a reliability objective is given. © 2020 John Wiley & Sons, Ltd.",change-points; nonhomogeneous Poisson process (NHPP); Parr curve; Rayleigh curve; reliability model; software reliability growth model (SRGM); testing effort; Weibull distribution,"Ke S.-Z., Huang C.-Y.",2020,Journal,Quality and Reliability Engineering International,10.1002/qre.2653,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082952504&doi=10.1002%2fqre.2653&partnerID=40&md5=8149deeff3c2e54696b1e62355047397,"Software Department, Altek Corp., Ltd., HsinChu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",John Wiley and Sons Ltd,English,07488017,
Scopus,Kernel Fuzzy Clustering with Output Layer Self-Connection Recurrent Neural Networks for Software Cost Estimation,"In the current world, the software cost estimation problem has been resolved using various newly developed methods. Significantly, the software cost estimation problems can be dealt with effectively with the recently grown recurrent neural network (RNN) than the other newly developed methods. In this paper, an improved approach is proposed to software cost estimation using Output layer self-connection recurrent neural networks (OLSRNN) with kernel fuzzy c-means clustering (KFCM). The proposed OLSRNN method follows the basics of traditional RNN models for integrating self-connections to the output layer; thereby, the output temporal dependencies are better captured. Also, the performance of neural networks is improved using the kernel fuzzy clustering algorithm to enhance software estimation results. Ultimately, five publicly available software cost estimation datasets are adapted to verify the efficacy of the proposed KFCM-OLSRNN method using the validation metrics such as MdMRE, PRED (0.25) and MMRE. The experimental results proved the efficiency of the proposed method for solving the software cost estimation problem. © 2020 World Scientific Publishing Company.",kernel fuzzy c-means; Output layer; Recurrent neural networks; software cost estimation,"Resmi V., Vijayalakshmi S.",2020,Journal,"Journal of Circuits, Systems and Computers",10.1142/S0218126620500917,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070574008&doi=10.1142%2fS0218126620500917&partnerID=40&md5=2c21f80a33ef37d0096fd3b7a2c9aa33,"Department of Computer Applications, Udaya School of Engineering, Vellamodi, Kanyakumari District, 629204, India; Department of Computer Applications, Thiagarajar College of Engineering, Madurai, 625015, India",World Scientific Publishing Co. Pte Ltd,English,02181266,
Scopus,The role of neural networks and metaheuristics in agile software development effort estimation,"In any software development, accurate estimation of resources is one of the crucial tasks that leads to a successful project development. A lot of work has been done in estimation of effort in traditional software development. But, work on estimation of effort for agile software development is very scant. This paper provides an effort estimation technique for agile software development using artificial neural networks (ANN) and a metaheuristic technique. The artificial neural networks used are radial basis function neural network (RBFN) and functional link artificial neural network (FLANN). The metaheuristic technique used is whale optimization algorithm (WOA), which is a nature-inspired metaheuristic technique. The proposed techniques FLANN-WOA and RBFN-WOA are evaluated on three agile datasets, and it is found that these neural network models performed extremely well with the metaheuristic technique used. This is further empirically validated using non-parametric statistical tests. Copyright © 2020, IGI Global.",Friedman test; Functional link artificial neural network; Non-parametric tests; Radial basis function neural network; Statistical tests; Whale optimization algorithm; Wilcoxon matched pair test,"Kaushik A., Tayal D.K., Yadav K.",2020,Journal,International Journal of Information Technology Project Management,10.4018/IJITPM.2020040104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085063880&doi=10.4018%2fIJITPM.2020040104&partnerID=40&md5=f1215fadec4e560b1b473042ee2f6c0e,"Maharaja Surajmal Institute of Technology, Delhi, India; Indira Gandhi Delhi Technical University for Women, Delhi, India",IGI Global,English,19380232,
Scopus,Evaluating deep learning paradigms with TensorFlow and Keras for software effort estimation,"Deep learning is an arm of Artificial Intelligence that uses deep neural networks to achieve artificial intelligence. It has made its mark in computer vision, speech recognition, language processing, and automatic engines. Google made a significant contribution to AI technologies by releasing TensorFlow (TF), its proprietary AI platform, in 2015, as an open-source software library to define, train and deploy learning models, including Machine Learning and Deep Learning. In this study, we aim to improve software estimation using the most recent deep learning paradigms. We employ TensorFlow and a high-level wrapper API to TF and evaluate a composite hyper-parameter tuning method employing the Cartesian grid and random search. We observe significant performance improvement, achieved (29.8%) from the base model, using the hybrid hyper-parameter tuning methodology. However, even While literature reports significant performance in cognitive imaging with TF and Keras, we have not been able to validate any substantial improvement in prediction, in the case of a software effort estimation data such as ISBSG 2018 by employing these techniques. © 2020 IJSTR.",Artificial intelligence; Deep Learning; Effort Prediction; Feature Engineering; Hyperparameters; Keras; Machine Learning; Neural Networks; Software Cost Estimation; Software Effort Estimation; TensorFlow,"Pillai S.P., Radha Ramanan T., Madhu Kumar S.D.",2020,Journal,International Journal of Scientific and Technology Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083527708&partnerID=40&md5=7948807889249d723e5a86b2f3e0726a,,International Journal of Scientific and Technology Research,English,22778616,
Scopus,Novel Grey Relational Feature Extraction Algorithm for Software Fault-Proneness Using BBO (B-GRA),"The inherent uncertainty of software gives a vague and imprecise solution when it is solved by human judgment. As the project expands, the issues of missing data values, outlier detection, feature subset selection and prediction of faultiness behaviour should be addressed. The feature selection process may lead to the production of high-dimensional data sets that may contribute to many irrelevant or redundant features. In this paper, we focussed on the optimal feature subset selection and fault prediction at the early stage of a project. We propose the novel approach of grey relational analysis (GRA) from grey system theory by optimizing the grey relational grade function using biogeography optimization referred to as B-GRA. The proposed algorithm gives resilience to users to select features for both continuous and categorical attributes. The issues such as feature subset selection, heterogeneity of data sets, outlier analysis and fault prediction are addressed, and then, B-GRA and GRA approaches on five publically available data sets are evaluated using statistical and machine learning techniques. Experimental results show significant results indicating that the proposed methodology can be used for the prediction of faults and produce conceivable results when compared with the GRA feature selection approach. © 2020, King Fahd University of Petroleum & Minerals.",Biogeography-based optimization; Fault classification; Genetic algorithm; Regression,"Aarti, Sikka G., Dhir R.",2020,Journal,Arabian Journal for Science and Engineering,10.1007/s13369-020-04445-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082971583&doi=10.1007%2fs13369-020-04445-2&partnerID=40&md5=f1c3184c2ff9a2be99ef33cd403cfc68,"Department of Computer Science and Engineering, Lovely Professional University, Jalandhar, India; Department of Computer Science and Engineering, NIT, Jalandhar, India",Springer,English,2193567X,
Scopus,Software Quality Metrics Calculations for Java Programming Learning Assistant System,"A Web-based Java Programming Learning Assistant System (JPLAS) has been proposed to assist Java programming educations in universities. In the code writing problem, the correctness of an answer code from a student is verified by running the test code on JUnit. Besides, their quality should be measured using the metrics to assess them. The currently using plugin could only be measured on eclipse for offline answering in JPLAS. To calculate the metrics and implement in web-based JPLAS, there are several equations that have been reported. In this paper, we find the proper equations to calculate the metrics that provide the same results as from Eclipse plugin. The application results for 45 source codes showed that the adopted metrics equations provide the same results as the plugin. © 2020 IEEE.",code writing problem; JPLAS; JUnit; test code,"Zaw K.K., Hnin H.W., Kyaw K.Y., Funabiki N.",2020,Conference,"2020 IEEE Conference on Computer Applications, ICCA 2020",10.1109/ICCA49400.2020.9022823,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082395312&doi=10.1109%2fICCA49400.2020.9022823&partnerID=40&md5=d4deb66d5025cf4067095c9665811107,"Yangon Technological University, Department of Computer Engineereing and Information Technology, Yangon, Myanmar; Okayama University, Department of Electrical and Communication Engineering, Okayama, Japan",Institute of Electrical and Electronics Engineers Inc.,English,,9781728159256
Scopus,A Hybrid Machine Learning Framework for Prediction of Software Effort at the Initial Phase of Software Development,"In the era of software application, the prediction of the effort of software plays an essential role in the success of project software. The inconsistent, inaccurate, and unreliable prediction of software leads to failure. As the requirement and specification changes as per the software needs, accurate prediction of effort is a difficult task for developing software. This prediction of effort must be calculated accurately to avoid unpredicted results. At the early stages of development, these inaccurate, unreliability, and uncertainty are the drawback of previously developed models. The main aim of the study is to overcome the drawbacks and develop a model for the prediction of the effort of software. A combination of regression analysis and genetic algorithm has been used to develop the model. The model is trained and validated using the ISBSG dataset. The proposed model is compared for performance with a few baseline models. The results show that the proposed model outperforms most of the baseline models against different performance metrics. © 2020, Springer Nature Singapore Pte Ltd.",Deep learning; Evaluation metrics; Genetic algorithm; Regression analysis; Software effort estimation,"Rai P., Kumar S., Verma D.K.",2020,Conference,Communications in Computer and Information Science,10.1007/978-981-15-6634-9_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089241171&doi=10.1007%2f978-981-15-6634-9_18&partnerID=40&md5=8dfc4911bce1a085e20ac848b1b2e67f,"Department of Computer Science and Engineering, Jaypee University of Engineering and Technology, Raghogarh-Vijaypur, India",Springer,English,18650929,9789811566332
Scopus,Systematic review study of decision trees based software development effort estimation,"The role of decision trees in software development effort estimation (SDEE) has received increased attention across several disciplines in recent years thanks to their power of predicting, their ease of use, and understanding. Furthermore, there are a large number of published studies that investigated the use of a decision tree (DT) techniques in SDEE. Nevertheless, in reviewing the literature, a systematic literature review (SLR) that assesses the evidence stated on DT techniques is still lacking. The main issues addressed in this paper have been divided into five parts: prediction accuracy, performance comparison, suitable conditions of prediction, the effect of the methods employed in association with DT techniques, and DT tools. To carry out this SLR, we performed an automatic search over five digital libraries for studies published between 1985 and 2019. In general, the results of this SLR revealed that most DT methods outperform many techniques and show an improvement in accuracy when combined with association rules (AR), fuzzy logic (FL), and bagging. Additionally, it has been observed a limited use of DT tools: it is therefore suggested for researchers to develop more DT tools to promote the industrial utilization of DT amongst professionals. © 2020 Science and Information Organization.",Decision tree; Regression tree; Software development effort estimation; Systematic literature review,"Najm A., Marzak A., Zakrani A.",2020,Journal,International Journal of Advanced Computer Science and Applications,10.14569/IJACSA.2020.0110767,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088983260&doi=10.14569%2fIJACSA.2020.0110767&partnerID=40&md5=3ae66f960c0916dc4c0cef4f83d7b0fc,"Department of Mathematics and Computer Sciences FSB M'sik, Hassan II University, Casablanca, Morocco; Department of Industrial, Engineering, ENSAM, Casablanca, Morocco",Science and Information Organization,English,2158107X,
Scopus,Learning actionable analytics from multiple software projects,"The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that generate demonstrably useful guidance on “what to do” within the context of a specific software project. Specifically, we propose XTREE (for within-project planning) and BELLTREE (for cross-project planning) to generating plans that can improve software quality. Each such plan has the property that, if followed, it reduces the expected number of future defect reports. To find this expected number, planning was first applied to data from release x. Next, we looked for change in release x + 1 that conformed to our plans. This procedure was applied using a range of planners from the literature, as well as XTREE. In 10 open-source JAVA systems, several hundreds of defects were reduced in sections of the code that conformed to XTREE’s plans. Further, when compared to other planners, XTREE’s plans were found to be easier to implement (since they were shorter) and more effective at reducing the expected number of defects. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Actionable analytics; Bellwethers; Data mining; Defect prediction; Planning,"Krishna R., Menzies T.",2020,Journal,Empirical Software Engineering,10.1007/s10664-020-09843-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088519438&doi=10.1007%2fs10664-020-09843-6&partnerID=40&md5=5585c124ac732d9667a13b9da22029f9,"Computer Science, Columbia University, New York, NY, United States; Computer Science, NC State University, Raleigh, NC, United States",Springer,English,13823256,
Scopus,Predicting maintenance work hours in maintenance planning,"Purpose: Current industry practices illustrate there is no standard method to estimate the number of hours worked on maintenance activities; instead, industry experts use experience to guess maintenance work hours. There is also a gap in the research literature on maintenance work hour estimation. This paper investigates the use of machine-learning algorithms to predict maintenance work hours and proposes a method that utilizes historical preventive maintenance order data to predict maintenance work hours. Design/methodology/approach: The paper uses the design research methodology utilizing a case study to validate the proposed method. Findings: The case study analysis confirms that the proposed method is applicable and has the potential to significantly improve work hour prediction accuracy, especially for medium- and long-term work orders. Moreover, the study finds that this method is more accurate and more efficient than conducting estimations based on experience. Practical implications: The study has major implications for industrial applications. Maintenance-intensive industries such as oil and gas and chemical industries spend a huge portion of their operational expenditures (OPEX) on maintenance. This research will enable them to accurately predict work hour requirements that will help them to avoid unwanted downtime and costs and improve production planning and scheduling. Originality/value: The proposed method provides new insights into maintenance theory and possesses a huge potential to improve the current maintenance planning practices in the industry. © 2020, Emerald Publishing Limited.",Estimation; Machine learning; Maintenance; Maintenance management; Planning; Prediction; Work hour,"Khalid W., Albrechtsen S.H., Sigsgaard K.V., Mortensen N.H., Hansen K.B., Soleymani I.",2020,Journal,Journal of Quality in Maintenance Engineering,10.1108/JQME-06-2019-0058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087130204&doi=10.1108%2fJQME-06-2019-0058&partnerID=40&md5=e8cb403de36a18041590db91e9058803,"Department of Mechanical Engineering, Technical University of Denmark, Lyngby, Denmark; Technical University of Denmark, Lyngby, Denmark",Emerald Group Publishing Ltd.,English,13552511,
Scopus,Adopting the appropriate performance measures for soft computingbased estimation by analogy,"Soft Computing based estimation by analogy is a lucrative research domain for the software engineering research community. There are a considerable number of models proposed in this research area. Therefore, researchers are of interest to compare the models to identify the best one for software development effort estimation. This research showed that most of the studies used mean magnitude of relative error (MMRE) and percentage of prediction (PRED) for the comparison of their estimation models. Still, it was also found in this study that there are quite a number of criticisms done on accuracy statistics like MMRE and PRED by renowned authors. It was found that MMRE is an unbalanced, biased, and inappropriate performance measure for identifying the best among competing estimation models. The accuracy statistics, e.g., MMRE and PRED, are still adopted in the evaluation criteria by the domain researchers, stating the reason for ""widely used, "" which is not a valid reason. This research study identified that, since there is no practical solution provided so far, which could replace MMRE and PRED, the researchers are adopting these measures. The approach of partitioning the large dataset into subsamples was tried in this paper using estimation by analogy (EBA) model. One small and one large dataset were considered for it, such as Desharnais and ISBSG release 11. The ISBSG dataset is a large dataset concerning Desharnais. The ISBSG dataset was partitioned into subsamples. The results suggested that when the large datasets are partitioned, the MMRE produces the same or nearly the same results, which it produces for the small dataset. It is observed that the MMRE can be trusted as a performance metric if the large datasets are partitioned into subsamples. © 2020, Insight Society.",Estimation by analogy; MMRE; Performance metrics; PRED; Software development effort,"Shah M.A., Jawawi D.N.A., Isa M.A., Younas M., Mustafa A.",2020,Journal,"International Journal on Advanced Science, Engineering and Information Technology",10.18517/ijaseit.10.2.10178,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085178189&doi=10.18517%2fijaseit.10.2.10178&partnerID=40&md5=6e8f0a6ff66fe6b9b661312c534b7692,"Software Engineering Department, School of Computing, Faculty of Engineering, Universit Teknologi Malaysia, Johor, Bahru, Malaysia; City University of Science and Information Technology, Peshawar, Pakistan; Government College University, Faisalabad, Pakistan",Insight Society,English,20885334,
Scopus,"InnoMetrics Dashboard: The Design, and Implementation of the Adaptable Dashboard for Energy-Efficient Applications Using Open Source Tools","Increasing amount of data the organizations worldwide have at their disposal lead to the need to structure, organize and present the information obtained from it. That is because, in today’s rapid-changing business environment, managers and executives need to be able to gain crucial insights about the ongoing project in as little time as possible. Recently, energy efficiency has become a greater field of research, and companies started concentrating on monitoring energy-related metrics. In addition, many of them have built their own internal tools (dashboards) to do just this. However, one of the major drawbacks of building specialized tools is the lack of adaptability. That is, they are often tailored to only one person (e.g. CEO), or a small group of them (e.g. board of directors, managers). Furthermore, the combination of metrics that is displayed to them does not change over time. This is a problem because most likely there exists a better metric combination that would allow users to get the crucial insights faster. To fill this gap, our ongoing research focuses on making the dashboards adaptable to multiple roles within the organization while optimizing for a certain goal. In some scenarios the dashboard’s goal may be to detect defects, in others it may be to generate the most profit. As our primary research interest is to amplify energy efficiency, we have chosen that to be our dashboard’s goal. Our previous work suggests that in order to handle compound metrics at scale it is needed to represent the dashboard as a complex system. This paper presents the design and the architecture of our proposed solution synergizing the notions from complexity theory, software architecture and user experience (UX) design. © 2020, IFIP International Federation for Information Processing.",Adaptable systems; Dashboards; Energy efficiency,"Ergasheva S., Ivanov V., Khomyakov I., Kruglov A., Strugar D., Succi G.",2020,Conference,IFIP Advances in Information and Communication Technology,10.1007/978-3-030-47240-5_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085029481&doi=10.1007%2f978-3-030-47240-5_16&partnerID=40&md5=fbdb26693f1d09c56acd80e813c6379e,"Innopolis University, Innopolis, 420500, Russian Federation",Springer,English,18684238,9783030472399
Scopus,The Development of Data Collectors in Open-Source System for Energy Efficiency Assessment,"The paper is devoted to the development of the data collectors for Windows OS and MacOS. The purpose of these plugins is to collect the process metrics from the user’s device and send it to the back-end for further processing. The overall open source framework is aimed at energy efficiency analysis of the developing software products. The development presented here as a sequence of the life cycle stages, including requirements analysis, design, implementation and testing. Specifics of the implementation for each targeted operating system are given. © 2020, IFIP International Federation for Information Processing.",Collector; Energy metrics; MacOS; Open source software; Process metrics; Windows,"Atonge D., Ivanov V., Kruglov A., Khomyakov I., Sadovykh A., Strugar D., Succi G., Vasquez X.Z., Zouev E.",2020,Conference,IFIP Advances in Information and Communication Technology,10.1007/978-3-030-47240-5_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085017787&doi=10.1007%2f978-3-030-47240-5_2&partnerID=40&md5=5166c6428c5fc9f85ba01137b7189bb9,"Innopolis University, Innopolis, Russian Federation",Springer,English,18684238,9783030472399
Scopus,Energy Efficient Software Development Process Evaluation for MacOS Devices,"InnoMetrics is the system that aims at collecting software development process metrics in a non-invasive way to access and optimize the development process and its efficiency. This paper demonstrates the development and analysis of energy consumption of MacOS systems based on the software process measurement data. It represents the experience of the development of MacOS system collector and Transfer, in addition to the user interface and early analysis of energy consumption metrics calculations. © 2020, IFIP International Federation for Information Processing.",Energy efficient software; MacOS software development; Software development process metrics,"Ergasheva S., Strugar D., Kruglov A., Succi G.",2020,Conference,IFIP Advances in Information and Communication Technology,10.1007/978-3-030-47240-5_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085002694&doi=10.1007%2f978-3-030-47240-5_20&partnerID=40&md5=f9a226b1554e8b1c7b78f99f82f04e63,"Innopolis University, Innopolis, Russian Federation",Springer,English,18684238,9783030472399
Scopus,Dynamic software management practices using genetically augmented neural networks,"The aim of research is to investigate real time projects and the utilisation of Ppc software packages for educational institutions. The main motivation behind the study is to resolve the scheduling problem in Ppc software projects, which is considered useful for students' curriculum. The scheduling problem is been a prominent issue in software projects, which creates a trade-off between project duration and skills. Hence, this work removes the trade-off between project duration and skills using genetic augmented neural network algorithm (GANN). The process runs in a structured way to obtain a better solution in software project duration over education sector. The result with GANN over software validation seems flexible and accurate and can work as a flexible tool for automated software management practices. Copyright © 2020 Inderscience Enterprises Ltd.",Educational sector; Genetic neural network algorithm; Project scheduling; Software project management,Alostad J.M.,2020,Journal,International Journal of Internet Technology and Secured Transactions,10.1504/IJITST.2020.107078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084672402&doi=10.1504%2fIJITST.2020.107078&partnerID=40&md5=2624ebf1c1ac48f22811270a280378a0,"College of Basic Education, Public Authority of Applied Education and Training (PAAET), P.O. Box, 23167, Safat, 13092, Kuwait",Inderscience Publishers,English,1748569X,
Scopus,Computational Intelligence Approaches for Software Quality Improvement,"Obtaining reliable, secure and efficient software under optimal resource allocation is an important objective of software engineering science. This work investigates the usage of classical and recent development paradigms of computational intelligence (CI) to fulfill this objective. The main software engineering steps asking for CI tools are: product requirements analysis and precise software specification development, short time development by evolving automatic programming and pattern test generation, increasing dependability by specific design, minimizing software cost by predictive techniques, and optimal maintenance plans. The tasks solved by CI are related to classification, searching, optimization, and prediction. The following CI paradigms were found useful to help software engineers: fuzzy and intuitionistic fuzzy thinking over sets and numbers, nature inspired techniques for searching and optimization, bio inspired strategies for generating scenarios according to genetic algorithms, genetic programming, and immune algorithms. Neutrosophic computational models can help software management when working with imprecise data. © 2020, Springer Nature Switzerland AG.",Computational intelligence; Immune algorithms; Neutrosophic computational models; Software quality; Software reliability,"Albeanu G., Madsen H., Popențiu-Vlădicescu F.",2020,Book Chapter,Springer Series in Reliability Engineering,10.1007/978-3-030-43412-0_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084001937&doi=10.1007%2f978-3-030-43412-0_18&partnerID=40&md5=df2c3ed06a01a389218d5432b3e809c9,"“Spiru Haret” University, Bucharest, Romania; Danish Technical University, Lyngby, Denmark; University Politehnica of Bucharest & Academy of Romanian Scientists, Bucharest, Romania",Springer,English,16147839,
Scopus,Prediction of Software Effort Using Design Metrics: An Empirical Investigation,"These days, prediction of effort in software project is the shove area for the researchers. The estimation of effort in software process is as essential as software product. Primarily, estimation models consist of relation between dependant and independent variable(s). The effectiveness of these models is to bring more accuracy to the work plan and reduce financial cost. The variables in these models may be considered as complexity, size, person per month, and other different software metrics. Most of these models only considered the static behaviour of the software product, in which the fixed value of the effort predicted at the starting of project. Hence, there is a need to formulate a methodology which considered the future changes in the software project for effort estimation. In this paper, a model has been formulated which can be use to make the prediction of software efforts with the help of software metrics, primarily design metrics, such as Depth of Inheritance Tree, Line of Code, Weighted Method per Class. The correlation between the metrics and effort is been shown with the help of regression model formulated in this paper. The model has been validated by the data set collected from the PROMISE repository. © Springer Nature Singapore Pte Ltd. 2020.",Regression; Software effort estimation; Software metrics; Software quality,"Rai P., Kumar S., Verma D.K.",2020,Book Chapter,Lecture Notes in Networks and Systems,10.1007/978-981-15-2071-6_51,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083996257&doi=10.1007%2f978-981-15-2071-6_51&partnerID=40&md5=8cef3317bb03110b75ddfd8d991a8be5,"Computer Science and Engineering, Jaypee University of Engineering and Technology, Guna, India",Springer,English,23673370,
Scopus,ERP implementation: Requirements engineering for ERP product customization,"Requirements Engineering (RE) is the basis for efficient software implementation and quality management. Tools which support RE in general are numerous nowadays; however, the task of providing a tool that specializes in RE for dynamic, customizable service-centric systems (like ERP systems) has been addressed rarely. According to this we proposed an artifact in a form of software functionalities list derived from performing literature review and through intensive discussion with our industry partners. Relevance of proposed application functionalities were discussed with experts in a form of interviews and those are presented in this research paper. A support application for collaborative requirements engineering and software artifacts traceability, with focus on ERP product customization will be developed and presented in further research work. © Springer Nature Switzerland AG 2020.",Collaborative requirements engineering; Customizable software (ERP) products; Enterprise Resource Planning; ERP implementation,"Kraljić T., Kraljić A.",2020,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-030-44322-1_42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083974123&doi=10.1007%2f978-3-030-44322-1_42&partnerID=40&md5=dec760991ca73c0808cb9967918e79ca,"Ghent University, Ghent, Belgium",Springer,English,18651348,9783030443214
Scopus,A Novel Model for Risk Estimation in Software Projects Using Artificial Neural Network,"Software projects generally involve more risks due to unexpected negative results. Therefore, the risks encountered in software projects should be detected and analyzed on time, and effective precautions should be taken in order to complete the projects successfully. The aim of this study was to estimate the deviations that may occur in the software project outputs according to risk factors by using artificial neural networks (ANNs). Thus we aimed to minimize loses that may occur in project processes with the developed model. Firstly, a comprehensive and effective list of risk factors was created. Later, a checklist form was prepared for Team Members and Managers. The data collected include general project data and risk factors, and these are the inputs of the model. The outputs of the model are the deviations in the project outputs. MATLAB package program was utilized to develop the model. The performance of the model was measured according to Regression Values and Mean-Squared Error. The model obtained has forty-five inputs, one hidden layer that has fifteen neurons, and five outputs (45-15-5). In addition, the training-R, testing-R, and MSE values of the model were found as 0.9978, 0.9935, and 0.001, respectively. It is seen that the estimation results obtained with the model using the real project data coincide with the actual results largely and the error rates were also very low (close to zero). The experimental results clearly revealed that model performance is high, and it is very effective to use ANNs in risk estimation processes for software projects. © 2020, Springer Nature Switzerland AG.",Artificial neural networks; Risk estimation; Risk factor; Risk management; Software project,"Calp M.H., Akcayol M.A.",2020,Book Chapter,Lecture Notes on Data Engineering and Communications Technologies,10.1007/978-3-030-36178-5_23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083466922&doi=10.1007%2f978-3-030-36178-5_23&partnerID=40&md5=565edc62608759b49cc84f1bfb37f82d,"Department of Management Information Systems, Karadeniz Technical University, Trabzon, Turkey; Department of Computer Engineering, Gazi University, Ankara, Turkey",Springer,English,23674512,
Scopus,Software quality analysis based on cost and error using fuzzy combined COCOMO model,"Software quality analysis and estimation is essential in developing a software to avoid faults and increase the reliability. Software quality model (SQM) is highly concerned with standard metrics to qualify the software modules to classify bug or no bug. By using these models, it is easy to identify the hurdles called as errors or faults Apriori to the development cycle. More likely the metrics will not follow the standard protocol in terms of size, performance, technology and the complexity involved. It will vary across the projects. Surprisingly there is no model-based architecture driven tool is available to intact the baseline estimates of the project based on the previous knowledge resource. In earlier research works, various quality assurance metrics are used for analysing the SQ. Also, there is no existing approaches can do earlier prediction of the faults/errors or reduced misclassification rates. But, the COCOMO (COnstructive COst MOdel) gives an approximate estimate in terms of the month constant will not be same for simulating the study. Hence By combining more than one model estimates COCOMO and Gaussian Membership Function software estimate relative error will be the best suite. A fuzzy-based analogy is obtained in the present study to select the nearest path from the history available to meet the project cost and time. Small or standard training sets were considered to deploy the estimate, and compare the performance with different estimators. From the experiment, it is concluded that the proposed fuzzy-COCOMO model outperforms than the existing approaches in terms of relative error. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",COCOMO model; Cost analysis; Cost estimation; Error analysis; Fuzzy logic; Software life cycle; Software quality model,"Manikavelan D., Ponnusamy R.",2020,Journal,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-01783-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082715207&doi=10.1007%2fs12652-020-01783-9&partnerID=40&md5=c2891e85d8d488955393384b2e6563ba,"Department of CSE, Sathyabama University, Chennai, India; Department of Computer Science and Engineering, CVR College of Engineering, Hyderabad, Telangana, India",Springer,English,18685137,
Scopus,Classification of heart disease using cluster based DT learning,"In the rural side, due to the absence of cardiovascular ailment centers, around 12 million people passing away worldwide reported by WHO. The principal purpose of coronary illness is a propensity of smoking. Our Cluster based disease Diagnosis (CDD) applies the ML classifiers to improve the prediction accuracy of cardiovascular diseases. For this we have taken a real Cleveland dataset from UCI. First, the ML performance is evaluated through all features. Then, the dataset is split through the class pairs through its distribution. From this class pair, the significant features are identified through entropy process. Through our CDD approach four significant features are identified from thirteen features. From this four features, the ML performance increases when compared to all other features. That is, in RF model the accuracy improves to 9.5%, SVM by 7.2% and DT model by 2.3%. © 2020 Senthilkumar Mohan, Chandrsegar Thirumalai and Abdalah Rababah.",Classification; Machine learning,"Mohan S., Thirumalai C., Rababah A.",2020,Journal,Journal of Computer Science,10.3844/jcssp.2020.50.55,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082008596&doi=10.3844%2fjcssp.2020.50.55&partnerID=40&md5=50a80ac1c9d6fc92dd28aa2e05c24c57,"School of Information Technology and Engineering, VIT University, Vellore, Tamilnadu, India; Department of Mathematical Sciences, United Arab Emirates University, Al Ain, 15551, United Arab Emirates",Science Publications,English,15493636,
Scopus,Fuzzy case-based-reasoning-based imputation for incomplete data in software engineering repositories,"Missing data is a serious issue in software engineering because it can lead to information loss and bias in data analysis. Several imputation techniques have been proposed to deal with both numerical and categorical missing data. However, most of those techniques used is simple reuse techniques originally designed for numerical data, which is a problem when the missing data are related to categorical attributes. This paper aims (a) to propose a new fuzzy case-based reasoning (CBR) imputation technique designed for both numerical and categorical data and (b) to evaluate and compare the performance of the proposed technique with the k-nearest neighbor (KNN) imputation technique in terms of error and accuracy under different missing data percentages and missingness mechanisms in four software engineering data sets. The results suggest that the proposed fuzzy CBR technique outperformed KNN in terms of imputation error and accuracy regardless of the missing data percentage, missingness mechanism, and data set used. Moreover, we found that the missingness mechanism has an important impact on the performance of both techniques. The results are encouraging in the sense that using an imputation technique designed for both categorical and numerical data is better than reusing methods originally designed for numerical data. © 2020 John Wiley & Sons, Ltd.",accuracy; categorical data; empirical software engineering; fuzzy analogy; imputation; missing data,"Abnane I., Idri A., Abran A.",2020,Journal,Journal of Software: Evolution and Process,10.1002/smr.2260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081904769&doi=10.1002%2fsmr.2260&partnerID=40&md5=762cb4270c6a55d209feaa8eb6df5c5f,"Software Project Management Research Team, ENSIAS, University Mohammed V, Rabat, Morocco; CSEHS, University Mohammed VI Polytechnic, Ben Guerir, Morocco; Department of Software Engineering, École de Technologie Supérieure, Montréal, Canada",John Wiley and Sons Ltd,English,20477481,
Scopus,Grey relational classification algorithm for software fault proneness with SOM clustering,"The estimation by the human judgment to deal with the inherent uncertainty of software gives a vague and imprecise solution. To cope with this challenge, we propose a new hybrid analogy model based on the integration of grey relational analysis (GRA) classification with self-organising map (SOM) clustering. In this paper, a new classification approach is proposed to distribute the data to similar groups. The attributes are selected based on GRC values. In the proposed, the similarity measure between reference project and cluster head is computed to determine the cluster to which target project belongs. The fault-proneness of reference project is estimated based on the regression equation of the selected cluster. The proposed algorithm gives resilience to users to select features for both continuous and categorical attributes. In this study, two scenarios based on the integration of proposed classification with regression have been proposed. Experimental results show significant results indicating that proposed methodology can be used for the prediction of faults and produce conceivable results when compared with the results of multilayer-perceptron, logistic regression, bagging, naïve Bayes and sequential minimal optimisation (SMO). Copyright © The Authors(s) 2019.",Fault-proneness; GRA; Grey relational analysis; Object-oriented; OO; Self-organising map; SOM; Unsupervised classification,"Aarti, Sikka G., Dhir R.",2020,Journal,"International Journal of Data Mining, Modelling and Management",10.1504/IJDMMM.2020.105599,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081603809&doi=10.1504%2fIJDMMM.2020.105599&partnerID=40&md5=cfbde38ee8edb3dff6294d1774c50278,"Department of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Department of Computer Science and Engineering, Dr. B.R. Ambedkar, NIT Jalandhar, IT Park144001, India",Inderscience Publishers,English,17591163,
Scopus,Data-driven benchmarking in software development effort estimation: The few define the bulk,"Context: The rapid evolvement of software development effort estimation models created the need for empirical evaluation of their quality. The empirical evaluation is based either on hypothesis tests with respect to a single criterion or on aggregating methods for multiple criteria. However, a model can be considered as a multidimensional entity performing differently on alternative datasets and its performance can be divergent when expressed by alternative criteria. Objective: In this study, we explore this multidimensional nature of models by considering them as points in two different spaces (domain and criteria spaces). Method: Introducing an alternative approach for data-driven benchmarking, a new framework based on archetypal analysis is proposed for evaluation purposes of multiple models. Results: The benefits of the framework are illustrated through a large-scale experimental setup on a set of 93 effort estimation models, trained and tested on 10 datasets under 8 criteria providing answers to critical research questions. Conclusion: The results indicate that a small minority of reference models is enough to define the performance of the bulk of all models. The framework focuses on models that have behavior close to archetypes and especially those that are close to a “best” archetype. © 2020 John Wiley & Sons, Ltd.",archetypal analysis; benchmarks; performance measures; software development effort estimation,"Mittas N., Angelis L.",2020,Journal,Journal of Software: Evolution and Process,10.1002/smr.2258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081587836&doi=10.1002%2fsmr.2258&partnerID=40&md5=6c927557fa958193ca6df6ddf23917d9,"Department of Chemistry, International Hellenic University, Kavala, Greece; School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece",John Wiley and Sons Ltd,English,20477481,
Scopus,Function Point Tree-Based Function Point Analysis: Improving Reproducibility Whilst Maintaining Accuracy in Function Point Counting,We propose a method to improve reproducibility whilst keeping accuracy for the Function Point Analysis (FPA) method. The proposed method is based on a new artifact model called Function Point Tree (FPT). FPT enables a standardized and systematic collection of all data required for FP counting. The new measurement method is called Function Point Tree-based Function Point Analysis (FPT-FPA). We designed FPT-FPA to comply with the IFPUG’s FPA steps. We implemented a prototype tool to show the feasibility of automation of the proposed method as well as to support its evaluation. We conducted an empirical study to evaluate FPT-FPA. Our results show general coefficients of variation lower than the maximum expected for both reproducibility and accuracy when compared to the standard FPA method. © Springer Nature Switzerland AG 2020.,Business processes; Function Point Analysis; Function Points; Functional size; Functional size measurement,"de Freitas M., Jr., Fantinato M., Sun V., Thom L.H., Garaj V.",2020,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-030-40783-4_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081586207&doi=10.1007%2f978-3-030-40783-4_10&partnerID=40&md5=e7d6f34fcd3d00abf5ef75a2147f7207,"School of Arts, Sciences and Humanities, University of São Paulo, São Paulo, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Department of Electronic and Computer Engineering, Brunel University London, Uxbridge, United Kingdom",Springer,English,18651348,9783030407827
Scopus,Software Effort Estimation Using Particle Swarm Optimization: Advances and Challenges,"The objective of software effort estimation is the analytical calculation for estimating the size, effort, time, and cost of the project, which are interrelated to the software development life cycle. Software effort estimation is a composite activity that requires information about various properties that influences the results of the project. The most basic issue is the part of the information required to keep away from unexpected outcomes, but it is difficult to get data in the required quantities for software estimation. At the initial stage of development, the lack of adequacy are certainty, off base, trickiness which are the major constraints of algorithmic and expert judgment estimation models. The non-algorithmic approaches depend on new methodologies like soft computing overcomes the above models. Evolutionary algorithms are theoretical pursuit strategies that copy the natural advancement and the social conduct of the species. Such calculations have been created to land close ideal solutions, for which conventional algorithmic methods may fizzle. The primary point of this study is to give an inside and out survey of particle swarm optimization technique and a general trend toward their use in diverse domains to progress the outcome of the particle swarm optimization algorithm. © 2020, Springer Nature Singapore Pte Ltd.",Particle swarm optimization; Soft computing; Software effort estimation,"Reddy D.K.K., S Behera H.",2020,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-15-2449-3_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081355272&doi=10.1007%2f978-981-15-2449-3_20&partnerID=40&md5=085a628a3a12aef3178a907a0f0e6dcb,"Department of Information Technology, Veer Surendra Sai University of Technology, Burla, 768018, India",Springer,English,21945357,9789811524486
Scopus,"Replication of Studies in Empirical Software Engineering: A Systematic Mapping Study, from 2013 to 2018","Context: In any discipline, replications of empirical studies are necessary to consolidate the acquired knowledge. In Software Engineering, replications have been reported since the 1990s, although their number is still small. The difficulty in publishing, the lack of guidelines, and the unavailability of replication packages are pointed out by the community as some of the main causes. Objective: Understanding the current state of replications in Software Engineering studies by evaluating current trends and evolution during the last 6 years. Method: A Systematic Mapping Study including articles published in the 2013-2018 period that report at least one replication of an empirical study in Software Engineering. Results: 137 studies were selected and analysed, identifying: {i}) forums; ii) authors, co-authorships and institutions; iii) most cited studies; iv) research topics addressed; {v}) empirical methods used; vi) temporal distribution of publications; and vii) distribution of studies according to research topics and empirical methods. Conclusions: According to our results, the most relevant forums are the Empirical Software Engineering and Information and Software Technology journals, and the Empirical Software Engineering and Measurement conference. We observed that, as in previous reviews by other researchers, most of the studies were carried out by European institutions, especially Italian, Spanish, and German researchers and institutions. The studies attracting more citations were published mainly in journals and in the International Conference on Software Engineering. Testing, requirements, and software construction were the most frequent topics of replication studies, whereas the usual empirical method was the controlled experiment. On the other hand, we identified research gaps in areas such as software engineering process, software configuration management, and software engineering economics. When analysed together with previous reviews, there is a clear increasing trend in the number of published replications in the 2013-2018 period. © 2013 IEEE.",Empirical software engineering; Replications; Systematic mapping study,"Cruz M., Bernardez B., Duran A., Galindo J.A., Ruiz-Cortes A.",2020,Journal,IEEE Access,10.1109/ACCESS.2019.2952191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081158422&doi=10.1109%2fACCESS.2019.2952191&partnerID=40&md5=cb8d4bc14bd2f5ceda66a5fba9dd24f7,"Department of Computer Languages and Systems, Universidad de Sevilla, Seville, 41004, Spain",Institute of Electrical and Electronics Engineers Inc.,English,21693536,
Scopus,Software Analysis Method for Assessing Software Sustainability,"Software sustainability evaluation has become an essential component of software engineering (SE) owing to sustainability considerations that must be incorporated into software development. Several studies have been performed to address the issues associated with sustainability concerns in the SE process. However, current practices extensively rely on participant experiences to evaluate sustainability achievement. Moreover, there exist limited quantifiable methods for supporting software sustainability evaluation. Our primary objective is to present a methodology that can assist software engineers in evaluating a software system based on well-defined sustainability metrics and measurements. We propose a novel approach that combines machine learning (ML) and software analysis methods. To simplify the application of the proposed approach, we present a semi-automated tool that supports engineers in assessing the sustainability achievement of a software system. The results of our study demonstrate that the proposed approach determines sustainability criteria and defines sustainability achievement in terms of a traceable matrix. Our theoretical evaluation and empirical study demonstrate that the proposed support tool can help engineers identify sustainability limitations in a particular feature of a software system. Our semi-automated tool can identify features that must be revised to enhance sustainability achievement. © 2020 World Scientific Publishing Company.",machine learning; software-based approach; Sustainability assessment,"Saputri T.R.D., Lee S.-W.",2020,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194020500047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080937041&doi=10.1142%2fS0218194020500047&partnerID=40&md5=72dcbea755d4effc6257f5f34efb6093,"Department of Computer Engineering, Ajou University, Suwon, 16499, South Korea",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,New factors affecting productivity of the software factory,"Productivity is very important because it allows organizations to achieve greater efficiency and effectiveness in their activities; however, it is affected by numerous factors. While these factors have been identified for over two decades, all of the previous works limited the software factory to the programming work unit and did not analyze other work units that are also relevant. 90% of a software factory’s effort is absorbed by the software production component, 85% of which is concentrated in the efforts of the analysis and design, programming, and testing work units. The present work identifies three new factors that influence the software factory, demonstrating that the use of rules and events influences analysis & design, team heterogeneity negatively affects analysis and design and positively affects programming; and the osmotic communication affects programming. An empirical study on software factories in Peru, determined that 95% of the influence came from these factors, which corroborated as well that team size and trust within the team influences in software production. Copyright © 2020, IGI Global.",Factors; Heterogeneity; Osmotic Communication; Productivity; Rules; Software Factory; Software Production; Trust,"Castañeda P., Mauricio D.",2020,Journal,International Journal of Information Technologies and Systems Approach,10.4018/IJITSA.2020010101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074487967&doi=10.4018%2fIJITSA.2020010101&partnerID=40&md5=7c6f73f6f8c8cbca8c677a5870a0009f,"Universidad Peruana de Ciencias Aplicadas (UPC), National University of San Marcos (UNMSM), Lima, Peru; National University of San Marcos, Lima, Peru",IGI Global,English,1935570X,
Scopus,A novel technique of software cost estimation using flower pollination algorithm,"Estimating Cost of Software (ECS) is a very essential aspect of a software development life cycle. Accurate estimations are needed in terms of person month and development time for software projects. Several project estimation techniques have been developed such as parametric and non-parametric. Estimation by parametric is one of the convenient methods in the field of software engineering. However, parametric approaches used in the development of software cost estimation is not able to handle the definite data in an explicit and accurate way. Different computational intelligence algorithms like machine learning, evolutionary algorithms, and swarm intelligence algorithms have been applied to optimize the parameters of various ECS model. But however, accurate cost estimation is still a big issue in ECS. In this work, a Flower Pollination Algorithm (FPA) is proposed to optimize the parameters of a Constructive Cost Model II (COCOMO-II) via using a standard Turkish industry dataset. Experimental results demonstrate that the proposed algorithm gives a better estimation as compared to existing approaches such as the Bat algorithm and original COCOMO-II in terms of Manhattan distance (MD) and mean magnitude of relative errors (MMRE). © 2019 IEEE.",COCOMO-II; Computational intelligence algorithms; Estimating cost of software (ECS); Turkish industry software project,"Ullah A., Wang B., Sheng J., Long J., Asim M., Riaz F.",2019,Conference,"Proceedings - 2019 International Conference on Intelligent Computing, Automation and Systems, ICICAS 2019",10.1109/ICICAS48597.2019.00142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083452083&doi=10.1109%2fICICAS48597.2019.00142&partnerID=40&md5=7e967bfc1e1daa4a4b913e9c2804a16b,"School of Computer Science and Engineering, Central South University, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781728161068
Scopus,An empirical investigation on requirements change management practices in Pakistani Agile based industry,"Requirements engineering is the key process of software development. Effectively dealing with change in requirements is a challenge. Agile methods are flexible and accept change effectively. The question arises in our mind that which requirements change management practices are the utmost to follow? Through a questionnaire-based survey, we explored the Pakistani software development industry to identify the processes, methods, practices being followed by the agile practitioners for requirements change management, and ranked them based on the perceived importance. We identified 30 practices for agile requirements change management from literature study, and conducted a survey with 140 agile practitioners, countrywide. We used a Multi-criteria Decision Method (MCDM) ranking method i.e. PROMETHEE family of methods for ranking of the relative importance of the practices. The findings reveal the most important and highly ranked agile requirements change management practices. Moreover, in numerous cases, the perceived importance of the change management practices depends on the type of the software project (e.g. methodology, domain and type of application or project). © 2019 IEEE.",Agile methods; Agile requirements change management; Requirements change management.; Requirements engineering,"Batool K., Inayat I.",2019,Conference,"Proceedings - 2019 International Conference on Frontiers of Information Technology, FIT 2019",10.1109/FIT47737.2019.00012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080123691&doi=10.1109%2fFIT47737.2019.00012&partnerID=40&md5=4f293a205d401f860541c743a996d2c7,"Software Engineering and Automation lab (SEAL), Department of Computer Science, National University of Computer and Emerging Sciences, Islamabad, Pakistan",Institute of Electrical and Electronics Engineers Inc.,English,,9781728166254
Scopus,Engineering Effort Estimation for Product Development Projects,"Cost estimation is an essential process for gaining competitive advantage in the bidding phase of every project. Besides, cost estimates are also vital for effective project control. For product development projects engineering hours is the main cost item and hence estimating engineering hours for potential product development projects is an important task. In this paper a case study, which is carried out at one of the leading manufacturers and suppliers of tracked and wheeled armored vehicles and weapon systems for the Turkish and Allied Armed Forces, is presented. In the company, currently there is no structured method for this purpose. The aim of this study is to create a method for estimating design engineering hours for potential projects based on available past data. Regression tree and k-Nearest Neighbor (k-NN) algorithm using Attribute Weighted Value Difference Metric methods are used for this purpose and the results are reported and discussed. © 2019 IEEE.",cost; engineering-effort; estimation; product-development,"Yurt Z.O., Iyigun C., Bakal P.",2019,Conference,IEEE International Conference on Industrial Engineering and Engineering Management,10.1109/IEEM44572.2019.8978764,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079662433&doi=10.1109%2fIEEM44572.2019.8978764&partnerID=40&md5=8ee7e01c61572c4051743dd4ddda6e7a,"FNSS Savunma Sistemleri A.Ş, Ankara, Turkey; Middle East Technical University, Department of Industrial Engineering, Ankara, Turkey",IEEE Computer Society,English,21573611,9781728138046
Scopus,Case based reasoning for optimal assignment of software development tasks [Razonamiento basado en casos para asignación óptima de tareas de desarrollo de software],"Project management means much more than dividing the work to assign parts to different people. Often, projects that may have been successful fail because of the assumptions that project management only involves dividing work into less complex tasks, leaving aside other important aspects. This paper presents ARDe (Developer Recommendation Assistant), a tool based on Case-Based Reasoning to build a solution that assists in making decisions about which members of the project team to select to perform the tasks established during project planning. As a result of various experiments, we found that the estimates resemble the actual metrics obtained from the execution and that ARDe also learns to order the recommendations according to the preferences of the team leader to select a developer. The latter is an important factor that helps mitigate the changes that occur before the departure of a team leader. © 2019, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.",Case based reasoning; Planning; Project management; Software metrics,"Rodríguez G., Berdun L., Araujo L.D.R.",2019,Journal,RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao,10.17013/risti.35.116–131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078348264&doi=10.17013%2fristi.35.116%e2%80%93131&partnerID=40&md5=17e98980fb16a927357994324ddb63fd,"ISISTAN (UNICEN-CONICET), Campus Universitario – Paraje Arroyo Seco, Tandil, 7000, Argentina",Associacao Iberica de Sistemas e Tecnologias de Informacao,Spanish,16469895,
Scopus,Combining data analytics and developers feedback for identifying reasons of inaccurate estimations in agile software development,"Background: Effort estimations are critical tasks greatly influencing the accomplishment of software projects. Despite their recognized relevance, little is yet known what indicators for inaccurate estimations exist, and which are the reasons of inaccurate estimations. Aims: In this manuscript, we aim at contributing to this existing gap. To this end, we implemented a tool that combines data analytics and developers’ feedback, and we employed that tool in a study. In that study, we explored the most common reasons of inaccurate user story estimations and the possible indicators of inaccurate estimations. Method: We relied on a mixed method approach used to study reasons and indicators for the identification and prediction of inaccurate estimations in practical agile software development contexts. Results: Our results add to the existing body of knowledge in multiple ways. We elaborate causes for inaccurate estimations going beyond the borders of existing literature; for instance, we show that lack of developers’ experience is the most common reason of inaccurate estimations. Further, our results suggest, for example, that the higher the complexity, the higher the uncertainty in the estimation. Conclusions: Overall, our results strengthen our confidence in the usefulness of using data analytics with human-in-the-loop mechanisms to improve effort estimations. © 2019 Elsevier Inc.",Agile methods; Data analytics; Empirical software engineering; Estimations; Mixed methods,"Conoscenti M., Besner V., Vetrò A., Fernández D.M.",2019,Journal,Journal of Systems and Software,10.1016/j.jss.2019.06.075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067949072&doi=10.1016%2fj.jss.2019.06.075&partnerID=40&md5=2a76ab2061c78a8115ece5d1248314f4,"Nexa Center for Internet & Society, DAUIN, Politecnico di Torino, Italy; Technical University of Munich, Germany",Elsevier Inc.,English,01641212,
Scopus,Future-proof software-systems: A sustainable evolution strategy,"This book focuses on software architecture and the value of architecture in the development of long-lived, mission-critical, trustworthy software-systems. The author introduces and demonstrates the powerful strategy of ""Managed Evolution,"" along with the engineering best practice known as ""Principle-based Architecting."" The book examines in detail architecture principles for e.g., Business Value, Changeability, Resilience, and Dependability. The author argues that the software development community has a strong responsibility to produce and operate useful, dependable, and trustworthy software. Software should at the same time provide business value and guarantee many quality-of-service properties, including security, safety, performance, and integrity. As Dr. Furrer states, ""Producing dependable software is a balancing act between investing in the implementation of business functionality and investing in the quality-of-service properties of the software-systems."" The book presents extensive coverage of such concepts as: Principle-Based Architecting. Managed Evolution Strategy. The Future. Principles for Business Value. Legacy Software Modernization/Migration. Architecture Principles for Changeability. Architecture Principles for Resilience. Architecture Principles for Dependability. The text is supplemented with numerous figures, tables, examples and illustrative quotations. Future-Proof Software-Systems provides a set of good engineering practices, devised for integration into most software development processes dedicated to the creation of software-systems that incorporate Managed Evolution. © Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2019. All right reserved.",,Furrer F.J.,2019,Book,Future-Proof Software-Systems: A Sustainable Evolution Strategy,10.1007/978-3-658-19938-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084275412&doi=10.1007%2f978-3-658-19938-8&partnerID=40&md5=71c21093dadb63d6cee4c5bd55f82028,"Computer Science Faculty, Technical University of Dresden, Dresden, Germany",Springer Fachmedien Wiesbaden,English,,9783658199388; 9783658199371
Scopus,Factors affecting software development productivity: An empirical study,"The competitiveness has demanded from the software industry shorter delivery times for its products resulting in optimized life cycles, generating a need to increase its performance to maintain competitiveness in the markets where they operate. This context has made productivity study so fundamental that organizations not only evaluate their performance, but also provide means to improve it. The main goal of this paper is to investigate which factors affect productivity in software development projects and in open-source projects. In this work a Systematic Literature Review (SLR) was carried out in order to answer the research questions and a survey with practitioners community about their perception in relation to the factors of the productivity of the team. This empirical study led to the discovery of interesting factors that show how the different factors do (or do not) affect productivity. It was also found out that some factors appear to allow independence and responsibility of team, while others appear to cause a better distribution of tasks. The results show how factors such as people, product, organization, investment in technology, lack of contractual relations and engagement of open-source project contributors influence productivity. © 2019 Association for Computing Machinery.",Empirical study; Influence Factors; Measurement; Metrics; Productivity,"Canedo E.D., Santos G.A.",2019,Conference,ACM International Conference Proceeding Series,10.1145/3350768.3352491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073226007&doi=10.1145%2f3350768.3352491&partnerID=40&md5=9528f76e114e4fef5c237aa60e32a8d8,"Department of Computer Science, University of Brasília (UnB), P.O. Box 4466, Brasília-DF, Brazil",Association for Computing Machinery,English,,9781450376518
Scopus,Evaluating imputation methods to improve data availability in a software estimation dataset,"Missing of partial data is a problem that is prevalent in most of the datasets used for statistical analysis. In this study, we analyzed the missing values in ISBSG R1 2018 dataset and addressed the problem through imputation, a machine learning technique which can increase the availability of data. Additionally, we compare the performance of three imputation methods: Classification and Regression Trees (CART), Polynomial Regression (PR), Predictive Mean Matching (PMM), and Random Forest (RF) applied to ISBSG R1 2018 dataset available from International Standards Benchmarks Group. Through imputation, we were able to increase data availability by four times. We also evaluated the performance of these methods against the original dataset without imputation using an ensemble of Linear Regression, Gradient Boosting, Random Forest, and ANN. Imputation using CART can increase the availability of the overall dataset but only at the loss of some predictive capability of the model. However, CART remains the option of choice to extend the usability of the data by retaining rows that are otherwise removed from the dataset in traditional methods. In our experiments, this approach has been able to increase the usability of the original dataset to 63%, but with 2 to 3% decrease in its overall predictive performance. ©BEIESP.",Artificial Neural Networks; Effort Prediction; Ensemble Models; Generalized Linear Model; Gradient Boosting Machines; Missing Data Imputation; Random Forests; Software Cost Estimation; Software Effort Estimation,"Pillai S.P., Radha Ramanan T., Madhu Kumar S.D.",2019,Journal,International Journal of Recent Technology and Engineering,10.35940/ijrte.B1025.0982S1119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074439766&doi=10.35940%2fijrte.B1025.0982S1119&partnerID=40&md5=08e6600558d00abe64f6f49ef3d1bf91,"School of Management Studies, NIT Calicut, Kozhikode, Kerala, India; Dept. of Computer Science, NIT Calicut, Kozhikode, Kerala, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22773878,
Scopus,Handling of categorical data in software development effort estimation: A systematic mapping study,"Producing reliable and accurate estimates of software effort remains a difficult task in software project management, especially at the early stages of the software life cycle where the information available is more categorical than numerical. In this paper, we conducted a systematic mapping study of papers dealing with categorical data in software development effort estimation. In total, 27 papers were identified from 1997 to January 2019. The selected studies were analyzed and classified according to eight criteria: publication channels, year of publication, research approach, contribution type, SDEE technique, Technique used to handle categorical data, types of categorical data and datasets used. The results showed that most of the selected papers investigate the use of both nominal and ordinal data. Furthermore, Euclidean distance, fuzzy logic, and fuzzy clustering techniques were the most used techniques to handle categorical data using analogy. Using regression, most papers employed ANOVA and combination of categories. © 2019 Polish Information Processing Society - as since.",,"Amazal F.A., Idri A.",2019,Conference,"Proceedings of the 2019 Federated Conference on Computer Science and Information Systems, FedCSIS 2019",10.15439/2019F222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074160747&doi=10.15439%2f2019F222&partnerID=40&md5=5cec31c0ece0e86cab4b44b32f889151,"LabSIV, Department of Computer Science, Faculty of Science, Ibn Zohr University, BP, 8106, Agadir, 80000, Morocco; Software Projects Management Research Team, ENSIAS, Mohamed v University, Madinate Al Irfane, Rabat, 10100, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9788395541605
Scopus,Application of bayesian regularization algorithm for evaluation of performance software complexity prediction model based on requirement,"Model performance evaluation is a method and process of evaluating the model that has been built. The model that will be evaluated is software complexity prediction model based on requirement. This model allows measuring software complexity before actual design and implementation. The experiment used three datasets namely training dataset, validation data set, and test dataset. For performance evaluation using Mean squared error. Mean squared error is very good at giving a description of how consistent the model is built. By minimizing the value of mean squared error, it means minimizing model variants. Models that have small variants are able to give relatively more consistent results for all input data compared to models with large variants. This research proposes the application of the Bayesian regularization algorithm for evaluating the performance of software complexity prediction model based on requirement. With this research it is expected to know how much the performance of the model that has been built. © BEIESP.",Algorithm; Complexity; Prediction; Software,"Wartika, Gaol F.L., Nugroho A., Abbas B.S.",2019,Journal,International Journal of Recent Technology and Engineering,10.35940/ijrte.C4715.098319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073501256&doi=10.35940%2fijrte.C4715.098319&partnerID=40&md5=6db051fd625342737b1e46d6ea4d82e2,"Binus University, Indonesia; BTPN Bank",Blue Eyes Intelligence Engineering and Sciences Publication,English,22773878,
Scopus,Crowdsourcing and probabilistic decision-making in software engineering: Emerging research and opportunities,"With today's technological advancements, the evolution of software has led to various challenges regarding mass markets and crowds. High quality processing must be capable of handling large groups in an efficient manner without error. Solutions that have been applied include artificial intelligence and natural language processing, but extensive research in this area has yet to be undertaken. Crowdsourcing and Probabilistic Decision-Making in Software Engineering: Emerging Research and Opportunities is a pivotal reference source that provides vital research on the application of crowd-based software engineering and supports software engineers who want to improve the manner in which software is developed by increasing the accuracy of probabilistic reasoning to support their decision-making and getting automation support. While highlighting topics such as modeling techniques and programming practices, this publication is ideally designed for software developers, software engineers, computer engineers, executives, professionals, and researchers. © 2020 by IGI Global. All rights reserved.",,Gupta V.,2019,Book,Crowdsourcing and Probabilistic Decision-Making in Software Engineering: Emerging Research and Opportunities,10.4018/978-1-5225-9659-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077730583&doi=10.4018%2f978-1-5225-9659-2&partnerID=40&md5=ac18ccbe6ec9a93fa5795acd4aae4fac,"University of Beira Interior, Covilha, Portugal",IGI Global,English,,9781522596615; 1522596593; 9781799810575
Scopus,Costing secure software development - A systematic mapping study,"Building more secure software is a recent concern for software engineers due to increasing incidences of data breaches and other types of cyber attacks. However, software security, through the introduction of specialized practices in the software development life cycle, leads to an increase in the development cost. Although there are many studies on software cost models, few address the additional costs required to build secure software. We conducted a systematic review in the form of a mapping study to classify and analyze the literature related to the impact of security in software development costs. Our search strategy strove to achieve high completeness by the identification of a quasi-gold-standard set of papers, which we then used to establish a search string and retrieve papers from research databases automatically. The application of inclusion/exclusion criteria resulted in a final set of 54 papers, which were categorized according to the approach to software security cost analysis. Perform Security Review, Apply Threat Modeling, and Perform Security Testing were the three most frequent activities related to cost, and Common Criteria was the most applied standard. We also identified ten approaches to estimating software security costs for development projects; however, their validation remains a challenge, which could be addressed in future studies. © 2019 Association for Computing Machinery.",Secure software development; Software cost model; Software development effort estimation; Software security,"Venson E., Guo X., Yan Z., Boehm B.",2019,Conference,ACM International Conference Proceeding Series,10.1145/3339252.3339263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071718485&doi=10.1145%2f3339252.3339263&partnerID=40&md5=0feedf6f3d1a079645bd08c62c1a44bf,"Center for Systems and Software Engineering, University of Southern California, United States; University of Southern California, United States",Association for Computing Machinery,English,,9781450371643
Scopus,Predicting pull request completion time: A case study on large scale cloud services,"Effort estimation models have been long studied in software engineering research. Effort estimation models help organizations and individuals plan and track progress of their software projects and individual tasks to help plan delivery milestones better. Towards this end, there is a large body of work that has been done on effort estimation for projects but little work on an individual checkin (Pull Request) level. In this paper we present a methodology that provides effort estimates on individual developer check-ins which is displayed to developers to help them track their work items. Given the cloud development infrastructure pervasive in companies, it has enabled us to deploy our Pull Request Lifetime prediction system to several thousand developers across multiple software families. We observe from our deployment that the pull request lifetime prediction system conservatively helps save 44.61% of the developer time by accelerating Pull Requests to completion. © 2019 ACM.",Case Studies; Effort Estimation; Empirical Studies; Prediction; Software Metrics,"Maddila C., Bansal C., Nagappan N.",2019,Conference,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,10.1145/3338906.3340457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071919194&doi=10.1145%2f3338906.3340457&partnerID=40&md5=cbe87c0ed5d8686d68f04ab39e2d2344,"Microsoft Research, Redmond, WA, United States","Association for Computing Machinery, Inc",English,,9781450355728
Scopus,Analogy Software Effort Estimation Using Ensemble KNN Imputation,"Missing data are a serious issue that influences the prediction accuracy of software development effort estimation (SDEE) techniques and especially analogy-based software effort estimation (ASEE). Hence, appropriate handling of missing data is necessary in order to ensure best performance. To deal with this issue K-nearest neighbors (KNN) imputation has been widely used. However, none of the studies investigating KNN imputation in SDEE have addressed the impact of parameter settings on the imputation process given that parameter optimization techniques are often used at the prediction level, as they highly impact the performance of SDEE techniques including ASEE. This paper proposes and evaluates an ensemble KNN imputation technique for ASEE. Thereafter, we compare ASEE performance using ensemble KNN imputation with those using either a grid search based single KNN imputation or KNN imputation without parameter optimization. For the six datasets used for comparison, the ensemble KNN imputation significantly improved ASEE performance compared with KNN imputation without optimization. Moreover, ensemble KNN imputation and grid search-based imputation behaved similarly. Given that grid search is time consuming, the ensemble KNN imputation may be an alternative to deal with missing data in the ASEE process. © 2019 IEEE.",Analogy-based-software-efoort-estimation; ensemble; grid-search; missing-data; parameter-optimisation,"Abnane I., Hosni M., Idri A., Abran A.",2019,Conference,"Proceedings - 45th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2019",10.1109/SEAA.2019.00044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075965838&doi=10.1109%2fSEAA.2019.00044&partnerID=40&md5=c5db2039dc213d82081b36642f22a7ab,"Software Project Management Research Team, ENSIAS, University Mohammed v, Rabat, Morocco; Dept. of Software Engineering and Information Technology, ETS, University of Quebec, Montreal, Canada",Institute of Electrical and Electronics Engineers Inc.,English,,9781728132853
Scopus,Analyzing Software Architecture Evolvability Based on Multiple Architectural Attributes Measurements,"When the erosion of software architecture occurs, there is an increase in software maintenance costs, a decrease in software quality, and degradation of software performance, etc. Therefore, it is particularly crucial to find a feasible way to evaluate software architecture to detect and avoid the erosion of software architecture in a timely manner. Through empirical study, we find that software architecture (SA) evolvability is one of the critical causes that leads to the erosion of software architecture. In this paper, we propose an approach to analyze SA evolvability based on multiple architectural attribute measurements and further solve the above problems in software architecture evolution. Our approach consists of the following steps: first, according to the evolutionary process, we propose four corresponding architectural attributes; second, these attributes are measured based on basic information and dependency information; third, SA evolvability is measured based on multiple architectural attribute measurements. Our experiments are conducted on thirteen Java open-source projects to verify the effectiveness of our approach. The experimental results show that our approach can effectively reflect the SA evolvability from the following two aspects: A single attribute can reflect a specific aspect of the SA evolvability; the composition of attributes can reflect the composite SA evolvability. Furthermore, we can locate the causes of the erosion of SA by combining the measurements and the evolutionary activities, and we further propose evolutionary proposals to improve the SA evolvability. © 2019 IEEE.",architectural attributes measurements; software architecture evolution; Software architecture evolvability,"Wang T., Li B.",2019,Conference,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",10.1109/QRS.2019.00037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073807061&doi=10.1109%2fQRS.2019.00037&partnerID=40&md5=9c5f7803b2d5fa0f67185af316bb4097,"School of Computer Science and Engineering, Southeast University, Nanjing, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781728139272
Scopus,Automated estimation of predictive object points metric values for object-oriented code,"If we are to improve the OO software quality we develop, we must measure our designs by well-defined standards. Possible problems in our system designs can be detected during the development process. If we are to estimate and manage our efforts, we must measure our progress effectively. An object-oriented coding scheme is, in some ways, incompatible with ancient metrics. To measure the size of software system, a variety of techniques for code development estimation exist like SLOC and Function Point, SLOC, as a metric has a number of drawbacks one is SLOC, is not consistent across languages, applications, or developing environments, however, it cannot applied on object-oriented code. The efforts required to develop the code is being calculated using the Predictive Object Point (POP) metric. This counting technique is supported by the Function Point Technique. This paper address the way of effort calculation for object oriented code using POP. To measure the POP accurately, an automated tool has been developed and designed. The results of the tools has also been discussed in the paper. © BEIESP.",And improvement; Automation; Code estimation technique; Object-oriented metrics; Predictive object point metric; Software tools; System metric tools,"Yadav V., Yadav V., Singh R.",2019,Journal,International Journal of Recent Technology and Engineering,10.35940/ijrte.B1165.0782S619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073467747&doi=10.35940%2fijrte.B1165.0782S619&partnerID=40&md5=1cb971890b5eaabb9acab99739852944,"A.K.T.U, Lucknow, India; R.E.C, Banda, India; H.B.T.U, Kanpur, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22773878,
Scopus,PLHIM: Proposing an inheritance based object-oriented metric,"Software metrics has been utilized to evaluate inheritance as well as to assist the designer in order to focus on product quality as well as cost estimation in all the lifecycle stage of development of the final product. To pertain measurement through the diverse level of class hierarchy, a person can evaluate inheritance with reuse, to acquire the best computation of abstraction levels of a object oriented system. In our paper, a new metric of hierarchical inheritance is proposed that measures the quality of the program through different levels of Object-Orientedness, and we named it PLHIM: Per Level of Hierarchical Inheritance Metric. The main idea behind proposed metrics and research work was to make use of measurement as a criterion for improvement in software development at different levels to minimize risk and this has been done by taking the problems of C++ and Java. © BEIESP.",Abstraction; Inheritance metric; Object-oriented metrics; PLHIM; Reuse; Software development,"Kharb L., Chahal D.",2019,Journal,International Journal of Innovative Technology and Exploring Engineering,10.35940/ijitee.I1101.0789S419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073418218&doi=10.35940%2fijitee.I1101.0789S419&partnerID=40&md5=726000cc36b72a891f9e1b6456284001,"Jagan Institute of Management Studies, Delhi, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22783075,
Scopus,In search of scientific agile,"Over the last twenty years, authors of systematic literature reviews have repeatedly criticized the state of agile research, describing it as nascent, random, and lacking appropriate recruitment strategies. This paper highlights some historical misperceptions and assumptions that tend to misdirect researchers, especially those new to the field. It also calls attention to some challenges intrinsic to agile research and comments on how those challenges impact research design. The intent is to help new researchers more easily make sense of the existing literature, so that they can focus their energy and resources on meaningful contributions to the field. © 2019 IEEE.",Agile; Contingent design; Kanban; Paradigm shift; Scrum,"Ward R., Chang C.K.",2019,Conference,Proceedings - International Computer Software and Applications Conference,10.1109/COMPSAC.2019.00018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072702148&doi=10.1109%2fCOMPSAC.2019.00018&partnerID=40&md5=b1c144611ba0e93de9dbb2e953e54450,"Department of Computer Science, Iowa State University, Ames, IA, United States",IEEE Computer Society,English,07303157,9781728126074
Scopus,DRCE maintainability model for component based systems using soft computing techniques,"Effective software maintainability is one of the most significant and challenging activity in the field of component based software. Several maintainability models are proposed by the researchers to reduce the maintenance cost, to improve the quality and life span of the software product. The proposed model will assist the software designers to develop maintainable softwares. This paper discusses a maintainability model, which selects four crucial factors that highly affect maintainability of component based software system. Soft computing techniques are employed to demonstrate strong correlation of these factors with maintainability. MATLAB’s Fuzzy logic toolbox is used for predicting the maintainability level of component (such as Excellent, Fair, Good, Bad and worst). Data generated by fuzzy model are provided as input to artificial neural network model. Experimental results shows mean absolute error (MAE) to be .028 and Relative Error (RE) to be .045.To further improve the performance of the model; neuro-fuzzy tool was employed. With the use of self learning capability of this tool, MAE and RE are now improved to the value .0029 and .039. It means that the model was sound enough to provide satisfactory outcomes in comparison to neural network. © BEIESP.",Component based system; Coupling; Document quality; Extensibility; Maintainability; MATLAB fuzzy logic; Quality models; Reusability,"Narang K., Goswami P.",2019,Journal,International Journal of Innovative Technology and Exploring Engineering,10.35940/ijitee.i8245.078919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070186070&doi=10.35940%2fijitee.i8245.078919&partnerID=40&md5=37706c3b503710da4a5716d55615d6dc,"Department of Computer science and Engineering, SRM UniversityHaryana, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22783075,
Scopus,Factors Affecting Development Process in Small Software Companies,"The importance of small software companies (SSC) cannot be ignored in the general software industry because SSCs represent up to 95% of all software companies worldwide and contribute significantly to the world economy. However, quality of software is a big challenge especially to SSCs and efforts to improve software processes have mainly targeted big companies. The paper focuses on a systematic literature review (SLR) to identify factors affecting the development process of SSCs with the ultimate focus being on companies from the African continent. The results of the study indicate that factors such as organizational, governance and business environment are commonly cited factors which affect the development process of SSCs. © 2019 IEEE.",Factors affecting development process; Small software companies; Software development process; Software process improvement,"Tuape M., Ayalew Y.",2019,Conference,"Proceedings - 2019 IEEE/ACM Symposium on Software Engineering in Africa, SEiA 2019",10.1109/SEiA.2019.00011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073054578&doi=10.1109%2fSEiA.2019.00011&partnerID=40&md5=3f592f810e01076d01a3f349a6b19cbd,"Department of Computer Science, University of Botswana, Gaborone, Botswana",Institute of Electrical and Electronics Engineers Inc.,English,,9781728122786
Scopus,The quest for productivity in software engineering: A practitioners systematic literature review,"Software productivity is perceived by practitioners as one of the most important subjects of Software Engineering (SE), because it establishes a connection between technical and economic concerns. Nonetheless, software processes are complex and productivity means different things to different people. In order to realize the full contribution of productivity research to the practice of SE, the compilation and analysis of the diverse practitioner viewpoints and concerns is required. In this paper, we develop a systematic literature review to confirm the existence of different empirical perceptions of productivity from the distinct business sectors and knowledge areas covered in practice by SE, identifying also commonalities that may exist. This review was compiled by analyzing 73 papers on empirical studies published from 1987 to 2017. The review found great variability of study findings, particularly concerning the impacts of agile and hybrid development practices on software productivity, and research gaps that could be investigated in the future. © 2019 IEEE.",Empirical Studies; Software Productivity; Systematic Literature Reviews,Duarte C.H.,2019,Conference,"Proceedings - 2019 IEEE/ACM International Conference on Software and System Processes, ICSSP 2019",10.1109/ICSSP.2019.00027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072315749&doi=10.1109%2fICSSP.2019.00027&partnerID=40&md5=fdd6a007c62bc41b236992826967d329,"BNDES, Av. Chile 100, Rio de Janeiro, RJ, 20001-970, Brazil",Institute of Electrical and Electronics Engineers Inc.,English,,9781728133935
Scopus,Process-driven incremental effort estimation,"Effort estimation has shown its value in process decisions, such as feasibility analysis, resource allocation, risk mitigation, and project planning. In this paper, we propose an incremental effort estimation method that integrates phase-based effort estimation models to solve the concerns about software process compatibility and estimation accuracy, which one may often have when adopting effort estimation methods for project management. We define the process framework for incremental effort estimation in terms of the transition between the phase model that defines the analysis and design activities, the system models that specify a system's behavior and structure at different levels of detail, the sizing model that measures software functional size via transaction analysis, the Bayesian model that statistically models the effects that the size measurements have on project effort, and the phase-based effort estimation models that provide improved effort estimation accuracy over the targeted early phases. The phase-based effort estimation models are calibrated and evaluated based on an empirical dataset of 61 master-level student projects from USC CSSE. The evaluation results show their improvements in out-of-sample estimation accuracy, provide a perspective about how estimation accuracy evolves throughout a software process, and set the practical criteria to decide the investment in analysis and design activities for the return in estimation accuracy. © 2019 IEEE.",Bayesian-analysis; Effort-estimation; Model-calibration; Software-functional-size-analysis; Software-size-metrics; Terms - Software-process-management-and-improvement,"Qi K., Boehm B.W.",2019,Conference,"Proceedings - 2019 IEEE/ACM International Conference on Software and System Processes, ICSSP 2019",10.1109/ICSSP.2019.00030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072312232&doi=10.1109%2fICSSP.2019.00030&partnerID=40&md5=e1fbf45c321f8e06f1c705ad1983deb7,"University of Southern California, United States",Institute of Electrical and Electronics Engineers Inc.,English,,9781728133935
Scopus,ISENSE: Completion-Aware Crowdtesting Management,"Crowdtesting has become an effective alternative to traditional testing, especially for mobile applications. However, crowdtesting is hard to manage in nature. Given the complexity of mobile applications and unpredictability of distributed crowdtesting processes, it is difficult to estimate (a) remaining number of bugs yet to be detected or (b) required cost to find those bugs. Experience-based decisions may result in ineffective crowdtesting processes, e.g., there is an average of 32% wasteful spending in current crowdtesting practices. This paper aims at exploring automated decision support to effectively manage crowdtesting processes. It proposes an approach named ISENSE which applies incremental sampling technique to process crowdtesting reports arriving in chronological order, organizes them into fixed-size groups as dynamic inputs, and predicts two test completion indicators in an incremental manner. The two indicators are: 1) total number of bugs predicted with Capture-ReCapture model, and 2) required test cost for achieving certain test objectives predicted with AutoRegressive Integrated Moving Average model. The evaluation of ISENSE is conducted on 46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting platforms in China. Its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi-automation of task closing trade-off analysis. The results show that ISENSE can provide managers with greater awareness of testing progress to achieve cost-effectiveness gains of crowdtesting. Specifically, a median of 100% bugs can be detected with 30% saved cost based on the automated close prediction. © 2019 IEEE.",automated close prediction; Crowdtesting; crowdtesting management; test completion,"Wang J., Yang Y., Krishna R., Menzies T., Wang Q.",2019,Conference,Proceedings - International Conference on Software Engineering,10.1109/ICSE.2019.00097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072274408&doi=10.1109%2fICSE.2019.00097&partnerID=40&md5=1e2df2a3306cd5cf22676f63e4ae4207,"Laboratory for Internet Software Technologie,s; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, United States; Department of Computer Science, North Carolina State University, Raleigh, NC, United States",IEEE Computer Society,English,02705257,9781728108698
Scopus,A Metrics Tracking Program for Promoting High-Quality Software Development,"There has been substantial focus on software metrics over the last few decades. However, many activities within software engineering are often qualitative and are not consonant with automated approaches. Consequently, there are few tools to measure software development quality or to assess teamwork contribution. This paper uses ideas from the Goal-Questions-Metrics (GQM) paradigm to propose a set of metrics to track product and process quality throughout the software development process. The proposed metrics program consists of a set of quality metrics and associated standards that will encourage software development teams to produce high-quality products. We also propose a framework for a tool that implements the metrics tracking program and demonstrate its utility by developing an Eclipse plugin based on the proposed quality metrics. © 2019 IEEE.",assessment; coding standards; documentation standards; software engineering; software measurement; software metrics; software quality,"Slhoub K., Nembhard F., Carvalho M.",2019,Conference,Conference Proceedings - IEEE SOUTHEASTCON,10.1109/SoutheastCon42311.2019.9020395,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082392348&doi=10.1109%2fSoutheastCon42311.2019.9020395&partnerID=40&md5=adef063efb967abc3cc75490678ae60d,"Florida Institute of Technology, College of Engineering and Sciences, Melbourne, FL, United States",Institute of Electrical and Electronics Engineers Inc.,English,07347502,9781728101378
Scopus,An efficient approach for software maintenance effort estimation using particle Swarm Optimization Technique,"The main objective of software engineering community is to develop useful models that are able to calculate the accurate estimating software effort. COCOMO (Constructive Cost Model) is consider as mostly used algorithmic maintenance cost modeling technique among other software maintenance cost estimation techniques. It is mostly used technique due to its simplicity for estimating the effort in person-months for a project at different stages. In this paper we have proposed a new approach that is able to give better results. In proposed approach we have used Tomcat server dataset whose features are extracted using Principle component analysis approach which is further optimized using Particle Swarm Optimization. In previous work most of researchers have used Genetic algorithm but it is a time consuming part. So, in this paper we have used Particle swarm optimization that gives improved results. At the end we have used Linear discriminant analysis for classification that classifies the priority levels and tell how much your system is having the estimations for the cost based on Source lines of the codes or functional points or the efforts required. The proposed approach is tested in terms of functional point, set effort person per month and SLOC that gives best results. © BEIESP.",COCOMO; Functional point; LDA; PCA; PSO; Set effort per month; SLOC; Software engineering,"Singh C., Sharma N., Kumar N.",2019,Journal,International Journal of Recent Technology and Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065203414&partnerID=40&md5=f74cf3bd7dd6d7452ce18a2f51537ebb,"I.K. Gujral Punjab Technical University, Jalandhar, Punjab, India; Gian Jyoti Group of Institutions, Mohali, India; HNB Garhwal University, Srinagar Garhwal, Uttarakhand, India",Blue Eyes Intelligence Engineering and Sciences Publication,English,22773878,
Scopus,Analysis and comparison of neural network models for software development effort estimation,"Prediction of software development is the key task for the effective management of any software industry. The accuracy and reliability of the prediction mechanisms used for the estimation of software development effort is also important. A series of experiments are conducted to gradually progress towards the improved accurate estimation of the software development effort. However, while conducting these experiments, it was found that the size of the training set was not sufficient to train a large and complex artificial neural network (ANN). To overcome the problem of the size of the available training data set, a novel multilayered architecture based on a neural network model is proposed. The accuracy of the proposed multi-layered model is assessed using different criteria, which proves the pre-eminence of the proposed model. Copyright © 2019, IGI Global.",Feed-Forward Neural Network; Neural Networks; Radial Basis Function Neural Network; Regression Test; Software Development Effort Estimation,"Dutta K., Gupta V., Dave V.S.",2019,Journal,Journal of Cases on Information Technology,10.4018/JCIT.2019040106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061506538&doi=10.4018%2fJCIT.2019040106&partnerID=40&md5=9b8ec14c97eba2fe15a87d30f7e431bb,"Department of CSE, National Institute of Technology, Hamirpur, India; Amity School of Engineering and Technology, Amity University, Noida, India",IGI Global,English,15487717,
Scopus,Optimized machine learning model using Decision Tree for cancer prediction,"Decision making is used for the selection oriented process. But can we use it in the field of healthcare is the biggest question. The answer is yes here also we can have with machine learning (ML) classifiers like Decision Tree (DT), random forest (RF), support vector machine (SVM), and neural network (NN). In the case of breast cancer (BC) disease, we can predict using machine learning classifiers. For this, original BC samples experiment. Our model named clustering based feature selection (CFS) intends to increase the accuracy and to reduce the feature dimension. We experiment the classifiers in three stages: 1. all features of BC, 2. DT based cluster, and 3. Feature selection. In all feature, RF achieves 99.10% accuracy, in DT based cluster RF, achieves 99.37% accuracy, and finally in cluster-based feature selection (CFS) with only four features the RF achieves 99.50% accuracy. The experimental results of our CFS approach along with DT, RF, SVM, and NN classifier along with its performance are presented. © 2019 IEEE.",Breast cancer; Classification; Decision Tree; Machine learning; Support Vector Machine (SVM),"Chandrasegar T., Nikhilesh Vutukuri S.B.",2019,Conference,"2019 Innovations in Power and Advanced Computing Technologies, i-PACT 2019",10.1109/i-PACT44901.2019.8960129,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079012081&doi=10.1109%2fi-PACT44901.2019.8960129&partnerID=40&md5=20d53510cf1537f4e54ab6e9b5775de1,"SITE, VIT, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538681909
Scopus,Dimensionality reduction of a phishing attack using decision tree classifier,"Internet plays an important role in our day-to-day lives, and we are using many e-commerce websites. Cyber-criminals are using phishing attack technique to steal user's sensitive information like personal details, and bank details. It may lead to loss of both client and original seller. Our work involves the decision tree and best performing classifier to identify the significant dimension. The entropy features include SFH, pop up window, and URL of anchor. Through which, RF with 5 features at each split achieves 84.8% accuracy. In our work, RF with 6 features achieves 96.30% accuracy. However, the RF with all features performs only 93.20% accuracy. © 2019 IEEE.",Decision tree; Machine learning; Phishing detection; Random forest; Support vector machine,"Chandrasegar T., Viswanathan P.",2019,Conference,"2019 Innovations in Power and Advanced Computing Technologies, i-PACT 2019",10.1109/i-PACT44901.2019.8960117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079001504&doi=10.1109%2fi-PACT44901.2019.8960117&partnerID=40&md5=3144bcf421118b3b307d8a098070b45d,"VIT, SITE, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538681909
Scopus,Heart Disease Diagnosis using a Machine Learning Algorithm,"Machine Learning turns the extensive collection of raw healthcare data into information that can help to make informed decision and prediction. This research named fine-tune prediction (FTP) model aims to identify significant features and incorporate hybrid classifier to improve accuracy. For this, the Cleveland heart samples taken from the UCI repository. Experiment results show that the proposed FTP model achieves 93.49% accuracy. Without FTP, the RF, and LM achieve only 88.20% and 63.60% accuracy. We also present the hyper-parameter tuning of the classifier with significant features in the result section. © 2019 IEEE.",Decision tree; Heart disease diagnosis; hyper-parameter tuning; Machine learning; Random forest; Support vector machine,"Chandrasegar T., Choudhary A.",2019,Conference,"2019 Innovations in Power and Advanced Computing Technologies, i-PACT 2019",10.1109/i-PACT44901.2019.8959989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078988134&doi=10.1109%2fi-PACT44901.2019.8959989&partnerID=40&md5=cded1ae512a46a1eb4bd2e9459cd7057,"SITE, VIT, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538681909
Scopus,Empirical evaluation of an entropy-based approach to estimation variation of software development effort,"As effort estimation has gained increasing attention, most of the techniques proposed have focused on the accuracy of effort estimates. Yet no clear conclusions have been drawn on which techniques perform best in all contexts. We propose an entropy-based approach to effort estimate variation caused by measurement and model error sources whatever the effort estimation technique used. The proposed approach was empirically evaluated by exploring three entropy formulae, four interpolation methods, and two analogy-based effort estimation approaches (crisp and fuzzy analogy) over seven datasets using the Jackknife evaluation method. The obtained results show that the three entropy formulae have in general the same positive influence on the performance of the entropy-based approach measured in terms of absolute error of effort deviation. In addition, the spline interpolation outperformed all other interpolation methods, using any of the entropy formulae. Moreover, achievement percentages of the best variants of our approach closely approximated those of the Gaussian distribution confirming that the Gaussian distribution is useful for characterizing effort estimate variation. © 2018 John Wiley & Sons, Ltd.",effort estimation; entropy; error and estimation variation; Gaussian distribution; interpolation,"El Koutbi S., Idri A., Abran A.",2019,Journal,Journal of Software: Evolution and Process,10.1002/smr.2149,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063377815&doi=10.1002%2fsmr.2149&partnerID=40&md5=f8f11d4ac305f27600a3f8dec5af170c,"Software Projects Management Research Team, ENSIAS, University Mohamed V, Rabat, Morocco; Department of Software Engineering and I.T, Ecole de Technologie Supérieure, Montreal, Canada",John Wiley and Sons Ltd,English,20477481,
Scopus,A soft computing approach to optimize component based software complexity metrics,"Identification of software components is found to be a crucial task and remains as a challenging area in the software domain to extract optimal components from the component repository[18]. Several methods are deployed to identify the software components and observed that clustering based technique is frequently used and offer a solution with certain limitations such as prior specification of a set of clusters, overlapping and difficulty in selecting the correct distance metric. In this research, an optimization technique is applied on some components to get the best result. The genetic algorithm using the concept of number of chromosomes is applied on software complexity metrics such as Cohesion of Variables within a Component (COVC), Cohesion of Methods within a Component (COMC) and Total Cohesion Complexity of a Component (TCCC) which are proposed by Rana and Singh et.al [14]. Fitness function metrics is proposed for finding out the fitness value. Rana and Singh also empirically evaluate cohesion metrics(COVC, COMC, TCCC) and perform comparative analysis with existing metrics in another research paper [15]. In this paper genetic algorithm is applied on these metrics for optimization of results. After application of genetic algorithm, SPSS a statistic tool is applied on the result to find out the significance of result. The result obtained shows that better optimization of software metrics is obtained through application of genetic algorithm compared to without usage of genetic algorithm. © 2005 – ongoing JATIT & LLS.",Genetic algorithm; Maintainability; Reusability; Software cohesion; Software component metrics,"Rana P., Singh R.",2019,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063548342&partnerID=40&md5=e9d27d855c72ee1df5e5f49ade301070,"Department of Computer Science and Applications, M.D. University, Rohtak, India",Little Lion Scientific,English,19928645,
Scopus,Estimating the threshold of software metrics for web applications,"Estimating thresholds for software metrics is a key step towards assigning a quality index. In defect prediction, two approaches are widely used those based on statistics and, that which uses rigorous mathematical models. Although significant insights have been surmised, a general consensus on their results is still far from generalizations. In these perspectives, we attempt to check whether there exists any relationship between the two approaches. An empirical investigation is carried out in this work to study the relationship between estimated threshold values calculated at various risk levels using Bender’s approach and measures of central tendency using the Apache Click web application. The effect of these different threshold estimates on the performance of the developed defect prediction models is also studied and validated using different releases of the dataset. We find that the threshold indicator obtained from the representational models such as that due to Bender has an intricate relationship with the median value of the dataset. The close association between the model and statistical parameters mainly stems from the underlying characteristics of the data set itself. Descriptive statistical analysis of all Apache Click metrics dataset is found to be positively skewed, and hence median render the most relevant central measure for threshold estimation. Additionally, we also find that with increasing risk level, the threshold value subsequently shifts from median to mean value of the underlying metric data. Our preposition that the performance of the defect prediction model is best when threshold estimates are closer to the median is also verified with inter-version project comparison. © 2019, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.",Bender approach; Defect prediction; Logistic regression; Measures of central tendency; Threshold; Web applications,"Malhotra R., Sharma A.",2019,Journal,International Journal of Systems Assurance Engineering and Management,10.1007/s13198-019-00773-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063593305&doi=10.1007%2fs13198-019-00773-1&partnerID=40&md5=e33fc0b8b1e09b5040ff977b58f3b47c,"Delhi Technological University, Bawana Road, Delhi, Delhi  110042, India; National Physical Laboratory, Dr. K S Krishnan Marg, New Delhi, 110012, India",Springer,English,09756809,
Scopus,Data analysis in software effort estimation using improved delphi method,"Software Estimation Accuracy is one of the most difficult tasks for the software developers. Defining the project duration, effort estimation and estimated cost, early in the development phase is greatest challenge has to be achieved for software projects. Inaccurate effort estimation of software development is one of the most important reasons of computer and IT major project failures. Low effort estimates may lead to project management problems, delayed deliveries, budget overruns and poor software quality, too high effort estimates may lead to loss of business opportunities and improper and inefficient use of resources. The projects main focus at Simulate Research Laboratory is to improve judgment-based effort estimation methods, which is most frequently used by the software industries. By introducing better mental steps in effort estimation, we can achieve significant improvement in software development estimation processes. There are great challenge while studying expert judgment like Delphi Estimation. To understand the use of multidisciplinary competencies, especially financial resources enables studies in realistic software development process, psychology and software engineering. This paper explores the relationship between development effort, team size and software size. The main objective of this research is to improve the existing Delphi method for the estimation of software development effort using hybrid approach. Proposed, improved method has been validated by using 15 NASA project dataset and the results show that the improved Delphi method for software effort estimation resulted in slightly better as compared to results obtained earlier. © 2005 - ongoing JATIT & LLS.",Algorithmic model; Effort estimation; Mmre; Pred; Productivity; Variance,"Rai A., Gupta G.P., Kumar P.",2019,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062909489&partnerID=40&md5=41b254c650ea55b18e1305da6832a71e,"Department of Computer Science, Shia P.G. College, University of Lucknow, Uttar Pradesh, India; Department of Computer Science and Engineering, Shri Ramswaroop College of Engineering and Management, Uttar Pradesh, India",Little Lion Scientific,English,19928645,
Scopus,Software Architecture Social Debt: Managing the Incommunicability Factor,"Architectural technical debt is the additional project cost connected to technical issues nested in software architectures. Similarly, many practitioners have already experienced that there exists within software architectures a form of social debt, that is, the additional project cost connected to sociotechnical and organizational issues evident in or related to software architectures. This paper illustrates four recurrent antipatterns or community smells connected to such architectural social debt and outlines a means to measure the additional project cost connected to their underlying cause: decision incommunicability. Evaluating the results in multiple focus groups, this paper concludes that studying social debt and community smells at the architecture level may prove vital to rid software development communities of critical organizational flaws incurring considerable additional cost. © 2014 IEEE.",Social debt; social debt cost estimation; social debt in software architecting; technical debt,Tamburri D.A.,2019,Journal,IEEE Transactions on Computational Social Systems,10.1109/TCSS.2018.2886433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061655975&doi=10.1109%2fTCSS.2018.2886433&partnerID=40&md5=e6693e173e86e6f5ee46cff0ae2f0a06,"Jheronimus Academy of Data Science, Technische Universiteit Eindhoven, Eindhoven, 5612 AZ, Netherlands",Institute of Electrical and Electronics Engineers Inc.,English,2329924X,
Scopus,Extraction Cost of Quality and Testing in Software Project,"Implementation of quality and testing by outsourced test team is new field procurement in software project especially in software development. In Government Agency of Malaysia, cost procurement for implementing both of the quality and testing are blended together with the cost of overall project which is implemented by software development team. Therefore, the cost of quality and testing has to extract from the overall cost of software project. The problem is how to estimate the cost of quality and testing that will be provided to outsourced test team. This paper aim to extract the cost of quality and testing from the total cost of the software project based on Salleh and Primandaria model. The result shows that our extraction model produces an acceptable estimation for project and suitable apply by Government Agency of Malaysia. © 2018 IEEE.",COCOMO; Function point; Linear regression; Software cost estimation; Software testing,"Ahmad S.F., Samat P.A.",2019,Conference,"2018 IEEE Conference on e-Learning, e-Management and e-Services, IC3e 2018",10.1109/IC3e.2018.8632624,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062836185&doi=10.1109%2fIC3e.2018.8632624&partnerID=40&md5=de9dd9be3aa4d8fb2236d86c0515a4e2,"Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Serdang, Selangor, 43400, Malaysia",Institute of Electrical and Electronics Engineers Inc.,English,,9781538672631
Scopus,Search and Research on Economics of Pair Programming,"This research aims to answer two research questions. Is pair programming more cost-effective than solo programming? In what situations is pair programming more cost-effective than solo programming and vice versa? In order to answer these, we adopted and extended economic models from past research. Two main conclusions can be drawn from our study. First, across the ranges of parameters studied, pair programming is more economically feasible only in a limited number of instances. Second, in order to achieve the economic benefit, pair programming either needs to have advantages in all three parameters (speed, defect, and defect removing) or have substantial advantages in two of them if on the other one pair is roughly equivalent to solo programming. We also identified specific parameter ranges for situations where a) pair programming is more economical, b) solo programming is more economical, and c) the two programming methods are equivalent. Implications of this research are discussed. © 2019, © 2019 International Association for Computer Information Systems.",Breakeven unit value; net present value; pair programming; solo programming,"Sun W., Aguirre-Urreta M., Nam H.S.",2019,Journal,Journal of Computer Information Systems,10.1080/08874417.2016.1263167,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041598979&doi=10.1080%2f08874417.2016.1263167&partnerID=40&md5=e1663e8bb8a35e8e4eaa480c43238218,"Washburn University, Topeka, KS, United States; Texas Tech University, Lubbock, TX, United States; Kettering University, Flint, MI, United States",Taylor and Francis Inc.,English,08874417,
Scopus,Reengineering cost estimation using scrum agile methodology,"Estimating the budget for developing software is one of the prime tasks for software stakeholders. Good estimation increases the customer faith and goodwill for the development company. Many estimation techniques exist for estimating the cost of the software. Estimating reengineering projects are equally important. Researchers estimated cost of Reengineering using conventional algorithmic estimation methods. They also used classical software development approaches to perform reengineering. Conventional estimation methods are suitable in an environment where requirements are predefined and fixed. Practically, these methods can not fit in today's software development environment. We need more realistic approach to estimate. Since a decade, we have witnessed a change in the Software development approaches. Now software development process is more people centric and realistic for their stakeholders. This change in process is due to Agile. Agile methodology has gained the interest of both customers as well as developers. The main objective of this research is to estimate the cost of reengineering with consensus based estimation technique of Scrum development methodology. Agile Reengineering model is also proposed for estimation and performing reengineering. Thus the research is aimed to provide a model, which not only helps in performing the reengineering estimations but also guides how to perform reengineering. Scrum approach with sprint iteration of three weeks is used to perform reengineering. Chidamber and Kemerer (CK) metric is applied to determine the complexity metrics for various classes of the software. Reengineering is performed to make the project more maintainable by reducing the CK metric complexity. Various tools used in this work include CK java Metric tool (CKJM) ver-1.9 for calculation of CK metrics suit, IBM Rational Rose ver7.5 for Unified Modeling, Rapid Minor studio ver7.1 for determining the reengineering requirements of the software. © 2019 MIR Labs.",Agile scrum methodology; Reengineering cost estimation; Software engineering; Software reengineering,"Singh J., Singh K., Singh J.",2019,Journal,International Journal of Computer Information Systems and Industrial Management Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083169072&partnerID=40&md5=d115db5211cdfb0a61e250993c367269,"Department of Computer Application, IK Gujral Punjab Technical University Kapurthala, Punjab, India; Department of Computer Science and Engineering, Baba Banda Singh Bahadur Engineering College Fatehgarh Sahib, Punjab, India; Department of Computer Applications, Chitkara University Rajpura, Punjab, India",Machine Intelligence Research (MIR) Labs,English,21507988,
Scopus,40 years journey of function point analysis: Against real-time and multimedia applications,"For various project of software development, the size, effort, and cost are essential aspects for planning schedules and control software development to achieve the preferred outcome. These aspects require a method to measure software accurately and reliably. Function Point Analysis is used as a standard method for that purpose. It is now 40 years old since Albrecht founded it in 1979. Bringing its advantages and some of the drawbacks from earlier versions, this method is now faced with current software development needs, namely, the real-time and the multimedia applications. Broadly translated our findings indicate that current software development does not only focus on system functionality, but non-functional factors also begin to influence the value of a system for the users. Therefore, in 40 years journey of Function Point Analysis, this method is not adequate for current software development, especially in real-time and multimedia applications because of the inability to measure non-functional factors. © 2019 The Authors.",Application; Function point analysis; Multimedia; Non-functional; Real-time; Software development,"Hillman M.F., Subriadi A.P.",2019,Conference,Procedia Computer Science,10.1016/j.procs.2019.11.123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078933786&doi=10.1016%2fj.procs.2019.11.123&partnerID=40&md5=74b8eec2952c72e3f062c34e09a14ae9,"Department of Information Systems, Institut Teknologi Sepuluh Nopember, Jalan Raya Sukolilo, Surabaya, 60111, Indonesia",Elsevier B.V.,English,18770509,
Scopus,Software design using genetic quality components search,"The paper presents a software design methodology based on computational experiments for effective selection of software component set. The selection of components is performed with respect to the numerical quality criteria evaluated in the reproducible experiments with various sets of components in the virtual infrastructure simulating the operating conditions of a software system being developed. To reduce the number of experiments with unpromising sets of components the genetic algorithm is applied. For representing the sets of components in the form of natural genotypes, the encoding mapping is introduced, reverse mapping is used to decipher the genotype. In the first step of the technique, the genetic algorithm creates an initial population of random genotypes that are converted into the assessed sets of software components. The paper shows the application of the proposed methodology to find the effective choice of Node.js components. For this purpose, a MATLAB program of genetic search and experimental scenario for a virtual machine running Ubuntu 16.04 LTS operating system were developed. To guarantee the proper reproduction of the experimental conditions, the Vagrant and Ansible configuration tools were used to create the virtual environment of the experiment. © Science and Information Organization.",Genetic algorithm; Numerical quality criteria evaluated; Selection of software components set; Software design,"Nikulchev E., Ilin D., Gusev A.",2019,Journal,International Journal of Advanced Computer Science and Applications,10.14569/ijacsa.2019.0101207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078471112&doi=10.14569%2fijacsa.2019.0101207&partnerID=40&md5=6de3d0c585ccadd108b1874eaba28e9e,"MIREA - Russian Technological University, Russian Federation",Science and Information Organization,English,2158107X,
Scopus,Evaluation of 6 missing value imputation methods for effort estimation [工数予測における 6 種類の欠損値補完手法の実証的評価],"Machine learning has been commonly used to estimate the software development effort to assist project planning and/or management. Since project data sets for ML model construction often contain missing values, we need to build a complete data set that has no missing values either by using imputation methods. However, while there are several ways to build the complete data set, it is unclear which method is the most suitable for the project data set. In this paper, using project data of 1364 cases (34% missing value rate) collected from several companies, we applied six imputation methods (k-nn, applied CF, Miss Forest and Multiple Imputation method with three algorithm(Data Augmentation, Fully Conditional Specification and Expectation-maximization with Bootstrapping)) to build 6 machine learning methods. Then, using project data of 160 cases (having no missing values), we evaluated the estimation performance of each imputation method. Additionally, we evaluate an effect of log transformation to estimation accuracy. The result showed that k-nn was best performance in ideal situation, however if it can not be secured the estimation cost, multiple imputation with FCS algorithm is another choice. © 2019 Japan Society for Software Science and Technology. All rights reserved.",,"Toda K., Tsunoda M.",2019,Journal,Computer Software,10.11309/jssst.36.4_95,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077028931&doi=10.11309%2fjssst.36.4_95&partnerID=40&md5=dcab44e509399440670dbd1e01d55fb9,"Faculty of Information Engineering, Fukuoka Institute of Technology, Japan; Faculty of Science and Engineering, Kinki University, Japan",Japan Society for Software Science and Technology,Japanese,02896540,
Scopus,Web effort estimation techniques: A systematic literature review,"Web Effort Estimation is an important estimation measure for predicting the effort required to develop a web application. The completion of web projects within stipulated time and budget is not possible without accurate effort estimation. The numerous effort estimation models are present these days and they have achieved a pinnacle of success, but the uncertainty features are daunting its progress due to deviations in the data set collected, types of projects, and data set characteristics. The literature studied for this research task elaborated that this field still lacks in a significant direction for consolidated documentation, which guides the researchers to choose a specific technique in order to predict the effort required for web application development. The wide and versatile nature of this domain daunting the researchers to mine the literature in a more appropriate way and deploy ensemble techniques of effort prediction models in order to achieve better results for web application viz., schedule delays, budget overruns. The systematic literature review (SLR) in this research task has been done to inspect the various aspects affecting the prediction accuracy of web applications and these identified characteristics lead to a better effort estimation model. The literature review is conducted on a collection of 143 papers retrieved from online journals and conference proceedings. Only 53 relevant papers are selected for broad investigation. The study reveals that the expert judgment and algorithm-based models are very popular and used frequently for effort prediction, instead the machine learning (ML) based models are rare in use but cater comparatively better prediction accuracy. The authors suggest taking cognizance of this research domain for developing ensembles of early effort prediction models to overcome delays in schedule and budget. © 2019 National Institute of Science Communication and Information Resources (NISCAIR).",Algorithmic; Effort estimation; Expert opinion; Machine learning; Web applications,"Kaur M., Sood S.",2019,Journal,Compusoft,10.6084/ijact.v8i11.1017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076625214&doi=10.6084%2fijact.v8i11.1017&partnerID=40&md5=f4df56f1c293c2b5696809491d16eaed,"Research Scholar, IKG PTU, Jalandhar, Punjab, India; IKG Punjab Technical University, Punjab, India",National Institute of Science Communication and Information Resources (NISCAIR),English,23200790,
Scopus,Can Expert Opinion Improve Effort Predictions When Exploiting Cross-Company Datasets? - A Case Study in a Small/Medium Company,"Many studies have shown that the accuracy of the predictions obtained by estimation models built considering data collected by other companies (cross-company models) can be significantly worse than those of estimation models built employing a dataset collected by the single company (within-company models). This is due to the different characteristics among cross-company and within-company datasets. In this paper, we propose an approach based on the opinion of the experts that could help in the context of small/medium company that do not have data available from past developed projects. In particular, experts are in charge of selecting data from public cross-company datasets looking at the information about employed software development process and software technologies. The proposed strategy is based on the use of a Delphi approach to reach consensus among experts. To assess the strategy, we performed an empirical study considering a dataset from the PROMISE repository that includes information on the functional size expressed in terms of COSMIC for building the cross-company estimation model. We selected this dataset since COSMIC is the method used to size the applications by the company that provided the within-company dataset employed as test set to assess the accuracy of the built cross-company model. We compared the accuracy of the obtained predictions with those of the cross-company model built without selecting the observations. The results are promising since the effort predictions obtained with the proposed strategy are significantly better than those obtained with the model built on the whole cross-company dataset. © Springer Nature Switzerland AG 2019.",Cross-company estimation models; Delphi approach; Effort estimation; Expert opinion,"Ferrucci F., Gravino C.",2019,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-030-35333-9_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076513885&doi=10.1007%2f978-3-030-35333-9_20&partnerID=40&md5=6829747edc04ad59d594319a5848c843,"University of Salerno, Fisciano, Italy",Springer,English,03029743,9783030353322
Scopus,A Review of Software Effort Estimation by Using Functional Points Analysis,"The estimation of the Software Development Effort, (further only SDE), value is critical for the effective management of any software industry. Function Point Analysis, (further only FPA) - is a standardised method designed to systematically measure the functional size of the software. Although this method tool has been become widely-used by many software organisations, it still faces many problems. In this paper, we shall present a systematic review of Software Effort Estimation, (further only SEE), methods based on Functional Points Analysis, (further only FPA). The article focuses on an analysis of the limitations and accuracy of the FPA method - which was proposed many years ago. © 2019, Springer Nature Switzerland AG.",Effort Accuracy Improvement (EAI); Functional Points Analysis (FPA); Software Effort Estimation (SEE),"Van Hai V., Le Thi Kim Nhung H., Hoc H.T.",2019,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-030-31362-3_40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075601057&doi=10.1007%2f978-3-030-31362-3_40&partnerID=40&md5=412ed8eb55565dc80309d37688333a98,"Faculty of Applied Informatics, Tomas Bata University in Zlin, Nad. Stranemi 4511, Zlin, 76001, Czech Republic",Springer,English,21945357,9783030313616
Scopus,Development of cosmic scaling factors using classification of functional requirements,"The focus of this paper is estimating COSMIC function points early in the software development lifecycle. The main input to function point sizing is the set of functional requirements for a piece of software. However, very early in the lifecycle it is unrealistic to expect this set of requirements to describe the full scope of functionality, including all the necessary functional details. The application of the size scaling factors developed within this context is illustrated with two COSMIC case studies. While the scaling factors are specific to the case studies used, the approximation technique presented can be used in most organizations provided that data on past projects can be collected and relevant classifications of functionalities identified. For the purpose of this paper, the ISO 19761 COSMIC function points standard is taken as reference for discussion, while the majority of concepts presented are generic to other similar ISO standards. Copyright © 2019 for this paper by its authors.",COSMIC; Functional requirements quality; Functional size measurement; ISO 19761; Size estimation; Software estimation,"Abran A., Vedadi S.",2019,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074110323&partnerID=40&md5=e8fff09ac92873b4c8336ca4c5803573,"École de Technologie Supérieure – ETS, University of Quebec, Montréal, Canada",CEUR-WS,English,16130073,
Scopus,Validation of supplier estimates using cosmic method,"In the software development industry, it is well known that software development organizations (suppliers) need a better and formal estimation approaches in order to increase the success rate of software projects developed. Considering a systematic view, any project requested by a customer needs to be validated in the estimation provided by the supplier, regardless of how formal or not the estimation method utilized was. However, very often the customers do not know the information used by the suppliers to make their estimations. The software decision makers must face a validation estimates problem where the more useful solution is used the expert judgment, with several problems related to it. In this paper, a real case study is described where a validation estimates model was generated from a reference database based in COSMIC method. The defined model using a density function helps to the customer to define validation criteria considering the probability that the supplier estimate will be met according to an industry reference database. Copyright © 2019 for this paper by its authors.",COSMIC ISO 19761; FSM; Software estimation; Validation estimates,Valdés-Souto F.,2019,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074067329&partnerID=40&md5=0680d63637bc05076a1926a43efd8ef8,"National Autonomous University of Mexico, Science Faculty, CDMX, Mexico City, Mexico",CEUR-WS,English,16130073,
Scopus,A software cost estimation taxonomy for global software development projects,"Nowadays, software cost estimation plays an important role in the management and development of distributed projects. The state of the art and cost estimation practice for Global Software Development (GSD) have recently been identified. This knowledge has still not been structured. The objective of this paper is to structure the knowledge about cost estimation for GSD. We used a design method to organize the knowledge identified as a cost estimation taxonomy for GSD. The proposed taxonomy offers a classification scheme for the cost estimation of distributed projects. The cost estimation taxonomy consists of four dimensions: cost estimation context, estimation technique, cost estimate and cost estimators. Each dimension in turn has multiple facets. The taxonomy could then be used as a tool to developing a repository for cost estimation knowledge. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Global software development; Software cost estimation; Taxonomy,"El Bajta M., Idri A.",2019,Conference,ICSOFT 2019 - Proceedings of the 14th International Conference on Software Technologies,10.5220/0007841202180225,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073123782&doi=10.5220%2f0007841202180225&partnerID=40&md5=f9c27e8ceb0cd0164647d149b6723eca,"Software Project Management Research Team, ENSIAS, Mohammed V University of Rabat, Morocco",SciTePress,English,,9789897583797
Scopus,Empirical studies on software product maintainability prediction: A systematic mapping and review,"Background: Software product maintainability prediction (SPMP) is an important task to control software maintenance activity, and many SPMP techniques for improving software maintainability have been proposed. In this study, we performed a systematic mapping and review on SPMP studies to analyze and summarize the empirical evidence on the prediction accuracy of SPMP techniques in current research. Objective: The objective of this study is twofold: (1) to classify SPMP studies reported in the literature using the following criteria: publication year, publication source, research type, empirical approach, software application type, datasets, independent variables used as predictors, dependent variables (e.g. how maintainability is expressed in terms of the variable to be predicted), tools used to gather the predictors, the successful predictors and SPMP techniques, (2) to analyze these studies from three perspectives: prediction accuracy, techniques reported to be superior in comparative studies and accuracy comparison of these techniques. Methodology: We performed a systematic mapping and review of the SPMP empirical studies published from 2000 up to 2018 based on an automated search of nine electronic databases. Results: We identified 82 primary studies and classified them according to the above criteria. The mapping study revealed that most studies were solution proposals using a history-based empirical evaluation approach, the datasets most used were historical using object-oriented software applications, maintainability in terms of the independent variable to be predicted was most frequently expressed in terms of the number of changes made to the source code, maintainability predictors most used were those provided by Chidamber and Kemerer (C&K), Li and Henry (L&H) and source code size measures, while the most used techniques were ML techniques, in particular artificial neural networks. Detailed analysis revealed that fuzzy & neuro fuzzy (FNF), artificial neural network (ANN) showed good prediction for the change topic, while multilayer perceptron (MLP), support vector machine (SVM), and group method of data handling (GMDH) techniques presented greater accuracy prediction in comparative studies. Based on our findings SPMP is still limited. Developing more accurate techniques may facilitate their use in industry and well-formed, generalizable results be obtained. We also provide guidelines for improving the maintainability of software. © 2019 Wroclaw University of Science and Technology. All rights reserved.",Empirical studies; Software product maintainability; Systematic literature review; Systematic mapping study,"Elmidaoui S., Cheikhi L., Idri A., Abran A.",2019,Review,E-Informatica Software Engineering Journal,10.5277/e-Inf190105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072396433&doi=10.5277%2fe-Inf190105&partnerID=40&md5=e241518e8e1bcacad45cf0092beb7e7e,"Software Project Management Team (SPM), ENSIAS, Mohamed V University, Rabat, Morocco; Department of Software Engineering and Information Technology, École de Technologie Supérieure, Montréal, Canada",Wroclaw University of Science and Technology,English,18977979,
Scopus,Estimation of complexity by using an object oriented metrics approach and its proposed algorithm and models,"The high pace emergence of software applications and associated technologies have enabled us an efficient and luxurious. This research paper focus the procedure of investigating the different object oriented metrics by taking an example and finally evaluate complexities, statistical measurements. A proposed model, flowchart and algorithm are used for measuring the metrics in the software project. Comparison done between the programming languages like C++ and Python and conclude that Python is better than C++. Copyright © 2019 Inderscience Enterprises Ltd.",CK metrics suite; Frame work; Object oriented metrics; Object oriented programming examples; OOM; Proposed algorithms and models; Reusability estimation model,"Padhy N., Satapathy S.C., Singh R.P.",2019,Journal,International Journal of Networking and Virtual Organisations,10.1504/IJNVO.2019.101147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069938694&doi=10.1504%2fIJNVO.2019.101147&partnerID=40&md5=126b77fb84fc358fdab50df084cfd557,"Sri Satya Sai University of Technology and Medical Sciences, Opposite OILFED, Bhopal Indore Highway, Pachama, Sehore (M.P.), 466001, India; Department of Computer Science and Engineering, PVP Siddhartha Institute of Technology, Vijayawada Andhra Pradesh, India",Inderscience Publishers,English,14709503,
Scopus,A systematic literature review on methods for software effort estimation,"There have been many researchers who proposed research in an effort to develop the field of improving accuracy in the Software Effort Estimation (SEE). Collected results from a series of studies selected in the Software Effort Estimation, which was published in the period 2000-2017, using systematic mapping and review procedures. The purpose of this review is to provide a classification of study areas of SEE related to publication channels, research approaches, types of contributions, techniques used in combination. To analyze: 1) The precise estimation of SEE techniques; 2) Accuracy of the SEE model estimate compared with other models; 3) A favorable outcome context for the use of the SEE model; and 4) The impact of other techniques into the SEE model by combining models and implementation for models and tools. We have identified 74 major studies that are relevant to the purpose of this study. After investigating, we found that eight types of techniques were used in the Software effort estimation model. that techniques used for SEE usually produce acceptable estimation accuracy, and the facts are more accurate. © 2005 - ongoing JATIT & LLS.",Datasets; Methods; Software Effort Estimation; Systematic Literature Review; Validation,"Marco R., Suryana N., Ahmad S.S.S.",2019,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061401170&partnerID=40&md5=6c4927ac5c23d0972d349cd6a63c7ddc,"Department of Information Technology, University of Amikom Yogyakarta, Yogyakarta, Indonesia; University Teknikal Malaysia Melaka, Melaka, Malaysia",Little Lion Scientific,English,19928645,
Scopus,Effort estimation for mobile applications using use case point (UCP),"Estimating the software size and effort helps in early prediction of uncertainties in software development. Determining the size helps in ascertaining the effort, cost, and schedule for entire project. In context of mobile software, the existing techniques for software effort estimations could be adapted as such or with modifications. There are many software size and effort estimation metrics. In this paper, Use Case Point (UCP) metric is aimed for estimating size and effort for mobile application. Five android mobile applications are considered as a case study, and difference in actual effort and estimated effort is evaluated. Modified UCP has been also proposed to improve the results by considering mobile-specific characteristics. © Springer Nature Singapore Pte Ltd. 2019.",Effort; Estimation; Mobile application; Software metric; Use Case Point (UCP),"Kaur A., Kaur K.",2019,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-10-8968-8_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049314109&doi=10.1007%2f978-981-10-8968-8_14&partnerID=40&md5=fab8bdba92ace9d40c5ad83d6c79302a,"I.K. Gujral Punjab Technical University, Kapurthala, India; Apeejay Institute of Management Technical Campus, Jalandhar, India",Springer Verlag,English,21945357,9789811089671
Scopus,Object-oriented metrics for defect prediction,"Today, defect prediction is an important part of software industry to meet deadlines for their products. Defect prediction techniques help the organizations to use their resources effectively which results in lower cost and time requirements. Various metrics are used for defect prediction in within company (WC) and cross-company (CC) projects. In this paper, we used object-oriented metrics to build a defect prediction model for within company and cross-company projects. In this paper, feed-forward neural network (FFNN) model is used to build a defect prediction model. The proposed model was tested over four datasets against within company defect prediction (WCDP) and cross-company defect prediction (CCDP). The proposed model gives good results for WCDP and CCDP as compared to previous studies. © Springer Nature Singapore Pte Ltd 2019.",Artificial neural network (ANN); Defect prediction; Object-oriented metrics,"Singh S., Singla R.",2019,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-10-8848-3_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049079259&doi=10.1007%2f978-981-10-8848-3_30&partnerID=40&md5=9018c21102bcabc4d2a6a8f7c76fe709,"BBSBEC, Fatehgarh Sahib, Punjab, India",Springer Verlag,English,21945357,9789811088476
Scopus,Application of mutual information-based sequential feature selection to ISBSG mixed data,"There is still little research work focused on feature selection (FS) techniques including both categorical and continuous features in Software Development Effort Estimation (SDEE) literature. This paper addresses the problem of selecting the most relevant features from ISBSG (International Software Benchmarking Standards Group) dataset to be used in SDEE. The aim is to show the usefulness of splitting the ranked list of features provided by a mutual information-based sequential FS approach in two, regarding categorical and continuous features. These lists are later recombined according to the accuracy of a case-based reasoning model. Thus, four FS algorithms are compared using a complete dataset with 621 projects and 12 features from ISBSG. On the one hand, two algorithms just consider the relevance, while the remaining two follow the criterion of maximizing relevance and also minimizing redundancy between any independent feature and the already selected features. On the other hand, the algorithms that do not discriminate between continuous and categorical features consider just one list, whereas those that differentiate them use two lists that are later combined. As a result, the algorithms that use two lists present better performance than those algorithms that use one list. Thus, it is meaningful to consider two different lists of features so that the categorical features may be selected more frequently. We also suggest promoting the usage of Application Group, Project Elapsed Time, and First Data Base System features with preference over the more frequently used Development Type, Language Type, and Development Platform. © 2017, Springer Science+Business Media, LLC.",Feature selection; ISBSG; k-nearest neighbor; Mutual information; Software development effort estimation,"Fernández-Diego M., González-Ladrón-de-Guevara F.",2018,Journal,Software Quality Journal,10.1007/s11219-017-9391-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032470479&doi=10.1007%2fs11219-017-9391-5&partnerID=40&md5=d6ec2802abdf2b5e8f5810348b8c630a,"Department of Business Organisation, Universitat Politècnica de València, Valencia, 46022, Spain",Springer New York LLC,English,09639314,
Scopus,Safety Critical Embedded Software: Significance and Approach to Reliability,"Safety critical embedded software applications are developed for systems whose failures contribute to hazards in the system for safety of life. Such software, as a part of extremely critical component of any system, requires high reliability index in its design, development or maintenance. Enhancing reliability and thereby achieving best quality software is a concern for safety critical software. In order to build highly reliable software, attributes of quality that are applied at each phase of development lifecycle are necessary to be considered for improvement. Usage of formal method based software tools during the development improves overall quality of the software by removing ambiguities and early detection removal of faults. This paper highlights the requirements and significance of reliability on the overall performance of the safety critical software, the approach to higher reliability through software project planning, development with standard methodical process, automation and configuration control. Further, this paper emphasizes on enabling safety and reliability into the critical software systems by adopting factors such as development process, formal methods and relevant tools in order to build continued confidence. © 2018 IEEE.",Automation; DO-178B; Lightweight formal methods; Safety critical embedded software; Software reliability,"Prabhu S.S., Kapil H., Lakshmaiah S.H.",2018,Conference,"2018 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2018",10.1109/ICACCI.2018.8554566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060059327&doi=10.1109%2fICACCI.2018.8554566&partnerID=40&md5=b7ec7f81975c8626074879f376b18687,"Gas Turbine Research Establishment, Defence RD Organisation, Bangalore, Karnataka, India; Department of Computer Science, Mangalore University, Mangalore, Karnataka, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538653142
Scopus,The Need to Critical Review of Function Point Analysis,"This Critical Review aims to evaluate the Function Point Analysis (FPA) methodology to be more reliable. It also evaluates the calculation process to be more efficient, effective, and repeatable. This study modifies the function point by adding the complexity of business processes in General System Characteristic (GSC). This is because some studies criticized GSC since it was difficult to be repeated and subjective. Therefore, it is strongly recommended to modify the GSC so that the FPA get more attention on this area. GSC calibration should be done by modifying the Complexity of Business Processes (CBP) because of the change of CBP might result a more complex needs in software development. In addition, organizational business processes are different from each other, as well as their complexity. Finally, the CBP affected the size of the software developed. Therefore, this research proposes a modification of the function point by modifying/adding the CBP to the 14 General Characteristic System (GSC) factors that are expected to improve accuracy to be more objective and reliable. This modification is expected to provide a variety of different software sizes but it more accurate. This research could answer the differences in the point function test results that have the same size. © 2018 IEEE.",Business Process Complexity; Critical Review; Function Point Analysis,"Subriadi A.P., Putri A.Y.P.",2018,Conference,"2018 International Seminar on Research of Information Technology and Intelligent Systems, ISRITI 2018",10.1109/ISRITI.2018.8864261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082885799&doi=10.1109%2fISRITI.2018.8864261&partnerID=40&md5=c527a90c57e380e3d53bb1b2ac79854f,"Department of Information Systems, Institut Teknologi Sepuluh Nopember, Surabaya, 60111, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781538674222
Scopus,Coping with uncertainties in estimating requirements development effort,"Estimating requirements development effort is a critical activity in the software development process. Producing an accurate estimate, however, is extremely difficult, especially in the early stages of the software development process, where no clear and no complete description of the desired software behavior is available, hence leading to an estimate process with a high level of uncertainty. To handle this issue, in this paper, we present REEE (Requirements Elicitation Effort Estimation) - a framework for incrementally estimating software requirements elicitation efforts. We first categorize a project in terms of project's domain, different types of project parameters and requirements - functional requirements (FRs), non-functional requirements (NFRs), constraints and domain properties (DPs), while taking into consideration relationships among them. We then present an incremental approach to software requirements elicitation effort estimation that uses similar past projects in a case-based reasoning (CBR) model. In this approach, semantic similarity is quantified, once classification and normalization of requirements are done. To see the strengths and limitations of REEE, a dataset of 36(students') projects have been analyzed. Experimental results show that the use of REEE is a viable alternative to other cost estimation techniques, and can provide better results, with less uncertainty, for requirements elicitation effort estimation. © 2018 IEEE.",Case-based reasoning (CBR); Domain properties (DP); Functional requirement (FR); Non Functional requirement (NFR); Requirements effort estimation; Requirements elicitation,"Fellir F., Park G.E., Nafil K., Chung L.",2018,Conference,"2018 6th International Conference on Control Engineering and Information Technology, CEIT 2018",10.1109/CEIT.2018.8751789,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069162337&doi=10.1109%2fCEIT.2018.8751789&partnerID=40&md5=ff829d2f062672ba2c3df955b000f2f9,"Lastid Laboratory, Faculty of Sciences, University of IbnTofail, Kenitra, Morocco; Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX  75083-0688, United States; Software Project Management, Research Team, ENSIAS, University of Mohammed v in Rabat, Morocco",Institute of Electrical and Electronics Engineers Inc.,English,,9781538676417
Scopus,Effort estimation development model for web-based mobile application using fuzzy logic,"Effort estimation becomes a crucial part in software development process because false effort estimation result can lead to delayed project and affect the successful of a project. This research proposes a model of effort estimation for web-based mobile application developed using object oriented approach. In the proposed model, functional size measurement of object oriented based web application named OOmFPWeb, web metric and mobile characteristic for web-based mobile application size measurement are combnined. The estimation process is done by using mamdani fuzzy logic method. To evaluate the proposed model, the comparison between OOmFPWeb as the variable that affect effort estimation for web-based mobile application and the proposed model are performed. The evaluation result shows that effort estimation for web-based mobile application with the proposed model is better than just using OOmFPWeb. © 2018 Universitas Ahmad Dahlan.",Functional; Fuzzy logic; Hypermedia; Mobile application effort estimation,"Agusta S., Suharjito, Girsang A.S.",2018,Journal,Telkomnika (Telecommunication Computing Electronics and Control),10.12928/TELKOMNIKA.v16i5.6561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060162620&doi=10.12928%2fTELKOMNIKA.v16i5.6561&partnerID=40&md5=1843607469a8fab28ad829c479af2395,"Computer Science Department, BINUS Graduate Program-Master of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia",Universitas Ahmad Dahlan,English,16936930,
Scopus,Modeling software development process complexity,"Modern software systems are growing increasingly complex, requiring increased complexity of software and software development process (SDP). Most software complexity measurement approaches focus on software features such as code size, code defects, number of control paths, etc. However, software complexity measurement should not only focus on code features but on features that cover several aspects of SDP in order to have a more complete approach to software complexity. To implement this approach, an extensive literature review for identifying factors that contribute to the complexity of SDP was performed and seventeen complexity factors were identified. As there were indications that the identified factors were not independent from each other but there were interrelations between them, statistical methods for identifying the underlined relations and refining them were applied, resulting to the final set of measures used in the proposed model. Finally, the proposed model has been tested in five software projects and the results were evaluated. Copyright © 2018, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",Project management; Software measurement; Software metrics; Software project management; Statistical analysis,"Damasiotis V., Fitsilis P., O'Kane J.F.",2018,Journal,International Journal of Information Technology Project Management,10.4018/IJITPM.2018100102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052712112&doi=10.4018%2fIJITPM.2018100102&partnerID=40&md5=e6a665692e39f3cb3dbccc301b6b8c0f,"Department of Accounting and Finance, University of Applied Sciences of Thessaly - TEI Thessaly, Larisa, Greece; Department of Business Administration, University of Applied Sciences of Thessaly - TEI Thessaly, Larisa, Greece; Edinburgh Napier University Business, Edinburgh, United Kingdom",IGI Global,English,19380232,
Scopus,Software Fault Prediction Using Artificial Intelligence Techniques,"Detecting faults in the initial stage of the development process has become an important prospect for the codes cost estimation; so a fault predictor model is very much necessary in order to bring down the cost of development and maintenance. Due to these reasons, developing models for fault prediction has become a crucial part of research, and various techniques have been adapted in order to predict faults in a software. Few of them include Artificial Neural Network, Decision Tree, Genetic Algorithm, etc. Among these techniques, Neural Networks and Genetic Algorithms have become a growing concern over the years and are being applied in various fields such as optimization, prediction or classification. These techniques make use of various software metrics to assess the characteristic of any software system such as number of faults, maintenance of class, etc. Most commonly used are Chidamber and Kemerer (CK) metrics which are found to be efficient from many researches. © 2017 IEEE.",Artificial Neural Network; Back Propagation; Chi-damber and Kemerer suite of metrics; Genetic Algorithm,"Haveri A., Suresh Y.",2018,Conference,"2nd International Conference on Computational Systems and Information Technology for Sustainable Solutions, CSITSS 2017",10.1109/CSITSS.2017.8447615,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054154667&doi=10.1109%2fCSITSS.2017.8447615&partnerID=40&md5=f6579eef30cbf53bc10c1e8aeb92f0ad,"Department of Computer Science and Engineering, BMS Institute of Technology Management, Bengaluru, 560064, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538620441
Scopus,Detailed use case points (ducps): A size metric automatically countable from sequence and class diagrams,"Sequence and class diagrams are widely used to model the behavioral and structural aspects of a software system. A size metric that is defined automatically countable from sequence and class diagrams boosts both the efficiency and the accuracy of size estimation by producing reproducible software size measurements. To fulfill the purposes, a size metric called Detailed Use Case Points (DUCPs) is proposed based on the information automatically derived from sequence and class diagrams. The automation is largely supported by our proposed user-system interaction model (USIM) that fills the gap between the system abstraction by the sizing model and the metamodels of the UML diagrams. The effectiveness of our proposed size metric in project effort estimation is validated by an empirical study of 22 historical projects. © 2018 ACM.",Automated analysis; Automated model transformation; Effort estimation; Function point analysis; Model based analysis; Model calibration; Object oriented modeling; Software size metric; Unified modeling language (uml); Use case analysis; Use case points,"Qi K., Boehm B.",2018,Conference,"Proceedings - 2018 ACM/IEEE 10th International Workshop on Modelling in Software Engineering, MiSE 2018",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053903637&partnerID=40&md5=d2fd67038bd0bd9f8da90081d2898152,"University of Southern California, United States",Institute of Electrical and Electronics Engineers Inc.,English,,9781450357357
Scopus,LP based model to find optimal portfolio to maximize the profit index in software project billing,"Throughout the globe, billions of dollars are spent in IT sectors for building software projects. Building software is continuous process with numerous hazards and so many issues and changes. Those rigorous interruptions may cause soaring of cost from both client's and IT companies. In this paper we have the objective to evaluate the cost of wages of technical resources of any IT farm and the revenue coming after those resources are billed into a client's project. Subsequently we have designed an LP based model to maximize the profit index. © 2018 IEEE.",cost benefit analysis; LP method; project billing; Software project,"Das A., Sharma M., Halder C., Das A., Dhawa S.K., Banerjee K.",2018,Conference,"Proceedings of the 2nd International Conference on Green Computing and Internet of Things, ICGCIoT 2018",10.1109/ICGCIoT.2018.8753102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069532477&doi=10.1109%2fICGCIoT.2018.8753102&partnerID=40&md5=3d867cada1b3cd13ea89abd764b1054b,"Faculty, Amity University, Kolkata, India; Amity School of Engineering and Technology, Noida, India; University of Engineering Management, Kolkata, India; J.C. Bose Institute of Education and ResearchWest Bengal, India; Jemes AcademyWest Bengal, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538656570
Scopus,Effort Estimation of Agile Development using Fuzzy Logic,"Effort Estimation is one of the most important factors for a successful project. Mostly it is done in the starting of the project life cycle. The vital activity in Agile project organization is a prediction of Software development this is because a project that is available in the beginning is not sufficient and moreover it is not clear because of the confliction. Getting accurate Effort Estimation for any software project has been a challenge always. In this paper fuzzy logic model is used to find Effort Estimation of Agile software development. In this model Fuzzy method is used to take the raw data of actual figures and output is given as the estimated effort available from industry. This way is more suitable when data is not available for analyzing by orthodox method. Basic idea is taken from Human where action and Inaction are not obvious, they are not clear. In this paper we will use three input variable to calculate Efforts estimation i.e User soty, Team Expertise and Complexity. In this manner, it is quite difficult to discover goals in more exact and clear simple way. © 2018 IEEE.",Agile Software Development; Effort Estimation; Fuzzy logic; User story,"Saini A., Ahuja L., Khatri S.K.",2018,Conference,"2018 7th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2018",10.1109/ICRITO.2018.8748381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069216386&doi=10.1109%2fICRITO.2018.8748381&partnerID=40&md5=db738a6f065b046abb642c1d01dee8ae,"Amity Institute of Information Technology, Amity University, Noida, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781538646922
Scopus,Selecting estimating software: Perspectives from the construction industry,[No abstract available],,"McIntyre C., Adhikari S., Ray V.M., White J.W.",2018,Conference,"ASEE Annual Conference and Exposition, Conference Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051170758&partnerID=40&md5=32fc773fbf2a89f447e10b193f8106f2,"Indiana University-Purdue University of Indianapolis, Construction Engineering Management Technology Program, United States",American Society for Engineering Education,English,21535965,
Scopus,A framework of statistical and visualization techniques for missing data analysis in software cost estimation,"Software Cost Estimation (SCE) is a critical phase in software development projects. However, due to the growing complexity of the software itself, a common problem in building software cost models is that the available datasets contain lots of missing categorical data. The purpose of this chapter is to show how a framework of statistical, computational, and visualization techniques can be used to evaluate and compare the effect of missing data techniques on the accuracy of cost estimation models. Hence, the authors use five missing data techniques: Multinomial Logistic Regression, Listwise Deletion, Mean Imputation, Expectation Maximization, and Regression Imputation. The evaluation and the comparisons are conducted using Regression Error Characteristic curves, which provide visual comparison of different prediction models, and Regression Error Operating Curves, which examine predictive power of models with respect to under- or over-estimation. © 2018, IGI Global. All rights reserved.",,"Angelis L., Mittas N., Chatzipetrou P.",2018,Book Chapter,"Intelligent Systems: Concepts, Methodologies, Tools, and Applications",10.4018/978-1-5225-5643-5.ch014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059707328&doi=10.4018%2f978-1-5225-5643-5.ch014&partnerID=40&md5=e9821d84e631045f7cd2127e06e7302e,"Aristotle University of Thessaloniki, Greece",IGI Global,English,,9781522556442; 1522556435; 9781522556435
Scopus,Utilizing cluster quality in hierarchical clustering for analogy-based software effort estimation,"Analogy-based software effort estimation is one of the most popular estimation methods. It is built upon the principle of case-based reasoning (CBR) based on the k-th similar projects completed in the past. Therefore the determination of the k value is crucial to the prediction performance. Various research have been carried out to use a single and fixed k value for experiments, and it is known that dynamically allocated k values in an experiment will produce the optimized performance. This paper proposes an interesting technique based on hierarchical clustering to produce a range for k through various cluster quality criteria. We find that complete linkage clustering is more suitable for large datasets while single linkage clustering is suitable for small datasets. The method searches for optimized k values based on the proposed heuristic optimization technique, which have the advantages of easy computation and optimized for the dataset being investigated. Datasets from the PROMISE repository have been used to evaluate the proposed technique. The results of the experiments show that the proposed method is able to determine an optimized set of k values for analogy-based prediction, and to give estimates that outperformed traditional models based on a fixed k value. The implication is significant in that the analogy-based model will be optimized according the dataset being used, without the need to ask an expert to determining a single, fixed k value. © 2017 IEEE.",Analogy; Clustering; K-NN; Software Effort Estimation; Software Metrics and Measurements,"Wu J.H.C., Keung J.W.",2018,Conference,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",10.1109/ICSESS.2017.8342851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047014034&doi=10.1109%2fICSESS.2017.8342851&partnerID=40&md5=d1b5954223ea4facccce72eb7cba3dbb,"College of Professional and Continuing Education, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong",IEEE Computer Society,English,23270586,9781538645703
Scopus,Creating an Estimation Model Using a COSMIC Functional Size Approximation Technique,"In the actual competitive software industry, software development organizations need a better and formal estimation approach in order to increase the success rate of software projects. However, currently the estimation approach typically employed in industry is the expert judgment ('experience-based'). Using measures and estimations that continue to be based on the researchers' intuition does not contribute to obtaining successful projects nor to mature software engineering. Any organization that aims to have or develop estimation approaches needs historical data, and the majority of the organizations do not have this information. Additionally, acquisition of this information has expensive cost and time/effort consuming. This paper proposes the use of the EPCU approximation approach -that has demonstrated several benefits- aiming to create a formal database that can be employed to generate a formal estimation model with low cost and effort, improving software estimation in organizations without historical databases. © 2017 IEEE.",Approximate Sizing; COSMIC ISO 19761; EPCU Model; Estimation database; FSM; Functional Size,Valdes-Souto F.,2018,Conference,"Proceedings - 2017 5th International Conference in Software Engineering Research and Innovation, CONISOFT 2017",10.1109/CONISOFT.2017.00016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050977039&doi=10.1109%2fCONISOFT.2017.00016&partnerID=40&md5=a90a2b92188e4b75c08f2bd00d8d88dc,"Science Faculty, National Autonomous University of Mexico (UNAM), CDMX Mexico City, Mexico",Institute of Electrical and Electronics Engineers Inc.,English,,9781538639566
Scopus,Time distribution of software stage effort,"In the 1950s, Norden first employed the Rayleigh Curve to describe the time distribution rule of software project effort. However, to date, there is no new curve proposed for the later software projects. This research explores a new distribution rule which leads further to estimate software stage effort. We conducted a study on the basis of 4,516 actual projects, and derived general monthly effort distribution curves from them. The new curve has some features similar with Norden's curve, but there also exist some obvious differences. Benefit from a large number of projects, we also found the monthly effort statistical distribution and the typical value of each month. The research results give a new facet about software projects within the dataset, and can be helpful for software project management. © 2017 IEEE.",Software project management; Software stage effort; Stage effort distribution; Stage effort prediction,"Wang Y., Wang L., Li Y., Zhu X.",2018,Conference,"Proceedings - 2017 24th Asia-Pacific Software Engineering Conference Workshops, APSECW 2017",10.1109/APSECW.2017.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050589889&doi=10.1109%2fAPSECW.2017.9&partnerID=40&md5=9b3dc4243ef1683cc313d4d4c8b96911,"State Key Laboratory of Software Engineering, Wuhan University, WHU, Wuhan, China; Department of Computer Science and Technology, Ocean University of China, OUC, Qingdao, China; Department of Computer Science and Technology, Xi'An Jiaotong University, XJTU, Xi'an, China",Institute of Electrical and Electronics Engineers Inc.,English,,9781538626498
Scopus,EMSA: Extensibility Metric for Software Architecture,"Software extensibility, the capability of adding new functions to a software system, is established based on software architecture. Therefore, developers need to evaluate the capability when designing software architecture. To support the evaluation, researchers have proposed metrics based on quality models or scenarios. However, those metrics are vague or subjective, depending on specific systems and evaluators. We propose the extensibility metric for software architecture (EMSA), which represents the degree of extensibility of a software system based on its architecture. To reduce the subjectivity of the metric, we first identify a typical task of adding new functions to a software system. Second, we define the metrics based on the characteristics of software architecture and its changes and finally combine them into a single metric. The originality of EMSA comes from defining metrics based on software architecture and extensibility tasks and integrating them into one. Furthermore, we made an effort to translate the degree into effort estimation expressed as person-hours. To evaluate EMSA, we conducted two types of user studies, obtaining measurements in both a laboratory and a real-world project. The results show that the EMSA estimation is reasonably accurate [6.6% MMRE and 100% PRED(25%)], even in a real-world project (93.2% accuracy and 8.5% standard deviation). © 2018 World Scientific Publishing Company.",Extensibility; metrics; software architecture,"Kim J., Kang S., Ahn J., Lee S.",2018,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194018500134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044581770&doi=10.1142%2fS0218194018500134&partnerID=40&md5=54773cb5a3d73a2643d618c1c9ad66c5,"Department of Information and Communications Engineering, 373-1 Guseong-dong, Yuseong-gu, Daejeon, South Korea; School of Computing, KAIST, 373-1 Guseong-dong, Yuseong-gu, Daejeon, South Korea; Department of Aerospace and Software Engineering, Department of Informatics, Gyeongsang National University, 501 Jinju-daero, Jinju, South Korea",World Scientific Publishing Co. Pte Ltd,English,02181940,
Scopus,Data analysis using box and whisker plot for stationary shop analysis,"In statistical analysis, we have a collection of data, with the use of these data, we have to do analysis based on our requirements. With the collection of data using Statistical analysis, we deal collection, analysis, presentation and organizing the data. With the help of statistical analysis, we can find underlying patterns, relationships, and trends between data samples. The R system for statistical computing is an environment for data analysis and graphics. Here we are going to implement boxplot method and control chart methods for sales analysis in a stationary shop dataset. With the help of boxplot, we can easily make relations between samples and we can find the outliers. © 2017 IEEE.",Data analysis; Decision making,"Vignesh V., Pavithra D., Dinakaran K., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046695107&doi=10.1109%2fICOEI.2017.8300874&partnerID=40&md5=e64066316e289bbc457782c04fe56f70,"MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Analysis of LOC attributes using code analyzer and correlation methods,"Code Analyzer is a metrics analysis tool which is used to find Total files, Total Lines, Code Lines, comment lines. White Space Lines, Average Line length, Number of blank and comment lines, Code ratio, comment ratio etc. All the above-mentioned metrics are calculated for the whole project, for the functions and classes of the project separately. Code Analyzer is a Java application for C, C++, Java, assembly, HTML, and user-defined software source metrics. It calculates metrics across multiple source trees as one coherent code set. It allows the exclusion of portions of included tree by individual file or by a subtree. It has a nice tree view of the project with flexible report capabilities. It is built around the Java tree display component, JTree. © 2017 IEEE.",Box plot analysis; Control chart; Pearson method,"Srilatha G., Madhumathi R., Sreshta P., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300893,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046678517&doi=10.1109%2fICOEI.2017.8300893&partnerID=40&md5=1c4bb05d7da82ed1fe9e3443af7ff5d6,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Data analysis using box plot and control chart for air quality,"Contains the responses of a gas multi-sensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer. Here we are going to implement boxplot method and control chart methods for analyzing air quality. With the help of boxplot, we can easily make relations between samples and we can find the outliers. © 2017 IEEE.",Data analysis; Decision making,"Praveen V., Delhi Narendran T., Pavithran R., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046672427&doi=10.1109%2fICOEI.2017.8300877&partnerID=40&md5=326662a73c100c5e5fc8e45df91d589d,"MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Analyzing the linked list complexity using correlation methods,"Code Analyzer is a software metrics analysis tool which is used to find KLOC, LOC, Number of comment lines and white spaces, All the metrics which are mentioned above are calculated for the whole project, for the classes and functions of the project separately. This tool can be used for various programming languages like C, C++, Java, HTML, and Assemble. We can generate report files in various formats like text, HTML, CSV. Linked List is a data structure which is used to store data using pointers, In this paper, correlation coefficient can be obtained by using some linked list programs like Generic list versions and other linked list programs stacks, queues, binary tree. Using the Pearson and specimen we are going to find the correlation coefficient among those programs, Finally, we have are showing the relation between the codes of metrics data for various linked list programs. © 2017 IEEE.",LOC; software measurement,"Sravani K., Pavithra D., Dhanya S., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300888,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046668901&doi=10.1109%2fICOEI.2017.8300888&partnerID=40&md5=fb71d4c560da4b1e7dad1b17a954e77a,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Analyzing user knowledge by Pearson and spearman method,This data set is used to measure the knowledge of the user based on study time and exam performance of each user. We perform the software metric analysis on the given data set. Based on the Pearson correlation and spearman attribute analysis of data we can differentiate each user based on the knowledge level. © 2017 IEEE.,Pearson Correlation; Spearman Correlation; User Knowledge,"Yuvaraj P., Anirudh R., Sharmila J., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300878,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046666566&doi=10.1109%2fICOEI.2017.8300878&partnerID=40&md5=284598711a7c477b9c265e79a08a031f,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,"Analysis of age, astigmatic and tear protection rate in contact lenses selection","In this report, we use Pearson and Spearmen methodologies for the real-time dataset of finding the data efficiency in the contact lenses selection dataset. In statistics, the Pearson coefficient of correlation, conjointly cited because the Pearson's r or Pearson product-moment coefficient of correlation could be alive of the linear dependence between two variables X and Y. It's a worth between +1 and -1 comprehensive, wherever one is a total positive linear correlation, zero isn't any linear correlation, and -1 is a total negative linear correlation. The Spearmen methodology is that the correlation methodologies utilized to spot the complexes between the various attributes in the dataset. Then we use Pearson and Spearmen method to determine the correlation between the attributes in the contact lenses selection dataset. © 2017 IEEE.",Contact Lens; Pearson Correlation,"Jagadish D., Kumaran U., Kumar J.V., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300879,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046658242&doi=10.1109%2fICOEI.2017.8300879&partnerID=40&md5=0c18e72fd1042b357fd489a2febfb961,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Evaluation of McCabe's cyclomatic complexity metrics for secured medical image,Communication plays very important role in this world and Digital communication has grown wildly. Conversations that happen through the internet between the source and destination are not secured always. Security is achieved by covering data within other files. This we call it as Steganography. Digital Steganography is hiding data within data. This process is done by covering a message in a carrier file and different algorithms are used to implement steganography. Least Significant Bit algorithm for audio and image Steganography. Different StegAnalysis for LSB Embedding in Images. Enhanced Least Significant bit algorithm for audio and image steganography. An Improved LSB based Image Steganography Technique for RGB Images. © 2017 IEEE.,LSB; McCabe's Cyclomatic Complexity; Steganography,"Shanthi V., Jeevana P., Chaithanya G.K., Thirumalai C.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046630390&doi=10.1109%2fICOEI.2017.8300886&partnerID=40&md5=89c1515e65581085acb910ec442a3399,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,An assessment framework of SIAM/ARAI fuel efficiency using semi-supervised and similarity methods,"In this paper, we are going to use the Pearson and Spearman methodologies for the real-time application of finding the fuel efficiency in 4 wheelers. The Pearson and Spearman methodologies are the correlation methodologies used mainly in the Software development process to identify the complexity between the various modules of the software. Identifying the complexity is important because if the complexity is higher then there is a higher chance of occurrence of the risk in the software. Hence we choose the Pearson and Spearman methods to analyze the fuel consumption of 4 wheelers between the City and Highway. © 2017 IEEE.",Correlation; Linear Relationship; Monotonie function; Pearson; Ranking samples; Semi-supervised learning; Spearman; Transduction,"Thirumalai C., Kalyan Kumar D., Sidhardha K., Reddy D.V.K.",2018,Conference,"Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017",10.1109/ICOEI.2017.8300849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046620460&doi=10.1109%2fICOEI.2017.8300849&partnerID=40&md5=7318b6585ee737a8ab22fd540f3acb68,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509042579
Scopus,Exploration of development projects of renewable energy applications in the ISBSG dataset: Empirical study,"The International Software Benchmarking Standards Group (ISBSG) development and enhancement dataset is a source of data using by academia and industry over around the world. It contains several software projects developed and/or enhanced in different countries for many industrial types or to be used by academia for a systematic empirical studies. This paper explores empirically only the software Development Projects of Renewable Energy Applications in the ISBSG Dataset v. 13 based on software project factors such as effort and team work size to define the correlations between them. In this work, three data analysis techniques were applied: statistical analysis, data clustering, and data visualization. Both SPSS and Rapid Miner are used to conduct the statistical analysis and data visualization. © 2017 IEEE.",Data Clustering; Data Visualization; ISBSG; Renewable Energy Applications; Statistical Analysis; Team Size; Work Effort,"Meridji K., Al-Sarayreh K.T., Abu-Arqoub M., Hadi W.M.",2018,Conference,"Proceedings of 2nd International Conference on the Applications of Information Technology in Developing Renewable Energy Processes and Systems, IT-DREPS 2017",10.1109/IT-DREPS.2017.8277808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046678879&doi=10.1109%2fIT-DREPS.2017.8277808&partnerID=40&md5=44cbc9c9bcee438545aca009be0389a2,"Departments of Software Engineering, University of Petra, Amman, Jordan; Department of Software Engineering, Hashemite University, Zarqa, Jordan; Departments of Computer Science, University of Petra, Amman, Jordan; Department of Computer Information Systems, University of Petra, Amman, Jordan",Institute of Electrical and Electronics Engineers Inc.,English,,9781538619841
Scopus,Effort estimation for service-oriented computing environments,"The concept of service in Service-Oriented Architecture (SOA) makes possible to introduce other ideas like service composition, governance and virtual-ization. Each of these ideas, when exercised to an enterprise level, provides benefits in terms of cost and performance. These ideas bring many new opportunities for the project managers in making the estimates of effort required to produce SOA systems. This is because the SOA systems are different from traditional software projects and there is a lack of efficient metrics and models for providing a high level of confidence in effort estimation. Thus, in this paper, an efficient estimation methodology has been presented based on analyzing the development phases of past SOA based software systems. The objective of this paper is twofold: first, to study and analyze the development phases of some past SOA based systems; second, to propose estimation metrics based on these analyzed parameters. The proposed methodology is facilitated from the use of four regression(s) based estimation models. The validation of the proposed methodology is cross checked by comparing the predictive accuracy, using some commonly used performance measurement indicators and box-plots evaluation. The evaluation results of the study (using industrial data collected from 10 SOA based software systems) show that the effort estimates obtained using the multiple linear regression model are more accurate and indicate an improvement in performance than the other used regression models. © 2018 Slovak Academy of Sciences. All Rights Reserved.",Effort estimation; Orchestration; Regression; SOA; Web services,"Mishra S., Kumar C.",2018,Journal,Computing and Informatics,10.4149/cai_2018_3_553,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064405190&doi=10.4149%2fcai_2018_3_553&partnerID=40&md5=4cd1f91d341343158ad663df9b328fbe,"Department of Computer Science and Engineering, Indian Institute of Technology (Indian School of Mines), Dhanbad, Jharkhand, 826004, India",Slovak Academy of Sciences,English,13359150,
Scopus,The state of practice of software cost estimation: Evidence from thai software firms,"Software cost and effort estimation is crucial for software project management success. It is used for project planning and control of project resources. Parametric software cost estimation models have been introduced for decades since 1960 [1], [2]. Today, researchers are still introducing new software estimation models [3]. The practice of software cost estimation model is, therefore, one of the interest area for researchers [2], [4-9]. The objective of this study was to explore how the software estimation was actually practiced in Thai software firms. This paper presents the findings from the returned 26 the questionnaires from a survey of Thai software firms. The findings indicate the discrepancy between theory and practice. The companies studied have not applied software cost estimation models found in the literature. Expert judgment and analogy method are the most employed instead. The companies also indicate that they cannot find appropriate software cost estimation models or tools that are accurate, easy to learn and use. © 2018 Newswood Limited. All rights reserved.",,Arnuphaptrairong T.,2018,Conference,Lecture Notes in Engineering and Computer Science,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062637315&partnerID=40&md5=c5b83e9af1c090a300db7a1301af4af7,"Department of Statistics, Chulalongkorn Business School, Chulalongkorn University, Bangkok, 10250, Thailand",Newswood Limited,English,20780958,9789881404886
Scopus,Evaluating the quality of UCP-based framework using CK metrics,"Software effort estimation is one of the most important concerns in the software industry. It has received much attention since the last 40 years to improve the accuracy of effort estimate at early stages of software development. Due to this reason, many software estimation models have been proposed such as COCOMO, ObjectMetrix, Use Case Points (UCP) and many more. However, some of the estimation methods were not designed for object-oriented technology that actively encourages reuse strategies. Therefore, due to the popularity of UCP model and the evolution of the object-oriented paradigm, a UCP-based framework and supporting program were developed to assist software developers in building good qualities of software effort estimation programs. This paper evaluates the quality of the UCP-based framework using CK Metrics. The results showed that by implementing the UCP-based framework, the quality of the UCP-based program has improved regarding the understandability, testability, maintainability, and reusability. © 2018 International Journal of Advanced Computer Science and Applications.",Ck metrics; UCP-based framework; Use case points,"Ani Z.C., Hashim N.L., Harun H., Basri S., Sarlan A.",2018,Journal,International Journal of Advanced Computer Science and Applications,10.14569/ijacsa.2018.091188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059005357&doi=10.14569%2fijacsa.2018.091188&partnerID=40&md5=2ae20a887578b582e2f959a3ddd4b292,"School of Computing, Universiti Utara Malaysia, UUM Sintok, Kedah, 06010, Malaysia; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Tronoh, Perak, 31750, Malaysia",Science and Information Organization,English,2158107X,
Scopus,Methods for estimating agile software projects: Systematic literature review,"In recent years, agile methods of software development have gained a lot of attention in the field of software engineering. Several estimation techniques have been proposed by several authors and developers in recent years. This paper performs a Systematic Literature Review aiming to identify the most used metrics e/or methods in the development of agile software and the most used size metrics regarding effort estimates, deadlines and costs in a planning of agile software project. The results suggest that Planning Poker is the most popular technique for agile teams in the planning phase, Story Point and Point of Function are the most used metrics in agile projects for estimating size, time, effort, productivity and cost. © 2018 Universitat zu Koln. All rights reserved.",Agile Software Development; Planning Poker.; Software Estimates; Software Metrics; Story Point,"Canedo E.D., Aranha D.P., De Oliveira Cardoso M., Da Costa R.P., Leite L.L.",2018,Conference,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",10.18293/SEKE2018-031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056872664&doi=10.18293%2fSEKE2018-031&partnerID=40&md5=531f4d6049fe96568ab4c3508ad0a570,"University of Brasília (UnB), P.O. Box 4466-Brasília-DFCEP 70910-900, Brazil",Knowledge Systems Institute Graduate School,English,23259000,1891706446
Scopus,Challenges of using software size in agile software development: A systematic literature review,"Software size is a fundamental measure for software management. Size is used for a variety of purposes, such as benchmarking, normalization, and portfolio measurement, and it is frequently considered as the sole input of estimation. Estimations can be produced for various reasons; e.g., to predict effort, cost and duration of software development projects. There are different types of software size measures. Particularly in projects where agile methodologies are adopted, measurement becomes a significant challenge as it is perceived as a non-value-added task and records of tasks such as requirements identification are not always consistent. The difficulties of applying traditional size measurement techniques in agile contexts, however, do not diminish the need, and new methods and techniques are introduced to improve the manageability of the agile projects. In this paper, we discuss estimation and measurement approaches in relation with ―software size in agile contexts. Based on this review, we present the perceptions of software size and related challenges, such as misinterpretation of size, difficulties in implementation, and acceptability of the measurement processes. We anticipate that providing a baseline for the state of software size measures in agile contexts and presenting related challenges, particularly in terms of its acceptability by practitioners can shed light on the development of new techniques. © 2018 CEUR-WS. All rights reserved.",Agile software development; Estimation; Function points; Line of code; Measurement; Size; Story points; Use case points,"Hacaloglu T., Demirors O.",2018,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053777134&partnerID=40&md5=03753428b3b3b001642e7402b3341980,"Atilim University, Ankara, 06830, Turkey; Middle East Technical University, Ankara, 06800, Turkey; Izmir Institute of Technology, Izmir, 35430, Turkey; University of New South Wales, Sydney, NSW  2052, Australia",CEUR-WS,English,16130073,
Scopus,A survey on measures and measurement practices of Turkish software organizations [Yazılım Sektöründe Ölçümler ve Ölçüm Pratikleri Üzerine Bir Anket Çalışması],"In this paper, the findings and results of a survey, which is designed to understand the measurement techniques used in software development organizations, estimation methods and usage areas of collected data, are presented. The purpose of the survey is to learn software measurement and estimation techniques used in software development industry and opinions of the professionals about them. The survey includes questions about technical, operational, and strategic measures used in software development. Professionals from small, medium and large scaled software development organizations attended to the survey. The results of this study can be used to clarify what measurement techniques and approaches are used in the industry, their frequency, which estimation techniques and approaches are used, and what are the opinions of the participants about measurement.",Software Development Effort Estimation; Software Development Effort Estimation Techniques; Software Measurement; Software Measurement Methods; Software Metrics,"Ercan İ., Salmanoğlu M.",2018,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053684087&partnerID=40&md5=06a21afd05f2dc276fbed6c3f99f1938,"JotForm, Ankara, Turkey; Bilgi Grubu, Ankara, Turkey",CEUR-WS,Turkish,16130073,
Scopus,Analysis of COCOMO and UCP,"After an early requirement design, project managers mostly use the requirement specifications to get an estimate of functional size of software which helps in estimating effort required and tentative cost of the software. An accurate estimate is necessary to be able to negotiate price of a software project and to plan and schedule project activities. Function Point sizing method for estimation is used frequently to estimate functional size of software. Another popular method of functional sizing is Use Case Points (UCP). UCP method of estimation although less used than FP based estimation, but is simpler than FP based method. One reason for this is - FP sizing metric often uses COCOMO-II or other effort estimation method to convert size estimate in FPs into effort estimate where as direct conversion formula can be used for converting size in UCPs to effort estimate. This paper compares results of both approaches for two mid-size business applications and tries to understand the correlation between the results of two approaches. © Springer Nature Singapore Pte Ltd. 2018.",COCOMO (COnstructive COst MOdel); Effort estimation; FP (Function Point); UCP (Use Case Point),"Sharma B., Purohit R.",2018,Conference,Communications in Computer and Information Science,10.1007/978-981-10-8527-7_31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044047943&doi=10.1007%2f978-981-10-8527-7_31&partnerID=40&md5=1629317c77ce02f79fa81a330d0ae749,"Department of CSE, JIET, Jodhpur, India",Springer Verlag,English,18650929,9789811085260
Scopus,Method for Estimation of Software Requirements Using Neural Network Based Classification Technique,"Effective management of software projects depends on ability to make accurate time-predictions. Nowadays, software companies need to deliver their solutions in expected time and budget. There are many factors influencing duration and cost of software projects. This paper provides innovative approach for estimations in early phase of software development. It shows usage of standard methods and its combination with soft computing technique called classification that is used for time-estimation of requirements using two-layer feed-forward neural network, which classifies requirements into time-groups. © 2018, Springer International Publishing AG.",Backpropagation; Classification; Effort estimation; Feed-forward; Neural network; Software project,"Štrba R., Vondrák I., Ježek D., Štolfa S.",2018,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-319-60834-1_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028636742&doi=10.1007%2f978-3-319-60834-1_10&partnerID=40&md5=94d883d22103daef7f19c1740d4f8ae3,"Department of Computer Science, VSB - Technical University of Ostrava, 17. Listopadu 15, Ostrava, Poruba, Czech Republic",Springer Verlag,English,21945357,9783319608334
Scopus,A taxonomy of web effort predictors,"Web engineering as a field has emerged to address challenges associated with developing Web applications. It is known that the development of Web applications differs from the development of non-Web applications, especially regarding some aspects such as Web size metrics. The classification of existing Web engineering knowledge would be beneficial for both practitioners and researchers in many different ways, such as finding research gaps and supporting decision making. In the context of Web effort estimation, a taxonomy was proposed to classify the existing size metrics, and more recently a systematic literature review was conducted to identify aspects related to Web resource/effort estimation. However, there is no study that classifies Web predictors (both size metrics and cost drivers). The main objective of this study is to organize the body of knowledge on Web effort predictors by designing and using a taxonomy, aiming at supporting both research and practice in Web effort estimation. To design our taxonomy, we used a recently proposed taxonomy design method. As input, we used the results of a previously conducted systematic literature review (updated in this study), an existing taxonomy of Web size metrics and expert knowledge. We identified 165 unique Web effort predictors from a final set of 98 primary studies; they were used as one of the basis to design our hierarchical taxonomy. The taxonomy has three levels, organized into 13 categories. We demonstrated the utility of the taxonomy and body of knowledge by using examples. The proposed taxonomy can be beneficial in the following ways: i) It can help to identify research gaps and some literature of interest and ii) it can support the selection of predictors for Web effort estimation. We also intend to extend the taxonomy presented to also include effort estimation techniques and accuracy metrics. © Rinton Press.",Knowledge classification; Taxonomy; Web effort predictors; Web engineering,"Britto R., Usman M., Mendes E.",2017,Journal,Journal of Web Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021630561&partnerID=40&md5=1e5aa791f8c02d042edf6d29d39f6eb9,"Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, 371 49, Sweden; Department of Computer Science and Engineering, Blekinge Institute of Technology, Karlskrona, 371 49, Sweden",Rinton Press Inc.,English,15409589,
Scopus,What makes a task difficult? An empirical study of perceptions of task difficulty,"Estimating the difficulty of tasks is imperative for project planning, task assignment, and cost calculation. However, little is known about how and for what purpose software practitioners estimate task difficulty in their day-To-day work. In this paper, we interviewed 15 professionals to understand their needs and perceptions when estimating task difficulty. We find that practitioners do estimate the difficulty of tasks for scheduling and prioritizing their work. Additionally, performing such estimation requires more than one metric, and across more than one domain (i.e. code metrics, process metrics, and task metrics). The use of metrics that encapsulate different aspects of a task allows developers to gain a holistic view of the task and its potential difficulty. © 2017 IEEE.",effort estimation; empirical study; software metrics; task difficulty,"Leano R., Chattopadhyay S., Sarma A.",2017,Conference,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",10.1109/VLHCC.2017.8103452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040982554&doi=10.1109%2fVLHCC.2017.8103452&partnerID=40&md5=61c906bedff69e27170924bd1487e379,"Department of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, United States",IEEE Computer Society,English,19436092,9781538604434
Scopus,Evaluation of missing value imputation methods for effort estimation using liner regression,"Multivariate regression models have been commonly used to estimate the software development effort to assist project planning and/or management. Since project data sets for model construction often contain missing values, we need to build a complete data set that has no missing values either by using imputation methods. However, while there are several ways to build the complete data set, it is unclear which method is the most suitable for the project data set. In this paper, using project data of 1364 cases (34% missing value rate) collected from several companies, we applied four imputation methods (knn method, applied CF method, Miss Forest method and Multiple Imputation method) to build regression models. Then, using project data of 160 cases (having no missing values), we evaluated the estimation performance of models after applying each imputation method. The result showed that Multiple Imputation method showed the best performance.",,"Toda K., Tsunoda M.",2017,Journal,Computer Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032952870&partnerID=40&md5=216d4829de50149af5d28e838f98b34a,"Faculty of Information Engineering, Fukuoka Institute of Technology, Japan; Faculty of Science and Engineering, Kinki University, United Kingdom",Japan Society for Software Science and Technology,Japanese,02896540,
Scopus,When should we ignore examples with missing values?,"In practice, the dataset collected from data mining usually contains some missing values. It is common practice to perform case deletion by ignoring those data with missing values if the missing rate is certainly small. The aim of this paper is to answer the following question: When should one directly ignore sampled data with missing values? By using different types of datasets having various numbers of attributes, data samples, and classes, it is found that there are some specific patterns that can be considered for case deletion over different datasets without significant performance degradation. In particular, these patterns are extracted to act as the decision rules by a decision tree model. In addition, a comparison is made between cases with deletion and imputation over different datasets with the allowed missing rates and the decision rules. The results show that the classification performance results obtained by case deletion and imputation are similar, which demonstrates the reliability of the extracted decision rules. © 2017, IGI Global.",Case Deletion; Categorical Data; Classification; Data Mining; Imputation; Machine Learning; Missing Values; Numerical Data,"Lin W.-C., Ke S.-W., Tsai C.-F.",2017,Journal,International Journal of Data Warehousing and Mining,10.4018/IJDWM.2017100104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028038261&doi=10.4018%2fIJDWM.2017100104&partnerID=40&md5=6d77ba5c7c601f4dd374af7e4cf6b2cf,"Asia University, Taichung, Taiwan; Chung Yuan Christian University, Taoyuan, Taiwan; Department of Information Management, National Central University, Jhongli, Taiwan",IGI Global,English,15483924,
Scopus,Foreword to the special section on negative results in software engineering,[No abstract available],,"Paige R.F., Cabot J., Ernst N.A.",2017,Editorial,Empirical Software Engineering,10.1007/s10664-017-9498-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010749651&doi=10.1007%2fs10664-017-9498-0&partnerID=40&md5=367f805c82c9bed00230da4b70871015,"Department of Computer Science, University of York, Deramore Lane, York  YO10 5GH, United Kingdom; Internet Interdisciplinary Institute, Universitat Oberta de Catalunya, Av. Carl Friedrich Gauss, 5. Building B3, Castelldefels -, 08860, Spain; Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA  15206, United States",Springer New York LLC,English,13823256,
Scopus,Understanding the variation of software development tasks: A qualitative study,"In order to reduce cost, get to market faster and utilize global talents, large companies often organize their software development globally (distributed over Internet), a paradigm advocated by Internetware. Considering the complexity of distributed software development, it is important to understand the variation of various tasks, so the projects could work more efficiently on plan formulation, personnel organization, or task allocation. The main goal of this paper is to understand the variations of software development tasks. We conduct an interview with 47 interviewees and a survey with 148 people from 15 projects with different size and domain.Through the analysis of interviews and surveys, we find that a software task could be characterized through three aspects: value, difficulty and centrality. Among them, task value is influenced by the role of stakeholders and project context, and is related to the task difficulty and task centrality. Task difficulty is reflected by technology, domain difference, working relationships, customer related issues, and it is also related to the developers' personalities. Task centrality can be described by customer impact, system-wide impact, team impact and future impact. We believe our results can help project managers to optimize task allocation, or adjust project plan, and thus achieve efficient development. © 2017 Association for Computing Machinery.",Efficient development; Qualitative study; Software development task; Task properties,"Tan X., Qin H., Zhou M.",2017,Conference,ACM International Conference Proceeding Series,10.1145/3131704.3131719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032482966&doi=10.1145%2f3131704.3131719&partnerID=40&md5=ad16dc36f0e267e31fcca2cdae3000a4,"Peking University, School of Electronics Engineering and Computer Science, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, 100871, China",Association for Computing Machinery,English,,1595930361
Scopus,When partly missing data matters in software effort development prediction,"The major objective of the paper is to investigate a new probabilistic supervised learning approach that incorporates ""missingness"" into a decision tree classifier splitting criterion at each particular attribute node in terms of software effort development predictive accuracy. The proposed approach is compared empirically with ten supervised learning methods (classifiers) that have mechanisms for dealing with missing values. 10 industrial datasets are utilized for this task. Overall, missing incorporated in attributes 3 is the top performing strategy, followed by C4.5, missing incorporated in attributes, missing incorporated in attributes 2, missing incorporated in attributes, linear discriminant analysis and so on. Classification and regression trees and C4.5 performed well in data with high correlations among attributes while k-nearest neighbour and support vector machines performed well in data with higher complexity (limited number of instances). The worst performing method is repeated incremental pruning to produce error reduction.",Decision tree imputation; Missing data; Software effort prediction,Twala B.,2017,Journal,Journal of Advanced Computational Intelligence and Intelligent Informatics,10.20965/jaciii.2017.p0803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032784495&doi=10.20965%2fjaciii.2017.p0803&partnerID=40&md5=3048fae9fe5e4763f878172528168ed6,"Department of Electrical and Electronic Engineering Science, University of Johannesburg, P.O. Box 524, Auckland Park, Johannesburg, 2006, South Africa",Fuji Technology Press,English,13430130,
Scopus,Estimation of total effort and effort elapsed in each step of software development using optimal Bayesian belief network,"Accuracy in estimating the needed effort for software development caused software effort estimation to be a challenging issue. Beside estimation of total effort, determining the effort elapsed in each software development step is very important because any mistakes in enterprise resource planning can lead to project failure. In this paper, a Bayesian belief network was proposed based on effective components and software development process. In this model, the feedback loops are considered between development steps provided that the return rates are different for each project. Different return rates help us determine the percentages of the elapsed effort in each software development step, distinctively. Moreover, the error measurement resulted from optimized effort estimation and the optimal coefficients to modify the model are sought. The results of the comparison between the proposed model and other models showed that the model has the capability to highly accurately estimate the total effort (with the marginal error of about 0.114) and to estimate the effort elapsed in each software development step.",Bayesian belief network; Enterprise resource planning; Machine learning methods; Optimization; Software effort estimation,"Baghiabad F.Z., Zare H.K., Fallahnezhad M.S., Adibnia F.",2017,Journal,Journal of Information Technology Management,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029546005&partnerID=40&md5=92c87428cec4370525e7939dfb20cc40,"Dep. of Industrial Engineering, Yazd University, Yazd, Iran; Dep. of Computer Engineering, Yazd University, Yazd, Iran",University of Tehran,Arabic,20085893,
Scopus,Stepping towards dynamic measurement for object oriented software,"Software metrics are very helpful in measuring the different aspects of software like cohesion, coupling, polymorphism, inheritance etc. The objective of measuring software metrics are quality assurance, defect prediction, maintainability prediction, cost estimation, debugging, etc. Many authors proposed the use of static metrics for the software maintainability prediction (SMP) and were successful, but static metrics don't take into account the run-time behavior of software. Hence, to capture this dynamic behavior, dynamic metrics are necessary to be evaluated. This paper presents the empirical investigation of dynamic metrics for SMP and also compares them with static metrics. Six machine learning algorithms are used to build the prediction models for both the static and dynamic metrics. The performance of all models is compared using prevalent accuracy measures. Results show that dynamic metrics perform better than static metrics, and can be used as a sound alternative for SMP. © 2016 IEEE.",machine learning algorithms; prediction; software maintainability; software maintenance; software metrics,"Jain A., Chug A.",2017,Conference,"India International Conference on Information Processing, IICIP 2016 - Proceedings",10.1109/IICIP.2016.7975323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027448730&doi=10.1109%2fIICIP.2016.7975323&partnerID=40&md5=1014efbab9ae9500512bfd1c4da3d3e7,"University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, Dwarka, New Delhi, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781467369848
Scopus,Identifying functional requirements inconsistencies in multiteam projects framed into a model-based methodology,"REP (Requirements Engineering Process) is one of the most essential processes within the software project life cycle because it allows describing software product requirements. This specification should be as consistent as possible to enable estimating in a suitable manner the effort required to obtain the final product. REP is complex in itself, but this complexity is greatly increased in big, distributed and heterogeneous projects with multiple analyst teams and high integration among functional modules. This paper presents an approach for the systematic conciliation of functional requirements in big projects dealing with a modelbased approach. It also explains how this approach may be implemented in the context of NDT (Navigational Development Techniques) methodology and finally, it describes a preliminary evaluation of our proposal in CALIPSOneo project by analyzing the improvements obtained with our approach. © Rinton Press.",Ambiguity; Consistency; Distributed teams; Functional requirements; NDT; Requirement gathering,"García-García J.A., Urbieta M., Escalona M.J., Rossi G., Enríquez J.G.",2017,Journal,Journal of Web Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015721947&partnerID=40&md5=449b7e1795f0098e4996a1345f45de0e,"Web Engineering and Early Testing (IWT2) Research Group, University of Seville, Spain; LIFIA, Facultad de Informática, Universidad Nacional de La Plata, Argentina; CONICET, Argentina",Rinton Press Inc.,English,15409589,
Scopus,The development of method of the enhancement of Technical Factor (TF) and Environmental Factor (EF) to the Use Case Point (UCP) to calculate the estimation of software's effort,"Use Case Point (UCP) is one of many approaches used for software project estimation. This approach is implemented by calculating effort estimation based on prediction a total number of workers and software development time. UCP was fist introduced by Karner on 1993. This approach is widely used. It was because some research on UCP showed that UCP approach is better than those approaches developed by experts. The calculation of effort estimation that was mentioned in the paper (Chetan Nagar, 2011), it has been proven that calculation estimation using UCP sometimes doesn't fit the real effort. This leads to the difference in cost calculation and a total number of workers on software development project. © 2016 IEEE.",COCOMO; ECF (Enviromental Complexity Factor); Effort Estimation; TCP (Technical Complexity Factors); Use Case Point (UCP),"Sarwosri, Al Haiyan M.J., Husein M., Putra Ferza A.",2017,Conference,"Proceedings of 2016 International Conference on Information and Communication Technology and Systems, ICTS 2016",10.1109/ICTS.2016.7910299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019433571&doi=10.1109%2fICTS.2016.7910299&partnerID=40&md5=ea2a471d0fe760c37b2e6024d403c6db,"Informatics Department, Faculty of Information Technology, Institut Teknologi Sepuluh Nopember (ITS), St. Arief Rahman Hakim, Surabaya, 60111, Indonesia",Institute of Electrical and Electronics Engineers Inc.,English,,9781509013791
Scopus,A note on rank correlation and semi-supervised machine learning based measure,This paperwork deals with similarity measure using Kendall's coefficient tau and spearman's coefficient rho. The results for the same ordinal values of two variables that are related and we are also going to describe a note of study in sample data that we got using the semi-supervised machine learning predication. We will use simple linear regression model technique to predict. Here we use Pearson correlation to calculate simple linear regression. Though this two coefficient (rank correlation) looks similar it has some differences which give major change as far as rank correlation results are concerned. And finally we will elaborate which one has more advantage and which is better to use and we will elaborate the note of study in data using simple linear regression model. © 2017 IEEE.,correlation; kendall; machine learning; ordinal values; pearson; prediction; semi-supervised; spearman,"Radhakrishnan P., Vignesh B.",2017,Conference,"2017 Innovations in Power and Advanced Computing Technologies, i-PACT 2017",10.1109/IPACT.2017.8245035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045843153&doi=10.1109%2fIPACT.2017.8245035&partnerID=40&md5=fae5992d503ef9c8402dd2fd568d9e5b,"School of Information Technology and Engineering, VIT University, Vellore, India",Institute of Electrical and Electronics Engineers Inc.,English,,9781509056828
Scopus,Software programmer productivity: A complementary-based research model,"The identification of the factors that condition a software programmer’s productivity remains a key challenge for both scholars and practitioners. While a number of studies have focused on the impact of one or a few particular factors, the way these factors jointly condition programmer productivity is still unknown. This paper presents a conceptual model aimed at a comprehensive understanding of the factors that complement each other to govern the productivity of a software programmer. The model is based on complementarity theory and its systems approach and addresses an individual worker’s productivity, which accounts for cognitive, technological, and organizational characteristics. The analyzed factors are organized into a system of complementarities, offering two propositions that specify the conditions of a programmer’s productivity. The model’s key contribution lies in its unique configuration of two systems of complementarities, which have the potential to add to the literature on the productivity of software programmers. The proposed model can be employed as a guidance for the design of empirical investigations of the conditions of individual software programmers’ productivity as well as information worker productivity in general. © 2017 Proceedings of the 25th European Conference on Information Systems, ECIS 2017. All rights reserved.",Complementarity; Individual productivity; Software programming; Systems approach,"Pashkevich N., Haftor D.M.",2017,Conference,"Proceedings of the 25th European Conference on Information Systems, ECIS 2017",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058006703&partnerID=40&md5=0051ae95708f5d903d57f84d4c137c58,"Linnaeus University, Växjö, Sweden",Association for Information Systems,English,,9780991556700
Scopus,Quantifying software: Global and industry perspectives,"Software is one of the most important products in human history and is widely used by all industries and all countries. It is also one of the most expensive and labor-intensive products in human history. Software also has very poor quality that has caused many major disasters and wasted many millions of dollars. Software is also the target of frequent and increasingly serious cyber-attacks. Among the reasons for these software problems is a chronic lack of reliable quantified data. This reference provides quantified data from many countries and many industries based on about 26,000 projects developed using a variety of methodologies and team experience levels. The data has been gathered between 1970 and 2017, so interesting historical trends are available. Since current average software productivity and quality results are suboptimal, this book focuses on “best in class” results and shows not only quantified quality and productivity data from best-in-class organizations, but also the technology stacks used to achieve best-in-class results. The overall goal of this book is to encourage the adoption of best-in-class software metrics and best-in-class technology stacks. It does so by providing current data on average software schedules, effort, costs, and quality for several industries and countries. Because productivity and quality vary by technology and size, the book presents quantitative results for applications between 100 function points and 100,000 function points. It shows quality results using defect potential and DRE metrics because the number one cost driver for software is finding and fixing bugs. The book presents data on cost of quality for software projects and discusses technical debt, but that metric is not standardized. Finally, the book includes some data on three years of software maintenance and enhancements as well as some data on total cost of ownership. © 2018 by Taylor & Francis Group, LLC.",,Jones C.,2017,Book,Quantifying Software: Global and Industry Perspectives,10.1201/9781315314426,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052172094&doi=10.1201%2f9781315314426&partnerID=40&md5=87638d17fd799e6f34b2ad15a0cde075,"Namcook Analytics LLC, United States",CRC Press,English,,9781315314419; 9781138033115
Scopus,E-commerce system size using user based function points,"The accurate Size estimation of the logical product like software is a difficult task to the estimator. Now a days, many new unique featured and complex software are developing. To estimate the size of the software requires a unique and specialized approach. So the complex software like E-Commerce also required a new focused size estimation technique. ECSSIZE: E-Commerce System Size is a new technique to calculate the size of an E-Commerce software using User Based Function Points (UBFP). © Research India Publications.",E-Commerce (Electronic Commerce); ECSSIZE (E-Commerce System Size); UBFP (User Based Function Points),"Dhas J.T.M., Bharathi C.R.",2017,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051685526&partnerID=40&md5=a4a8b2362aa20b2db5c0a7da86f388fd,"VelTech University, Dept. of CSE, RVS Padhmavathy College of Engineering and TechnologyTamil Nadu, India; Department of Electronics and Communication Engineering, VelTech University, Chennai, Tamil Nadu, India",Research India Publications,English,09734562,
Scopus,A framework of statistical and visualization techniques for missing data analysis in software cost estimation,"Software Cost Estimation (SCE) is a critical phase in software development projects. However, due to the growing complexity of the software itself, a common problem in building software cost models is that the available datasets contain lots of missing categorical data. The purpose of this chapter is to show how a framework of statistical, computational, and visualization techniques can be used to evaluate and compare the effect of missing data techniques on the accuracy of cost estimation models. Hence, the authors use five missing data techniques: Multinomial Logistic Regression, Listwise Deletion, Mean Imputation, Expectation Maximization, and Regression Imputation. The evaluation and the comparisons are conducted using Regression Error Characteristic curves, which provide visual comparison of different prediction models, and Regression Error Operating Curves, which examine predictive power of models with respect to under- or over-estimation. © 2018 by IGI Global. All rights reserved.",,"Angelis L., Mittas N., Chatzipetrou P.",2017,Book Chapter,"Computer Systems and Software Engineering: Concepts, Methodologies, Tools, and Applications",10.4018/978-1-5225-3923-0.ch017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041609542&doi=10.4018%2f978-1-5225-3923-0.ch017&partnerID=40&md5=335c577a98fee654a66ca30642b6f9e7,"Aristotle University of Thessaloniki, Greece",IGI Global,English,,9781522539247; 9781522539230
Scopus,Building defect prediction models in practice,"The information about which modules of a future version of a software system will be defect-prone is a valuable planning aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. In this chapter, building a defect prediction model from data is characterized as an instance of a data-mining task, and key questions and consequences arising when establishing defect prediction in a large software development project are discussed. Special emphasis is put on discussions on how to choose a learning algorithm, select features from different data sources, deal with noise and data quality issues, as well as model evaluation for evolving systems. These discussions are accompanied by insights and experiences gained by projects on data mining and defect prediction in the context of large software systems conducted by the authors over the last couple of years. One of these projects has been selected to serve as an illustrative use case throughout the chapter. © 2018 by IGI Global. All rights reserved.",,"Ramler R., Himmelbauer J., Natschläger T.",2017,Book Chapter,"Computer Systems and Software Engineering: Concepts, Methodologies, Tools, and Applications",10.4018/978-1-5225-3923-0.ch014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041605628&doi=10.4018%2f978-1-5225-3923-0.ch014&partnerID=40&md5=6caa71052c8c95451fff62ced48a10e3,"Software Competence Center Hagenberg, Austria",IGI Global,English,,9781522539247; 9781522539230
Scopus,Software test effort estimation: Current practices in Turkey [Yazilimda Test Işgücü Kestirimi: Türkiye'deki Güncel Durum],"Software quality is continuously gaining importance for software organizations to stay competitive on the market. One of the most widely used techniques by the organizations to ensure the quality of the software products is Independent Software Verification and Validation. Since, the activities for this technique are conducted by an independent organization, creating plans to better utilize resources became crucial for both project and independent test team managers. This study presents the results of a systematic literature review and an industrial survey conducted in Turkish software industry, which we conducted to investigate the state of the art on test effort estimation.",Effort estimation; Independent verification and validation; Software quality; Software test,"Adali O.E., Karagöz A., Gürel Z., Ahir T.O.T., Gencel C.",2017,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035145548&partnerID=40&md5=039c145903bccbd1ff72c90a5b86a92e,"PROVEN Information Technologies Ltd., Ankara, Turkey; OnePIN, Inc., Ankara, Turkey; COMSATS Institute of IT, Lahore, Pakistan; Free University of Bozen, Bolzano, Italy",CEUR-WS,Turkish,16130073,
Scopus,An alternative approach of agile count for software product maintenance's size [Uma abordagem alternativa para contagem ágil do tamanho da manutenção do produto de software],"A functional software size metric was customized with focus on reducing the time and effort of the maintenance count process. A bibliographic, documentary and empirical research was carried out by means of the analysis of 10,405 counts of function points - APF (size up to 50 PF) of a Brazilian organization. Based on standards, the Agile Count was proposed, and the result was promising (0.93% considering the detailed count). Comparisons were made with other models whose results were below those of Agile Count. © 2017.",Agile; APF; Maintenance; NESMA,"Calazans A.T.S., Martins E.G.P., Masson E.T.S., Teixeira R.G.",2017,Journal,Espacios,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034984027&partnerID=40&md5=206d6418711a1ef50bc08e13b0e7254d,"Instituto CEUB de Pesquisa e Desenvolvimento - ICPD. Uniceub, Centro Universitário de Brasilia, Brazil; Centralizadora de Desenvolvimento Brasília - CEDESBR, Caixa Econômica Federal, Brazil; Faculdade de Tecnologia e Ciências Sociais Aplicadas - FATECS. Uniceub, Centro Universitário de Brasilia, Brazil",Revista Espacios,Portuguese,07981015,
Scopus,A metric for evaluating residual complexity in software,"A new metric for evaluating the complexity of software is proposed: The residual complexity. This is the combination of a complexity metric with a code coverage metric. It indicates how well the complexity of a software is handled by software tests, and how much complexity still remains untested. In this paper we give an overview over existing source code metrics and code coverage metrics. Afterwards the residual complexity is described and the consequences are discussed. In the end a use case is shown on a real life example of a software application implemented in.NET. © 2017, Springer International Publishing AG.",Branch coverage; Complexity metric; Cyclomatic complexity; Residual complexity; Software metric; Software quality,"Krisper M., Iber J., Kreiner C., Quaritsch M.",2017,Conference,Communications in Computer and Information Science,10.1007/978-3-319-64218-5_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030644562&doi=10.1007%2f978-3-319-64218-5_11&partnerID=40&md5=ad4e7576cf57b7770b1bb940e031c0ab,"Graz University of Technology, Graz, Austria",Springer Verlag,English,18650929,9783319642178
Scopus,An approach of statistical methods for improve software quality and cost minimization,"High quality software products play an important role for economic business growth of any organization. Software companies need to maintain their own existent in business world to facing lots of challenges like defect origins, defect tracking, defect removal, and finding the injected bugs or defects into the software development life cycle. These defects or bugs have breakdowns there economic business growth. In this article, the paper discuses Phase by analysis of software project have the highest probability for finding the errors and bugs during their development time and re-inspected the software products. Software engineers have great competitive pressure to improving the software quality and reducing their skyrocketing of software cost. Cost of quality has big challenges to minimizing failure cost and improving the prevention cost, cost of poor quality is affected by internal and external failure cost. In this article, it describes the approach of cost Models to improve the software quality and statistical process control to minimizing the failure cost. Defect removal matrices to improve the software quality as well as minimizing the internal or external failure cost. Phase by removal process and tracking the defects at each level which are injected during the software development life cycle process. In this methodology it enables the predictions of software quality and reducing the estimated cost. © Research India Publications.",Defect injection; Fault detection; Quality cost model; Quality cost reduction; Software quality; Statistical methods,"Marandi A.K., Khan D.A.",2017,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017315705&partnerID=40&md5=a9654c0744eb26cf72870e9786b56dc5,"Dept. of computer applications, National Institute of Technology, Jamshedpur, 831014, India",Research India Publications,English,09734562,
Scopus,The application of meta-heuristic algorithms to improve the performance of software development effort estimation models,"One of the major activities in effective and efficient production of software projects is the precise estimation of software development effort. Estimation of the effort in primary steps of software development is one of the most important challenges in managing software projects. Some reasons for these challenges such as: discordant software projects, the complexity of the manufacturing process, special role of human and high level of obscure and unusual features of software projects can be noted. Predicting the necessary efforts to develop software using meta-heuristic optimization algorithms has made significant progressions in this field. These algorithms have the potent to be used in estimation of the effort of the software. The necessity to increase estimation precision urged the authors to survey the efficiency of some meta-heuristic optimization algorithms and their effects on the software projects. To do so, in this paper, they investigated the effect of combining various optimization algorithms such as genetic algorithm, particle swarm optimization algorithm and ant colony algorithm on different models such as COCOMO, estimation based on analogy, machine learning methods and standard estimation models. These models have employed various data sets to evaluate the results such as COCOMO, Desharnais, NASA, Kemerer, CF, DPS, ISBSG and Koten & Gary. The results of this survey can be used by researchers as a primary reference. © 2017 by IGI Global. All rights reserved.",,"Saadi M.H., Bardsiri V.K., Ziaaddini F.",2016,Book Chapter,"Artificial Intelligence: Concepts, Methodologies, Tools, and Applications",10.4018/978-1-5225-1759-7.ch062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018535224&doi=10.4018%2f978-1-5225-1759-7.ch062&partnerID=40&md5=fb8ce92eaddb2ca4b538a0a9f124eab5,"Islamic Azad University, Kerman, Iran",IGI Global,English,,9781522517603; 1522517596; 9781522517597
Scopus,Hypermedia web software effort estimate with adaptive neuro fuzzy inference system,"Accurate software cost estimates are an important factor in the stability of the software companies in the world competitive and efficient use of resources. Nature and structure of web applications is quite different from traditional software. In 2003, The estimated cost of hypermedia web projects was based on seven features were obtained best results, using case base reasoning (CBR) using Stepwise Regression approaches with MMRE on 37 web hypermedia projects. We considered count of html, count of media files and count of inner links features, presented in this paper proposed approach to reduce predicted effort Error than the actual amount for web hypermedia projects and calculate average relative deviations (AAD), through adaptive neuro fuzzy system (ANFIS) Method that is achieved better and more accurate results. © 2005 - 2016 JATIT & LLS. All rights reserved.",Cost; Effort; Fuzzy; Hypermedia web software; Neural network,Iraji M.S.,2016,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995676366&partnerID=40&md5=d5dda17f602f141b5186ea16d31a0714,"Faculty Member of Department of Computer Engineering and Information Technology, Payame Noor University, Iran",Asian Research Publishing Network,English,19928645,
Scopus,An approach for multimedia software size estimation,"Although, the Function Point Analysis has offered an idea to estimate the size of software in both planned and existing one [1], use of multimedia technology has provided a different direction for delivering instruction. The interested users in the era of Graphics User Interfaces, are being attracted, gets benefited and have new learning capabilities and this is reason the two-way multimedia training is a process, rather than being termed as a technology. Multimedia software Developer should use suitable methods for designing the package which will not only enhance its capabilities but will also be user friendly. Mixed media projects can be utilized to present data in combined form in a number of ways to energize routes hypermedia strategies with guideline. Great presentations can be made when they depend on psychological targets that create attention on the learning of themes at distinctive levels of appreciation. All media segments, for example, design, sound and video and so forth it adds to learning. While developing multimedia programs, developer should focus mainly on three point; firstly grabbing the user's attention; secondly it should ease the user to find and organize all necessary information and finally to integrate all information into the user's knowledge data bank. All elements like text, graphics icon and audio visual elements need to be utilized at the maximum to create visual appeal and organized in structured program, and thus for the purpose of systematic organization it is convenient to divide the screen into functional areas by the Developers. Class and sequence diagrams were utilized solely for the generation of FP counts [2]. A similar technique for creating automated counts using COSMIC FFP methods likewise depended on UML graphs [3]. But, neither of these strategies measures will be used for the estimation mixed media software the developers. © 2016 IEEE.",MCAF-Multimedia Complexity Adjustment Factor; MFPA-Multimedia Function Point Analysis; MLI-Level of Impact for Multimedia; MUFP-Multimedia Unadjusted Function Points; MVAF-Value Adjustment Factor for Multimedia,"Kumar S., Nag R.",2016,Conference,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997235977&partnerID=40&md5=e81eaa8583d3be82831cf068b4b29c51,"Sharda University, Greater Noida, India; BIT, Extention Centre, Mesra, Noida, India",Institute of Electrical and Electronics Engineers Inc.,English,,9789380544199
Scopus,Background and overview of the COSMIC-ISO 19761,[No abstract available],,[No author name available],2016,Book Chapter,COSMIC Function Points: Theory and Advanced Practices,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052505049&partnerID=40&md5=980ccb5aa433e100fe18e5711f479bb6,,CRC Press,English,,9781439844878; 9781439844861
Scopus,Consideration of Similarity Factors in Integration of FP and SLOC for Software Project Estimation,"This research paper includes the use and explanations related to advantages of two ""public domain"" costing methods i.e., Function Point and Source Lines of Codes for size estimation. Research paper demonstrates the effect of deviation between SLOC and FP and use of homogeneous data can provide the acceptable results by reducing deviation as established. It is demonstrated and established that the combination of physical size and functional size using the LOC and function points can affect the productivity. Such estimates are of very high degree of accuracy. © 2015 IEEE.",COCOMO; Function Point; homogeneous data Productivity; Source Lines of Codes,"Singh B.K., Punhani A., Tiwari S., Misra A.K.",2016,Conference,"Proceedings - 8th International Conference on Advanced Software Engineering and Its Applications, ASEA 2015",10.1109/ASEA.2015.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964901403&doi=10.1109%2fASEA.2015.17&partnerID=40&md5=26292617f7134ce927a3107240e30a9e,"Department of CSE, FET, R.B.S. College, Agra, U. P., India; Department of CSE, GLA University, Mathura, U. P., India; Department of CSE, ITS Engg. College, Gr. Noida, U. P., India; Department of CSE, MNNIT, Allahabad, U. P., India",Institute of Electrical and Electronics Engineers Inc.,English,,9781467398374
Scopus,Software code generator in automotive field,"Rapid development of new technology has resulted changes in IT world affecting the way we work. Software use has increased in different fields of technology. In automation field, for example, the usage of Electronics Control Unit (ECU) has increased resulting in a different size of software. Due to persistent role of software in technology, the software cost and quality has become an important field. In automation field a number of tools are being developed which help to generate software codes. These tools can be used in software cost reduction because of reuse of software modules. In automatic code generation, the size estimates can hide the true effort of a program. In this paper we discuss the effect of using code generators in automation sector and how these tools have an effect on the cost of the software. We will compare the effort of code written manually and by using a code generator. © 2015 IEEE.",Automotive; Code effort; Code generator; Software engineering,"Nadir S., Streitferdt D.",2016,Conference,"Proceedings - 2015 International Conference on Computational Science and Computational Intelligence, CSCI 2015",10.1109/CSCI.2015.186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964434755&doi=10.1109%2fCSCI.2015.186&partnerID=40&md5=a22f82f0392feaa49c3b3924848708a5,"Technical University of Ilmenau, Ilmenau, Germany",Institute of Electrical and Electronics Engineers Inc.,English,,9781467397957
Scopus,Supporting mobile development project-based learning by Software Project and Product Measures,"Project-based learning is a kind of learning activity which has great educative effect, but which presents also several issues. In particular, if we consider an university course that requires the design and the implementation of a software project, may be difficult to estimate the number of hours that a team of students has to take to accomplish that project. There is the risk to underestimate the project (too difficult) or to overestimate it (too easy) with respect to the other projects of the same course and the amount of foreseen work hours. In this paper, we present the experience we gained in the adoption of Software Project and Product Measures for addressing the project size of projects performed during a Mobile Application Development course for Computer Science students at the University of Salerno. The course foresaw a project work conducted by students organized in teams. The goal of the project work was to design and develop an Android-based application with back-end for smart devices. Software estimation project measures are applied to some metrics extracted in the requirement analysis phase to get an estimation of the effort in terms of man/hours and consequently to adjust the project size by adding/reducing requirements. The metrics extracted from the projects of academic year 2013/14 have been used in the successive year for estimating the project effort and intervene on the project size variables.",,"Francese R., Gravino C., Risi M., Scanniello G., Tortora G.",2016,Conference,Proceedings - DMS 2016: 22nd International Conference on Distributed Multimedia Systems,10.18293/DMS2016-045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014415223&doi=10.18293%2fDMS2016-045&partnerID=40&md5=df778014423881dd3ee4294995d19418,"University of Salerno, Italy; University of Basilicata, Italy",Knowledge Systems Institute Graduate School,English,,1891706403; 9781891706400
Scopus,Software metrics and soft computing models for component based software: A review,"For the past ten years object oriented software development is not strong enough to handle changing requirements of the software. Component based software development deals with the different aspect that is use of components for the development of software. A reusable piece of software that is independent of any application is known as software component. Low cost, good quality and less time for the development software with others also are some of the important advantages. Literature review of software metrics for component based system and the soft computing models is done in this paper which are developed by number researchers. From the review we have concluded that more research in the field of soft computing can be done and implementation on the real projects can also be done. © International Science Press.",Component based system; Component-based software engineering; Software component; Software metrics,"Sagar S., Mathur P., Sharma A.",2016,Journal,International Journal of Control Theory and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010917031&partnerID=40&md5=d145938c632b5523df1d0d79112972ff,"Department of Computer Science, Banasthali University, Rajasthan, India; Department of IT, Indira Gandhi Delhi Technical University for Women, Delhi, India",Serials Publications,English,09745572,
Scopus,Analytic study of fuzzy-based model for software cost estimation,"The need for successful software projects has been a major area of discourse amongst researchers and software developers in academia and software industry respectively. Failure of software projects has been tied to flawed estimation at the early stages of software development life cycle. Recently, soft computing techniques such as Fuzzy logic models has been seen as an alternative to handle uncertainties and vagueness of input parameters to the early software estimation models. In order to analyze the various conditions which affect estimation accuracy of fuzzy-based models, a sample of 93 COCOMO NASA projects was used to develop two groups of fuzzy models. One was the controlled group while the other was the experimental group varying in conditions of model structure, linguistic variables, parameters of input and output variables. A comparative analysis of the Mean Magnitude of Relative Error (MMRE) and Prediction accuracy Pred(l) evaluation criteria for the models was made and findings recorded. Results from the experiments show that the performance of a fuzzy-based software cost estimation model utilizing Takagi-Sugeno inference, Gaussian/Sigmoid membership function with more number of input variables and linguistics variables is more efficient. Copyright © 2016 Ibadan ACM.",Fuzzy Inference System; Fuzzy model; Membership function (MF); Performance; Software cost estimation (SCE),"Nwaiwu J.C., Oluwadare S.A.",2016,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009376475&partnerID=40&md5=108c6aadc8734ce283fffae33f57c39c,"Computer Science Department, Federal University of Technology Akure, Nigeria",CEUR-WS,English,16130073,
Scopus,A Survey Report on Various Software Estimation Techniques and Practices,"Paper offers thoroughly examine of software and project analysis methods established in industry and literature, its skills and flaws The Software Estimation is very important task for completing the project successfully. A successful software project development not only relies on the product efficiency but also the accurate estimation The estimation in software development depends on various factors especially on cost and effort factors for which further AI(Artificial Intelligence) and Algorithmic models have been put into usage. The low accuracy and non- reliable structures of the algorithmic models led to high risks of software projects. So, it is needed to estimate the cost of the project annually and compare it to the other techniques. The Metaheuristic algorithms have been developed well lately in software fields. Metaheuristics like Genetic Algorithms (GA) and Ant Colony Optimization (ACO) solve the problems according to the optimization of the problems and are very efficient in optimizing the algorithmic models and the effective factors in cost estimation. This review aims to discuss the methods for calculating and optimizing the metrices by using metaheuristic algorithms for software development. For this purpose survey of already implemented meta- heuristic algorithms like MOPSO, Bee Colony Optimization and Firefly is done. © International Science Press.",Effort estimation; Metaheuristic optimization; Software quality,"Nandal D., Sangwan O.P.",2016,Journal,International Journal of Control Theory and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006931247&partnerID=40&md5=3773f0a7333409750fba13d2cdbab015,"GJUS and T, Hisar, India",Serials Publications,English,09745572,
Scopus,Executable Behavioral Modeling of System and Software Architecture Specifications to Inform Resourcing Decisions,"The size, cost, and slow rate of change of DoD Information Technology (IT) systems in comparison with commercial IT makes introduction of a new DoD system or capability challenging. Making design decisions without consideration of the whole system and its environment may result in unintended behaviors that have operational and financial impacts, often not visible until later testing. The complexity of these system interactions isn't cheap, impacting intellectual, programmatic, and organizational resources. Precise behavioral modeling offers a way to assess architectural design decisions prior to, during, and after implementation to mitigate the impacts of complexity, but in and of itself does not lead to estimates of the effort and the cost of those design decisions. This research introduces a methodology to extract Unadjusted Function Point (UFP) counts from architectural behavioral models utilizing a framework called Monterey Phoenix (MP), lightweight formal methods, and high level pseudocode for use in cost estimation models such as COCOMO II. Additionally, integration test estimates are informed by extracts of MP model event traces. These unambiguous, executable architecture models and their views can be inspected and revised, in order to facilitate communication with stakeholders, reduce the potential for software failure, and lower costs in implementation. © 2016 Author.",architecture; behaviors; COCOMO II; complexity; event traces; function point analysis; lightweight formal methods,"Farah-Stapleton M., Auguston M., Giammarco K.",2016,Conference,Procedia Computer Science,10.1016/j.procs.2016.09.292,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998880891&doi=10.1016%2fj.procs.2016.09.292&partnerID=40&md5=d24da104a09cde2a14af4e9526ca8659,"PEO DHMS, 1700 N. Moore St, Rosslyn, VA, United States; Naval Postgraduate School, Monterey, CA, United States",Elsevier B.V.,English,18770509,
Scopus,Clustering and artificial neural network ensembles based effort estimation,"Accurate effort estimation of software development projects plays a key role in project success. However, it is still a challenge activity to researchers and practitioners because of the nature of software products and dynamics in software industry and development environment. Artificial neural network (ANN) is as an effective method and has been widely used in various areas of software engineering. This paper proposes a new effort estimation method based on clustering and ANN ensembles. The contribution of the paper is twofold. First, the impact of clustering projects on the estimation accuracy is investigated. Second, the impact of using ANN ensembles instead of a single ANN is also investigated. The proposed method includes three phases called pre-processing, k-means clustering, and ANN ensembles effort estimation. The method starts with exploring the historical projects dataset. Afterward, k-means is used to cluster the projects. Finally, the proposed method as well as two other estimation methods (i.e. a single ANN and expert-based) were applied to the created clusters and results were compared using MMRE and PRED measures. The simulation results show that the proposed method significantly outperforms the two other estimation methods. Copyright © 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.",Artifical neural network ensembles; Clustering; Effort estimation; K-means; Mean magnitude relative error; Percentage of/predictions,"Ibrahim H., Far B.H.",2016,Conference,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",10.18293/seke2016-250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988447189&doi=10.18293%2fseke2016-250&partnerID=40&md5=8cd3fca8f03f80f61536ff9647059b95,"Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; Department of Information Systems, Menoufia University, Shebin Elkom, Egypt",Knowledge Systems Institute Graduate School,English,23259000,189170639X; 9781891706394
Scopus,A brief analysis of reported problems in the use of function points,"Know the software size is a key issue to guide the planning and management of a software project. In this context, Function Point Analysis (FPA) has been consolidated as a strategic tool for measuring the functional size of software. The function point metric is the most widespread in the world, but despite its growth has received several criticisms from its users. This paper presents an investigation of the problems and difficulties on the application of FPA. As a result, the reported problems were analyzed and proposed solutions to these problems were presented. © Springer International Publishing Switzerland 2016.",Function point analysis; Reported problems; Software project,"Silva A., Pinheiro P., Albuquerque A.",2016,Conference,Advances in Intelligent Systems and Computing,10.1007/978-3-319-33622-0_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964780406&doi=10.1007%2f978-3-319-33622-0_11&partnerID=40&md5=bc62c005c94b0b64d5e5dac729d03e28,"University of Fortaleza, Av. Washinton Soares, 1321, BL J, SL 30, Fortaleza, Ceará  60833-155, Brazil",Springer Verlag,English,21945357,9783319336206
Scopus,An experimental comparison of three machine learning techniques for web cost estimation,"Many comparative studies on the performance of machine learning (ML) techniques for web cost estimation (WCE) have been reported in the literature. However, not much attention have been given to understanding the conceptual differences and similarities that exist in the application of these ML techniques for WCE, which could provide credible guide for upcoming practitioners and researchers in predicting the cost of new web projects. This paper presents a comparative analysis of three prominent machine learning Techniques-Case-Based Reasoning (CBR), Support Vector Regression (SVR) and Artificial Neural Network (ANN)-In terms of performance, applicability, and their conceptual differences and similarities for WCE by using data obtained from a public dataset (www.tukutuku.com). Results from experiments show that SVR and ANN provides more accurate predictions of effort, although SVR require fewer parameters to generate good predictions than ANN. CBR was not as accurate, but its good explanation attribute gives it a higher descriptive value. The study also outlined specific characteristics of the 3 ML techniques that could foster or inhibit their adoption for WCE. © 2016 SERSC.",Artificial neural networks; Case based reasoning; Machine learning; Support vector regression; Web cost estimation,"Daramola O., Ajala I., Akinyemi I.",2016,Journal,International Journal of Software Engineering and its Applications,10.14257/ijseia.2016.10.2.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960455414&doi=10.14257%2fijseia.2016.10.2.16&partnerID=40&md5=6b7274c7588264db521949fcbea88ad2,"Department of Computer and Information Sciences, Covenant University, Ota, Nigeria",Science and Engineering Research Support Society,English,17389984,
Scopus,A novel framework for integrating data mining techniques to software development phases,"In software development process, phases such as development effort estimation, code optimization, source code defect detection and software reuse are very important in order to improve the productivity and quality of the software. Software repository data produced in each phases have increased as component of software development process and new data analysis techniques have emerged in order to optimize the software development process. There is a gap between the software project management practices and the need of valuable data from software repository. To overcome this gap, a novel integrated framework is proposed, which integrates data mining techniques to extract valuable information from software repository and software metrics are used in different phases of software development process. Integrated framework can be used by software development project managers to improve quality of software and reduce time in predicting effort estimation, optimizing source code, defect detection and classification. © Springer India 2016.",Code optimization; Defect prediction and classification; Software development process; Software effort estimation; Software reuse,"Ajay Prakash B.V., Ashoka D.V., Manjunath Aradhya V.N.",2016,Conference,Advances in Intelligent Systems and Computing,10.1007/978-81-322-2752-6_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959036722&doi=10.1007%2f978-81-322-2752-6_47&partnerID=40&md5=5eb0b37f9dc86e8ff5f88548fe0fd22f,"Department of Computer Science and Engineering, SJBIT, Bengaluru, India; Department of Computer Science and Engineering, JSSATE, Bengaluru, India; Department of MCA, Sri Jayachamarajendra College of Engineering, Mysuru, India",Springer Verlag,English,21945357,9788132227502
Scopus,Evolving the reliability for cloud system using priority metric,"The evolution of cloud in the past several years has shifted the IT industry toward utilizing the cloud as software, as a platform, or even as an entire infrastructure. As the demand for different cloud services is growing rapidly, it becomes a challenge for the cloud service providers to ensure the quality of the delivery of services. Cloud service quality refers to the ability to guarantee an intended level of performance or to provide different priorities to different users or applications. There are many ways to provide quality of cloud services, such as scheduling, admission control, traffic control, dynamic resource provisioning, etc. The essential criteria of QoS (quality of service) are reliability, availability, latency, price, etc. Reliability is the quality changing over time, i.e., failure-free service within a specific period of time. Therefore, reliability of cloud services is one of the most important issues in today’s scenario. While scheduling the cloud resources to the users in the form of virtual machines, it is important to allocate the virtual machines according to their reliability and the users’ priority. The authors have proposed an algorithm and also done simulation work regarding this problem. The authors also intend to deploy this algorithm in a controlled environment with 100 virtual machines. © Springer Science+Business Media Singapore 2016.",Cloud computing; Data center; Failure; Priority; Reliability; Virtual machine,"Chowdhury A., Agrawal K., Tripathi P.",2016,Conference,Advances in Intelligent Systems and Computing,10.1007/978-981-10-0129-1_70,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959019422&doi=10.1007%2f978-981-10-0129-1_70&partnerID=40&md5=2c2ac986e5e4343d9206309c56e33ba7,"Computer Engineering and Applications, National Institute of Technical Teachers’ Training and Research, Bhopal, India",Springer Verlag,English,21945357,9789811001277
Scopus,Measuring the utility of functional-based software using centroid-adjusted class labelling,"The functional programming paradigm involves stateless computation on immutable data constructs. While this paradigm’s historical context dates back to the early twentieth century with lambda calculus and a formal study of computability and function definition, there has been a resurgence in functional programming, especially in the area of predictive analytics. New, purely functional, languages have recently emerged, and functional extensions have been added to several popular programming languages. It is sometimes difficult to estimate the overall utility and extensibility of functional programming software components. At the same time, many software metrics exist that attempt to quantify various qualitative attributes of software components. Here, we use a computational intelligence strategy that uses a set of software metrics to predict the qualitative utility of a software system’s underlying components. Centroid-adjusted class labelling is a pattern classification preprocessing method that compensates for the possible imprecision of an established external reference test (gold standard) by adjusting, when necessary, design pattern class labels while maintaining the reference test’s discriminatory power. The adjusted design labels incorporate within-class centroid information using robust measures of location and dispersion. This method is applied to a biomedical data analysis software system written in a functional programming style. It is shown that significant improvement to the discriminatory power of the classifier is obtained when using this preprocessing method. © Springer International Publishing Switzerland 2016.",Functional programming; Java lambda; Pattern classification; Software engineering; Software metrics; Software utility,Pizzi N.J.,2016,Book Chapter,Studies in Computational Intelligence,10.1007/978-3-319-25964-2_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955259659&doi=10.1007%2f978-3-319-25964-2_6&partnerID=40&md5=5dc4363dfdcda387e5fd84d618548553,"InfoMagnetics Technologies Corporation, Research and Technology Development, Winnipeg, MB  R3C 3Z5, Canada; Department of Computer Science, University of Manitoba, Winnipeg, MB  R3T 2N2, Canada",Springer Verlag,English,1860949X,
Scopus,Cost estimation in software engineering projects with web components development [Estimación de costes en proyectos de ingeniería software con desarrollo de componentes web],"A variety of models for the prediction of effort costs in software projects have been developed, including some that are specific for Web applications. In this research we tried to determine if the use of specific models is justified by comparing the cost behavior of Web and no- Web projects. We focused on two aspects of the cost calculation: the diseconomies of scale in software development and the impact of some project features that are used as cost drivers. We hypothesized that for these kinds of projects, diseconomies of scale are higher but the cost-increasing effect of the cost drivers is mitigated. We tested such hypotheses using a set of real projects. Our results suggest that both hypotheses hold. Thus, the present research’s main contribution to the literature is that the development of specific models for the estimation of effort costs for the case of Web developments is justified. © The author; licensee Universidad Nacional de Colombia.",Costs; Estimation; Software projects; Web,"De Andrés J., Fernández-Lanvin D., Lorca P.",2015,Journal,DYNA (Colombia),10.15446/dyna.v82n192.43366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941637771&doi=10.15446%2fdyna.v82n192.43366&partnerID=40&md5=0ed381dc70b6cf58a3dbe306d5a535b6,"Accounting Department, University of Oviedo, Asturias, Spain; Department of Computer Sciences, University of Oviedo, Asturias, Spain",Universidad Nacional de Colombia,English,00127353,
Scopus,Towards an automation of the traceability of bugs from development logs - A study based on open source software,"Context: Information and tracking of defects can be severely incomplete in almost every Open Source project, resulting in a reduced traceability of defects into the development logs (i.e., version control commit logs). In particular, defect data often appears not in sync when considering what developers logged as their actions. Synchronizing or completing the missing data of the bug repositories, with the logs detailing the actions of developers, would benefit various branches of empirical software engineering research: prediction of software faults, software reliability, traceability, software quality, effort and cost estimation, bug prediction and bug fixing. Objective: To design a framework that automates the process of synchronizing and filling the gaps of the development logs and bug issue data for open source software projects. Method: We instantiate the framework with a sample of OSS projects from GitHub, and by parsing, linking and filling the gaps found in their bug issue data, and development logs. UML diagrams show the relevant modules that will be used to merge, link and connect the bug issue data with the development data. Results: Analysing a sample of over 300 OSS projects we observed that around 1/2 of bug-related data is present in either development logs or issue tracker logs: the rest of the data is missing from one or the other source. We designed an automated approach that fills the gaps of either source by making use of the available data, and we successfully mapped all the missing data of the analysed projects, when using one heuristics of annotating bugs. Other heuristics need to be investigated and implemented. Conclusion: In this paper a framework to synchronise the development logs and bug data used in empirical software engineering was designed to automatically fill the missing parts of development logs and bugs of issue data. Copyright 2015 ACM.",,"Romo B.A., Capiluppi A.",2015,Conference,ACM International Conference Proceeding Series,10.1145/2745802.2745833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961154423&doi=10.1145%2f2745802.2745833&partnerID=40&md5=a46cf854cfa018244aed438a4d3389ad,"Department of Computer Science, Brunel University, London, United Kingdom",Association for Computing Machinery,English,,9781450333504
Scopus,Case consistency: A necessary data quality property for software engineering data sets,"Data quality is an essential aspect in any empirical study, because the validity of models and/or analysis results derived from an empirical data is inherently in uenced by its quality. In this empirical study, we focus on data consistency as a critical factor in uencing the accuracy of prediction mod- els in software engineering. We propose a software metric called Cases Inconsistency Level (CIL) for analyzing con icts within software engineering data sets by leveraging probabil- ity statistics on project cases and counting the number of con icting pairs. The result demonstrated that CIL is able to be used as a metric to identify either consistent data sets or inconsistent data sets, which are valuable for building robust prediction models. In addition to measuring the level of con- sistency, CIL is proved to be applicable to predict whether or not an effort model built from data set can achieve higher accuracy, an important indicator for empirical experiments in software engineering. Copyright 2015 ACM.",Data consistency; Data quality; Empirical software engi-neering; Heterogeneous distance function; Software effort estimation,"Phannachittay P., Mondeny A., Keungz J., Matsumotoy K.",2015,Conference,ACM International Conference Proceeding Series,10.1145/2745802.2745820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961143219&doi=10.1145%2f2745802.2745820&partnerID=40&md5=148823f16fe61f9f7647baf3a86dfad7,"Graduate School of Information Science, Department of Computer Science, Nara Institute of Science and Technology, City University of Hong Kong, Hong Kong",Association for Computing Machinery,English,,9781450333504
Scopus,A concern-oriented framework for dynamic measurements,"Evolving software programs requires that software developers reason quantitatively about the modularity impact of several concerns, which are often scattered over the system. To this respect, concern-oriented software analysis is rising to a dominant position in software development. Hence, measurement techniques play a fundamental role in assessing the concern modularity of a software system. Unfortunately, existing measurements are still fundamentally module-oriented rather than concern-oriented. Moreover, the few available concern-oriented metrics are defined in a non-systematic and shared way and mainly focus on static properties of a concern, even if many properties can only be accurately quantified at runtime. Hence, novel concern-oriented measurements and, in particular, shared and systematic ways to define them are still welcome. This paper poses the basis for a unified framework for concern-driven measurement. The framework provides a basic terminology and criteria for defining novel concern metrics. To evaluate the framework feasibility and effectiveness, we have shown how it can be used to adapt some classic metrics to quantify concerns and in particular to instantiate new dynamic concern metrics from their static counterparts. © 2014 Elsevier B.V. All rights reserved.",Software feature and concern; Software measurements and metrics; Static and dynamic software artifact analysis,"Cazzola W., Marchetto A.",2015,Journal,Information and Software Technology,10.1016/j.infsof.2014.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983590411&doi=10.1016%2fj.infsof.2014.08.006&partnerID=40&md5=29987858c8651dbea90909c3e18f021d,"Department of Computer Science, Universitá Degli Studi di Milano, Italy; Fondazione Bruno Kessler, Trento, Italy",Elsevier B.V.,English,09505849,
Scopus,CEUR Workshop Proceedings,The proceedings contain 21 papers. The topics discussed include: towards proactive management of technical debt by software metrics; defining metrics for continuous delivery and deployment pipeline; metrics for Gerrit code review; test suite evaluation using code coverage based metrics; accounting testing in software cost estimation: a case study of the current practice and impacts; ICDO: integrated cloud-based development tool for DevOps; internal marketplace as a mechanism for promoting software reuse; lean startup meets software product lines: survival of the fittest or letting products bloom?; model-based technology of software development in large; requirements management in GitHub with a lean approach; and preventing malicious attacks by diversifying Linux shell commands.,,[No author name available],2015,Conference Review,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962594558&partnerID=40&md5=22e820441d057765225c7bfc109fc322,,CEUR-WS,English,16130073,
Scopus,Statistical analysis of defect removal effectiveness to improve the software quality and reducing the estimated cost,"Software companies need to reduce their estimated cost for surviving the business world. Now the present scenario software industries faces with greater competitive pressures and skyrocketing costs of software breakdown, to push high quality software within their limits to achieve new heights. Software quality and reducing the estimated cost, in this approach method for finding the solution of parameters in linear regression models with cost estimating method. It describes the approach of software and cost analysis with historical project data. In this methodology, Software quality model can make timely predictions of reliability indications; it's enabling to improve software development processes by target reducing the estimated cost for software products and improve the techniques for more effectively and efficiently. © 2015 IEEE.",Cost estimation; Defect injection; Defect removal; Defect tracking; Linear regression model; Quality management; Software quality; Statistical analysis,"Marandi A.K., Khan D.A.",2015,Conference,"2015 International Conference on Computing for Sustainable Global Development, INDIACom 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960874569&partnerID=40&md5=9d67dd79ce8314e6d66676b9317af4f7,"Department of Computer Applications, National Institute of Technology, Jamshedpur, India",Institute of Electrical and Electronics Engineers Inc.,English,,9789380544168
Scopus,Form tabanli uygulamalar için çaba kestirimi,[No abstract available],,"Haslak U.K., Selçuk Y.E.",2015,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954569580&partnerID=40&md5=c269a1849e0b37959feea84f95fc759e,"Yildiz Teknik Üniversitesi, Bilgisayar Müh., Besiktas, Istanbul, 34349, Turkey",CEUR-WS,Turkish,16130073,
Scopus,Evidence-based software portfolio management,"In this paper, we describe the research proposal for an approach for Evidence-Based Software Portfolio Management; a new way to help software companies in steering their software portfolio's based on cost, duration, defects found on the one hand and stakeholder satisfaction and perceived value on the other. The research approach is based on instruments such as a Cost / Duration Matrix, the identification of success and failure factors for software projects, and the collection of data on finalized software projects from portfolios of different companies in a research repository.Categories and Subject Descriptors D.2.8 [Software Engineering]: Metrics - Process Metrics, Performance Measures.General Terms Management, Measurement, Economics. © 2015 for this paper by its authors.",Evidence-based software engineering; Software benchmarking; Software economics; Software portfolio management,Huijgens H.,2015,Conference,CEUR Workshop Proceedings,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954514869&partnerID=40&md5=59688b3cbfe668e8670408d0500846f2,"Delft University of Technology, Delft, Netherlands",CEUR-WS,English,16130073,
Scopus,Mobile application estimate the design phase,"When addressing mobile applications, it is a technological landscape that is emerging with new requirements and restrictions requires a reassessment of current knowledge about the processes of development of these types of systems. These new systems have different features, ranging from planning to completion of the design, and therefore a particular area that is being addressed differently when it comes to estimating software. The estimation processes in general are based on characteristics of the systems to attempt to quantify the complexity of the implementation. For this reason, it is important to analyze the main models currently proposed for estimating software projects and consider whether it is suitable for mobile computing. Thus, the main objective of this paper is to present an estimation method for mobile applications still in the design phase, giving basis for all the features addressed in this scenario. © Springer International Publishing Switzerland 2015.",Estimating software; Mobile applications; Mobile computing; Software engineering; Software quality; Systematic review,"de Souza L.S., de Aquino G.S., Jr.",2015,Conference,Communications in Computer and Information Science,10.1007/978-3-319-27218-4_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951994369&doi=10.1007%2f978-3-319-27218-4_11&partnerID=40&md5=198f7887655e56b1f90bae1268642d02,"Department of Informatics and Applied Mathematics, Federal University of Rio Grande do Norte, Natal, 59078-970, Brazil",Springer Verlag,English,18650929,9783319272177
Scopus,Intelligent software cost estimation through multiple comparison algorithm,"The rapidly increased need for managers is to settle Software Cost Estimation (SCE) for large scaled complex systems. The SCE depends on success or failure of the whole development process. The proposed cost estimation model identifies the differences in accuracy of data set, and also clusters it into nonoverlapping groups. Specific technologies are not bound in modern software development methodologies. To overcome this problem a system is proposed to improve cost effort estimation methods and then compared using appropriate statistical procedures for ensuring the appropriate results. The Scott Knott test is to perform multiple comparisons without ambiguity. The proposed modification related to the partitioning and means grouping to obtain results without ambiguity among models. In the proposed methodology, models that did not participate in the initial group are joined for a new analysis, which allows for a better group distribution. © Research India Publications.",Software cost estimation; Software effort estimation; Software metrics; Statistical methods,"Sujitha M., Vivek Pandian S., Abirami S.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944894653&partnerID=40&md5=b2e206339d7bab0ffa71fcb36dc4e2ed,"Dept of Computer science and Engg, R.V.S Educational Trust’s Group of Institution, Dindigul, India",Research India Publications,English,09734562,
Scopus,Comparative analysis of sizing techniques in the sense of E-learning system,"Software size estimation is a key factor to determine the amount of time and effort needed to develop software systems, and the e-learning systems are no exception. The success of any software project largely depends on effective estimation of project effort, time and cost. Estimation helps in setting realistic targets for completing the project. The basic element for estimating all is size. The software industry uses various sizing techniques they are Lines of code, Function points, feature points, use case points, object points, internet points, etc. are not effectively supported for determine the size of E-Learning system that affect all estimates. The wrong estimates lead incompleteness, loss and customer dissatisfaction. This paper presents the popular sizing techniques and their inabilities in sizing and also the necessities of new sizing approach for E-Learning system. © Research India Publications.",FPA - Function point analysis; LOC - Line of code; OP - Object points; UCP - Use case points,"Shiny Angel T.S., Rodrigues P.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942035442&partnerID=40&md5=a3030af4752f5196ab8761c354a82388,"Department of Software Engineering, SRM University, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, DMI Engineering College, Chennai, Tamil Nadu, India",Research India Publications,English,09734562,
Scopus,Contemplation of human factors in impelling software project management,"With its origins in Industrial Revolution, Human Factors became widely incorporated discipline during the World War II. Many giant companies came to recognize that the success of a product depends upon a solid Human Factors design. Human Factors discovers and applies information about human behaviour, abilities, limitations, and other characteristics to the design of tools, machines, systems, tasks, jobs, and environments for productive, safe, comfortable, and effective human use. Chatzoglou and Macaulay (1997) [1] claim that the scientific discussion about the work situation in software development and about productivity factors in such projects is done based on an insufficient empirical basis. According to them, it is dominated by shallow surveys and qualitative experience reports. Moreover, the software engineering literature in that area often has a strong emphasis on mainly technical factors such as the software size or the product complexity. While half of the typical programmer’s time is spent interacting with other colleagues, thirty percent of the time he/she will spend working alone and the other twenty will be spent in activities such as travel and training. There is scant research in this area and this article aims to alleviate these gaps by contemplating human factors in impelling software project management. Through review of literature, it’s endeavoured to identify those human factors (dimensions and related variables) which play a major role in impacting software project management. © Research India Publications.",Analyst capability; Developer temperaments; Manager application experience; Organisational culture; Team identity,"Vinoth M.P., Ramasamy M.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940393601&partnerID=40&md5=964ee4dcf0584b59c9ce8bd3bc72d505,"Sathyabama University, Chennai, India; Siva Institute of Frontier Technology, Chennai, India",Research India Publications,English,09734562,
Scopus,Effort estimation on e-learning projects using function point analysis,"In the recent developments in cloud and interactive web based environment, e-learning projects are intensified, so that the e-learning effort estimation plays a significant role to reduce the cost of the project, within the stipulated time and budget. There is a crucial need to estimate the effort spent on e-learning project planning and development. Several factors are involved in the e-learning project development like animation, simulation, audio, video, content development and storyboarding. This paper aims to obtain an estimating method for e-learning projects effort based on function point. This paper describes how to estimate the size and effort of the three different types of projects like simple, medium and complex using Function Point Analysis (FPA). Next the actual project data is used to obtain the linear equation between function point and software effort. In this study, the FPA is used to obtain the results of the size, effort and estimated time to complete the e-learning projects under various categories. © Research India Publications.",Effort estimation; Effort estimation of e-learning project; Function point; Function point analysis; Software qualit,"Rajankrupa C., Srinath M.V.",2015,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940204142&partnerID=40&md5=67255a12897902af242b97cdde323729,"Department of MCA, Kumaraguru College of Technology, Coimbatore, India; Dept of CSE, Mahendra Engineering College, Tiruchengode, India",Research India Publications,English,09734562,
Scopus,Neural model for software project development effort estimation (MONEPS) [Modelo neuronal de estimación para el Esfuerzo de desarrollo en proyectos de software (MONEPS)],[No abstract available],,"Almache C. M.G., Ruiz R. J.A., Raura G., Fonseca C. E.R.",2015,Conference,"XI Jornadas Iberoamericanas de Ingenieria de Software e Ingenieria del Conocimiento, JIISIC 2015",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935121117&partnerID=40&md5=e6582ee522d6f448ce32c27c513831fe,"Universidad de las Fuerzas Armadas, ESPE, Sangolquí, Ecuador",Escuela Superior Politecnica de Chimborazo,English; Spanish,,9781510802087
Scopus,Security quality model: an extension of Dromey’s model,"The quantity of sensitive data that is stored, processed and transmitted has increased many folds in recent years. With this dramatic increase, comes the need to ensure that the data remain trustworthy, confidential and available at all times. Nonetheless, the recent spate of high-profile security incidents shows that software-based systems remain vulnerable due to the presence of serious security defects. Therefore, there is a clear need to improve the current state of software development to guide the development of more secure software. To this end, we propose a security quality model that provides a framework to identify known security defects, their fixes, the underlying low-level software components along with the properties that positively influence the overall security of the product. The proposed model is based on Dromey’s quality model that addresses the core issue of quality by providing explicit guidelines on how to build quality into a product. Furthermore, to incorporate security, we have introduced several new model components and model construction guidelines as Dromey’s model does not address security explicitly and the model construction guidelines are not specific enough. We use well-known defects and security controls to construct the model as a proof of concept. The constructed model can be used by the programmers during development and can also be used by the quality engineers for audit purposes. We also propose an automated environment in which the model can be used in practice. © 2013, Springer Science+Business Media New York.",Application security; Dromey’s quality model; Security; Security engineering; Security quality model; Software defects,"Zafar S., Mehboob M., Naveed A., Malik B.",2015,Journal,Software Quality Journal,10.1007/s11219-013-9223-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924223988&doi=10.1007%2fs11219-013-9223-1&partnerID=40&md5=ae7e170e55e7ac4cc69cbdcc34458e6e,"Faculty of Computing, Riphah International University, Islamabad, Pakistan; International Islamic University, Islamabad, Pakistan; Foundation University College of Liberal Arts and Science, Islamabad, Pakistan",Kluwer Academic Publishers,English,09639314,
Scopus,Analysis of cost estimation metrics to review the cost of testing,"Reliable software is produced by specifying, assuring and controlling its quality within budget. Researchers and practitioners have expressed concern over their inability to accurately estimate costs associated with software development which has become even more pressing as costs associated with development continues to increase. As a result, considerable research attention is now directed at gaining a better understanding of the software development process as well as constructing and evaluating software measurement tools for cost estimation and testing. An effective testing cost measurement technique requires a metrics that could not only increase the reliability, reusability, correctness and maintainability but also measures the quality and productivity of software development process within budget. Keeping this goal in mind, we have provided a set of useful cost estimation metrics that could be effectively implemented to evaluate costs of testing the software products, for improvement in overall software product reliability and also become one of the metrics used for the measurement of overall expenses incurred during the course of software testing and thus make way towards a reliable and quality software system in recent future. © IDOSI Publications, 2014.",Quality improvement; Quality software system; Quality within budget; Testing cost evaluation metrics,"Udayakumar R., Khanaa V., Thooyamani K.P.",2014,Journal,World Applied Sciences Journal,10.5829/idosi.wasj.2014.29.csea.2276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897724988&doi=10.5829%2fidosi.wasj.2014.29.csea.2276&partnerID=40&md5=b83f84f57955d565b2dd2ef96375bb8f,"School of Computing Science, Bharath University - 73, India",,English,18184952,
Scopus,Software cost and duration estimation based on distributed project data: A general framework,"Effort estimation is one of the most challenging tasks in the process of software project management. Enhancing the accuracy of effort estimation remains a serious problem for software professionals. Accurate estimation is difficult to achieve. The main difficulty is to collect distributed knowledge as data and information are often dispersed over different services, departments or organisations. Other main difficulty is to propose a model representative enough of this multi-partner behaviour. The objective of this study is to propose a general framework of the estimation starting from the analysis of the available projects database, the choice and establishment of estimation model, up to the use of this model to make estimation for new projects. In this paper, a comparative study between regression models and neural network models is performed. The proposed study is applied on a dataset of an automotive company. © 2014, Springer International Publishing Switzerland.",Comparison; Cost estimation; Duration estimation; Neural network; Regression,"Laqrichi S., Marmier F., Gourc D.",2014,Book Chapter,Proceedings of the I-ESA Conferences,10.1007/978-3-319-04948-9_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047434808&doi=10.1007%2f978-3-319-04948-9_18&partnerID=40&md5=ff30def2a03e9e2936417662b3b964a4,"Mines Albi, University of Toulouse, Route de Teillet, Campus Jarlard, Albi Cedex 09, 81013, France",Springer International Publishing,English,21992533,
Scopus,Effort estimation for architectural refactoring to introduce module isolation,"The decomposition of software architecture into modular units is driven by both functional and quality concerns. Dependability and security are among quality concerns that require a software to be decomposed into separate units isolated from each other. However, it appears that this decomposition is usually not aligned with the decomposition based on functional concerns. As a result, introducing module isolation forced by quality attributes, while preserving the existing decomposition, is not trivial and requires a substantial refactoring effort. In this work, we introduce an approach and a toolset to predict this effort prior to refactoring activities. As such, a selection can be made among potential decomposition alternatives based on quantitative estimations. These estimations are obtained from scalable analysis of module dependencies based on a graph database and reusable query templates. We discuss our experiences and evaluate our approach on a code base used in a commercial Digital TV and Set-top Box software. © 2014 Springer International Publishing Switzerland.",dependability; effort estimation; module isolation; refactoring; reverse engineering; security; Software architecture,"Öztürk F., SarIlI E., Sözer H., Aktemur B.",2014,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-09970-5_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958549687&doi=10.1007%2f978-3-319-09970-5_26&partnerID=40&md5=401a8c78ab7d459be22262b72ce2042e,"Vestel Electronics, Manisa, Turkey; Department of Computer Science, Ozyegin University, Istanbul, Turkey",Springer Verlag,English,03029743,9783319099699
Scopus,Scaling up analogy-based software effort estimation: A Comparison of multiple hadoop implementation schemes,"Analogy-based estimation (ABE) is one of the most time consuming and compute intensive method in software de- velopment effort estimation. Optimizing ABE has been a dilemma because simplifying the procedure can reduce the estimation performance, while increasing the procedure com- plexity with more sophisticated theory may sacrifice an ad- vantage of the unlimited scalability for a large data input. Motivated by an emergence of cloud computing technology in software applications, in this study we present 3 different implementation schemes based on Hadoop MapReduce to optimize the ABE process across multiple computing in- stances in the cloud-computing environment. We experimentally compared the 3 MapReduce implementation schemes in contrast with our previously proposed GPGPU approach (named ABE-CUDA) over 8 high-performance Amazon EC2 instances. Results present that the Hadoop solution can pro- vide more computational resources that can extend the scalability of the ABE process. We recommend adoption of 2 different Hadoop implementations (Hadoop streaming and RHadoop) for accelerating the computation specifically for compute-intensive software engineering related tasks. Copyright © 2014 ACM.",Analogy-based estimation; Cloud computing; CUDA; Map reduce; Software effort estimation,"Phannachitta P., Keung J., Monden A., Matsumoto K.",2014,Conference,"International Workshop on Innovative Software Development Methodologies and Practices, InnoSWDev 2014 - Proceedings",10.1145/2666581.2666582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942782262&doi=10.1145%2f2666581.2666582&partnerID=40&md5=8c354793bdceb8b4887972162febe56d,"Nara Institute of Science and Technology City, Graduate School of Information Science, Japan; Department of Computer Science, University of Hong Kong, Hong Kong, Hong Kong","Association for Computing Machinery, Inc",English,,9781450332262
Scopus,A critical survey to the challenges of software estimation,Now a day’s estimating software is quite difficult and risky task. During estimation we face lot of challenges as considering the environmental factors. Cost estimation is important task for software modeling. There are so many algorithmic and non algorithmic techniques are already developed to face the challenges of software estimation. This paper is about the critical study and compressive description of the models that were presented in the early stages of the software estimation field and covers most of the famous available parametric models and few non parametric techniques that help to estimate the software cost and efforts. Here all these models are sketched together to enhance our readers to find out the similarity and dissimilarity of the models. © Research India Publications.,KLOC; MMRE; MRE; PRED; SLIM; SLOC,"Patra H.P., Rajnish K.",2014,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941143515&partnerID=40&md5=e475491523387d9a027db4aa302142b4,"BIT, Mesra, Ranchi, India",Research India Publications,English,09734562,
Scopus,Empirical-based extension of the COSMIC FP method,"This paper discusses the extensions of the COSMIC Function Point method using empirical aspects in order to support the broader application of this method for effort estimation and other software system und processes characterization. The method extensions are based on our experience in different COSMIC applications for embedded systems, agile development, SOA implementations, cloud computing and apps implementation in the last ten years. After a short introduction about this well-known COSMIC method, empirical aspects of software products and processes are described and applications of effort estimation based on sizing the quality, technology and methodology are discussed. © 2014 IEEE.",COSMIC method; cost and effort estimation; empirical aspects; functional size measurement,"Dumke R., Neumann R., Schmietendorf A., Wille C.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929648739&doi=10.1109%2fIWSM.Mensura.2014.20&partnerID=40&md5=018cc2cdd0d893c18bbe8b12aaa73bc9,"University of Magdeburg, Dept. of Computer Science, Germany; Business School of Economics and Law, Germany; University of Applied Science, Bingen, Germany",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,Cost estimation of transition projects in application outsourcing: An empirical model,"Application outsourcing is a major IT business, with an estimated global market size of 49 billion for 2014. In a transition project the involved parties in an application outsourcing contract need to move from a current situation to a target situation. Generally the proprietor asks the IT service provider to provide a (fixed) quotation for the application services including the transition project, without supplying extensive details about the application portfolio. In this paper the authors present a model for cost estimation of transition projects in application outsourcing. © 2014 IEEE.",application outsourcing; cost estimation; transition,"Wams N., Vogelezang F.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929643266&doi=10.1109%2fIWSM.Mensura.2014.17&partnerID=40&md5=c764a4065d728644c5178f915a6afba4,"Ordina, Netherlands",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,Application of function points to software based on open source: A case study,"Many software are developed from Open Source. There is a crucial need to estimate the efforts spent in a project development based on Open Source. These estimates give room for comparison between the development model based on Open Source and others and lead to a decision making concerning a project, if it is judged worthy to be continued, discarded or combined with traditional approaches. In the present paper, a case study is made using approximation of functional size to the software based on Open Source in order to show that the development of applications based on Open Source has many advantages in terms of efforts. For this purpose, the present work sets out to adapt an Open Source software called TRIADE and to evaluate its functional size matching. The results show that this adaptation can decrease the functional size of software up to 90% compared to a development from scratch. The present study might constitute a research frame on the development and/or validation of a model of cost estimate of application development based on Open Source. © 2014 IEEE.",function points; Open Source software; software adaptation; software estimation; source lines of code,"Moulla D.K., Damakoa I., Kolyang D.T.",2014,Conference,"Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014",10.1109/IWSM.Mensura.2014.15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929618405&doi=10.1109%2fIWSM.Mensura.2014.15&partnerID=40&md5=3dc0d2d85b89bd07d63c1e6bcd8b9c55,"Department of Mathematics and Computer Science, University of Ngaoundere, Ngaoundere, Cameroon; Higher Teachers' Training College, University of Maroua, Maroua, Cameroon",Institute of Electrical and Electronics Engineers Inc.,English,,9781479941742
Scopus,Software cost estimation using similarity difference between software attributes,"The apt estimate of the software cost in advance is one of the most challenging, difficult and mandatory task for every project manager. Software development is a critical activity which requires various considerable resources and time.A prior assessment of software cost directly depends on the expanse of these resources and time, which in turn depends in the software attributes and its characteristics. Since there are many precarious and dynamic attributes attached to every software project, the accuracy in prediction of the cost will rely on the prudential treatment of these attributes. This paper deals with the methods of selection, quantification and comparison of different attributes related to different projects. We have tried to find the similarity difference between project attributes and then consequently used this difference measurement for creating the initial cost proposals of any software project that has some degree of correspondence with the formerly completed projects whose total cost is fairly established and well known. © Springer India 2014,",Analogy and similarity difference; Cost estimation; K-nearest neighbor classifier; Software attributes; Software development cost,"Kashyap D., Misra A.K.",2014,Conference,Advances in Intelligent Systems and Computing,10.1007/978-81-322-1602-5_126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928027719&doi=10.1007%2f978-81-322-1602-5_126&partnerID=40&md5=c98f763e3a3f05e702d39c443b80d034,"Department of Computer Science and Engineering, MNNIT, Allahabad, India",Springer Verlag,English,21945357,9788132216018
Scopus,Factors influencing the effort of EAI projects-a repertory grid investigation,"Today's enterprises often face heterogeneous application landscapes. Many of those companies struggle with effective and efficient accomplishment of enterprise application integration (EAI), which results in significant time and budget overruns. As regards EAI project management, a major reason for failure is considered to be underestimation of effort. The underestimation has been found to be an aftermath of applying estimation methods that do not account for all relevant factors influencing EAI project effort. We therefore explore factors affecting the effort of such projects in this study. Applying Repertory Grid, we conduct 22 semi-structured expert interviews. 91 factors influencing the effort of EAI projects in nine categories emerge from these interviews. We provide an extensive overview of effort-influencing factors and their classification, which can be used as a checklist in EAI projects. Future research can additionally use our findings as basis for development of more accurate effort estimation models.",Effort estimation; Enterprise application integration; Project management; Repertory grid,"Wagner H., Pankratz O., Basten D., Mellis W.",2014,Conference,"35th International Conference on Information Systems ""Building a Better World Through Information Systems"", ICIS 2014",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923463992&partnerID=40&md5=f5fc3d70b9b25df926e428a79da4bc9f,"KMA-Umwelttechnik GmbH, Eduard-Rhein-Straße 2, Königswinter, 53639, Germany; University of Cologne, Albertus-Magnus-Platz, Cologne, 50923, Germany",Association for Information Systems,English,,
Scopus,Development and application of STDCM for long-term software projects,This study presents a new model for estimation of remaining residual defects based on software products. It is an initial input in Software Testing Defect corrective Model (STDCM). This paper proposes a more reliable and simpler approach for estimating residual defects in application software built in software quality. © Research India Publications.,Cost estimation; FMEA; Function point analysis; Inspections; QFD; Reviews and residual defects,"Karnavel K., Dillibabu R.",2014,Journal,International Journal of Applied Engineering Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919784768&partnerID=40&md5=cc15cfd4b2cf0eb3bb2492eef3edc0fc,"Department of Industrial Engineering, Anna University, Chennai – 25, India",Research India Publications,English,09734562,
Scopus,Fallacies and biases when adding effort estimates,"Software professionals do not always clarify what they mean by their effort estimates. Knowing what is meant by an estimate is, however, essential when adding individual effort estimates from a work breakdown structure to find the estimated total effort. Adding the most likely instead of the mean effort of a set of cost elements may result in substantial underestimation of the total effort. In a survey of forty-four software companies we found only two companies that clarified the meaning of their estimates and had a proper method for adding these estimates. The other companies typically added single point estimates without clarifying what they added or with types of estimates likely to give too low estimates of the total effort. We examine the effect of improper addition of estimates and find, for the studied contexts, that summing the most likely effort estimates would lead to a substantial under-estimation of the most likely total effort. We also find that the use of the PERT-method, which provides a proper statistical basis for adding effort estimates and is used by many software companies, is likely to underestimate the effort in software development contexts. This is caused by, we argue and illustrate with empirical data, people's tendency towards providing too narrow minimum and maximum effort intervals. We outline a method that, we believe, better ensures that proper estimates of the total effort are produced. © 2014 IEEE.",adding estimates; Cost estimation; project management; uncertainty analysis; work breakdown structures (WBS),Jorgensen M.,2014,Conference,"Proceedings - 40th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2014",10.1109/SEAA.2014.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916631393&doi=10.1109%2fSEAA.2014.16&partnerID=40&md5=fad52b8aad808cb3b8e429734a2e80f6,"Simula Research Laboratory, Fornebu, Norway",Institute of Electrical and Electronics Engineers Inc.,English,,9781479957941
Scopus,Software cost estimation on e-learning technique using a classical fuzzy approach,"Software Cost Estimation with resonating dependability, gainfulness and advancement exertion is a testing and cumbersome undertaking. This has actuated the product group to give much required push and dig into broad research in programming exertion estimation for developing complex strategies. Estimation by similarity is one of the convenient methods in programming exertion estimation field. Notwithstanding, the strategy used for the estimation of programming exertion by similarity is not ready to handle the straight out information in an express and exact way. Another approach has been produced in this paper to gauge programming exertion for ventures spoke to by absolute or numerical information utilizing thinking by similarity and fluffy methodology. The current chronicled datasets; broke down with fluffy rationale, produce exact brings about correlation to the dataset investigated with the prior systems. © 2014 SERSC.",Analogy; Categorical data; Datasets; Fuzzy logic; Software effort,"Veeranjaneyulu N., Suresh S., Salamuddin S., Kim H.-J.",2014,Journal,International Journal of Software Engineering and its Applications,10.14257/ijseia.2014.8.11.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913538196&doi=10.14257%2fijseia.2014.8.11.20&partnerID=40&md5=b08634105b3823eb6ccc6a8db26a749e,"Dept. of Information Technology, VFSTR University Vadlamudi, Guntur, India; Dept. of Child Welfare, Vision University College of Jeonju, 235, Cheonjam-ro, Wansan-gu, Jeonju-city, Cheonbuk-do, 560-760, South Korea",Science and Engineering Research Support Society,English,17389984,
Scopus,Research on improved staged software cost estimation method based on COCOMO model,"The accuracy of software cost estimation is essential for software development management. By introducing and analyzing the estimation methods of software cost systematically, the paper discussed the necessary of considering the software maintenance stage and estimating the software cost by separating the procedure of software development into several small stages. Then a staged software cost estimation method based on COCOMO model was proposed. The use of the new software cost estimation method proposed by this paper not only contributes to the cost control of software project, but also effectively avoids the bias problem due to using by single cost estimation method so that the accuracy of cost estimation could be improved. © (2014) Trans Tech Publications, Switzerland.",COCOMO model; Cost estimation; Software project; Stages divided method,Yang H.,2014,Conference,Advanced Materials Research,10.4028/www.scientific.net/AMR.989-994.1501,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905851563&doi=10.4028%2fwww.scientific.net%2fAMR.989-994.1501&partnerID=40&md5=dab6620a528699b38ce28abbe7fba500,"College of Information Science and Electricity Engineering, Shandong Jiaotong University, Jinan, Shandong,250023, China",Trans Tech Publications Ltd,English,10226680,9783038351733
Scopus,Impact of the performance metrics in the improvement of accuracy in software effort estimation,"Software effort estimation plays vital role in the software project management, since it is a base for many activities like planning, scheduling and tracking the software projects. In most existing research on the effort estimation models are limited by their inability to manage a performance requirements of the software projects in the development phase. A software effort estimation model which includes the performance metrics in the estimation provides accurate and better quality efforts. This paper proposes a method which incorporates correctness factor of performance metrics in function point analysis to overcome the uncertainty in the estimation error like COCOMO model. The main objectives of this research are to investigate the impact of performance metrics on the accuracy of the effort estimation. Experimental results showed that applying this method for effort estimation resulted in high accuracy as compared with the results obtained from original model. © 2014 International Information Institute.",Accuracy; COCOMO model; Performance requirements; Software effort estimation; Software project management,"Kumar M.S., Rajan B.C.",2014,Journal,Information (Japan),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905279204&partnerID=40&md5=6caa5ed3d9b82fde1a3e76af5895150b,"Department of Computer Science and Engineering, Valliammai Engineering College, Anna University, SRM Nagar, Kattankulathur - 603 203, Tamil Nadu, India",International Information Institute Ltd.,English,13434500,
Scopus,Creating a framework for quality decisions in software projects,"This work analyzes the challenges that quality decisions represent to software project managers. Projects' goals are normally determined by the paradigm of the Iron Triangle of project management. Managers need to know which are the effects of a quality assurance (QA) decision on the three axis: which effects in quality they can get but at what cost and which effects may appear in terms of schedule. This decision problem is clearly related to existing disciplines like SBSE, multi-objective optimization and methods for ROI calculation and value-based software engineering. This survey paper critically reviews the contributions of these disciplines to support QA decisions together with basic information from a pilot survey carried out as part of the developments of the Iceberg project funded by EU Programme Marie Curie. © 2014 Springer International Publishing.",influence factors; Iron triangle; QA decisions; software quality,"Potena P., Fernandez-Sanz L., Pages C., Diez T.",2014,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-319-09156-3_31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904909352&doi=10.1007%2f978-3-319-09156-3_31&partnerID=40&md5=54d470b8cf52c732490695d3f22474f7,"Computer Science Department, University of Alcalá, 28871 Alcalá de Henares, Madrid, Spain",Springer Verlag,English,03029743,9783319091556
Scopus,ECO-FOOTPRINT: An innovation in enterprise system customization processing,"In the overall ownership cost of enterprise system, the maintenance cost consists of a major percentage. During the lifetime of an enterprise system, process customization is the most frequent maintenance efforts. However, current processing method has limited scalability and efficiency. In this case study, we explained how a scalable and efficient customization processing method was implemented. This method used the carbon emission trading mechanism to facilitate the cost benefit analysis of customization request. It also used distributed processing principle to improve the overall processing efficiency. Feedback from a pilot implementation in a large manufacturer included. © Springer International Publishing Switzerland 2014.",Cost benefit analysis; Distributed processing; ERP customization,"Wan Y., Kalidindi V.",2014,Conference,Lecture Notes in Business Information Processing,10.1007/978-3-319-06505-2_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904552646&doi=10.1007%2f978-3-319-06505-2_9&partnerID=40&md5=dd30317deaa302e90f675749b9931feb,"University of Houston - Victoria, Sugar Land, TX, United States; e-CO Matrix LLC., Missouri City, TX, United States",Springer Verlag,English,18651348,9783319065045
Scopus,Software development effort estimation using ANFIS,"Software cost estimation is one of the major challenges confronted in the development of a software project and it is very much related to, the decision making in an organization to bid, plan and budget the system that is to be developed. Development effort is the basic parameter in the software cost estimation which when computed manually is less accurate because, it is not known properly all the requirements at the earlier stage of the project. So several methods based on regression, iteration etc., were developed to estimate the Development effort In this paper a soft computing based approach is introduced to estimate the Development effort. The methodology involves an Adaptive Neuro Fuzzy Inference System (ANFIS) using the Fuzzy C Means clustering (FCM) and Subtractive clustering (SC) techniques. Thus an ANFIS based on FCM and SC was developed to provide a better estimate of the Development effort. The performance characteristics of the ANFIS based FCM and SC are verified using evaluation parameters. © 2014 International Information Institute.",Cost estimation; Development effort; Fuzzy neural networks; Process planning; Software development management,"Praynlin E., Latha P.",2014,Journal,Information (Japan),,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902687903&partnerID=40&md5=fc3eef32b491c459c9ebb5b5bf38a9dc,"Department of Computer Science and Engineering, Government College of Engineering, Tirunelveli Tamilnadu, India; Computer Science and Engineering, Government College of Engineering, Tirunelveli Tamilnadu, India",International Information Institute Ltd.,English,13434500,
Scopus,An efficient approach to develop software cost estimation model using case-based reasoning and agent technology,"One of the most important tasks for IT professionals is software development cost estimation. This critical task affects the firm's software investment decisions before bidding for a contract or committing required resources to that project. Under-estimation may lead to unexpected increase in budget, delay of project completion or its low quality, while over-estimation may lead to losing business opportunities. In this work authors investigate possibility of building a software cost estimation model by using multi-agent system to collect software cost data from distributed predefined project cost databases. The developed model implements Case-Based Reasoning (CBR) to find similar projects in historical data retrieved from measured software projects developed by different organizations, which will assist the project managersh to perform an appropriate cost estimation of a software project. © 2005 - 2014 JATIT & LLS. All rights reserved.",Agent technology; Analogy; Case-based reasoning; Magnitude of relative error,"Al-Sakran H., Abu Tair H.Y.A.",2014,Journal,Journal of Theoretical and Applied Information Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901927174&partnerID=40&md5=447af5e99f041ad4168a5f56f706cd13,"Department of Management Information Systems, King Saud University, Saudi Arabia; Department of Computer Science, King Saud University, Saudi Arabia",Asian Research Publishing Network (ARPN),English,19928645,
Scopus,A method for software product platform design based on features,"Due to the increased competition and the advent of mass customization, software firms are applying the Software Product Line Engineering (SPLE) approach to provide product variety in a cost-effective manner. Although the key to designing a successful software product family is the product platform, yet there is lack of measures and methods that are useful to optimize the product platform design. This paper proposes a method to provide decision support to determine the optimized product platform design. The method targets at identifying the optimized product platform design in order to maximize the cost savings and the amount of commonality while meeting the goals and needs of the envisioned customers' segments. It generates, validates, and evaluates alternative product platform designs while considering market concerns (e.g., customer preferences) and technical product platform concerns (e.g., decisions regarding shared features, economic benefit). We demonstrate its applicability with an example of platform design problem in smart phones domain. Copyright 2013 ACM.",Commonality index; Kano scheme; Product platform design; Software product line,"Alsawalqah H., Kang S., Lee D.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2499777.2500723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890466281&doi=10.1145%2f2499777.2500723&partnerID=40&md5=3a6c8fa47491c3b5f80b00121860ab85,"Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea",,English,,9781450323253
Scopus,It is possible to overcome the precision based on expert judgment of effort estimation of software products? [Es posible superar la precisión basada en el juicio de expertos de la estimación de esfuerzo de productos de software?],[No abstract available],,"Robiolo G., Castillo O., Rossi B., Santos S.",2013,Conference,"CIbSE 2013: 16th Ibero-American Conference on Software Engineering - Memorias del 10th Workshop Latinoamericano Ingenieria de Software Experimental, ESELAW 2013",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889803901&partnerID=40&md5=be39d4f28072c53eb077b3c5fbe3423c,"Universidad Austral, Buenos Aires, Argentina; Universidad Argentina de la Empresa, Buenos Aires, Argentina; Universidad Nacional de La Plata, La Plata, Argentina",,English; Spanish,,9789974837935
Scopus,Continuous process improvement,"Nowadays, a variety of different processes for the development and maintenance of software-intensive systems exists, ranging from agile development processes to classical plan-based approaches. There is no ultimate process that can be applied in each and every situation. It depends on the project goals and environment as well as on the required characteristics of the system under development. Development processes support organizations in developing software-intensive systems with certain quality characteristics, within a certain time span, and requiring a certain amount of effort. Continuous process improvement deals with the establishment and maintenance of high-quality processes, with analyzing their performance and effectiveness, and with initiating corresponding improvement actions if needed. In this chapter, we will take a closer look at how to systematically define and continuously improve development processes based on documented best practices and the use of measurement data collected during the enactment of the development process. The chapter highlights current challenges and presents solution approaches for establishing continuous process improvement in practice. © 2013 Springer-Verlag Berlin Heidelberg. All rights are reserved.",,Heidrich J.,2013,Book Chapter,Perspectives on the Future of Software Engineering: Essays in Honor of Dieter Rombach,10.1007/978-3-642-37395-4_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929550344&doi=10.1007%2f978-3-642-37395-4_8&partnerID=40&md5=5f4c8ccc27b7af1b1263da1f781ea6d4,"Fraunhofer Institute for Experimental Software Engineering, Kaiserslautern, Germany",Springer-Verlag Berlin Heidelberg,English,,9783642373954; 3642373941; 9783642373947
Scopus,Understanding factors contributing to the escalation of software maintenance costs,"We examine the main drivers of software maintenance effort and cost. We use the 'Distributed Cognition' framework to hypothesize about how 'discovery work' in maintenance is effected by two types of cost drivers: system attributes (size, complexity, age, etc.) and personnel attributes (number of maintainers, location dispersion, etc.). We test our hypotheses using archival data about over 5,000 maintenance projects carried out between 2009 and 2011 on 412 different operational systems in a large financial institution. We find that personnel attributes are significantly more influential than system attributes. In particular, a marginal change in personnel factors is associated with effort growing much faster than cost, indicating an escalating marginal cost of spreading maintenance work across more maintainers and site locations. We also find, counter to expectation, that two system attributes are negatively linked to maintenance effort and cost. Implications of these findings for research and practices are discussed. © (2013) by the AIS/ICIS Administrative Office All rights reserved.",Cost drivers; Discovery work; Distributed cognition; Personnel attributes; Software maintenance; System attributes,Benaroch M.,2013,Conference,International Conference on Information Systems (ICIS 2013): Reshaping Society Through Information Systems Design,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897749932&partnerID=40&md5=d30c34bfe0c6e78f19d5b6dacf772055,"Whitman School of Management, Syracuse University, Syracuse, NY, United States",,English,,9781629934266
Scopus,The innovation of the teaching pattern driven by the software company,The paper presents one teaching pattern driven by the practice in order to overcome the problems found in the existing teaching pattern. The new teaching pattern stimulates the students to work in the class as they are in the real software companies. So the students can apply the theory to the practice. The practice lays the foundation of the subsequent work after graduation. © 2013 IEEE.,Software Engineering; Teaching Pattern,"Zhang L., Pan Y., Xu M.",2013,Conference,"2013 IEEE International Conference on Information and Automation, ICIA 2013",10.1109/ICInfA.2013.6720400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894196720&doi=10.1109%2fICInfA.2013.6720400&partnerID=40&md5=c46ec4de6b406244164ce670a24e9d13,"School of Information Science and Engineering, University of Jinan, 106, Jiwei Road, Jinan, Shandong Province, China",,English,,9781479913343
Scopus,Improving the performance of neuro-fuzzy function point backfiring model with additional environmental factors,"Backfiring is a technique used for estimating the size of source code based on function points and programming. In this study, additional software environmental parameters such as Function Point count standard, development environment, problem domain, and size are applied to the Neuro-Fuzzy Function Point Backfiring (NFFPB) model. The neural network and fuzzy logic designs are introduced for both models. Both estimation models are compared against the same data source of software projects. It is found that the original NFFPB model outperforms the extended model. The results are investigated, and it is explained why the extended model performed worse. © 2015, IGI Global. All right reserved.",,"Wong J., Ho D., Capretz L.F.",2013,Book Chapter,Exploring Innovative and Successful Applications of Soft Computing,10.4018/978-1-4666-4785-5.ch014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957019489&doi=10.4018%2f978-1-4666-4785-5.ch014&partnerID=40&md5=c0dd585a0a3cc903721888ba2908db8d,"University of Western Ontario, Canada; Department of Software Engineering, Faculty of Engineering, University of Western Ontario, Canada; NFA Estimation Inc., Canada",IGI Global,English,,9781466647862; 146664785X; 9781466647855
Scopus,MIDD - An architecture for inter-domain mobile content distribution,"The proliferation of mobile markets and subsequently mobile application development has led to new markets for content distributors. Whilst architectures have been developed to aid with the distribution of content within mobile software domains, this has not extended to inter-domain distribution. For instance MXit, Android and Facebook developers are required to develop separate applications when distributing content. Little research has been conducted to investigate mobile inter-domain content distribution. This paper proposes a new architecture Mobile Inter-Domain Development (MIDD) to address some of the issues with existing architectures and allow for inter-domain content distribution. This paper discusses the design, implementation and evaluation of the MIDD architecture as a proof of concept. The proof of concept architecture was evaluated and results showed that whilst memory consumption is of concern, networking and lines of code (LOC) efficiency improved through the use of MIDD. Future work will include increased component level mapping capabilities, lower level event handling and higher level user interface generation. Dynamic module loading and unloading will also be incorporated to lower the memory requirements.",Mobile application; Mobile content distribution; Mobile user interface; User interface generation,"Cilliers C., Barnard L., Hibbers T., Koorsse M.",2013,Conference,ACM International Conference Proceeding Series,10.1145/2513456.2513498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886245609&doi=10.1145%2f2513456.2513498&partnerID=40&md5=7eb259c68e4023030a4f1fe79008dd03,"Nelson Mandela Metropolitan University, Department of Computing Sciences, P.O. Box 77000, Port Elizabeth 6031, South Africa",,English,,9781450321129
Scopus,Software War Stories: Case Studies in Software Management,"A comprehensive, practical book on software management that dispels real-world issues through relevant case studies Software managers inevitably will meet obstacles while trying to deliver quality products and provide value to customers, often with tight time restrictions. The result: Software War Stories. This book provides readers with practical advice on how to handle the many issues that can arise as a software project unfolds. It utilizes case studies that focus on what can be done to establish and meet reasonable expectations as they occur in government, industrial, and academic settings. The book also offers important discussions on both traditional and agile methods as well as lean development concepts. Software War Stories: Covers the basics of management as applied to situations ranging from agile projects to large IT projects with infrastructure problems Includes coverage of topics ranging from planning, estimating, and organizing to risk and opportunity management Uses twelve case studies to communicate lessons learned by the author in practice Offers end-of-chapter exercises, sample solutions, and a blog for providing updates and answers to readers' questions Software War Stories: Case Studies in Software Management mentors practitioners, software engineers, students and more, providing relevant situational examples encountered when managing software projects and organizations. © 2014 by the IEEE Computer Society. All rights reserved.",,Reifer D.J.,2013,Book,Software War Stories: Case Studies in Software Management,10.1002/9781118717257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015244387&doi=10.1002%2f9781118717257&partnerID=40&md5=d9890661fea78726964a9d46ce340cbc,,Wiley-IEEE Press,English,,9781118717257; 9781118650721
Scopus,Sizing cloud applications with ISO/IEC 19761: A case study,"Despite business opportunities created by cloud software development becoming an irresistible trend, enterprises are daunted by the high complexity of managing cloud software development. Accurate cost estimations and time scheduling are essential to successfully manage cloud software development; quantification of the size of cloud software has a key role in supporting decisions that may occur. The international standard ISO/IEC 1976-1, a widely used functional-size measurement method, has the advantage of being able to be applied to real-time systems and more easily performing quantitative calculations. Using a case study method, this study guides and explains how to use ISO/IEC 1976-1 to measure the functional size of cloud software. The contributions of this research are primarily divided into the following three points: (a) introducing ISO/IEC 1976-1 to provide practical fields a systematic method to quantitatively measure the functional size of cloud software to support managerial decision-making, (b) using ISO/IEC 1976-1 as a practical guide and system for the management and applications measuring the size of cloud software, and (c) verifying the usability of ISO/IEC 1976-1 on size measurement of cloud software. © 2013 Springer Science+Business Media New York.",Cloud software; Functional-size measurement; ISO/IEC 19761; Software measurement,"Han W.-M., Chen W.-T.",2013,Conference,Lecture Notes in Electrical Engineering,10.1007/978-1-4614-6747-2_48,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881083242&doi=10.1007%2f978-1-4614-6747-2_48&partnerID=40&md5=d373a227181db4562dea292b65a7827e,"Department of Management Information Systems, Takming University of Science and Technology, Taipei 11451, Taiwan; Graduate Institute of Information Technology and Management, Takming University of Science and Technology, Taipei 11451, Taiwan",,English,18761100,9781461467465
Scopus,Hybrid effort estimation based on multivariate liner regression and analogy based estimation,"We propose a hybrid effort estimation method based on the multivariate liner regression analysis and the analogy based estimation method (ABE). First, our method calculates the unreliability index of ABE on an estimation target project. Next, our method selects log-log regression estimation when the value of the index is low. Otherwise our method selects ABE estimation or combined estimation (the average of ABE and log-log regression estimation). In the experiment, we compared estimation accuracies of our method with conventional methods, and the results showed that the median of Balanced Relative Error (estimation accuracy index) was improved from 47.2% to 39.7%, when the variance of similar projects' effort was used as the reliability index.",,"Toda K., Tsunoda M., Monden A., Matsumoto K.",2013,Journal,Computer Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883418284&partnerID=40&md5=212ad83c89f499b3044cd85645254d0f,"Department of Computer Science and Engineering, Fukuoka Institute of Technology, Japan; Faculty of Information Sciences and Arts, Toyo University, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan",,Japanese,02896540,
Scopus,Developmental size estimation for object-oriented software based on analysis model,"Software size estimation at the early analysis phase of software development lifecycle is crucial for predicting the associated effort and cost. Analysis phase captures the functionality addressed in the software to be developed in object-oriented software development life-cycle. Unified modeling language captures the functionality of the software at the analysis phase based on use case model. This paper proposes a new method named as use case model function point to estimate the size of the object-oriented software at the analysis phase itself. While this approach is based on use case model, it also adapts the function point analysis technique to use case model. The various features such as actors, use cases, relationship, external reference, flows, and messages are extracted from use case model. Eleven rules have been derived as guidelines to identify the use case model components. The function point analysis components are appropriately mapped to use case model components and the complexity based on the weightage is specified to calculate use case model function point. This proposed size estimation approach has been evaluated with the object-oriented software developed in our software engineering laboratory to assess its ability to predict the developmental size. The results are empirically analysed based on statistical correlation for substantiating the proposed estimation method. © 2013 World Scientific Publishing Company.",analysis; function point; Object-oriented software; software estimation; use case model,"Arumugam C., Babu C.",2013,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194013500083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880537856&doi=10.1142%2fS0218194013500083&partnerID=40&md5=1d0f6d5edd157811e267c94f9d45c6ef,"Department of Computer Science and Engineering, Sri SivaSubramaniya Nadar College of Engineering, Rajiv Gandhi Salai, SSN Nagar, Tamil Nadu-603 110, India",,English,02181940,
Scopus,Performance management in software engineering,"Performance measurement in software engineering has to meet a multiplicity of challenges. Oftentimes, traditional metrics focus on sequential development instead of using incremental and iterative development. Output is measured on a pure quantitative (e.g., SLOC), quality-disregarding basis. A project's input is hard to assign properly using enterprise-unspecific forecasting tools which have to be calibrated at first and which do not account for time preferences. Requirements necessary for behaviourally adjusted project management and control are rarely discussed. Focusing on these shortcomings, this paper proposes an enterprise-specific approach which combines lifecycle and activity based costing techniques for software development following the incremental and iterative Unified Process model. Key advantages are calibration effort can be avoided, project management decisions are supported by a clear managerial accounting emphasis, precise milestone-depending cost objectives can be determined as the basis for personnel management and control of development teams, and cost and time variance analysis can be supported in a sophisticated way. © 2013, IGI Global.",,"Ilg M., Baumeister A.",2013,Book Chapter,Perspectives and Techniques for Improving Information Technology Project Management,10.4018/978-1-4666-2800-7.ch005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949591088&doi=10.4018%2f978-1-4666-2800-7.ch005&partnerID=40&md5=31709bc564545933d137267f1f0d5506,"Department und Management and Business Administration, Vorarlberg University of Applied Sciences, Austria; Faculty of Law and Business, Saarland University, Germany",IGI Global,English,,9781466628014; 1466628006; 9781466628007
Scopus,Software effort estimation using regularized radial basis function neural networks,"The value of Artificial Neural Networks (ANNs) methods in performing complicated pattern recognition and nonlinear estimation tasks has been demonstrated across an impressive spectrum of applications. ANNs methods has been used extensively, in the software cost estimation process, due to the complexity of the relations between the project's attributes. ANNs Radial Basis Function (RBF) networks have advantages of easy design, and strong tolerance to input Noise. This paper, studies the effect of using Regularized Radial Basis Function Networks in improving the accuracy of the software cost estimation, using different training algorithms like K-means method, Genetic algorithm and Particle Swarm Optimizer (PSO). The relative improvements were found to be around 40 %, by using the Regularized Radial basis function with the PSO Algorithm. Copyright © 2013 by Knowledge Systems Institute Graduate School.",Estimation using ann; Genetic algorithm; Particle swarm optimizer; Software effort estimation; Survey on software estimation,"Shams K., Hamza H., Kamel A.",2013,Conference,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937713872&partnerID=40&md5=ca818d0d17f9bef1a5bc49dde4f2a16a,"Faculty of Computers and Information, Cairo University, Egypt",Knowledge Systems Institute Graduate School,English,23259000,
Scopus,Statistical issues on optimization for software metric models with missing data,"When developing new software, software metric models are often applied in predicting certain important elements such as total work effort or error rate and so on. The procedures during the regression model construction using certain historical data, such as determination of the independent metrics, imputation of missing values and combining levels for independent categorical metrics, have been discussed already. In this paper, how to choose some important parameters for the proposed procedures during the model construction is considered in depth. The selection of critical parameters in the k-nearest neighbors (k-NN) multiple imputation is specifically focused. An example is given for illustration with data from widely used database. © 2013 IEEE.",k-NN imputation; kernel function; model optimization; software metrics,"Xie T., Ding W.",2013,Conference,Proceedings - International Conference on Natural Computation,10.1109/ICNC.2013.6818152,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901789197&doi=10.1109%2fICNC.2013.6818152&partnerID=40&md5=0658042d008213f0ab15090c44e090a2,"College of Applied Sciences, Beijing University of Technology, Beijing, China; China National Institute of Standardization, 4 Zhichun Road Haidian District, Beijing, China",IEEE Computer Society,English,21579555,9781467347143
Scopus,Technical factors of consistency in COSMIC measurement,"Consistency of COSMIC measurement is the most important prerequisite to meet user's needs. In fact, due to technical and nontechnical reasons, measurement results are difficult to get consistent. Based on researches of COSMIC method and large number of case analysis, this paper proposed three technical factors of consistency: structure, functional process and object of interest. Then put forward to corresponding control methods. In the end, we validate these methods by typical cases. © 2012 IEEE.",Consistency; Control; COSMIC; Technical factors,"Mai X., He H., Liu Q., Xu Z.",2012,Conference,"2012 Spring World Congress on Engineering and Technology, SCET 2012 - Proceedings",10.1109/SCET.2012.6342100,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870689160&doi=10.1109%2fSCET.2012.6342100&partnerID=40&md5=e75904bf5941f56c349e5600dd128fbf,"National University of Defense Technology, Changsha, China",,English,,9781457719646
Scopus,Software metrics in student projects,"Software Engineering is an important part of modern Computer Engineering education. A comprehensive course in Software Engineering should include a topic on software metrics. Also, a number of courses practice handing team projects to students. In this paper a number of software metrics are applied to projects developed by students with the purpose of estimating overall team effort. In this way, conclusions are formed about usefulness of these metrics in the context of teaching software engineering. © 2012 IEEE.",,"Ljubovic V., Nosovic N.",2012,Conference,"2012 20th Telecommunications Forum, TELFOR 2012 - Proceedings",10.1109/TELFOR.2012.6419495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874184414&doi=10.1109%2fTELFOR.2012.6419495&partnerID=40&md5=0a5a52386c21101b33435231a4cf9cea,"Elektrotehnički Fakultet Sarajevo, Zmaja od Bosne b.b., 71000 Sarajevo, Bosnia and Herzegovina",,English,,9781467329842
Scopus,The role of systematic reviews in identifying the state of the art in web resource estimation,"The goal of this position paper is to motivate the importance of SRs, and to present a SR of Web resource estimation. The SR results suggest that there is plenty of work to be done in the field of Web resource estimation whether it be investigating a more comprehensive approach that considers more than a single resource facet, evaluating other possible resource predictors, or trying to determine guidelines that would help simplify the process of selecting a resource estimation technique. Copyright 2012 ACM.",Systematic review; Web resource estimation,"Mendes E., Azhar D.",2012,Conference,EAST'12 - Proceedings of the 2nd International Workshop on Evidential Assessment of Software Technologies,10.1145/2372233.2372237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867514050&doi=10.1145%2f2372233.2372237&partnerID=40&md5=fe64dcb75ff04ecd0365750ddc4eb56e,"College of Information Technology, Zayed University, United Arab Emirates; Department of Computer Science, University of Auckland, New Zealand",,English,,9781450315098
Scopus,Estimating database size and its development effort at conceptual design stage,"In recent years, the database size of information system software is increasing rapidly along with the development of the technology of software and its estimation techniques. Generally, any software product may consist of basic three components: data, functional, and document. The estimation of development effort for software depends on the effort required for the development of these components. The database development effort primarily depends on the database volume. In this paper, a set of metrics have been proposed and validated for estimating database size using ER and Enhanced ER diagram artifacts. The effort of database development based on the proposed size metrics have been validated using COCOMO model. © 2012 Springer-Verlag.",database size; effort estimation; ER diagram; size estimation,"Mishra S., Aisuryalaxmi E., Mall R.",2012,Conference,Communications in Computer and Information Science,10.1007/978-3-642-29216-3_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865993489&doi=10.1007%2f978-3-642-29216-3_14&partnerID=40&md5=6a40b5393f3d25e199bbb2803baa8124,"School of Computer Engineering, KIIT University, Bhubaneswar, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India",,English,18650929,9783642292156
Scopus,A unifying framework for the definition of syntactic measures over conceptual schema diagrams,"There are many approaches that propose the use of measures for assessing the quality of conceptual schemas. Many of these measures focus purely on the syntactic aspects of the conceptual schema diagrams, e.g. their size, their shape, etc. Similarities among different measures may be found both at the intra-model level (i.e., several measures over the same type of diagram are defined following the same layout) and at the intermodel level (i.e., measures over different types of diagrams are similar considering an appropriate metaschema correspondence). In this paper we analyse these similarities for a particular family of diagrams used in conceptual modelling, those that can be ultimately seen as a combination of nodes and edges of different types. We propose a unifying measuring framework for this family to facilitate the measure definition process and illustrate its application on a particular type, namely business process diagrams. © 2012 IEEE.",conceptual schema diagram; conceptual schema measure; metamodelling; MOF,"Costal D., Franch X.",2012,Conference,Proceedings - International Conference on Research Challenges in Information Science,10.1109/RCIS.2012.6240423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864997204&doi=10.1109%2fRCIS.2012.6240423&partnerID=40&md5=cea52fd2ca506b42effb36728c177992,"Universitat Politècnica de Catalunya (UPC), c/ Jordi Girona 1-3, Barcelona E-08034, Spain",,English,21511349,9781457719387
Scopus,Efficient estimation of effort using machine-learning technique for software cost,"Several useful models have been developed by the software engineering community to elucidate the periodic growth of life cycle and calculate the effort of cost estimation in a precise manner. One of the commonly used machine learning techniques is the analogy method that cannot handle the categorical variables efficiently. In general, project attributes of cost estimation are often measured in terms of linguistic values. These imprecise values leads to analogous while explaining the process. The proposed fuzzy analogy method is a new approach based on reasoning by analogy using fuzzy logic for handling both numerical and categorical variables where the uncertainty and imprecision solution is also identified by the behavior of linguistic values utilized in the software projects. The performance of this method validates the results based on historical NASA dataset. The outcome of fuzzy analogy method is analyzed which indicates its improvement over the existing fuzzy logic methods. © Indian Society for Education and Environment (iSee).",Analogy; Categorical variables; Cost estimation; Fuzzy logic; Linguistic,"Malathi S., Sridhar S.",2012,Journal,Indian Journal of Science and Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868285038&partnerID=40&md5=4810e37901415884c62263ae7b2640ac,"Department of CSE, Sathyabama University, Chennai-600119, India; Department of CSE and IT, Sathyabama University, Chennai-600 119, India",,English,09746846,
Scopus,Using test cases to size systems: A case study,"Productivity, quality and speed are the three key areas that all technology organizations strive to understand. Yet despite the keen interest, the ability to quantitatively measure these aspects of performance often eludes us. Central to the issue is that software development varies significantly in size from project to project making comparisons challenging. Measurement of the size of software is a key component to comparing dissimilar projects. Without a means to normalize for size, drawing conclusions between projects is a nearly impossible task. While solutions have existed to size systems in both lines of code (KLOC) and function points (FP), this paper explores a successful alternative approach to sizing that exhibits the benefits of existing methods with less incentive to manipulate the measurement system and a significantly lower cost. This paper proposes that the number of test cases is a viable measure of system size, available early in the process. © 2012 IEEE.",defects; function points; metrics; software sizing; test cases,Schwartz A.,2012,Conference,"Proceedings of the 9th International Conference on Information Technology, ITNG 2012",10.1109/ITNG.2012.41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863971629&doi=10.1109%2fITNG.2012.41&partnerID=40&md5=f879a7a3c8c220f509122639e61038c0,"Trueleandevelopment.com, Holden, MA, United States",,English,,9780769546544
Scopus,Scheduling quality related activities in incremental software development projects based on Monte Carlo simulation,"Quality is very important in software development project especially for those related to medical treatment, aerospace, finance and public infrastructure. Quality related activities including feasibility analysis, requirement review, architecture inspection, code review, software testing and auditing run through the entire software development life cycle. Quality related activities are a set of umbrella task corresponding to the defects contained inside the requirement specification, documents and applications. While quality related activity may account for nearly one third or even a half effort and duration in typical software projects, it has been considered as the bottleneck to deliver the software applications in timely model. Based on the effort analysis on the defect finding in incremental software development projects, this paper introduces the effort model of the quality related activities, the calibration based on local historic incremental software projects and detail scheduling method including the efficiency matrix, capability matrix and the definition of the constraints and the goal and suggests scheduling quality related activities while balancing time-to-market and high quality based on Monte Carlo simulation. Pilot in some incremental financial software projects shows that the suggested method will benefit the scheduling of quality related activities in the incremental software development projects. © 2012 Asian Network for Scientific Information.",Defect removal efficiency; Effort optimization; Metric calibration; Software testing solution,"Xu B., Chen J., Ge Y., Chen Z., Ling Y.",2012,Journal,Information Technology Journal,10.3923/itj.2012.751.759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863206798&doi=10.3923%2fitj.2012.751.759&partnerID=40&md5=2aedf150d8836291e1301e84a0614e4d,"College of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou 310018, China",,English,18125638,
Scopus,Information theory and best practices in the IT industry,"The importance of benchmarking in the service sector is well recognized as it helps in continuous improvement in products and work processes. Through benchmarking, companies have strived to implement best practices in order to remain competitive in the product- market in which they operate. However studies on benchmarking, particularly in the software development sector, have neglected using multiple variables and therefore have not been as comprehensive. Information Theory and Best Practices in the IT Industry fills this void by examining benchmarking in the business of software development and studying how it is affected by development process, application type, hardware platforms used, and many other variables. Information Theory and Best Practices in the IT Industry begins by examining practices of benchmarking productivity and critically appraises them. Next the book identifies different variables which affect productivity and variables that affect quality, developing useful equations that explaining their relationships. Finally these equations and findings are applied to case studies. Utilizing this book, practitioners can decide about what emphasis they should attach to different variables in their own companies, while seeking to optimize productivity and defect density. © Springer Science+Business Media, LLC 2012. All rights reserved.",,Mohapatra S.,2012,Book,Information Theory and Best Practices in the IT Industry,10.1007/978-1-4614-3043-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949177772&doi=10.1007%2f978-1-4614-3043-8&partnerID=40&md5=0e76969aa368a9e6afc501c651f4c8a9,"Xavier Institute of Management, Bhubaneswar, Orissa, India",Springer US,English,,9781461430438; 1461430429; 9781461430421
Scopus,A perspective on software engineering education with open source software,"As the development and use of open source software (OSS) becomes prominent, the issue of its outreach in an educational context arises. The practices fundamental to software engineering, including those related to management, process, and workflow deliverables, are examined in light of OSS. Based on a pragmatic framework, the prospects of integrating OSS in a traditional software engineering curriculum are outlined, and concerns in realizing them are given. In doing so, the cases of the adoption of an OSS process model, the use of OSS as a computer-aided software engineering (CASE) tool, OSS as a standalone subsystem, and open source code reuse are considered. The role of openly accessible content in general is discussed briefly. Copyright © 2013, IGI Global.",Computer-aided software engineering (CASE); Open source code reuse; Open source software (OSS); Pragmatic framework; Software engineering; Standalone subsystem,Kamthan P.,2012,Review,International Journal of Open Source Software and Processes,10.4018/ijossp.2012070102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902960392&doi=10.4018%2fijossp.2012070102&partnerID=40&md5=a9d5dc054be12b72266589f9beae364f,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",IGI Global,English,19423926,
Scopus,Towards understanding the use of patterns in software engineering,"There are a number of avenues of articulating experiential knowledge, including patterns. However, the mere availability of patterns does not lead to their suitable use, if at all. In order to establish a systematic approach for using patterns, a pattern stakeholder model and a general, process environment-neutral and domain-independent pattern usage model are proposed, and the relationships between them are underscored. The underlying essential and accidental concerns in putting patterns into practice by pattern stakeholders are highlighted and, in some cases, possible resolutions are suggested. In particular, challenges in acquisition, selection, and application of patterns are discussed. © 2011, IGI Global.",,Kamthan P.,2011,Book Chapter,Knowledge Engineering for Software Development Life Cycles: Support Technologies and Applications,10.4018/978-1-60960-509-4.ch007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898551997&doi=10.4018%2f978-1-60960-509-4.ch007&partnerID=40&md5=c3eadada73f828ddc809f44e8e62d649,"Concordia University, Canada",IGI Global,English,,9781609605094
Scopus,A decision support scheme for software process improvement prioritization,"Software managers pursuing process improvement initiatives are confronted with the problem of selecting potential improvements. In the field of software quality assurance, suitable decision support for prioritizing the optimization of activities according to their return on investment is not yet available. Our paper addresses this research gap.We develop a decision support scheme that facilitates the selection and prioritization of quality assurance activities. We demonstrate the scheme's applicability in three industrial case studies. By relying on the wellknown COQUALMO model's characteristics and calibration data, our approach is industrially applicable with little data collection efforts. © Springer-Verlag Berlin Heidelberg 2011.",COQUALMO; Decision support; Software process improvement,"Beckhaus A., Karg L.M., Graf C.A., Grottke M., Neumann D.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-20116-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879479020&doi=10.1007%2f978-3-642-20116-5&partnerID=40&md5=75e4ecb673060c4be860496c2c186698,"SAP Research Darmstadt, Germany; Imbus AG, Germany; University of Erlangen-Nuremberg, Germany; University of Freiburg, Germany",,English,18650929,9783642201158
Scopus,An empirical analysis of metrics to predict thesoftware defect fix-effort,"Software defect fix-effort is an important software process metric that plays a critical role in software quality assurance. It is defined as the effort required in person-hours to fix a defect. It can assist to focus on planning of effort, duration and staff and hence costs to deliver the project on time during development as well as maintenance phase. But the prediction of software defect fix-effort in person-hours has long been a complex problem attracting lot of researchers' attention. The present paper reports on an empirical study which aims to construct defect fix-effort estimation model for a large object-oriented system. We use a set of product and process metrics as input variables to predict the fix-time/effort (personhours). Software metrics-based approach used to build quality model has been used. Both the univariate and the multivariate regression analysis have been conducted on the data set.",Defect fix-effort; Process metrics; Product metrics; Regression analysis,"Goel B., Singh Y.",2011,Journal,International Journal of Computers and Applications,10.2316/Journal.202.2011.2.202-2749,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957846573&doi=10.2316%2fJournal.202.2011.2.202-2749&partnerID=40&md5=f374293d22bfab0c2ad351d045b7c67c,"University School of Information Technology, Guru Gobind Singh Indraprastha University, Kashmere Gate, Delhi, India",,English,1206212X,
Scopus,Web engineering and metrics,"The objective of this chapter is three-fold. First, it provides an introduction to Web Engineering, and discusses the need for empirical investigations in this area. Second, it defines concepts such as metrics and measurement, and details the types of quantitative metrics that can be gathered when carrying out empirical investigations in Web Engineering. Third, it presents the three main types of empirical investigations - surveys, case studies, and formal experiments. © 2011 Springer-Verlag Berlin Heidelberg.",,Mendes E.,2011,Journal,Studies in Computational Intelligence,10.1007/978-3-642-17551-0_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952084162&doi=10.1007%2f978-3-642-17551-0_3&partnerID=40&md5=51202c0af68637341ee880b051dd304c,"Computer Science Department, University of Auckland, Private Bag 92019, Auckland, New Zealand",,English,1860949X,9783642175503
Scopus,Quality of the software measures proposed to the industry,There are currently available hundreds of software measures and quantitative models proposed to the practitioners for estimating software projects and measuring the quality of the software delivered. But how many software organizations have today in place software measurement programs and used these measures and models as a basis for decision-making? There must be then something at work that impairs the use of quantitative data for decision making in software-based organizations. Within the software community there is a presumption that the lack of use of software measures in industry is caused by the practitioners' and managers' resistance to change. This presentation is based on a different analysis: this lack of use of software measures by industry comes from the immaturity and unreliability of the measures themselves proposed to date to the industry [1].,,Abran A.,2011,Conference,"Proceedings of International Conference on Software Engineering: Software Quality: The Road Ahead, CONSEG 2011",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901756195&partnerID=40&md5=3fbbd88042d08688498e4f8624222f53,"École de Technologie Supérieure, Université du Québec, Montréal, Canada",Tata McGraw Hill Education Private Limited,English,,0071078169; 9780071078160
Scopus,State of software metrics to forecast variety of elements in the software development process,"Software metrics are mainly utilized to measure and characterize the software development process. Cost, Time and Productivity are key attributes in the software development process. Predictability is the concept which leads to know prior the outcome of the system development. Predicting the various elements which relates to the software development process in advance is quite complex. The main aim of this paper is to propose a comprehensive set of software metrics for predicting the cost, time and productivity in advance with help of available inputs in the project instantiated. These metrics tends to be the horoscope of a project development. Our main objective is to resolve the complexity, enhancing the efficiency of the system development in earliest manner. This paper is mainly deals with analyzing and evaluating predictive metrics with predefined models. The implementation of the metrics to predict the outcome of the project whether the project leads to success or failure even before the project instantiated. If the project leads to success, predicting whether it attains massive or moderate profit. If the project leads to failure, check the alternative way to recover from failure i.e., to reduce the complexity and enhancement of efficiency. © 2011 Springer-Verlag.",COCOMO; Cost and Productivity; Predictability Metrics; Software Predictability; Time,"Sangaiah A.K., Arun Kumar T.",2011,Conference,Communications in Computer and Information Science,10.1007/978-3-642-24037-9_56,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054793493&doi=10.1007%2f978-3-642-24037-9_56&partnerID=40&md5=68d7154765e9342510dc3e13ec814c2f,"School of Computing Science and Engineering, VIT University, Vellore, Tamil Nadu, India",Springer Verlag,English,18650929,9783642240362
Scopus,Demonstrating the usage of GCL software for quatity take-off of building in a virtual high-rise building case,"With the acceleration of the process of global revolution in science and technology led by information technology, the human society is in a gradual transition from an industrial society into an information society. This has already become a major trend in the informatization of various industries including the field of engineering cost. In order to meet the requirements of the development of its own industry, the Chinese construction industry is strengthening its informatization and vigorously promoting the application of information technology. This paper describes the usage of GCL software in a high-rise building case, which was developed by Glodon company, had released to be used and is prevalent in high performance computing quatity take-off. Based on 3D modeling GCL Software can calculate automatically quatity take-off of building processing automatically deducted relations between components. Its core is the construction process of the modeling for building, which is divided into such steps as setting the floor elevation, establishment of axis network, establishment of building components, drawing component, formation of floor, building formation. After three-dimensional model of buildings built the quatity take-off of building can be automatically calculated, which can be seen in output forms. In this paper we present two case studies. The results of the study identify higher efficiency in the GCL software calculation for quatity take-off of building than manual calculation. This study also revealed that GCL software can well meet the bid requirements quickly and accurately. © 2010 IEEE.",Building components; Calculation; Floor component; Modeling; Quatity take-off,Xiaoyong L.,2010,Conference,"ICSTE 2010 - 2010 2nd International Conference on Software Technology and Engineering, Proceedings",10.1109/ICSTE.2010.5608822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650013238&doi=10.1109%2fICSTE.2010.5608822&partnerID=40&md5=aed222a5803b697715f96c10798c7a07,"College of Architecture, North China University of Technology, Beijing, 100144, China",,English,,9781424486656
Scopus,Methods for statistical and visual comparison of imputation methods for missing data in software cost estimation,"Software Cost Estimation is a critical phase in the development of a software project, and over the years has become an emerging research area. A common problem in building software cost models is that the available datasets contain projects with lots of missing categorical data. The purpose of this chapter is to show how a combination of modern statistical and computational techniques can be used to compare the effect of missing data techniques on the accuracy of cost estimation. Specifically, a recently proposed missing data technique, the multinomial logistic regression, is evaluated and compared with four older methods: listwise deletion, mean imputation, expectation maximization and regression imputation with respect to their effect on the prediction accuracy of a least squares regression cost model. The evaluation is based on various expressions of the prediction error and the comparisons are conducted using statistical tests, resampling techniques and a visualization tool, the regression error characteristic curves. © 2011, IGI Global.",,"Angelis L., Sentas P., Mittas N., Chatzipetrou P.",2010,Book Chapter,Modern Software Engineering Concepts and Practices: Advanced Approaches,10.4018/978-1-60960-215-4.ch009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898595120&doi=10.4018%2f978-1-60960-215-4.ch009&partnerID=40&md5=f34efe57e499f841b5bb7b92e5180be7,"Department of Informatics, Aristotle University of Thessaloniki, Greece",IGI Global,English,,9781609602154
Scopus,Measuring the size of business sector and business software,"The paper shows a proposition of metrics for measuring the size of business sector and business software which covers it. It is based on counting the elements of which the observed structures consists. Since the structure of business sector (which is seen as the desired structure of a business organization involved in the sector) and the structure of business software can be shown by same elements, the suggested metrics can be applied to both structures and the obtained measures are mutually comparable. The article also presents a short overview of existing metrics for measuring the size of business sector and business software, and the application of the metrics on the example of an ERP solution.",,"Jakupovic A., Pavlic M., Simunovic D.",2010,Conference,"19th International Conference on Software Engineering and Data Engineering 2010, SEDE 2010",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883629464&partnerID=40&md5=4d96b993ba3bbee59bd2174b16f76ff4,"Business Department, Polytechnic of Rijeka, Vukovarska 58, 51000 Rijeka, Croatia; Department of Informatics, University of Rijeka, Omladinska 14, 51000 Rijeka, Croatia; Croatian Radiotelevision, Prisavlje 3, 10000 Zagreb, Croatia",,English,,9781617386077
Scopus,Success of the process and product in IS development projects. The perspective of the contractor [Prozess- Und produkterfolg in IS-Entwicklungsprojekten Die perspektive der auftragnehmer],[No abstract available],,"Basten D., Joosten D., Mellis W.",2010,Conference,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875023874&partnerID=40&md5=9f9d213ee4948419b268d2a7fe0f4856,"Seminar für Wirtschaftsinformatik und Systementwicklung, Universität zu Köln, Pohligstraße 1, 50969 Köln, Germany",,German,16175468,9783885792727
Scopus,Software metrics for collaborative software engineering projects,"Many software metrics have been established in the past to measure the various aspects of the software development process. The scopes of the metrics span across the artifacts, the end product, the process to produce these artifacts, as well as the project management for the process. In recent years, driven by advances in telecommunication, the internet, and wireless technology, and also by economic factors, collaboration in software engineering project has become increasingly popular. As collaboration becomes more widespread, software engineering metrics for collaboration, and new or adapted metrics for collaborative projects, will become more important. Nonetheless, no comprehensive study has been done on the impact of collaboration on development productivity, process structure, or software quality. In this paper, we study some commonly used software metrics to investigate whether collaboration can easily be incorporated, and where possible, to suggest strategies for that incorporation.",Collaborative software engineering; Process evaluation; Software measurement; Software metrics; Software quality,"Ku C.S., Marlowe T.J.",2010,Conference,"WMSCI 2010 - The 14th World Multi-Conference on Systemics, Cybernetics and Informatics, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870183768&partnerID=40&md5=a822c843b8383f0dd00349dab86a7b4c,"Department of Computer Science, William Paterson University, Wayne, NJ 07470, United States; Department of Mathematics and Computer Science, Seton Hall University, South Orange, NJ 07079, United States",,English,,9781936338009
Scopus,Comparative study on applicability of two novel effort estimation models in WEB projects,"In software engineering research area, cost/effort estimation is one of the most important issues. Effort estimation accuracy will affect the availability of resource allocation and task scheduling. This article discusses the need for new metrics and models to estimate the effort and duration for Web development projects. It then describes a new size metrics, presents two novel methods (WEBMO+ and VPM+), based on WEB model (WEBMO) using Web objects instead of SLOC and Vector Prediction Model (VPM), to fast estimate the development effort of Web-based information systems. We also empirically validate the approach with a four projects study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life cycle to within +/-20 percent across a range of application types. In contrast with other existing methods, WEBMO+ and VPM+ uses raw historical information about development capability and high granularity information about the system to be developed, in order to carry out such estimations. This method is simple and specially suited for small or medium-size Web based information systems.",Effort estimation method; Sizing metric; Web engineering; Web-based metrics,"Lazić L., Mastorakis N.E.",2010,Conference,"Proceedings of the 4th European Computing Conference, ECC '10",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952691821&partnerID=40&md5=ea26a2c9165331080e2ac7d6213c686e,"Department for Mathematics and Informatics, State University of Novi Pazar, Serbia; Technical University of Sofia, English Language Faculty of Engineering, Industrial Engineering, Sofia, Bulgaria",,English,,9789604741786
Scopus,Reliability of transaction identification in use cases,"Context: The concept of transaction is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method many different definitions of use-case transaction (and approaches to their identification) have been proposed thus far. Therefore, a question arises whether all of them define the same concept, and do they provide similar results when used by different people. Objective: The goal of this study was to investigate differences and similarities between the existing definitions of use-case transactions that can have an impact on reliability of the use-case-based functional size measurement. Method: A controlled experiment was conducted on a group of 120 students. The independent variable was a technique used for transaction identification (four methods were investigated), while the dependent variable was the number of transactions identified by participants in a use-case-based requirements specification. Results: A significant difference in the median number of transactions was observed between groups using the original Karner's definition, and the definition proposed by Diev, which is based on the elementary process known from Function Point Analysis. In addition, a list of problems influencing intramethod reliability was defined based on the qualitative analysis of the experiment data. Conclusions: It seems that there are two main sources of use-case transactions definitions. The Karner's definition of transaction is based on the concept of use-case transaction proposed by Jacobson - the inventor of use cases, while the second one proposed by Diev, is based on the elementary process. There are also other methods for transaction identification that follow one of these definitions (e.g. Robiolo and Orosco stimuli-verbs approach). The important observation is that both main definitions of transactions yield different on-average results when used by different people. Therefore, it is important to consistently use only one in order to create a reliable historical database. © 2010 ACM.",Functional size measurement; Use Case Points; Use-case transactions,"Ochodek M., Alchimowicz B., Jurkiewicz J., Nawrocki J.",2010,Conference,"24th European Conference on Object-Oriented Programming, ECOOP 2010 Workshop Proceedings - Workshop 1: Workshop on Advances in Functional Size Measurement and Effort Estimation, FSM'10",10.1145/1921705.1921710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952366650&doi=10.1145%2f1921705.1921710&partnerID=40&md5=14b15517f74e7cdbf9d3a786c795b43c,"Institute of Computing Science, Poznań University of Technology, ul. Piotrowo 2, 60-965 Poznań, Poland",,English,,9781450305396
Scopus,A cost estimation model for OEM based military software support,"Military software supportability increasingly becomes a key factor of equipment system supportability, and the proportion of software support cost in the system is rising significantly. Based on the analysis of the processes of supportability design and implementation, the paper proposes several effective methods for estimating software costs in the different stages and aspects, and constructs a general-purpose cost estimation model for military software support. Case studies on real-world projects demonstrate the feasibility and rationality of the model. © 2010 IEEE.",Cost estimation; Military software supportability; Software metrics,"Zhu Y., Yujun Z.",2010,Conference,"2nd International Conference on Information Science and Engineering, ICISE2010 - Proceedings",10.1109/ICISE.2010.5690862,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951989930&doi=10.1109%2fICISE.2010.5690862&partnerID=40&md5=7635e0c075dd849e8312a5276f17a84b,"General Department of Armaments, Systems Engineering Institute of Engineer Equipments, Beijing, China; Institute of Software, Chinese Academy of Sciences, Beijing, China",,Chinese,,9781424480968
Scopus,Estimation of the size of business sectors covered by ERP solutions,"The paper describes a method for estimation of the size of business sectors covered by ERP solutions. It presents a short overview of existing methods for measuring the size of business sectors and software in general, including the ERP solution as its special type. A list of business sectors which are supported by ERP solutions is given. The described method for the estimation of size was applied over this business sectors list. Then, an estimation of the number of common elements which are part of the structure of the observed business sectors was performed. Based on the estimated number of common elements and estimated size of business sectors, the relationship between common and specific elements in the size of business sectors supported by ERP solutions was calculated. © 2010 IEEE.",Business sector; ERP solutions; Size,"Jakupovic A., Pavlic M., Poscic P.",2010,Conference,"Proceedings - 5th International Multi-Conference on Computing in the Global Information Technology, ICCGI 2010",10.1109/ICCGI.2010.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951892500&doi=10.1109%2fICCGI.2010.30&partnerID=40&md5=b8676b262f875c15ef1ffc22e328da68,"Business Department, Polytechnic of Rijeka, Rijeka, Croatia; Department of Informatics, University of Rijeka, Rijeka, Croatia",,English,,9780769541815
Scopus,A method to measure productivity trends during software evolution,"Better measures of productivity are needed to support software process improvements. We propose and evaluate indicators of productivity trends that are based on the premise that productivity is closely related to the effort required to complete change tasks. Three indicators use change management data, while a fourth compares effort estimates of benchmarking tasks. We evaluated the indicators using data from 18 months of evolution in two commercial software projects. The productivity trend in the two projects had opposite directions according to the indicators. The evaluation showed that productivity trends can be quantified with little measurement overhead. We expect the methodology to be a step towards making quantitative self-assessment practices feasible even in low ceremony projects. © 2010 Springer-Verlag.",,"Benestad H.C., Anda B., Arisholm E.",2010,Conference,Communications in Computer and Information Science,10.1007/978-3-642-14819-4_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650736423&doi=10.1007%2f978-3-642-14819-4_11&partnerID=40&md5=af1f476ffdb668061c1784059d6370e4,"Simula Research Laboratory, University of Oslo, P.O. Box 134, Lysaker 1325, Norway",,English,18650929,3642148182; 9783642148187
Scopus,Software project effort assessment,"Software project assessments and postmortem analyses can increase the success of future projects and forthcoming project phases. However, although assessments and analyses are well-presented in the software engineering literature, they are short of descriptions to assess effort. This paper proposes a stepwise effort assessment method that strives to facilitate learning from past experiences and to increase effort estimation accuracy by exploring realized project effort. The proposed method can be used either to assess the realized project effort of a time span from project's lifecycle or as a part of the whole project postmortem analysis in producing effort information for the project iteration assessment, final report, and annual project portfolio report. In addition, the effort information increases the project team's knowledge on realized activity sets and their relationships, and the information can be utilized in estimation method and model calibration for both re-estimations and future estimations. The method was evaluated with case projects, a sample supplied by Tieto Finland Oy. Copyright © 2010 John Wiley & Sons, Ltd.",assessment; effort; final report; postmortem; project management; retrospective,"Haapio T., Eerola A.",2010,Journal,Journal of Software Maintenance and Evolution,10.1002/smr.454,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649841501&doi=10.1002%2fsmr.454&partnerID=40&md5=2d9499b7d9192cffebf29b9be6f5d56c,"Tieto Finland Oy, P.O. Box 1779, FI-70601 Kuopio, Finland; Department of Computer Science, University of Eastern Finland, P.O. Box 1627, FI-70211 Kuopio, Finland",,English,1532060X,
Scopus,Understanding the influential factors to development effort in Chinese software industry,"A good understanding of the influential factors to software development effort and further precise effort estimate are undoubtedly crucial to any cost-effective and controllable software development projects. In most effort estimation researches, a large dataset is always a necessary basis of estimation modeling, model calibration and method validation. Among them, different attributes and characteristics of project data will to a large extent affect the applicable scope of particular research result. This research aims to identify the factors that significantly influence development effort, and to investigate how the influence works in Chinese software industry. In this study, six factors and their relationships to development effort are analyzed, prioritized and discussed based upon the dataset recording 999 projects from 140 software organizations in China. In terms of our analysis and findings, some suggestions for effort estimation and control are extracted to assist software practitioners in coping with various types of software projects. © 2010 Springer-Verlag.",,"He M., Zhang H., Yang Y., Wang Q., Li M.",2010,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-642-13792-1_24,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955437123&doi=10.1007%2f978-3-642-13792-1_24&partnerID=40&md5=941776cfc0e5975969a83eb97390d630,"Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, China; National ICT Australia, University of New South Wales, Australia",,English,03029743,3642137911; 9783642137914
Scopus,An effect of data size on performance of effort estimation with missing data techniques,"To deal with missing data in historical project data sets is an important issue for constructing effort estimation models. Past researches have showed that the similarity-based imputation showed high estimation performance. However, it is unclear if it is still effective for small data sets. In this paper, using multiple data sets with different project cases each extracted from ISBSG data set, we present an experimental evaluation among four methods: mean imputation, similarity-based imputation, row-column deletion and pairwise deletion. The result showed that the row-column deletion showed better performance than similarity-based imputation for data sets not exceeding 220 cases.",,"Tamura K., Monden A., Matsumoto K.-I.",2010,Journal,Computer Software,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956451371&partnerID=40&md5=364ee90a40e8186e5b0c8a8b4d11fbb6,"Graduate School of Information Science, Nara Institute of Science and Technology, Japan",,Japanese,02896540,
Scopus,Comparison of imputation techniques for efficient prediction of software fault proneness in classes,"Missing data is a persistent problem in almost all areas of empirical research. The missing data must be treated very carefully, as data plays a fundamental role in every analysis. Improper treatment can distort the analysis or generate biased results. In this paper, we compare and contrast various imputation techniques on missing data sets and make an empirical evaluation of these methods so as to construct quality software models. Our empirical study is based on NASA's two public dataset. KC4 and KC1. The actual data sets of 125 cases and 2107 cases respectively, without any missing values were considered. The data set is used to create Missing at Random (MAR) data Listwise Deletion(LD), Mean Substitution(MS), Interpolation, Regression with an error term and Expectation-Maximization (EM) approaches were used to compare the effects of the various techniques.",Imputation; Missing data; Missing Data Techniques,"Sikka G., Takkar A.K., Uddin M.",2010,Journal,"World Academy of Science, Engineering and Technology",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651526458&partnerID=40&md5=7208cb6e2ba6dc66408bf42e9589ca5a,"Department of Computer Science and Engineering, Dr. B R Ambedkar National Institute of Technology, Jalandhar, 14401 1, Punjab, India; School of Information Technology, Guru Gobind Singh Indrapratha University, Delhi, India; Dr. B R Ambedkar National Institute of Technology, Jalandhar, 144011, Punjab, India",,English,2010376X,
Scopus,Effects of company age and size on software processes and maturity in the Turkish software industry,"Software engineering industry is growing very rapidly with significant updates in process methodologies and more flexibility in basic practices of software engineering. In such an era, the maturity in development and growth of IT firms around the world cannot be the same. In this study we analyze Turkish software industry by conducting a comprehensive survey and present the snapshot of current profile in terms of four major process areas: requirements, design, development, and test. Our results illustrate that software process maturity is directly affected by industrial variables such as age and company size. Old companies are more traditional in terms of techniques used to collect requirements, implement design, whereas New and Large companies adapt the state-of-the-art technologies.",Statistical tests; Survey; Turkish software industry,"Tosun A., Bener A., Koch S., Aytac T., Yilmaz H., Cetinkaya O.",2010,Conference,"Business Transformation through Innovation and Knowledge Management: An Academic Perspective - Proceedings of the 14th International Business Information Management Association Conference, IBIMA 2010",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905122957&partnerID=40&md5=7d17e378474ed1c2c4c14940d60a9988,"Department of Computer Engineering, Bogazici University, Turkey; Department of Management, Bogazici University, Turkey; Logo Business Solutions, Turkey; Department of Computer Engineering, Bogazici University, Turkey","International Business Information Management Association, IBIMA",English,,9780982148938
Scopus,A graph theoretic approach to estimate software evolution costs,"A key issue in software engineering is being able to estimate software evolution costs consistently and accurately. Poor cost estimation can lead management to make bad decisions about which projects to pursue and result in cost overruns and low profit margins. A graph-theoretic framework could dramatically increase the predictability and accuracy of software evolution cost estimations. If a graphtheoretic framework for estimating costs of software evolution were developed and tuned and then adopted by program managers, they could make better decisions about which projects to pursue in order to reduce risk and improve margins.",Cost estimation; Project management; Software design and development; Software evolution; Software methodologies; Software metrics,Stanek E.J.,2010,Conference,"Proceedings of the IASTED International Conference on Software Engineering, SE 2010",10.2316/p.2010.677-072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954609042&doi=10.2316%2fp.2010.677-072&partnerID=40&md5=430922b268d05686fe73d6380b1ce130,,Acta Press,English,,9780889868212
Scopus,Development of an effort estimation model - A case study on delivery projects at a leading IT provider within the electric utility industry,"When projects are sold with fixed prices, it is utterly important to quickly and accurately estimate the effort required to enable an optimal bidding. This paper describes a study performed at a leading IT provider within the electric utility industry, with the purpose of improving the ability to early produce effort estimates of projects where standard functionality is delivered. In absence reliable historic data, an estimation model suitable for incorporating expert estimates was developed. The model is based on decomposition of projects and bottom-up estimation of them, where impact of relevant variables is estimated by assessing discrete scenarios. In addition to a estimating the expected effort of a project the uncertainty of provided estimates are visualised. Together with the transparency of the model this makes it possible to analyse and refine the estimates as more details of a project are known. Copyright © 2010 Inderscience Enterprises Ltd.",Bidding; Delivery projects; Effort estimation; Estimation model; Project cost; Technology management,"Sommestad T., Lilliesköld J.",2010,Journal,"International Journal of Services, Technology and Management",10.1504/IJSTM.2010.029675,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72049113578&doi=10.1504%2fIJSTM.2010.029675&partnerID=40&md5=bacbf139fd20f496e71d376da7916a9f,"Department of Industrial Information and Control Systems, Royal Institute of Technology (KTH), Osquldas väg 12, S-100 44, Stockholm, Sweden",Inderscience Publishers,English,14606720,
Scopus,Web 2.0 effort estimation,"Web effort models and techniques provide the means for Web companies to formalise the way they estimate effort for their projects, and potentially help in obtaining more accurate estimates. Accurate estimates are fundamental to help project managers allocate resources more adequately, thus supporting projects to be finished on time and within budget. The aim of this chapter is to introduce the concepts related to Web effort estimation and effort forecasting techniques, and to discuss effort prediction within the context of Web 2.0 applications. © 2010, IGI Global.",,Mendes E.,2009,Book Chapter,"Handbook of Research on Web 2.0, 3.0, and X.0: Technologies, Business, and Social Applications",10.4018/978-1-60566-384-5.ch025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898115419&doi=10.4018%2f978-1-60566-384-5.ch025&partnerID=40&md5=5f701c613462bf724da063c3d12b84e1,"The University of Auckland, New Zealand",IGI Global,English,,9781605663845
Scopus,Software size estimation using expert estimation: A fuzzy logic approach,"Software Managers have been using Function Points as a methodology to estimate the size of software projects during the early stages of project development for many years now. While the Function Point methodology has proven to be successful, many software managers feel that it is overly complicated to use and essentially trades one estimation model for another. Instead, research has shown that many software managers feel that the use of a less formal methodology called ""expert estimation"" or ""estimation by analogy"" can be extremely accurate when used by a manager who has a significant amount of domain experience. While expert estimations can be quite effective, their accuracy relies on the vague, imprecise, or incomplete information available at the start of a software project. In addition, they rely on the tacit knowledge held by the manager and therefore other managers cannot duplicate their results. This paper demonstrates the use of expert estimation with fuzzy logic to determine the effort required to develop a real world software engineering project.",,"Stevenson G., Bryant W.",2009,Conference,"International Conference on Software Engineering Theory and Practice 2009, SETP 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878243240&partnerID=40&md5=824e3dcf945c807c6808170564676f26,"CIS, Mt. San Jacinto College, Menifee CA, United States; Mission Analysis, Northrop Grumman, Azusa CA, United States",,English,,9781615676590
Scopus,Designing change request forms for better effort estimates on requirements changes,"Managing change request (CR) effectively with the appropriate estimation of cost and effort needed to correct or modify a requirement change (RC) is a challenge present in all software development projects. Despite software project estimation tools available in the market, there is no enough data available on which to base an estimation model and no useful categorization of requirements types when estimating the amount of effort needed for a RC. In this paper, new criteria are suggested for CR forms aim to help improve better requirements change categorisation and increase accurate cost estimation effort reworks. A theoretical effort rework model based on COCOMO II is developed. The aim of this theoretical model is to benefit IT practitioners by providing them with a better effort estimation method and in turns to help with mitigation of project risks.",,"Chua B.B., Verner J.",2009,Conference,"International Conference on Software Engineering Theory and Practice 2009, SETP 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878235426&partnerID=40&md5=cdca6e7f247bf204143e6bfefcd9347e,"University of Technology, Sydney, Australia; University of New South Wales, National ICT Australia, Sydney, Australia",,English,,9781615676590
Scopus,"The relationship among development skills, design quality, and centrality in open source projects","In a previous paper, we have found empirical evidence supporting a positive relationship between network centrality and success. However, we have also found that more successful projects have a lower technical quality. A first, straightforward argument explaining previous findings is that more central contributors are also highly skilled developers who are well known for their ability to manage the complexity of code with a lower attention to the software structure. The consolidated metrics of software quality used by the authors in their previous research represent measures of code structure. This paper provides empirical evidence supporting the idea that the negative impact of success on quality is caused by the careless behaviour of skilled developers, who are also hubs within the social network. Research hypotheses are tested on a sample of 56 OS applications from the SourceForge.net repository, with a total of 378 developers. The sample includes some of the most successful and large OS projects, as well as a cross-section of less famous active projects evenly distributed among SourceForge.net's project categories.",Social networks; Software design skills; Software quality,"Barbagallo D., Francalanci C.",2009,Conference,"17th European Conference on Information Systems, ECIS 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870666545&partnerID=40&md5=90449f016d530cb93bb69c3969f9a578,"Dipartimento di Elettronica e Informazione, Politecnico di Milano, Via Ponzio 34/5, 20133 Milano, Italy",,English,,9788861293915
Scopus,An approach to software project scheduling using personal software process and soft computing techniques,"This paper presents an approach for the modeling and handling of time management features in a software project scheduling framework. It is achieved in two main steps. The first one is an automatic features extraction process with the aim of defining the elements involved in a software project. This knowledge is represented by means fuzzy sets and fuzzy prototypes. The source of data is the Personal Software Project time recording logs. The second one is the successful integration of the extracted knowledge of a scheduler able to handle these uncertainties into a scheduling framework. For this purpose, it is used a Takagi-Sugeno fuzzy controller and reactive scheduling algorithms. The result is a flexible scheduler able to handle requirements and circumstances that change very frequently.",Personal software process; Soft-computing; Software projects scheduling,"Peralta A., Romero F.P., Polo M., Olivas J.A.",2009,Conference,"Proceedings of the 2009 International Conference on Artificial Intelligence, ICAI 2009",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866129032&partnerID=40&md5=810a55bfbfdeef8284e03989e8356310,"Dept. of Information Systems and Technologies, University of Castilla La Mancha, Ciudad Real, Spain",,English,,9781601321091
Scopus,Prioritization of software process improvements: A COQUALMO-based case study and derived decision support scheme,"Quality assurance has always been a major concern for software engineers. While a lot of work has been devoted to technical quality assurance aspects, its economics have rarely been addressed. This is remarkable in the light of the current endeavor of the software industry to learn from more mature industries and to adapt their 'lean' and process improvement philosophies. We advance this debate by proposing a decision support scheme. It facilitates the selection and prioritization of quality assurance activities for improvement initiatives. The prioritizing order is based on the expected quality gains in terms of reduced number of defects in the software product. Our scheme supports managers in their decision process, as shown in three industrial case studies. It can be instantiated with low data-collection effort because it makes use of calibration data and model characteristics of COQUALMO.",COQUALMO; Decision support; Software process improvement; Software quality assurance,"Beckhaus A., Karg L.M., Graf C.A., Grottke M., Neumann D.",2009,Conference,"ICSOFT 2009 - 4th International Conference on Software and Data Technologies, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549176620&partnerID=40&md5=53c8e69348be8ea49279e0bddb17c584,"SAP Research Darmstadt, Germany; Imbus AG, Germany; University of Erlangen-Nuremberg, Germany; University of Freiburg, Germany",,English,,9789896740092
Scopus,Validity verifying method of software project management,"To ensure the validity of software project management method and improve the success rate of software projects, a quantitative and systemic validation method of software project management method validity was studied combined with some actual project cases. By the data revision of the schedule result, the usability of the data could be ensured. By the short-term and small-amount data verification, the organization could obtain the implementation result in time. By the long-term and large-amount data verification, the organization could track the implementation result of software project management method chronically. The statistics of the actual data indicate that these verification methods could reflect the validity of the project management methods adopted by the organization, and could obtain continuous improvement. ©2009 IEEE.",Method; Software project management; Validity; Verification,Zhang J.-G.,2009,Conference,"Proceedings - International Conference on Management and Service Science, MASS 2009",10.1109/ICMSS.2009.5302352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849138231&doi=10.1109%2fICMSS.2009.5302352&partnerID=40&md5=c90241ea273bd23966877b6612b1e205,"School of Economics and Management, University of Science and Technology Beijing, USTB, Beijing, China",,English,,9781424446391
Scopus,Investigation of domain effects on software,"It has been stated that there is very less variability in cohesion, coupling and complexity of software packages within specific domains such as Graphical User Interface (GUI). This implies that software metrics show low variability within single domain and high variability between domains. This paper investigates the domain issue by creating hierarchical model of four different domains and two software packages within each domain. Metrics are collected on each package and compared against the domains and packages. Results confirm metrics are not domain centric. ©2009 ACM.",Cohesion; Complexity; Coupling; Domain effects; Software metrics; Software quality,"Virani S., Etzkorn L., Gholston S., Farrington P., Utley D., Fortune J.",2009,Conference,"Proceedings of the 47th Annual Southeast Regional Conference, ACM-SE 47",10.1145/1566445.1566496,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449711511&doi=10.1145%2f1566445.1566496&partnerID=40&md5=e198aac419283b47ee691a8a67624782,"University of Alabama at Huntsville, 301 Sparkman Drive, Huntsville, AL 35811, United States; University of Alabama at Huntsville, 301 Sparkman Drive, Huntsville, AL 35899, United States",,English,,9781605584218
Scopus,Predicting software stage effort with sequence changing ratio,"Software stage effort has the features of data starvation and uncertainty. It is difficult to use the current methods (e.g. regression) to make predictions. This paper proposes a novel prediction method, which gets the effort sequence changing feature-""changing ratio"" from the completed stage effort sequences, and gets the ""changing ratio threshold"" from historical projects by machine learning methods, then uses grey models to make predictions. The experimental results on 10 real world software engineering datasets show that, compared with linear regression method, the prediction accuracy of the proposed method has been improved by 20%-80%. This is very encouraging and indicates that the method has considerable potential.",Grey model; Project cost prediction; Software cost; Stage cost,"Wang Y., Song Q.-B., Shen J.-Y.",2009,Journal,Jisuanji Xuebao/Chinese Journal of Computers,10.3724/SP.J.1016.2009.01346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69249232377&doi=10.3724%2fSP.J.1016.2009.01346&partnerID=40&md5=02460dc3e5c26eb6751b6b4bf56e02ea,"Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an 710049, China",,Chinese,02544164,
Scopus,A pattern-oriented methodology for engineering high-quality E-commerce applications,"This article proposes, develops, and explores a methodology for engineering electronic commerce applications (ECA) aiming for ""high-quality."" In doing so, the development and maintenance of ECA is undertaken from the perspective of Web Engineering. The relevant quality attributes and corresponding stakeholder types for the ECA are identified, and the role of a flexible development process and the challenges in making optimal use of patterns are analyzed. The activities of a systematic selection and application of patterns to the macro- and micro-architecture design of business-to-consumer (B2C) ECA are given. The scope and limitations of the proposed methodology are discussed, and some possible directions for its evolution are outlined. [Article copies are available for purchase from InfoSci-on-Demand.com]. © 2009, IGI Global.",Accessibility; B2c e-commerce; E-commerce quality issues; Information quality; Is development methodologies; User satisfaction; User-centered design; Web site design,Kamthan P.,2009,Journal,Journal of Electronic Commerce in Organizations,10.4018/jeco.2009040101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70249140274&doi=10.4018%2fjeco.2009040101&partnerID=40&md5=48545aeb6d46c636364295fd4ac6384c,"Concordia University, Canada",IGI Publishing,English,15392937,
Scopus,First steps towards validating a cost-benefit model of reviews and tests,"Software project managers' decisions on reviews and tests are difficult. This paper describes a cost-benefit model for specific decisions on quality assurance. The quantitative model is based on single relationships and is quantified with historical data. Its results are shown and are compared with cost estimations. The model is able to reflect reported results of process improvement. Data collected in student projects is used to evaluate the model. Project averages and single projects are considered. Furthermore, results of a cross-validation are shown. © 2008 Springer Berlin Heidelberg.",,Hampp T.,2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-89403-2-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049101077&doi=10.1007%2f978-3-540-89403-2-5&partnerID=40&md5=23f363291d5fb8c4f7a41fc6b1f047e7,"Institut für Softwaretechnologie, Universität Stuttgart, Stuttgart 70569, Germany",,English,03029743,3540894020; 9783540894025
Scopus,Adjusting analogy software effort estimation based on fuzzy logic,"Analogy estimation is a well known approach for software effort estimation. The underlying assumption of this approach is the more similar the software project description attributes are, the more similar the software project effort is. One of the difficult activities in analogy estimation is how to derive a new estimate from retrieved solutions. Using retrieved solutions without adjustment to considered problem environment is not often sufficient. Thus, they need some adjustment to minimize variation between current case and retrieved cases. The main objective of the present paper is to investigate the applicability of fuzzy logic based software projects similarity measure to adjust analogy estimation and derive a new estimate. We proposed adaptation techniques which take into account the similarity between two software projects in terms of each feature. In earlier work, a similarity measure between software projects based on fuzzy Cmeans has been proposed and validated theoretically against some well known axioms such as: Normality, Symmetry, transitivity, etc. This similarity measure will be guided towards deriving a new estimate.",Analogy software effort estimation; Fuzzy logic; Software project similarity measurement,"Azzeh M., Neagu D., Cowling P.",2008,Conference,ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649217891&partnerID=40&md5=653456c30aefe8dcf07d8a170ef66cd5,"Department of Computing, University of Bradford, Bradford, BD7 1DP, United Kingdom",,English,,9789898111524
Scopus,Empirical studies for web effort estimation,"Web technologies are being even more adopted for the development of public and private applications, due to the many intrinsic advantages. Due to this diffusion, estimating the effort required to develop Web applications represents an emerging issue in the field of Web engineering since it can deeply affect the competitiveness of a software company. To this aim, in the last years, several estimation techniques have been proposed. Moreover, many empirical studies have been carried out so far to assess their effectiveness in predicting Web application development effort. In the chapter, we report on and discuss the results of the most significant empirical studies undertaken in this field. © 2009, IGI Global.",,"Di Martino S., Ferrucci F., Gravino C.",2008,Book Chapter,"Information Systems Research Methods, Epistemology, and Applications",10.4018/978-1-60566-040-0.ch017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901522660&doi=10.4018%2f978-1-60566-040-0.ch017&partnerID=40&md5=98c39122d89d7803091c5f73190dcd3a,"Università di Salerno, Italy; Università degli Studi di Napoli sFederico II', Italy",IGI Global,English,,9781605660400
Scopus,Using patterns for engineering high-quality web applications,"In this chapter, we view the development and maintenance of Web applications from an engineering perspective. A methodology, termed as POWEM, for deploying patterns as means for improving the quality of Web applications is presented. To that end, relevant quality attributes and corresponding stakeholder types are identified. The role of a process, the challenges in making optimal use of patterns, and feasibility issues involved in doing so, are analyzed. The activities of a systematic selection and application of patterns are explored. Following a top-down approach to design, examples illustrating the use of patterns during macro- and micro-architecture design of a Web application are given. Finally, the implications towards Semantic Web applications and Web 2.0 applications are briefly outlined. © 2008, IGI Global.",,Kamthan P.,2008,Book Chapter,Software Engineering for Modern Web Applications: Methodologies and Technologies,10.4018/978-1-59904-492-7.ch006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899210556&doi=10.4018%2f978-1-59904-492-7.ch006&partnerID=40&md5=2608fbb52e8c0c6c5461bc9e0e92c969,"Concordia University, Canada",IGI Global,English,,9781599044927
Scopus,Sizing Web applications for Web effort estimation,"Surveying and classifying previous work on a particular field brings several benefits, which are: 1) to help organise a given body of knowledge; 2) to provide results that can help identify gaps that need to be filled; 3) to provide a categorisation that can also be applied or adapted to other surveys; and 4) to provide a classification and summary of results that may benefit practitioners and researchers who wish to carry out meta-analyses. This chapter presents a survey literature of size measures (attributes) that have been proposed for Web effort estimation. These measures are classified according to a proposed taxonomy. We also discuss ways in which Web companies can devise their own size measures. © 2008, IGI Global.",,Mendes E.,2008,Book Chapter,Handbook of Research on Web Information Systems Quality,10.4018/978-1-59904-847-5.ch001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898566042&doi=10.4018%2f978-1-59904-847-5.ch001&partnerID=40&md5=6ff2275612b15fa3a0f2006db6bcf9f3,"The University of Auckland, New Zealand",IGI Global,English,,9781599048475
Scopus,Quantifying functional reuse from object oriented requirements specifications,"Software reuse is essential in improving efficiency and productivity in the software development process. This paper analyses reuse within requirements engineering phase by taking and adapting a stsndard functional size measurement method. COSMIC FFP. Our proposal attempts to quantify reusability from Object Oriented requirements specifications by identifying potential primitives with a high level of reusability and applying a reuse indicator. These requirements are specified using OO-Method, an automatic software production method based on transformation models. We illustrate the application of our proposal in a Car Rental real system.",Functional reuse; Functional size; Measurement; Requirement specification,"Condori-Fernandez N., Pastor O., Daneva M., Abran A., Castro J.",2008,Conference,"11th Workshop on Requirements Engineering, WER 2008 - Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870406531&partnerID=40&md5=d9af57fcc04713ab23d3edf2fbef7136,"Centro de Investigation en Metodos de Production de Software, Universidad Politecnica de Valencia, Valencia, Spain; University OfTwente, Drienerlolaan 5, 7522 NB Enschede, Netherlands; Ecole de Technologie Superieure ETS, 1100 Notre-Dame Ouest, Montreal, H3C 1K3, Canada; Departamento de Informatica, Universidade Federal de Pernambuco, Recife, Brazil",,English,,9788476531440
Scopus,Proposing an effort estimation model for complex web applications,"Most Web development projects suffer from unrealistic project schedules, leading to applications that are rarely developed on time and within budget. There have been numerous attempts to model resource estimation of Web projects, but none yielded a complete causal model incorporating all the necessary component parts. This paper introduces a new effort estimation predicting model for complex web applications. This model combines Object-oriented Method Function Points for the web (OOmFPWeb) procedure with the Case-Based Reasoning (CBR) as a prediction technique.",Effort estimation; OOmFPWeb; Terms: CBR; Web applications,"Tawfik S.M., Kassem H.F., Irgens C.",2008,Conference,"Proceedings of the 9th IASTED International Conference on Software Engineering and Applications, SEA 2008",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549118853&partnerID=40&md5=50c818b10d49bc647274d50535f85048,"Arab Academy for Science and Technology and Maritime Transport, P.O. Box: 1029, Miami, Alexandria, Egypt",,English,,9780889867765
Scopus,An investigation on performance of software enhancement projects in China,"As two major performance measures, software productivity and quality convey critical information in supporting many decision making situations during project planning, management, as well as process/organization benchmarking processes. However, there is a lack of investigation on these performance measures with respect to enhancement projects, this leads to, in many enhancement/ maintenance cases, the inappropriate application of techniques or benchmarking data resulted from studies using development project data. In this paper, through analysis of 264 enhancement projects in China, we seek to develop in-depth and comprehensive understanding about software enhancement projects, by examining the variance of productivity and defect density by several significant influencing factors, such as business area, region, language, programming tool, project size, and team size. © 2008 IEEE.",,"Mei H., Ye Y., Qing W., Mingshu L.",2008,Conference,"Neonatal, Paediatric and Child Health Nursing",10.1109/APSEC.2008.42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60849126433&doi=10.1109%2fAPSEC.2008.42&partnerID=40&md5=8e583051e55395030ca94d3d5f1d4c0d,"Institute of Software, Chinese Academy of Sciences, China; Graduate University, Chinese Academy of Sciences, China",,English,14416638,
Scopus,E-learning courseware Effort Estimate Model,"This paper presents an improved software estimation model, which uses to estimate developing effort of e-Learning's contents. This model is called the e-Learning courseware Effort Estimate Model (EEEM). This model uses special designed storyboard to delineate the e-Learning courseware's contents and define the measurement metrics. Research was conducted using statistical data from the real world e-Learning courseware development. Detail implementation data of 60 courseware were collected and analyzed. The ""Web Model"" was adapted to assess the webpage development. The number of web page, test-page, graphics and multimedia elements in each courseware are analyzed. The EEEM model is designed to estimate the e-Learning courseware development time and effort. The proposed model is more accurate than the previous models. The previous Web Model has Mean Absolute Percentage Error: MAPE =83.28 % and the new EEEM Model :MAPE=16.86%. The output of this model is the courseware development effort (in person-hour). The new model can help developers to save money and time that cause from estimation. © 2008 IADIS.",Courseware; E-learning; Effort estimate model; Software development,"Vanijja V., Chokananratana A.",2008,Conference,MCCSIS'08 - IADIS Multi Conference on Computer Science and Information Systems; Proceedings of e-Learning 2008,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449122028&partnerID=40&md5=4bb679086991f19e694591c83a9ed832,"King Mongkut's University of Technology Thonburi, Thailand",,English,,9789728924584
Scopus,Size and effort-based computational models for software cost prediction,"Reliable and accurate software cost estimations have always been a challenge especially for people involved in project resource management. The challenge is amplified due to the high level of complexity and uniqueness of the software process. The majority of estimation methods proposed fail to produce successful cost forecasting and neither resolve to explicit, measurable and concise set of factors affecting productivity. Throughout the software cost estimation literature software size is usually proposed as one of the most important attributes affecting effort and is used to build cost models. This paper aspires to provide size and effort-based estimations for the required software effort of new projects based on data obtained from past completed projects. The modelling approach utilises Artificial Neural Networks (ANN) with a random sliding window input and output method using holdout samples and moreover, a Genetic Algorithm (GA) undertakes to evolve the inputs and internal hidden architectures and to reduce the Mean Relative Error (MRE). The obtained optimal ANN topologies and input and output methods for each dataset are presented, discussed and compared with a classic MLR model.",Artificial neural networks; Genetic algorithms; Software cost estimation,"Papatheocharous E., Andreou A.S.",2008,Conference,ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849144765&partnerID=40&md5=3e1cfd8fc411b716eb89684fb369df47,"University of Cyprus, Dept. of Computer Science, 75 Kallipoleos str., CY1678 Nicosia, Cyprus",,English,,9789898111388; 9789898111364
Scopus,Analysis of Software Functional Size Databases,"Parametric software effort estimation models rely on the availability of historical project databases from which estimation models are derived. In the case of large project databases with data coming from heterogeneous sources, a single mathematical model cannot properly capture the diverse nature of the projects under consideration. Clustering algorithms can be used to segment the project database, obtaining several segmented models. In this paper, a new tool is presented, Recursive Clustering Tool, which implements the EM algorithm to cluster the projects, and allows use different regression curves to fit the different segmented models. This different approaches will be compared to each other and with respect to the parametric model that is not segmented. The results allows conclude that depending on the arrangement and characteristics of the given clusters, one regression approach or another must be used,and in general, the segmented model improve the unsegmented one. © 2008 Springer-Verlag Berlin Heidelberg.",Clustering; Effort estimation; EM algorithm; Recursive Clustering Tool (RCT); Segmented parametric model; Software Engineering,"Cuadrado-Gallego J.J., Garre M., Rejas R.J., Sicilia M.-Á.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85553-8_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249110015&doi=10.1007%2f978-3-540-85553-8_16&partnerID=40&md5=5de923221166ff2e84846039c55ea7c1,"University of Alcalá, Autovia A-2 km 33.6, Madrid, Alcalá de Henares 28871, Spain",,English,03029743,3540855521; 9783540855521
Scopus,"Cases, predictions, and accuracy learning and its application to effort estimation","Estimation by analogy EBA (effort estimation by analogy) is one of the proven methods for effort prediction in software engineering; in AI this would be called Case-Based Reasoning. In this paper we consider effort predictions using the EBA () method AQUA and pay attention to two aspects: (i) The influence of the set of analogs on the quality of prediction. The set of analogs is determined by a learning process incorporating the number of nearest neighbors and the threshold of the similarity measure used, (ii) Analyzing and understanding the conditions under which the prediction can be expected to be the most or the least accurate. We study two types of learning: One for finding the ""best"" set of analogs, and one for finding out factors for reliability. While both questions are relevant for different areas and disciplines, the focus of the paper is on estimation of effort in software engineering. For EBA method AQUA, the cases can be features or past projects characterized by attributes of various type. Classical estimation approaches just investigate the overall estimated quality of a system. However, in that case information is missing if and why estimation was performing the way it did. Bad estimates are often due to external influences. Therefore it is valuable for to find out under which conditions the estimates are more or less reliable. © Springer-Verlag Berlin Heidelberg 2008.",Accuracy; AQUA; Estimation by analogy; Learning; Software effort,"Li J., MacKas B., Richter M.M., Ruhe G.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-85502-6_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149092338&doi=10.1007%2f978-3-540-85502-6_20&partnerID=40&md5=480a264aab97715236d6e18011210e70,"Department of Computer Science, University of Calgary, Canada",,English,03029743,3540855017; 9783540855019
Scopus,Software cost estimation inhibitors - A case study,"Software cost estimation errors are increasing in the automotive industry along with the number of software components in modern vehicles. As the software cost estimation is an important and problematic part of project planning there is a need of process improvement to decrease estimation errors. However, to improve the process of cost estimation there is a need to explore whether the perceived cost estimation problem is an estimation problem or if it is a management problem. This paper focuses on inhibitors in the process of software cost estimation in system development projects and reports the results of a study carried out at a Swedish automotive company in order to gain an understanding for the factors that affect the cost estimation process. © 2008 Springer-Verlag Berlin Heidelberg.",Automotive Systems; Case Study; Cost Estimation; Empirical Software Engineering,"Magazinovic A., Pernstål J., Öhman P.",2008,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/978-3-540-69566-0_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249126390&doi=10.1007%2f978-3-540-69566-0_8&partnerID=40&md5=410fa74a4d2cad89e924a4bfd7c1d477,"Chalmers, Computer Science and Engineering, Gothenburg SE 421 96, Sweden",,English,03029743,3540695648; 9783540695646
Scopus,A quasi-experiment for effort and defect estimation using least square linear regression and function points,"Software companies are currently investing large amounts of money in software process improvement initiatives in order to enhance their products' quality. These initiatives are based on software quality models, thus achieving products with guaranteed quality levels. In spite of the growing interest in the development of precise prediction models to estimate effort, cost, defects and other project's parameters, to develop a certain software product, a gap remains between the estimations generated and the corresponding data collected in the project's execution. This paper presents a quasi-experiment reporting the adoption of effort and defect estimation techniques in a large worldwide IT company. Our contributions are the lessons learned during (a) extraction and preparation of project historical data, (b) the use of estimation techniques on these data, and (c) the analysis of the results obtained. We believe such lessons can contribute to the improvement of the state-of-the-art in prediction models for software development. © 2009 IEEE.",Human judgment approaches; Linear regression; Software metrics estimation,"Tenõrio Jr. N.N., Ribeiro M.B., Ruiz D.D.",2008,Conference,"32nd Annual IEEE Software Engineering Workshop, SEW-32 2008",10.1109/SEW.2008.20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951195144&doi=10.1109%2fSEW.2008.20&partnerID=40&md5=06af9265bca6d29ca1b8cfb6ad2ae201,"Faculty of Informatics, Pontifical Catholic University of Rio Grande do Sul, Porto Alegre, Brazil",IEEE Computer Society,English,,9780769536170
Scopus,A component-based approach to effort estimation,"Over the past ten couple of years, there is a variety of effort models proposed by academicians and practitioners at early stage of software development life cycle. Some authors addressed that efforts could be predicted using Lines of Codes(LOC) or COCOMO model, while others emphasized that it could be made using Function Point Analysis(FPA) and others. The study seeks to develop a model which estimates software effort by studying and analyzing small and medium scale application software. To develop such a model, 50 completed software projects are collected from a software company. From the sample, components affecting effort estimation are identified and extracted. By applying them to simple regression analyses, a prediction of software effort estimates with accuracy of MMRE = 9% was constructed. The results give several benefits. First, estimation problems are minimized due to the simple procedure used in identifying those components. Second, the predicted software projects are only applicable to a specific environment rather than being based upon industry environment. We believe the accuracy of effort estimates can be improved. According to the results analyzed, the work shows that it is possible to build up simple and useful prediction model based on data extracted at early stage of software development life cycle. We hope this model can provide valuable ideas and suggestions to project designers for planning and controlling software projects in near future. © 2008 IEEE.",Application software; Design team factors; Effort estimation,"Simon W., Iok K.",2008,Conference,"2008 International Conference on Wireless Communications, Networking and Mobile Computing, WiCOM 2008",10.1109/WiCom.2008.2896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049087723&doi=10.1109%2fWiCom.2008.2896&partnerID=40&md5=c31d7100b4eb0fb97d274f96ee778f8d,"Faculty of Business Administration, University of Macau, Macao, Macau",IEEE Computer Society,English,,9781424421084
Scopus,Agent-based and discrete-event modeling: A quantitative comparison,"Although agent-based modeling is widely regarded as a concept well-suited to the simulation of logistics systems, there has not been any quantitative comparison with discreteevent modeling or other more established concepts. As a timely check on the rising popularity of this paradigm for modeling complex systems, this paper describes a study comparing an agent-based model with a traditional discrete-event model, each of which has been implemented to a common specification. The model scenario is based on an existing application - the global repair operation of a fleet of civil aircraft high-bypass turbofan engines. Initial results, including metrics for runtime, and software size, structure, and complexity highlight the basic similarities and differences between the models. Future estimation of model maintainability will further quantify their benefits and drawbacks. It is hoped that characterizing the paradigms more fully will enable a modeler to match various parts of a model more appropriately to its problem domain. © 2007 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.",,"Yu T.-T., Scanlan J.P., Wills G.B.",2007,Conference,"Collection of Technical Papers - 7th AIAA Aviation Technology, Integration, and Operations Conference",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885035737&partnerID=40&md5=7898f2f341a0e78c858253b9b0f450a3,"University of Southampton, Southampton, Hampshire, SO17 1BJ, United Kingdom",,English,,1563479087; 9781563479083
Scopus,Activity-based software estimation using work break down structure,"Software Cost estimation at activity level is very much accurate than macro estimation with respect to phases of software development life cycle, but the same is very difficult to achieve[1]. Activity based estimation focus on key activities should not be left out and if any effort variance occurs it will be possible to track at particular activity level rather than affecting the entire activities[1]. Activity-based Software estimation based on work break down structure has been explained by collecting and analyzing the data for 12 Enhancements from Application service Maintenance project which were already delivered. This paper explains how to arrive accurate estimation at different micro level activities of Software Development Life Cycle(SDLC). © 2007 Springer.",,"Basavaraj M.J., Shet K.C.",2007,Conference,Innovations and Advanced Techniques in Computer and Information Sciences and Engineering,10.1007/978-1-4020-6268-1-20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879672730&doi=10.1007%2f978-1-4020-6268-1-20&partnerID=40&md5=3ebf06c44080d8f0bf1b6ed0be505fc2,"Perot Systems, EPIP Phase II, Whitefield Industrial Area, Bangalore-560 066, India; Computer Department, National Institute of Technology, Karnataka, Surathkal, India",,English,,9781402062674
Scopus,A software size measure for estimating effort based on a software development life cycle,"This paper describes a new software size measure that is based on the artifacts produced during a software development project. This measure leverages characteristics associated with most software development processes and requires the counting of attributes, where an attribute represents a distinct type of knowledge stored in a software artifact. Given the amount of data this measure requires, the empirical study measured software size of student projects, and then used this data to estimate effort using estimation by analogy. The goal of the empirical study was to assess whether this software size measure warrants further study with industry projects. Since the pred(0.25) results from this study are similar to other effort estimation studies that used academic projects, the authors' have concluded that this new software size measure warrants further study.",,"Voorhees D.P., Mitropoulos F.J.",2007,Conference,"International Conference on Software Engineering Theory and Practice 2007, SETP 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878243212&partnerID=40&md5=796036fc69ab5bb59584267026d293ed,"Le Moyne College, 1419 Salt Springs Road, Syracuse, NY 13214, United States; Nova Southeastern University, 3301 College Avenue, Fort Lauderdale, FL 33314, United States",,English,,9781615677221
Scopus,Lessons learned and results from applying data-driven cost estimation to industrial data sets,"The increasing availability of cost-relevant data in industry allows companies to apply data-intensive estimation methods. However, available data are often inconsistent, invalid, or incomplete, so that most of the existing data-intensive estimation methods cannot be applied. Only few estimation methods can deal with imperfect data to a certain extent (e.g., Optimized Set Reduction, OSR®). Results from evaluating these methods in practical environments are rare. This article describes a case study on the application of OSR® at Toshiba Information Systems (Japan) Corporation. An important result of the case study is that estimation accuracy significantly varies with the data sets used and the way of preprocessing these data. The study supports current results in the area of quantitative cost estimation and clearly illustrates typical problems. Experiences, lessons learned, and recommendations with respect to data preprocessing and data-intensive cost estimation in general are presented. © 2007 IEEE.",,"Heidrich J., Trendowicz A., Münch J., Ishigai Y., Yokoyama K., Kikuchi N., Kawaguchi T.",2007,Conference,QUATIC 2007 - 6th International Conference on the Quality of Information and Communications Technology,10.1109/QUATIC.2007.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48449106260&doi=10.1109%2fQUATIC.2007.16&partnerID=40&md5=3782f08fa6435346905d5a05b1ccdf61,"Fraunhofer IESE, Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; IPA-SEC, 2-28-8 Honkomagome, Bunkyo-Ku, Tokyo, 113-6591, Japan; Toshiba Information Systems (Japan) Corporation, 7-1 Nissin-Cho, Kawasaki-City 210-8540, Japan",,English,,0769529488; 9780769529486
Scopus,Development of an effort estimation model: A case study on delivery projects at a leading IT provider within the electric utility industry,"When projects are sold with fixed prices, it is utterly important to quickly and accurately estimate the effort required to enable an optimal bidding. This paper describes a case study performed at a leading IT provider within the electric utility industry, with the purpose of improving the ability to early produce effort estimates of projects where standard functionality is delivered. The absence of reliable historic data made expert judgment the only appropriate foundation for estimates, with difficutties of quickly develop estimates and reuse or modify estimates already made. To overcome these troubling issues, the expert estimates were incorporated into a model where they and the factors influencing them are traceable and readily expressed. The model is based on decomposition of projects and bottom-up estimation of them, where impact of relevant variables is estimated by assessing discrete scenarios. It provides quick and straightforward means of developing estimates of the decomposed elements and whole projects in various circumstances, where not only expected effort is considered, but the uncertainly of the individual estimates is visualized as well. Which together with the traceability enables the estimates produced by the model to be assessed, analyzed and refined as more details of the project is known. ©2007 PICMET.",,"Sommestad T., Lilliesköld J.",2007,Conference,Portland International Conference on Management of Engineering and Technology,10.1109/PICMET.2007.4349549,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47849123458&doi=10.1109%2fPICMET.2007.4349549&partnerID=40&md5=b9e2639df6ea0367d9351188fa4efa3b,"Department of Industrial Information and Control Systems, Royal Institute of Technology (KTH), Stockholm, Sweden",,English,,1890843164; 9781890843168
Scopus,Estimation practices efficiencies: A case study,"Software Project Estimation has been one of the hot topics of research in the software engineering industry for a long time. Solutions for estimation are in great demand. By knowing the estimates early in the software project life cycle, project managers can manage resources efficiently. The objective of this paper is to investigate the estimation practices within an individual software company and to assess their reliability. We perform a methodical review of predictions from a within-company model, based on our analysis of their historical project data. We analyze their estimation practices and compute prediction accuracies and thereby, suggest improvements or modifications. The data analysis revealed that the company used expert judgment in the early years but gradually switched to parametric approaches (calibrated COCOMO II hybrid model). We describe our systematic review of the estimation process, perform experiments and analyze the results. Our findings suggest that these methods should be employed. © 2007 IEEE.",,"Yenduri S., Munagala S., Perkins L.A.",2007,Conference,"Proceedings - 10th International Conference on Information Technology, ICIT 2007",10.1109/ICOIT.2007.4418293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349086447&doi=10.1109%2fICOIT.2007.4418293&partnerID=40&md5=a2f464e69aba3ae68bdf9f1661fbcfbb,"730 East Beach Blvd., Long Beach, MS 39560, United States",,English,,0769530680; 9780769530680
Scopus,Bayesian analysis for software development's effort estimation factors,"For better software metrics, management and adapting new software effort estimation, this paper analyzed project attributes which were extracted from International Software Benchmarking Groups (abbr. ISBSG) and analyzed from component-based software development. And on this base, an effort estimation factors set was proposed for component-based software development.",Bayesian analysis; Components; Estimation; Software management,"Yu X.-J., Zhang S.-J., Hu D.-H.",2007,Journal,Beijing Gongye Daxue Xuebao / Journal of Beijing University of Technology,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248377381&partnerID=40&md5=2dedf83cc980fcc691f03ef975696880,"School of Software Engineering, Beijing University of Technology, Beijing 100022, China; Institute of Computer Science, Beijing University of Technology, Beijing 100022, China; Civil aviation Telecomn. corp., Beijing 100021, China",,Chinese,02540037,
Scopus,Software economics: A roadmap,"The fundamental goal of all good design and engineering is to create maximal value added for any given investment. There are many dimensions in which value can be assessed, from monetary profit to the solution of social problems. The benefits sought are often domain-specific, yet the logic is the same: design is an investment activity. Software economics is the field that seeks to enable significant improvements in software design and engineering through economic reasoning about product, process, program, and portfolio and policy issues. We summarize the state of the art and identify shortfalls in existing knowledge. Past work focuses largely on costs, not on benefits, thus not on value added; nor are current technical software design criteria linked clearly to value creation. We present a roadmap for research emphasizing the need for a strategic investment approach to software engineering. We discuss how software economics can lead to fundamental improvements in software design and engineering, in theory and practice. © 2007 by IEEE Computer Society. All rights reserved.",,"Boehm B.W., Sullivan K.J.",2007,Book Chapter,"Software Engineering: Barry W. Boehm'S Lifetime Contributions to Software Development, Management, and Research",10.1109/9780470187562.ch2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043307596&doi=10.1109%2f9780470187562.ch2&partnerID=40&md5=da0f6f33c70bc6c12ecfd8ccfa7adf04,,Wiley-IEEE Press,English,,0818676094; 9780470187562; 047014873X; 9780818676093
Scopus,Software size estimation of object-oriented systems,"Software size estimation has been the object of a lot of research in the software engineering community due to the need of reliable size estimates in tile utilization of existing software project cost estimation models. This paper discusses tlte strengths and weaknesses of existing size estimation techniques, considers the nature of software size estimation, and presents a software size estimation model which has the potential for providing more accurate size estimates than existing methods. The proposed method takes advantage or a characteristic of object-oriented systems, the natural correspondence between specification and implementation, in order to enable users to come up with better size estimates at early stages of the software development cycle. Through a statistical approach tbe method also provides a confidence interval for the derived size estimates. The relation between the presented software sizing model and project cost estimation has also been considered. © 2006 by the IEEE Computer Society. All rights reserved.",,Laranjeira L.A.,2007,Book Chapter,"Software Management, Seventh Edition",10.1109/9780470049167.ch6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036666290&doi=10.1109%2f9780470049167.ch6&partnerID=40&md5=ad9b5483a6e10adfe084e52d94453861,"Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX  78712, United States",Wiley-IEEE Press,English,,9780470049167; 0471775622; 9780471775621
Scopus,Software engineering economics,"This paper summarizes the current state ofthe art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available and the state of the art in algorithmic cost models. © 2006 IEEE Computer Society.",,"Chulani S., Boehm B.",2007,Book Chapter,"Software Management, Seventh Edition",10.1109/9780470049167.ch6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036642966&doi=10.1109%2f9780470049167.ch6&partnerID=40&md5=5598bfffaf7c7a2e40375675692e147f,"Research Staff Member IBM T.J., Watson Research Center, 650 Harry Road, San Jose, CA  95120, United States; Software Engineering Computer Science Department, Center for Systems and Software Engineering, University of Southern California, United States",Wiley-IEEE Press,English,,9780470049167; 0471775622; 9780471775621
Scopus,Software project estimation: An overview,[No abstract available],Biological system modeling; Estimation; Measurement; Productivity; Schedules; Software; Software engineering,Stutzke R.D.,2007,Book Chapter,"Software Management, Seventh Edition",10.1109/9780470049167.ch6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036618921&doi=10.1109%2f9780470049167.ch6&partnerID=40&md5=7550f4e771caa989be5129c4fb016ff1,"Vice President Science Applications International Corp. (SAIC), 6725 Odyssey Drive, Huntsville, AL  35806, United States",Wiley-IEEE Press,English,,9780470049167; 0471775622; 9780471775621
Scopus,A neural network approach for web cost estimation,"Accurate cost estimates are an essential element to remain successful in the market, so cost estimation initiatives have been in the center of attention for many firms. Web development projects are certainly different from traditional software development projects and hence, require differently tailored measures for accurate estimation. The use of neural network in estimating software cost by Nasser Tadayon [1] produced accurate results, but it can't be applied to web applications, because they do not take all of the web objects into consideration. In this paper, author explores the use of expert judgment and machine learning techniques using neural network as well as referencing WebMo Estimation model to predict the cost of software. The proposed network improves the accuracy of the estimation as the number of dataset increases with input from expert judgment that affects the learning procedure.",Cost estimation; Neural network; Web application; Web object; WebMo,"Reddy Ch.S., Raju K.V.S.V.N., Srinivas T., Devi G.L.",2007,Conference,"Proceedings of the 11th IASTED International Conference on Software Engineering and Applications, SEA 2007",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959055132&partnerID=40&md5=2172e12404b121138f04c33c0f7836e9,"Andhra University, Visakhapatnam, India; Bisro Inc., United States; GITAM University, Visakhapatnam, India",ACTA Press,English,,9780889867055
Scopus,Implementing integration of quality standards Capability Maturity Model Integration and ISO 9001:2000 for software engineering,"In this paper, we present the implementation of an integrated quality reference frame. This reference frame integrates two quality standards Capability Maturity Model Integration and ISO 9001:2000. As a first process to implement, we choose a software estimation model to evaluate effort, schedule and cost of software development during all the phases of a project. An approach based on COnstructive COst MOdel (COCOMO) II model is used, and we propose an adaptation to a particular context of an organisation. Both human and cultural aspects of the company are considered in order to mitigate the problem of acceptability. Our model is based on the work of Dr. Allan Albrecht on Function Points and Dr. Barry Boehm on COCOMO II model. Copyright © 2007 Inderscience Enterprises Ltd.",Business process; Capability Maturity Model Integration; CMMI; COCOMO II; COnstructive COst MOdel; Enterprise modelling; FP; Function Points; ISO 9001:2000; Project evaluation; Quality standards; Reference frame; Software estimation,"Ferchichi A., Bourey J.-P., Bigand M., Lefebvre H.",2007,Journal,International Journal of Product Lifecycle Management,10.1504/IJPLM.2007.018298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867062877&doi=10.1504%2fIJPLM.2007.018298&partnerID=40&md5=bfeb7128f0dd09dd4523808e20d5a66e,"Laboratoire de Génie Industriel de Lille (LGIL), Ecole Centrale de Lille, B.P. 48, F-59651, Villeneuve d'Ascq, France; Recherche Opérationnelle Innovation, Sylis, Parc du Pont Royal - Bat. G, 251 Avenue du Bois, 59831 Lambersart, France",Inderscience Publishers,English,17435110,
Scopus,An empirical research of the software project measures model,"In this research, the software project measures model be proposed for the software firms in Korea. In this KOSPMM (KOSPMM; Korea's software project measures model), there are the definitions of 74 measure items which are suggested considering measurability, scalability and acquisition possibility. And the metric for each measure is specified by considering the common attributes and applicability on the software firms. For validating of KOSPMM model, the questionnaires were developed, and the survey on the 25 items of the software project measures was performed as of initial phase. The data was collected from the 8 major Korean software firms and 45 software development projects. To the result, the basic statistics about some measures were shown. Especially, the result explains that the level of productivity and quality of software project in Korea can be compared with the others.",Metrics; Software measures; Software productivity,Ahn Y.,2006,Conference,"Proceedings of the ISCA 15th International Conference on Software Engineering and Data Engineering, SEDE 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883542585&partnerID=40&md5=cbb81e29b695b83073b91b4ef40cf0ea,"Kyungwon College, Sungnam City, South Korea",,English,,9781604235821
Scopus,Detailed theoretical considerations for a suite of metrics for integration of software components,"This paper defines two suites of metrics, which cater static and dynamic aspects of component assembly. The static metrics measure complexity and criticality of component assembly, wherein complexity is measured using Component Packing Density and Component Interaction Density metrics. Further, four criticality conditions namely, Link, Bridge, Inheritance and Size criticalities have been identified and quantified. The complexity and criticality metrics are combined into a Triangular Metric, which can be used to classify the type and nature of applications. Dynamic metrics are collected during the runtime of a complete application. Dynamic metrics are useful to identify super-component and to evaluate utilisation of components. In this paper both static and dynamic metrics are evaluated using Weyuker's set of properties. The result shows that the metrics provide a valid means to measure issues in component assembly. © 2006 Springer.",Component assembly; Component metrics; CORBA component model,"Lakshmi Narasimhan V., Hendradjaya B.",2006,Conference,"Advances in Systems, Computing Sciences and Software Engineering - Proceedings of SCSS 2005",10.1007/1-4020-5263-4-41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878068684&doi=10.1007%2f1-4020-5263-4-41&partnerID=40&md5=a1067a68e9625faee939587a011edca7,"Faculty of Electrical Engineering and Computer Science, University of Newcastle, NSW 2308, Australia",,English,,1402052626; 9781402052620
Scopus,Efficiency implications of open source commonality and reuse,"This paper analyzes the reuse choices made by open source developers and relates them to cost efficiency. We make a distinction between the commonality among applications and the actual reuse of code. The former represents the similarity between the requirements of different applications and, consequently, the functionalities that they provide. The latter represents the actual reuse of code. No application can be maintained for ever. A fundamental reason for the need for periodical replacement of code is the exponential growth of costs with the number of maintenance interventions. Intuitively, this is due to the increasing complexity of software that grows in both size and coupling among different modules. The paper measures commonality, reuse and development costs of 26 open-source projects for a total of 171 application versions. Results show that reuse choices in open-source contexts are not cost efficient. Developers tend to reuse code from the most recent version of applications, even if their requirements are closer to previous versions. Furthermore, the latest version of an application is always the one that has incurred the highest number of maintenance interventions. Accordingly, the development cost per new line of code is found to grow with reuse.",Commonality; Maintenance and replacement policies; Open source; Software cost; Software reuse,"Capra E., Francalanci C., Merlo F., Tosetti M.",2006,Conference,"Proceedings of the 14th European Conference on Information Systems, ECIS 2006",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870640320&partnerID=40&md5=96343fb68ce4eb73dc346d2e403b5d80,"Politecnico of Milan, Department of Electronics and Information, Via Ponzio 34/5, 20133 Milano, Italy; Department of Electronics and Information, Italy",,English,,
Scopus,Web metrics selection through a practitioners' survey,"There are a lot of web metrics proposals. However, most previous work does not include their practical application. The risk of doing so, is to limit all the effort made just to an academic exercise. In order to eliminate this gap as well as to be able to apply the work developed, it is necessary to involve the different stakeholders related to web technologies as an essential part of web metrics definition. So, it is crucial to know the perception they have about web metrics, especially those related to the development and maintenance of web sites and applications. In this paper, we present the work we have done to find out which web metrics are considered useful by web developers and maintainers. This study has been performed on the basis of the 385 web metrics classified in WQM, a Web Quality Model defined in a previous work, using as validation tool, a survey made by professionals of web technologies. As a result, we have found out that the most weighted metrics were related to usability. That means that web professionals give more importance to the user of metrics than to their own effort.",Quality; Web Metrics,"Ruiz J., Calero C., Piattini M.",2006,Conference,"ICSOFT 2006 - 1st International Conference on Software and Data Technologies, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954023017&partnerID=40&md5=f5d858b09919da80e05d8a1f4b4b54e5,"Information Systems and Technologies Department, UCLM-Soluziona Research and Development Institute, University of Castilla-La Mancha, Paseo de la Universidad, 4, 13071 - Ciudad Real, Spain",,English,,9728865694; 9789728865696
Scopus,Software cost estimating: A cyclical conundrum,"This article describes the dilemma of some organizations in establishing credible software estimates, proposes some guiding principles and practices for improving the process and addresses how current software best practices may play a role in the journey to achieving accurate software estimations.",,Walker E.,2006,Review,CrossTalk,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748324469&partnerID=40&md5=e9728876a0f9773b6d381c91f6b57bbf,"Data and Analysis Center for Software; Data and Analysis Center for Software (DACS), 775 Daedalian Drive, Rome, NY 13441, United States",,English,,
Scopus,Quantifying the evolution of goals in requirements engineering: A study on the quality assurance review assistant tool,"Goal-oriented requirements engineering is becoming increasingly important, as goals help to specify requirements, provide rationales to stakeholders, and detect causes behind requirements conflicts. Measuring and understanding the manner whereby goals are understood and modeled over time can have a vital implication to many aspects of software development. Despite the importance, however, there seems to be little help available in measuring the evolution of goals. This paper presents a scheme for quantifying the evolution of goals in requirements engineering. This paper also presents a case study for this scheme using a groupware application, called the Quality Assurance Review Assistant Tool (QARAT). The data collected are analyzed in relation to several conjectures regarding the structural evolution of the goals over 11 iterations. Ultimately, studies such as these can be used to help improve cost estimation, project planning, and risk analysis activities. © 2005 by Trop Chowdhury, Lawrence Chung & Kendra Cooper.",,"Chowdhury T., Chung L., Cooper K.",2005,Conference,"15th Annual International Symposium of the International Council on Systems Engineering, INCOSE 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883307897&partnerID=40&md5=d7cbc485a94df327a58c41f8f07b0271,"The Univ. of Texas at Dallas, 2601 N. Floyd Road, Richardson, TX 75083-0688, United States",,English,,9781622769285
Scopus,Fuzzy set based software measurement and estimation,"In this paper, we investigate how fuzziness in software measuring emerges and how to take this fuzziness into account. It involves software metrics with values in categorical data, which are represented by software metrics with values in fuzzy sets and linguistic variables. This study is aimed to support the development of high quality software. The process of program design as a transition from a problem to a program is studied. A classification of software metrics is developed with the aim of better structuring and optimization of the software fuzzy metric design. Processes of constructing new measures from existing ones often use aggregation operations. That is why, aggregation operations for fuzzy sets, which are necessary for building efficient software metrics and their relevant utilization.",Fuzzy set; Fuzzy software metric; Problem; Software; Uncertainty,"Burgin M., Debnath N.",2005,Conference,"18th International Conference on Computer Applications in Industry and Engineering 2005, CAINE 2005",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883287564&partnerID=40&md5=0ddda9a2366e34266bbf3a12b7416c8e,"Department of Computer Science, University of California, Los Angeles, CA 90095, United States; Department of Computer Science, Winona State University, Winona, MN 55987, United States",,English,,9781604234817
Scopus,"Understanding manager and developer perceptions of the relative advantage, compatibility, and complexity of function points and source lines of code","Software measures are recommended for the effective management of software development projects. Innovation diffusion theory (IDT) provides perspective for understanding managers' and software developers' perceptions of the relative advantage, complexity, and compatibility of software measures. This paper describes the results of a survey in which software developers and managers identified a software measure and then answered IDT-based questions about the measure. Two of the most commonly identified measures were source lines of code (SLOC) and function points (FP). Overall, participants indicated that FP have greater relative advantage, compatibility, and complexity than SLOC. Developers indicated that FP have greater relative advantage, compatibility, and complexity than SLOC. Managers, however, did not perceive a significantly greater relative advantage and compatibility for FP over SLOC, but did perceive FP to be more complex than SLOC.",Function points; Innovation diffusion; Practitioners; SLOC; Software measures; Survey,"Sheetz S., Henderson D., Wallace L.",2005,Conference,"Association for Information Systems - 11th Americas Conference on Information Systems, AMCIS 2005: A Conference on a Human Scale",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869828553&partnerID=40&md5=c21d1d7742fc50b2f065393d4b64c0db,"Virginia Tech, United States",,English,,9781604235531
Scopus,Optimizing software construction,"This article considers the problems of optimal allocation of resources to the production of software, and optimal scheduling of software development and testing activities. There exist several models focused on careful estimation of costs and times needed for software implementation. The purpose of this paper is to use these models to formulate the optimization problems faced by software managers when planning the development and testing activities.",Cost estimation; Optimization; scheduling; Soft computing; Software engineering; Software metrics,"Pacciarelli D., Pranzo M., Cuadrado-Gallego J.J., Dolores M., Moreno R.",2005,Conference,"Proceedings of the 2005 International Conference on Software Engineering Research and Practice, SERP'05",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749107226&partnerID=40&md5=63963ef3d11df7ed68cf51a3cc8cee92,"Dipartimento di Informatica e Automazione, Università Roma Tre, Roma, Italy; Departamento de Ciencias de la Computación, Universidad de Alcalá, Madrid, Spain",,English,,9781932415506
Scopus,An information theory word-based metric to evaluate software maintainability and reusability,"The English language and its use in software is a significant factor in its ability to be reused because it impacts largely on understandability and maintainability. The choice of naming convention for programming variables (classes, methods, variables, etc.) in representing the problem domain is critical. Presently there is no known method other than visual inspection to judge maintainability and reusability on the basis of program element names. However, information theory may be helpful in assessing these attributes by determining the degree of descriptiveness of program variables. Scoring the programming variable names and comparing them to words used in the English language, even unique words in a problem domain, would allow for the automation of maintainability and reusability assessments on this basis.",Entropy software metrics; Information theory; Software maintenance programming,"Olague H.M., Etzkorn L.H.",2005,Conference,"Proceedings of the 2005 International Conference on Software Engineering Research and Practice, SERP'05",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749085914&partnerID=40&md5=0d572e4f8e8bd0ad2a74dff8ebde42f8,"Computer Science Department, University of Alabama in Huntsville, Huntsville, AL, United States",,English,,9781932415506
Scopus,Software project cost estimation using Multivariate Adaptive Regression Splines,"The software cost estimation is an important task within projects. It determines the success or failure of a project. In order to improve the estimation, it is very important to identify and study the most relevant factors and variables. This paper describes a method to perform this estimation based on Multivariate Adaptive Regression Splines (MARS), a multivariate non-parametric regression procedure. MARS is capable of reliably tracking very complex data structures that often hide in high-dimensional data. The implementation of the algorithm used is a modification of the original MARS method, named API-MARS. Previously of the algorithm application, an exhaustive data processing is done.",Artificial intelligent; Data mining; Multivariate Adaptive Regression Splines; Sofware cost estimation,"Rodriguez Montequín V., Villanueva Balsera J., Alba González C., Martínez Huerta G.",2005,Journal,WSEAS Transactions on Information Science and Applications,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645243758&partnerID=40&md5=9f94a8347b97dd274081ccbb66b5c60e,"Project Management Area, University of Oviedo, C/Independencia 4, 33004 Oviedo, Spain",,English,17900832,
Scopus,The viability of fuzzy logic modeling in software development effort estimation: Opinions and expectations of project managers,"There is a growing body of evidence to suggest that significant benefits may be gained from augmenting current approaches to software development effort estimation, and indeed other project management activities, with models developed using fuzzy logic and other soft computing methods. The tasks undertaken by project managers early in a development process would appear to be particularly amenable to such a strategy, particularly if fuzzy logic models are used in a complementary manner with other algorithmic approaches, thus providing a range of predictions as opposed to a single point value. As well as providing a more intuitively acceptable set of estimates, this would help to reduce or remove the unwarranted level of certainty associated with a point estimate. Furthermore, such an approach would enable organizations to ""store"" their project management knowledge, making them less susceptible to employee resignations and the like. If fuzzy logic modeling is to be implemented in industry, however, managers must first believe it to be a realistic and workable option. This issue is addressed here by considering two related questions: one, what expectations do project managers have in relation to effort estimation? And two, what is their opinion of the methods that might be useful in this regard? This is followed by a discussion of the results of two surveys of project managers aimed at deriving membership functions using polling methods, the first using an interval declaration approach and the second using votes on fixed points. It is concluded that there is indeed support in the software engineering practitioner community for the use of methods based on the principles of fuzzy logic modeling. © World Scientific Publishing Company.",Effort estimation; Fuzzy logic; Industry acceptibility; Project management,"Macdonell S.G., Gray A.R.",2005,Journal,International Journal of Software Engineering and Knowledge Engineering,10.1142/S0218194005002555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27744599244&doi=10.1142%2fS0218194005002555&partnerID=40&md5=3c294454f96b33f874602a49de41ed48,"School of Computer and Information Sciences, Auckland University of Technology, Private Bag 92006, Auckland 1020, New Zealand; Department of Preventive and Social Medicine, University of Otago, New Zealand",,English,02181940,
Scopus,Testing the predictive ability of a requirements pattern language,This paper looks at a case study in the commercial procurement of an IT system to support learners on short educational courses. It compares the use case model created before the system was built with the use case model after the system was delivered. The original use case model was created through the application of a requirements pattern language designed to be employed during the procurement phase of an IT system. The final use case model was reverse engineered from the working application. The objective was to discover how accurately the original model represented the final application to provide a measure of the potential usefulness of the pattern language during procurement. © Springer-Verlag London Limited 2004.,Procurement; Requirements patterns; Use case modelling,"Merrick P., Barrow P.",2005,Journal,Requirements Engineering,10.1007/s00766-004-0193-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19844382060&doi=10.1007%2fs00766-004-0193-5&partnerID=40&md5=637f26eab2fecca9b5c1d29c5b2c0b11,"School of Computer Science, Universtiy of East Anglia, NR4 7TJ Norwich, United Kingdom",,English,09473602,
Scopus,Cost estimation modeling techniques for web applications: An empirical study,"The problem of estimating the effort required to develop web applications represents an emerging issue in the field of web engineering. In the paper, we report on an empirical analysis we have carried out in order to construct suitable prediction models. In particular, we have considered several features characterizing web applications and constructed some prediction models by employing different techniques, such as multiple linear regression, stepwise regression, and regression tree. The empirical analysis has been performed by using two different data sets: the first was obtained by considering web projects developed by academic students while the second is related to web projects developed by a software company. Indeed, the analysis had a twofold goal. On one hand, it allowed us to establish which features could be considered indicators of effort development and which technique could be suitable to construct a prediction model. On the other hand, it allowed us to analyze possible differences/similarity in the empirical results obtained with the two different data sets. © 2005 by Knowledge Systems Institute Graduate School.",,"Costagliola G., Di Martino S., Ferrucci F., Gravino C., Tortora G., Vitiello G.",2005,Conference,Proceedings:  DMS 2005 - 11th International Conference on Distributed Multimedia Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923859839&partnerID=40&md5=c4101b35a2f5195e39223e3da7c210a7,"Dipartimento di Matematica e Informatica, Università degli Studi di Salerno, Via Ponte Don Melillo, Fisciano, SA  84084, Italy",Knowledge Systems Institute Graduate School,English,,1891706179
Scopus,Software effort prediction models using maximum likelihood methods require multivariate normality of the software metrics data sample: Can such a sample be made multivariate normal?,"Missing data often appear in software metrics data samples used to construct software effort prediction models1. So far, the least biased and thus the most strongly recommended family of such models capable of handling missing data are those using maximum likelihood methods. However, the theory of such maximum likelihood methods assumes that the data samples underlying the model construction are multivariate normal. Previous researches on such models simply ignored the violation of such an assumption by the empirical data samples. This paper proposes and empirically illustrates a not-so-complicated but effective technique to transform the data sample for the purpose of meeting such an assumption. This technique is empirically proven to work for typical software metrics data samples and the author recommends applying such a technique in any further researches on and practical industrial application of software effort prediction models using maximum likelihood methods. © 2004 IEEE.",,Chan V.K.Y.,2004,Conference,Proceedings - International Computer Software and Applications Conference,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18744415700&partnerID=40&md5=ed21d45d22cd76e5b75b57837df4b8f2,Macau Polytechnic Institute,,English,07303157,
Scopus,Visualizing software project analogies to support cost estimation,"Software cost estimation is a crucial task in software project portfolio decisions like start scheduling, resource allocation, or bidding. A variety of estimation method's have been proposed to support estimators. Especially the analogy - based approach - based on a project's similarities with past projects - has been reported as both efficient and relatively transparent. However, its performance was typically measured automatically and the effect of human estimators' sanity checks was neglected. Thus, this paper proposes the visualization of high-dimensional software project portfolio data using multidimensional scaling (MDS). We (i) propose data preparation steps for an MDS visualization of software portfolio data, (ii) visualize several real-world industry project portfolio data sets and quantify the achieved approximation quality to assess the feasibility, and (iii) outline the expected benefits referring to the visualized portfolios' properties. This approach offers several promising benefits by enhancing portfolio data understanding and by providing intuitive means for estimators to assess an estimate's plausibility.",Analogy-based cost estimation; Multidimensional scaling; Portfolio decisions; Portfolio visualization; Software project portfolio,"Auer M., Graser B., Biffl S.",2004,Conference,ICEIS 2004 - Proceedings of the Sixth International Conference on Enterprise Information Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8444242950&partnerID=40&md5=717fbc3595c45e2fe34c4512608c76af,"Institute for Software Technology, Vienna University of Technology, Austria",,English,,9728865007; 9789728865009
Scopus,Application of fuzzy logic for improved software project management estimations,"Project Management (PM) is the application of knowledge, skills, tools and techniques to project activities in order to meet project requirements. The success of any project relies heavily on the initial estimation of all project parameters. Fuzzy Logic is a soft-computing technique used to effectively solve uncertainties due to imprecise inputs to generate linguistic or quantitative outputs. This paper investigates the application of fuzzy logic as a feasible technique for improved estimation accuracy to all the tasks within the project management knowledge areas to ensure higher software project success rates.",Estimations; Fuzzy Logic; Project Management; Project variables; Soft-computing,"Siwani I., Capretz M.",2004,Conference,Canadian Conference on Electrical and Computer Engineering,10.1109/CCECE.2004.1347582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4944229700&doi=10.1109%2fCCECE.2004.1347582&partnerID=40&md5=bae4ebeacb7957f8e60238679d88cf06,"University of Western Ontario, Department of Electrical Engineering, London, Ont. N6A 5B9, Canada",,English,08407789,0780382536
Scopus,Fuzzy logic in estimation of effort in software projects,"This paper proposes the use of fuzzy logic in different estimation models. Some of these models apply variables not clearly defined that could be expressed by fuzzy sets. This formalism would help to make more gradual transitions. Furthermore, fuzzy logic may play a central role in estimation by analogy. The similarity with previous software projects depend on a number of factors (some of them fuzzy) that should be weighted according to their relevance. Finally an adaptive fuzzy reputation of experts and a fuzzy measure of the accuracy of estimation models may also contribute to improve the results.",,Carbo J.,2003,Conference,Proceedings of the 10th ISPE International Conference on Concurrent Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442358802&partnerID=40&md5=d70c37f5470d8b8206566020e4a1e53d,"Computer Science Dept., Universidad Carlos III de Madrid, Spain",,English,,905809524X; 9789058095244
Scopus,Effort estimation based on data structures in entity-relationship diagrams,"This paper proposes a simple method of estimating the effort required to build transaction processing systems. Four basic data structures that occur in entity relationship diagrams are identified and their existence confirmed by empirical evidence based on fifty projects. A set of heuristics is described to assist the developer to code the diagram in terms of these structures. A model is proposed that associates with each structure a set of implied components, representing the essential components needed to implement a transaction processing system based on that structure. The implied components model is confirmed by empirical evidence based on eighteen projects. Effort estimation is based on a matrix of the implied components. The implied components method requires less effort than other estimation methods and can be applied earlier in the development cycle. The method is validated empirically.",Data modelling; Effort estimation; Entity-relationship diagram; Software metrics,Kennedy G.J.,2003,Conference,IASTED International Multi-Conference on Applied Informatics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442351202&partnerID=40&md5=8337efe51418f084ec8d39a44c6f9639,"School of Information Technologies, University of Sydney, Australia",,English,,0889863415; 9780889863415
Scopus,Function points in object oriented analysis and design,"Function points are still a frequently used technique for estimating project size. The FPA method can be applied to both object oriented projects and projects where structured techniques are used. However for object oriented projects additional guidelines are needed. In the paper, these guidelines are described together with a simple example of their use with Unified Modeling Language (UML) notation. Counting Function points on real projects needs appropriate tool support. A prototype of such a tool, written in Java programming language, is presented in the paper. Its integration with the industry-leading CASE tool has been made using XML Metadata Interchange (XMI) format. We also made tests on sample projects to prove both the method and the tool. The results can be found in this paper.",Function Points; Project Management; Software Metrics; Software Size Estimation; UML,"Živkovič A., Heričko M., Rozman I.",2003,Conference,IASTED International Multi-Conference on Applied Informatics,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442302377&partnerID=40&md5=f0c9292e4dd4ed94eddc418ea699d3f4,"University of Maribor, Fac. of Elec. Eng./Computer Science, Smetanova 17, SI-2000 Maribor, Slovenia",,English,,0889863415
Scopus,A model for software error estimation,"This paper presents an original algorithm for estimation of trends of software. In order to obtain high reliability at an acceptable cost, developers need to be able to estimate the reliability of software under development, and for management and planning purposes, they should be able to project the additional efforts needed for their software to reach a certain reliability level. The paper also introduces the capabilities of the software package developed according to this method.",Error trending; Software engineering; Software reliability,"Davani D., Rosenberg L., Trajkovski G.",2003,Conference,Simulation Series,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907440497&partnerID=40&md5=7650bd8632b86466442ae68e5a1b2dc0,"Computer and Information Sciences Department, Towson University, 8000 York Road, Towson, MD  21252, United States; Software Assurance Technology Center, Goddard Space Flight Center, Greenbelt, MD  20771, United States",The Society for Modeling and Simulation International,English,07359276,
Scopus,UML based software process management,"The main objective of software project management is to assure that a software product will be delivered in time, keeping the cost limits and a proper quality. The key problems are the proper estimation of the effort needed to implement a specific design, the sufficient and effective allocation of resources and the development environment. An appropriate project plan has to have a good or optimal scheduling of the individual development sub-tasks. Finally, a project management methodology has to cope with the risks evolving during the project. These project management activities have to start in the very early phases of the development process in order to keep the deadlines and have to continuously accommodate with the progress of the development process. UML, the Unified Modelling Language is increasingly widely used to design applications in a very broad range of software products. One of the major benefits of using UML as a design language is that it can be thoroughly used from the very initial phases to the implementation. Exploiting the property of the UML by which it's able to model also the dynamic behaviour of a system, it can be interpreted for workflows, this way can be used to describe also the development process itself. In this way our main objective was to provide a methodology, which is able to take into account all the main factors influencing the project efforts and scheduling for UML based design, and to generate the mathematical model of a software process optimization problem based on UML diagrams.",Cost estimation; Process optimization; Software development; UML,"Dobán O., Pataricza A.",2003,Conference,Periodica Polytechnica Electrical Engineering,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042815920&partnerID=40&md5=846ea199faf3afb8f6ceed563f9ffffe,"Department of Measurement Systems, Budapest Univ. of Technol./Economics, H-1521 Budapest, Hungary",Technical University of Budapest,English,03246000,
Scopus,Static analysis of Java applications - Are lines-of-code metric and Halstead length suitable? [Statische Analyse von Java-Anwendungen - Eignen sich Lines-of-Code-Metrik und Halstead-Länge?],"Many of the recently developed software systems are implemented in Java. For these systems, activities presently are mainly related to software development tasks rather than to dedicated software maintenance tasks. For these Java systems, therefore, experimental confirmation of established metrics for measuring code quantities that are related to software maintenance is not available. This also includes very basic size measures such as the LOC metric and the Halstead length. In this article, the application of these metrics for Java systems as well as some of the associated difficulties are outlined. The presented results are based on experimental data and include empirical correlations between the basic size metrics as well as newly derived scaling laws which are suitable for maintenance related software measurement.",Halstead metric; Java systems; LOC metric; Size measures; Software maintenance,Wolle B.,2003,Review,Wirtschaftsinformatik,10.1007/bf03250881,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348219304&doi=10.1007%2fbf03250881&partnerID=40&md5=65bb624458fabac96b2b6aaac4a734da,"CC GmbH, Flachstraße 13, 65197 Wiesbaden, Germany",Friedr. Vieweg und Sohn Verlags GmbH,German,09376429,
Scopus,A data collection case study supporting requirements oriented prediction and management in software developments,"Consider the statement ""this project should cost X and has risk of Y"". Such statements are used daily in industry as the basis for making decisions. The work reported here is part of a study aimed at providing a rational and pragmatic basis for such statements. Of particular interest are predictions made in the requirements and early phases of projects. A preliminary model has been constructed using Bayesian Belief Networks and in support of this, a programme to collect and study data during the execution of various software development projects commenced in May 2002. The data collection programme is undertaken under the constraints of a commercial industrial regime of multiple concurrent small to medium scale software development projects. Guided by pragmatism, the work is predicated on the use of data that can be collected readily by project managers; including expert judgements, effort, elapsed times and metrics collected within each project.",,"Boness K., Harrison R.",2002,Journal,Proceedings-IEEE Computer Society's International Computer Software and Applications Conference,10.1109/CMPSAC.2002.1045092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036389608&doi=10.1109%2fCMPSAC.2002.1045092&partnerID=40&md5=1307f45d4563f022882ae60485fe5a82,"BMC Software Ltd., Corinthian Court, 80 Milton Park, Abingdon, Oxon. OX14 4RY, United Kingdom; School of Computer Science, Cybernetics and Electronic Engineering, The University of Reading, Reading, RG6 6AY, United Kingdom",,English,07303157,
Scopus,A pilot study in effort estimation for the generalization of object-oriented components for reuse,"Generalizing components for future reuse is advocated as a worthwhile investment of time and resources. Some studies have attempted to quantify the additional costs of generalization of object-oriented components for reuse, with varying success. In this pilot study, we not only analyze one small data set for values of generalization effort but, more importantly, we develop an algorithmic approach that can be used to predict how much data is needed before a reliable estimate can be made. © 2001 IEEE.",,"Verner J.M., Henderson-Sellers B.",2001,Conference,"Proceedings of the Australian Software Engineering Conference, ASWEC",10.1109/ASWEC.2001.948513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949205419&doi=10.1109%2fASWEC.2001.948513&partnerID=40&md5=18970fe47ea0140e7f2e978d4eac2fa2,"College of Information Science and Technology, Drexel University, Philadelphia, United States; Department of Software Engineering, University of Technology, Sydney, NSW, Australia",IEEE Computer Society,English,,0769512542
Scopus,The measurement in software engineering,"Measurement can be used throughout a software project to assist in estimation, quality control and project control. Software measurement is concerned with capturing information about attributes of entities. Cost estimation models are developed using data from completed software projects and accuracy tests are usually done using data sets with known project size. Software metrics provide a quantitative way to assess the quality of internal attributes. This paper will focus on ways and methods to measure the software system.",Measurement of electrical and non-electrical quantities; Software Metrics; Software Project Management,Kalipsiz O.,2001,Conference,11th IMEKO TC4 Symposium on Trends in Electrical Measurements and Instrumentation and 6th IMEKO TC4 Workshop on ADC Modelling and Testing 2001,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943311768&partnerID=40&md5=26c10e538ccaeb4f62b5f9c8fe6981cf,"Department of Computer Science, Yildiz Technical University, Istanbul, 80750, Turkey",IMEKO-International Measurement Federation Secretariat,English,,9781510803978
Scopus,Proceedings of the Seventh International Software Metrics Symposium,"The proceedings contains 34 papers from the Seventh International Software Metrics Symposium, Metrics 2001. Topics discussed include: building a software cost estimation model based on categorical data; investigating the impact of reading techniques on the accuracy; assessing the benefits of imputing enterprise resource planning (ERP) projects with missing data; influence of team size and defect detection technique on inspection effectiveness; usage measurement for statistical web testing of distributed real-time systems; and evaluating software degradation through entropy.",,[No author name available],2001,Conference Review,"International Software Metrics Symposium, Proceedings",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035005465&partnerID=40&md5=4e4cccab97f05017441063e04d2cd2ba,,,English,,
Scopus,Software project control and metrics,"A selection of six papers from the combined 11th European Software Control and Metrics Conference (ESCOM) and the 3rd Software Certification Program in Europe (SCOPE) Conference held in April 2000 is presented. Focus is on software-related issues of project and risk control, process improvement, quality and certification, cost estimation and productivity, and the measurement to support the activities.",,"Maxwell K.D., Kusters R.J.",2000,Journal,Information and Software Technology,10.1016/S0950-5849(00)00147-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034323324&doi=10.1016%2fS0950-5849%2800%2900147-6&partnerID=40&md5=ffdd0943d0e4c7064a2e186a372080b7,"DATAMAX, 7 Bis Boulevard Marechal Foch, Fontainebleau 77300, France; Eindhoven University of Technology, Open University, Netherlands",,English,09505849,
Scopus,A monitoring framework for software project development,"Software project development includes a number of activities that result in a delivered product (software). As software becomes more and more expensive to develop, monitoring is an important task for project development and has been recognized as a difficult task in practice. There are a lot of unpredictable factors existing in the software development cycle that have become contributing factors to this problem. This paper gives an overview of the present state of the art of the software development projects. Moreover, a monitoring framework is proposed to help project management to get better understanding and make all activities running on schedule. © 1999 IEEE.",,"Tsoi H.-L., Cheung D.",1999,Conference,"Proceedings of the 2nd International Conference on Intelligent Processing and Manufacturing of Materials, IPMM 1999",10.1109/IPMM.1999.791530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040072509&doi=10.1109%2fIPMM.1999.791530&partnerID=40&md5=8cc5846f1ca2b80fdbd6bce2d2e4f8a2,"Software Quality Institute, Griffith University, Australia; Division of Computer Studies, City University of Hong Kong, Hong Kong, Hong Kong",Institute of Electrical and Electronics Engineers Inc.,English,,0780354893; 9780780354890
Scopus,Experimental evaluation of two-phase project control for software development process,"In this paper, we consider a simple development process consisting of design and debug phases, which is derived from actual concurrent development process for embedded software at a certain company. Then we propose two-phase project control that examines the initial development plan at the end of design phase, updates it to the current status of the development process and executes the debug phase under the new plan. In order to show the usefulness, we define three imaginary projects based on actually executed projects in a certain company: the project that executes debug phase under initial plan, the project that applies the proposed approach, and the project that follows a uniform plan. Moreover, to execute these projects, we use the project simulator, which has already been developed based on GSPN model. Judging from the number of residual faults in all products, we found that project B is the best among them.",Petri-net model; Project management; Software development; Software fault; Software test and debug,"Mizuno O., Kusumoto S., Kikuno T., Takagi Y., Sakamoto K.",1998,Journal,"IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032049822&partnerID=40&md5=6d4dc3b4b9671ba6fdde73c0a1c09a17,"School of Engineering Science, Osaka University, Toyonaka-shi, 560-8531, Japan; OMRON Corporation, Kusatsu-shi, 525-0035, Japan","Institute of Electronics, Information and Communication, Engineers, IEICE",English,09168508,
Scopus,Towards an adaptation of the cocomo cost model to the software measurement theory,"When the COCOMO cost model was published in the beginning of the eighties, software measurement was not grounded on solid theoretical foundations. This has been achieved until the nineties by Fenton ond others. Thus, it is not surprising that some of the concepts defined or used in the COCOMO model are somewhat incompatible with the software measurement theory. In this work, we mainly stress some of the incompatibilities and propose altemaive ways to avoid them. © Springer-Verlag Berlin Heidelberg 1997.",,"Idri A., Griech B., Iraki A.E.",1997,Conference,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10.1007/3-540-63531-9_37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949429488&doi=10.1007%2f3-540-63531-9_37&partnerID=40&md5=7db0b936d5d22e66ac87ca5d3e8f9e86,"ENSIAS, BP. 713, Agdal, Rabat, Morocco",Springer Verlag,English,03029743,3540635319; 9783540635314
Scopus,Software estimating and metrics - a pragmatic approach,"This paper reviews current software estimating practice, with particular emphasis on practical experiences of software size and cost estimation in a military simulation environment. A key issue in any form of estimation is the gathering of information on which the estimate can be based. The activities associated with these metrics data and their relationship to the software estimation process are also described. Although the paper focuses on the techniques required to estimate software for military simulation systems it is believed that this particular application environment has much in common with any other software estimating activity. Many of the problems associated with this particular estimating environment are apparent in other application domains.",,Rutherford J.,1995,Journal,GEC journal of research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029190457&partnerID=40&md5=bb8d8e65fdb3691f394fd9a8c49d0e30,,"GEC-Marconi Research Centre, Chelmsford, United Kingdom",English,02649187,
Scopus,Incremental resources estimation with real-time feedback from measurement,"The quality and usability of software project estimation models can be substantially improved by integrating metrics in the software development process to provide feedback in real-time. Active metrics by be used to adjust and improve the initial estimates by including data from the early phases of software development. In this paper, a method is proposed for incrementally proceeding from a priori project estimation and modelling to measuring the project and product during development to provide real-time feedback and refinement to the initial estimation. This method enables the estimates to be incrementally developed and to use them to control and guide project execution in real-time. A hierarchical system or project decomposition can be used to implement distributed estimation models which include a constraint propagation network. These techniques are based on an extended object-oriented modelling technique for software engineering elements. © 1993.",,Oivo M.,1993,Journal,Microprocessing and Microprogramming,10.1016/0165-6074(93)90156-F,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027657038&doi=10.1016%2f0165-6074%2893%2990156-F&partnerID=40&md5=9649c1bfcaf3ff538a414c76f5ad8e3a,"Technical Research Centre of Finland (VTT) Computer Technology Laboratory P.O. Box 201, SF-90571 Oulu, Finland",,English,01656074,
Scopus,Long-term controlling of software reuse,"The reuse of software is good practice in the development of new software systems. While parts are reused over a period of many years and in many projects, the economic control of software development is still focused on single projects. This is inadequate, especially as it gives no way of spreading the high costs incurred when a project is the first in a new domain or on different hardware. It is argued that software development costs can be regarded as an investment. Later projects that reap the profits should be required to pay royalties on the investments. The development teams must get the incentives and information that help them to improve their productivity. In the long run a company should take a strategic approach, investigate the probable number of reuses, and include the result into the cost estimation. It is shown how this gives the ability to bid at low prices for a number of projects that considered together can be highly profitable. © 1992.",cost estimation; software development; software economics; software reuse,Wolff F.,1992,Journal,Information and Software Technology,10.1016/0950-5849(92)90029-O,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049125277&doi=10.1016%2f0950-5849%2892%2990029-O&partnerID=40&md5=0bb6dced2c24629f57f34c31bf002eaf,"Wirtschaftswissenschaftliche Fakultät, Universität Witten-Herdecke, Stockumer Str. 10, W-5810 Witten-Annen, Germany",,English,09505849,
Scopus,Controlling complexity and cost of software projects with a spreadsheet software,"The use of a spreadsheet software in the field of software measurement technology is disccussed. A model of metric's life cycle is defined and used together with the spreadsheet software to develop a new complexity metric. It's evaluation has shown that it is supperior to most other complexity metrics in estimating development times and allocating testing resources. The spreadsheet implementation of the information flow metric to controll the complexity and the COCOMO cost prediction model to controll costs of software projects in real software development process is presented next. Finally, the spreadsheet and conventional programming are compared, and it is not unreasonable to state that spreadsheet better supports activities performd during metric's life cycle then conventional programming. © 1990.",complexity; computer software; controll; management; Measurement; modelling; prediction; spreadsheet software,"Kokol P., Zumer V.",1988,Journal,Annual Review in Automatic Programming,10.1016/0066-4138(90)90014-I,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024187859&doi=10.1016%2f0066-4138%2890%2990014-I&partnerID=40&md5=7c9960f70926f3b52cef9f8d5fd205b8,"The Faculty of Technical Sciences, Smetanova 17, 62000 Maribor",,English,00664138,
Scopus,It is easy to forget that software must be as fail-safe as hardware,"While hardware reliability is seen as a major problem by computer users, software reliability is not taken so seriously. A group of academics and industrialists have formed the Centre for Software Reliability in an attempt to increase awareness of the need for reliable software. Research has been carried out into software metrics and cost estimation, system design and structuring and program testing. The US DoD is working on 'Star Wars' projects which will contain probably the largest software programs ever. To make these work research is being carried out into artificial intelligence and software reliability. © 1985.",data processing; software reliability; software techniques,Cowan D.,1985,Journal,Data Processing,10.1016/0011-684X(85)90031-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46549095730&doi=10.1016%2f0011-684X%2885%2990031-0&partnerID=40&md5=152986a5bf8bc951c6f7e9b5ac3b8ee0,,,English,0011684X,
